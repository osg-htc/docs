{
    "docs": [
        {
            "location": "/", 
            "text": "Installation Guides\n\n\nThe Open Science Grid consists primarily of a fabric of services at\nparticipating sites.\n\n\nOne of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.\n\n\nOur most common software products include:\n\n\n\n\nHTCondor CE Installation\n: Provides a \ngateway\n\n  between the grid and your batch system.\n\n\nHTTP Proxy\n: Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable \nsquid2\n software).\n\n\nCVMFS\n The CernVM File System (CVMFS) is a global-scale, read-only,\n  hierarchical filesystem.  CVMFS volumes distribute the majority of the scientific\n  software used on OSG in addition to the OSG worker node client.\n\n\nWorker node client\n and \nglexec\n: An RPM-based install\n  of the worker node software; this includes the \nglexec\n binary (not provided by the\n  CVMFS install) which allows pilots to securely isolate payload jobs.\n\n\n\n\nYou can help!\n\n\nDocumentation is a task that is never done!  Feel free to fork this on github and\nsend us any updates or corrections.\n\n\nIn particular, many documents are being converted from our old twiki to this site.\nWe have used an automatic converter, but most documents need a human touch prior\nto being included in the table of contents.\n\n\nIf you'd like to contribute, please consider trying to clean up one of these\ndocuments:\n\n\n\n\nCA Certificate Updater script\n\n\nHost certificate management package\n\n\nJob Router Recipes\n\n\nInstalling the Worker Node from OASIS\n\n\nInstalling a GridFTP Server", 
            "title": "Home"
        }, 
        {
            "location": "/#installation-guides", 
            "text": "The Open Science Grid consists primarily of a fabric of services at\nparticipating sites.  One of the most common ways of participating in the OSG is to install\none of our software services at your site and provide computational\npower opportunistically to the grid.  Our most common software products include:   HTCondor CE Installation : Provides a  gateway \n  between the grid and your batch system.  HTTP Proxy : Caches the most commonly-used files at your\n  site to preserve bandwidth (a custom packaging of the venerable  squid2  software).  CVMFS  The CernVM File System (CVMFS) is a global-scale, read-only,\n  hierarchical filesystem.  CVMFS volumes distribute the majority of the scientific\n  software used on OSG in addition to the OSG worker node client.  Worker node client  and  glexec : An RPM-based install\n  of the worker node software; this includes the  glexec  binary (not provided by the\n  CVMFS install) which allows pilots to securely isolate payload jobs.", 
            "title": "Installation Guides"
        }, 
        {
            "location": "/#you-can-help", 
            "text": "Documentation is a task that is never done!  Feel free to fork this on github and\nsend us any updates or corrections.  In particular, many documents are being converted from our old twiki to this site.\nWe have used an automatic converter, but most documents need a human touch prior\nto being included in the table of contents.  If you'd like to contribute, please consider trying to clean up one of these\ndocuments:   CA Certificate Updater script  Host certificate management package  Job Router Recipes  Installing the Worker Node from OASIS  Installing a GridFTP Server", 
            "title": "You can help!"
        }, 
        {
            "location": "/Common/yum/", 
            "text": "YUM Repositories\n\n\nAbout This Document\n\n\nThis document introduces YUM repositories and how OSG uses them.\n\n\nRepositories\n\n\nOSG hosts four public-facing repositories at\n\nrepo.grid.iu.edu\n:\n\n\n\n\nrelease\n: This repository contains software that we are willing\n    to support and can be used by the general community.\n\n\ncontrib\n: RPMs contributed from outside the OSG.\n\n\ntesting\n: This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.\n\n\ndevelopment\n: This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.\n\n\n\n\nOSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:\n\n\n\n\nyour OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)\n\n\nEPEL repositories\n\n\nthe OSG repositories you\u2019d like to use\n\n\n\n\nIf one of these repositories is missing you may have missing\ndependencies.\n\n\n We did not test other\nrepositories. If you use packages from other repositories, like\n\njpackage\n, \ndag\n, or \nrpmforge\n, you may encounter problems.\n\n\nEnabling Repositories\n\n\nIn \nour advice on using\nyum\n you will learn many\ntricks and tips on using yum.\n\n\nTo use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.\n\n\nInstall the Yum Repositories required by OSG\n\n\nThe OSG RPMs currently support {{ supportedOs }}\n\n\nOSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the\n\nEPEL\n repositories. So both\nrepositories must be enabled.\n\n\nInstall EPEL\n\n\nInstall the EPEL repository, if not already present. \nNote:\n This\nenables EPEL by default. Choose the right version to match your OS\nversion.\n\n\n# EPEL 5 (For RHEL 5, CentOS 5, and SL 5)\n$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\nWARNING\n: if you have your own mirror or configuration of the EPEL\nrepository, you \nMUST\n verify that the OSG repository has a better yum\npriority than EPEL (\ndetails\n).\nOtherwise, you will have strange dependency resolution (\ndepsolving\n) issues.\n\n\nInstall the Yum priorities package\n\n\nFor packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.\n\n\n\n\n\n\nChoose the correct package name based on your operating\n    system\u2019s major version:\n\n\n\n\nFor EL\u00a05 systems, use \nyum-priorities\n\n\nFor EL\u00a06 and EL\u00a07 systems, use \nyum-plugin-priorities\n\n\n\n\n\n\n\n\nInstall the Yum priorities package:\n    \nyum install *PACKAGE*\n\n    Replace \nPACKAGE\n with the package name\n    from the previous step.\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the\n    \n[main]\n section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:\n\n\nplugins=1\n\n\nNOTE\n: If you do not have a\nrequired key you can force the installation using\n\n--nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities\n.\n\n\n\n\n\n\nInstall OSG Repositories\n\n\n\n\nIf you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:\n\n\n\n\n# yum clean all\n   # rpm -e osg-release\n\n\nThis step ensures that local changes to \n*.repo\n files will not\n   block the installation of the new OSG repositories. After this step,\n   \n*.repo\n files that have been changed will exist in \n/etc/yum.repos.d/\n\n   with the \n*.rpmsave\n extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the \n*.rpmsave\n files to the new \n*.repo\n files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from\n\nrepo.grid.iu\n:\n\n\n    ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```\n\n\n\nPriorities\n\n\n Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.\n\n\nThe only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as \nosg-testing\n, then edit its file\n(e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option\nfrom 0 to 1:\n\n\n[osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled=\n1\n\ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\nIf you have your own mirror or\nconfiguration of the EPEL repository, you \nMUST\n verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.\n\n\nReferences\n\n\n\n\nBasic use of Yum\n\n\nBest practices in using Yum", 
            "title": "Yum Repos"
        }, 
        {
            "location": "/Common/yum/#yum-repositories", 
            "text": "", 
            "title": "YUM Repositories"
        }, 
        {
            "location": "/Common/yum/#about-this-document", 
            "text": "This document introduces YUM repositories and how OSG uses them.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Common/yum/#repositories", 
            "text": "OSG hosts four public-facing repositories at repo.grid.iu.edu :   release : This repository contains software that we are willing\n    to support and can be used by the general community.  contrib : RPMs contributed from outside the OSG.  testing : This repository contains software ready for testing. If\n    you install packages from here, they may be buggy, but we will\n    provide limited assistance in providing a migration path to a fixed\n    version.  development : This repository is the bleeding edge. Installing\n    from this repository may cause the host to stop functioning, and we\n    will not assist in undoing any damage.   OSG\u2019s RPM packages rely also on external packages provided by supported\nOSes and EPEL. You must have the following repositories available and\nenabled:   your OS repositories (SL 5/6/7, CentOS 5/6/7, or RHEL 5/6/7\n    repositories)  EPEL repositories  the OSG repositories you\u2019d like to use   If one of these repositories is missing you may have missing\ndependencies.   We did not test other\nrepositories. If you use packages from other repositories, like jpackage ,  dag , or  rpmforge , you may encounter problems.", 
            "title": "Repositories"
        }, 
        {
            "location": "/Common/yum/#enabling-repositories", 
            "text": "In  our advice on using\nyum  you will learn many\ntricks and tips on using yum.  To use the packages in a repository without adding special options to\nthe yum command the repository must be enabled.", 
            "title": "Enabling Repositories"
        }, 
        {
            "location": "/Common/yum/#install-the-yum-repositories-required-by-osg", 
            "text": "The OSG RPMs currently support {{ supportedOs }}  OSG RPMs are distributed via the OSG yum repositories. Some packages\ndepend on packages distributed via the EPEL  repositories. So both\nrepositories must be enabled.", 
            "title": "Install the Yum Repositories required by OSG"
        }, 
        {
            "location": "/Common/yum/#install-epel", 
            "text": "Install the EPEL repository, if not already present.  Note:  This\nenables EPEL by default. Choose the right version to match your OS\nversion.  # EPEL 5 (For RHEL 5, CentOS 5, and SL 5)\n$ curl -O https://dl.fedoraproject.org/pub/epel/epel-release-latest-5.noarch.rpm\n$ rpm -Uvh epel-release-latest-5.noarch.rpm\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n$ rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm  WARNING : if you have your own mirror or configuration of the EPEL\nrepository, you  MUST  verify that the OSG repository has a better yum\npriority than EPEL ( details ).\nOtherwise, you will have strange dependency resolution ( depsolving ) issues.", 
            "title": "Install EPEL"
        }, 
        {
            "location": "/Common/yum/#install-the-yum-priorities-package", 
            "text": "For packages that exist in both OSG and EPEL repositories, it is\nimportant to prefer the OSG ones or else OSG software installs may fail.\nInstalling the Yum priorities package enables the repository priority\nsystem to work.    Choose the correct package name based on your operating\n    system\u2019s major version:   For EL\u00a05 systems, use  yum-priorities  For EL\u00a06 and EL\u00a07 systems, use  yum-plugin-priorities     Install the Yum priorities package:\n     yum install *PACKAGE* \n    Replace  PACKAGE  with the package name\n    from the previous step.    Ensure that  /etc/yum.conf  has the following line in the\n     [main]  section (particularly when using ROCKS), thereby enabling\n    Yum plugins, including the priorities one:  plugins=1  NOTE : If you do not have a\nrequired key you can force the installation using --nogpgcheck=; e.g., =yum install --nogpgcheck yum-priorities .", 
            "title": "Install the Yum priorities package"
        }, 
        {
            "location": "/Common/yum/#install-osg-repositories", 
            "text": "If you are upgrading from OSG 3.1 (or 3.2) to OSG 3.2\n   (or 3.3), remove the old OSG repository definition files and clean the\n   Yum cache:   # yum clean all\n   # rpm -e osg-release  This step ensures that local changes to  *.repo  files will not\n   block the installation of the new OSG repositories. After this step,\n    *.repo  files that have been changed will exist in  /etc/yum.repos.d/ \n   with the  *.rpmsave  extension. After installing the new OSG\n   repositories (the next step) you may want to apply any changes made in\n   the  *.rpmsave  files to the new  *.repo  files.\n2. Install the OSG repositories using one of the following methods\ndepending on your EL version:\n    1. For EL versions greater than EL5, install the files directly from repo.grid.iu :      ```\n    rpm -Uvh URL\n    ```\n\n    Where `URL` is one of the following:\n\n    | Series      | EL6 URL (for RHEL 6, CentOS 6, or SL 6) | EL7 URL (for RHEL 7, CentOS 7, or SL 7) |\n    |----------   | ----------------------------------------| --------------------------------------- | \n    | **OSG 3.2** | `https://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm`  | N/A\n    | **OSG 3.3** | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el6-release-latest.rpm`  | `https://repo.grid.iu.edu/osg/3.3/osg-3.3-el7-release-latest.rpm` |\n\n2. For EL5, download the repo file and install it using the following:\n    ```\n    # curl -O https://repo.grid.iu.edu/osg/3.2/osg-3.2-el5-release-latest.rpm\n    # rpm -Uvh osg-3.2-el5-release-latest.rpm\n    ```", 
            "title": "Install OSG Repositories"
        }, 
        {
            "location": "/Common/yum/#priorities", 
            "text": "Make sure you installed the Yum\npriorities plugin, as described above. Not doing so is a common mistake\nthat causes failed installations.  The only OSG repository enabled by default is the release one. If you\nwant to enable another one, such as  osg-testing , then edit its file\n(e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option\nfrom 0 to 1:  [osg-testing]\nname=OSG Software for Enterprise Linux 5 - Testing - $basearch\n#baseurl=http://repo.grid.iu.edu/osg/3.2/el5/testing/$basearch\nmirrorlist=http://repo.grid.iu.edu/mirror/osg/3.2/el5/testing/$basearch\nfailovermethod=priority\npriority=98\nenabled= 1 \ngpgcheck=1\ngpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  If you have your own mirror or\nconfiguration of the EPEL repository, you  MUST  verify that the OSG\nrepository has a better yum priority than EPEL. Otherwise, you will have\nstrange dependency resolution issues.", 
            "title": "Priorities"
        }, 
        {
            "location": "/Common/yum/#references", 
            "text": "Basic use of Yum  Best practices in using Yum", 
            "title": "References"
        }, 
        {
            "location": "/Common/ca/", 
            "text": "Installing Certificate Authorities Certificates and related RPMs\n\n\nThis document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).\n\n\nWhen installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:\n\n\n\n\nWhat set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)\n\n\nHow do you want to update them?\n\n\nDo you want to centrally manage the CA certificates or install them on each computer at your site?\n\n\n\n\nYou have four options for installing CA certificates:\n\n\n\n\nInstall an RPM for a specific set of CA certificates.\n\n\nInstall \nosg-update-certs\n, a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.\n\n\nInstall an RPM that installs \nno\n CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.\n\n\nMake no choice, let \nyum\n decide for you.\n\n\n\n\nAdditionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.\n\n\nPrior to following the instructions on this page, you must enable our \nyum repositories\n\n\nInstall CA certificates: Options\n\n\nPlease choose one of the four options to install the CA certificates.\n\n\nOption 1: Install an RPM for a specific set of CA certificates\n\n\nIf you want to install an RPM for one of our predefined CA certificates, you have two choices to make:\n\n\nWhich set of CAs?\n\n\n\n\n(\nrecommended\n) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See \nhere\n for details)\n\n\nThe default \nIGTF\n CA certificates.\n\n\n\n\nDepending on your choice, you select one of two RPMs:\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n --\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg-ca-certs\n\n\nyum install osg-ca-certs\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf-ca-certs\n\n\nyum install igtf-ca-certs\n\n\n\n\n\n\n\n\nHow do I keep CAs updated?\n\n\nPlease follow the \nupdate instructions\n to make sure that the CAs are kept updated.\n\n\nOption 2: Install osg-update-certs\n\n\nInstall this with:\n\n\n[root@client ~]$ yum install osg-ca-scripts\n\n\n\n\nYou have the same choices for CA certificates as above. In order to choose, you will run \nosg-ca-manage\n, which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nCA certs name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\nOSG\n\n\nOpenSSL-both\n\n\nosg\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url osg\n\n\n\n\n\n\nIGTF\n\n\nOpenSSL-both\n\n\nigtf\n\n\n/usr/sbin/osg-ca-manage setupCA --location root --url igtf\n\n\n\n\n\n\n\n\nHere is an example:\n\n\n[root@client ~]$ /usr/sbin/osg-ca-manage setupCA --location root --url osg \nSetting up CA Certificates for OSG installation\nCA Certificates will be installed into /etc/grid-security/certificates\nosg-update-certs\n  Log file: /var/log/osg-update-certs.log\n  Updates from: http://software.grid.iu.edu/pacman/cadist/ca-certs-version-new\n\nWill update CA certificates from version unknown to version 1.21NEW.\nUpdate successful.\n\nSetup completed successfully.\n\n\n\n\nInitially the CA certificates will not be updated. You can tell by looking at:\n\n\n[root@client ~]$ /sbin/service osg-update-certs-cron  status\nPeriodic osg-update-certs is disabled.\n\n\n\n\nYou can enable the \ncron\n job that updates the CA certs with:\n\n\n[root@client ~]$ /sbin/service osg-update-certs-cron  start\nEnabling periodic osg-update-certs:                        [  \nOK\n  ]\n\n\n\n\nA complete set of options available though \nosg-ca-manage\n command, including your interface to adding and removing CAs, could be found at \nosg-ca-manage documentation\n\n\nOption 3: Install an RPM that installs no CAs\n\n\nInstall this with:\n\n\nyum install empty-ca-certs \u2013-enablerepo=osg-empty\n\n\n\n\n\n\nWarning\n\n\nIf you choose this option, you are responsible for installing the CA certificates yourself. You must install them in \n/etc/grid-security/certificates\n, or make a symlink from that location to the directory that contains the CA certificates.\n\n\n\n\nOption 4: Make no choice, let yum decide for you\n\n\nIf you use \nyum\n to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above (\nInstall an RPM for a specific set of CA certificates\n), and the osg-ca-certs RPM is chosen.\n\n\nInstall other CAs\n\n\nIn addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, \nosg-ca-certs\n and the like, but not \nosg-ca-scripts\n.) They are in addition to the above RPMs, so do not only install these extra CAs.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nFormat\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\ncilogon-basic \n cilogon-openid\n\n\nOpenSSL-both\n\n\ncilogon-ca-certs\n\n\nyum install cilogon-ca-certs\n\n\n\n\n\n\n\n\nManaging Certificate Revocation Lists\n\n\nIn addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack use these to ensure that you are talking to valid clients or servers. We use a tool named \nfetch-crl\n that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.\n\n\nfetch-crl\n is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The \nfetch-crl-cron\n service runs \nfetch-crl\n every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the\n\nfetch-crl-cron\n service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.\n\n\nInstall \nfetch-crl\n\n\nNormally \nfetch-crl\n is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ yum install fetch-crl3\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n[root@client ~]$ yum install fetch-crl\n\n\n\n\nEnable and Start \nfetch-crl\n\n\nTo enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron on\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n \n[root@client ~]$ /sbin/chkconfig fetch-crl-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron on\n\n\n\n\nTo start fetch-crl:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /sbin/service fetch-crl3-boot start\n[root@client ~]$ /sbin/service fetch-crl3-cron start\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n \n[root@client ~]$ /sbin/service fetch-crl-boot start\n[root@client ~]$ /sbin/service fetch-crl-cron start\n\n\n\n\n\n\nNote\n\n\nWhile it is necessary to start \nfetch-crl-cron\n in order to have it active, \nfetch-crl-boot\n is started automatically at boot time if enabled. The start command will run \nfetch-crl-boot\n at the moment when it is invoked and it may take some time to complete.\n\n\n\n\nConfigure \nfetch-crl\n\n\nTo modify the times that fetch-crl-cron runs, edit \n/etc/cron.d/fetch-crl\n (or \n/etc/cron.d/fetch-crl3\n depending on the version you have).\n\n\nBy default, \nfetch-crl\n connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as \nsquid\n) the worker nodes can utilize; OSG provides\n\npackaging of squid\n.\n\n\nTo configure fetch-crl to use an HTTP proxy server:\n\n\n\n\n\n\nIf using \nfetch-crl\n version 2 (the \nfetch-crl\n package on RHEL5 only), then create the file \n/etc/sysconfig/fetch-crl\n and add the following line:\n\n\nexport http_proxy=\nhttp://your.squid.fqdn:port\n\n\nAdjust the URL appropriately for your proxy server.\n\n\n\n\n\n\nIf using \nfetch-crl\n version 3 on RHEL5 via the \nfetch-crl3\n package\n    or on RHEL6/RHEL7 via the \nfetch-crl\n package, then create or edit the\n    file \n/etc/fetch-crl3.conf\n (RHEL5) or \n/etc/fetch-crl.conf\n\n    (RHEL6/RHEL7) and add the following line:\n\n\nhttp_proxy=\nhttp://your.squid.fqdn:port\n\n\nAgain, adjust the URL appropriately for your proxy server.\n\n\n\n\n\n\nNote that the \nnosymlinks\n option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself (\ninfodir\n) is a link to a directory.\n\n\nAny modifications to the configuration file will be preserved during an RPM update.\n\n\nCurrent versions of \nfetch-crl\n and \nfetch-crl3\n produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:\n\n\n\n\n\n\nChange the configuration file to enable syslog:\n\n\nlogmode = syslog\n  syslogfacility = daemon\\\n/pre\\\n\n\n\n\n\n\nMake sure the file \n/var/log/daemon\n exists, e.g. touching the file\n\n\n\n\nChange \n/etc/logrotate.d\n files to rotate it\n\n\n\n\nStart/Stop fetch-crl: A quick guide\n\n\nYou need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /usr/sbin/fetch-crl3 # This fetches the CRLs\n[root@client ~]$ /sbin/service fetch-crl3-boot start\n[root@client ~]$ /sbin/service fetch-crl3-cron start\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7\n\n[root@client ~]$ /usr/sbin/fetch-crl # This fetches the CRLs\n[root@client ~]$ /sbin/service fetch-crl-boot start\n[root@client ~]$ /sbin/service fetch-crl-cron start\n\n\n\n\nTo enable the \nfetch-crl\n service to keep the CRLs up to date after reboots:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron on\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n[root@client ~]$ /sbin/chkconfig fetch-crl-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron on\n\n\n\n\nTo stop \nfetch-crl\n:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /sbin/service fetch-crl3-boot stop\n[root@client ~]$ /sbin/service fetch-crl3-cron stop\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n[root@client ~]$ /sbin/service fetch-crl-boot stop\n[root@client ~]$ /sbin/service fetch-crl-cron stop\n\n\n\n\nTo disable the fetch-crl service:\n\n\n# For RHEL 5, CentOS 5, and SL5 \n\n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot off\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron off\n\n# For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n\n[root@client ~]$ /sbin/chkconfig fetch-crl-boot off\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron off\n\n\n\n\nUpdating CAs/CRLs\n\n\nWhy maintain up-to-date Trusted CA /CRL information\n\n\nThe Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA. CRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.) This is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you. However, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.\n\n\nHow to ensure you are get up-to-date CA/CRL information\n\n\n\n\nIf you installed CAs using rpm packages (\nosg-ca-certs\n,\nigtf-ca-certs\n) (Options 1, 4), you will need to install the software described in \nthe CA update document\n, and enable \nosg-ca-certs-updater\n service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.\n\n\nIf you use Option 2 (i.e. \nosg-update-certs\n) then make sure that you have the corresponding service enabled.\n\n\n\n\n[root@client ~]$ /sbin/service osg-update-certs-cron  status\n   Periodic osg-update-certs is enabled.\n\n\n\n\nEnsure that fetch-crl cron is enabled\\\n\n\n\n\n[root@client ~]$ /sbin/service fetch-crl-cron  status\n  Periodic fetch-crl is enabled.\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nCA File Location\n\n\n/etc/grid-security/certificates\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nIndex files\n\n\n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n\n\nLatest version also available at \nhttp://software.grid.iu.edu/pacman/cadist/\n\n\n\n\n\n\nAll CA Packages\n\n\nChange Log\n\n\n/etc/grid-security/certificates/CHANGES\n\n\nLatest version also available at \nhttp://software.grid.iu.edu/pacman/cadist/CHANGES\n\n\n\n\n\n\nosg-ca-certs or igtf-ca-certs\n\n\ncontain only CA files\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nfetch-crl-2.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl.conf\n\n\n\n\n\n\n\n\nfetch-crl-3.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl3.conf\n\n\n\n\n\n\n\n\n\n\nThe index and change log files contain a summary of all the CA distributed and their version.\n\n\nLogs files:\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out\n\n\n\n\n\n\n\n\nTests\n\n\nTo test the host certificate of a server \nopenssl s_client\n can be used. Here is an example with the gatekeeper:\n\n\nUCL_PROMPT\n openssl s_client -showcerts -cert /etc/grid-security/hostcert.pem -key /etc/grid-security/hostkey.pem -CApath /etc/grid-security/certificates/ -debug -connect osg-gk.mwt2.org:2119\n\n\n\n\nFrequently Asked Questions\n\n\nLocation of Certificates?\n\n\n /etc/grid-security/certificates \n\n\n\n\nWhat is the version of OSG CA package I have installed and what are its contents?\n\n\nThe version of the CA package ca be found at \n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n. The changes file can be found at \n/etc/grid-security/certificates/CHANGES\n.\n\n\nContents of OSG CA package?\n\n\nThe OSG CA Distribution contains:\n\n\n\n\nIGTF Distribution of Authority Root Certificates\n (CAs accredited by the \nInternational Grid Trust Federation\n)\n\n\nPurdue TeraGrid CA\n\n\n\n\nDetails of CAs in OSG distribution can be found \nhere\n. For additional details what is in the current release, see the \ndistribution site\n and \nchange log\n.\n\n\nHow can I add or remove a particular CA file?\n\n\nAdd and remove of CA files are supported only if you CA files are being installed using \nosg-update-certs\n, which is included in the \nosg-ca-scripts\n package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use \nosg-ca-manage\n. For adding a new CA \nosg-ca-manage add [--dir \nlocal_dir\n] --hash \nCA_hash\n may be used, while a CA is removed using \nosg-ca-manage remove --hash \nCA_hash\n.\n\n\nAre there any log files or configuration files associated with CA certificate package?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4) no log or configuration files are present.\n\n\nLog and configuration files are however present for \nosg-ca-scripts\n rpm package (option 2).\n\n\nConfig files: \n/etc/osg/osg-update-certs.conf\n Log files: \n/var/log/osg-update-certs.log\n, \n/var/log/osg-ca-certs-status.system.out\n, \n/var/log/osg-ca-manage.system.out\n, \n/var/log/osg-setup-ca-certificates.system.out\n\n\nAre CA packages automatically updated?\n\n\nIf CA files are installed using \nosg-ca-certs\n or \nigtf-ca-certs\n rpms (i.e. options 1, 4), you will need to install the software described in OsgCaCertsUpdater, and enable osg-ca-certs-updater service to keep the CAs automatically updated.\n\n\nIf CA files are being installed using \nosg-ca-scripts\n rpm package (option 2), CA files are kept up-to-date as long as \nosg-update-certs-cron\n service the package provides has been started.\n\n\nHow do I manually update my CA package?\n\n\nFor Option 1: run one of the following \nyum update osg-ca-certs\n or \nyum update igtf-ca-certs\n depending on the rpm package you installed.\n\n\nFor Option 4: run \nyum update osg-ca-certs\n\n\nFor Option 2: You do not need to do a manual update, make sure \nosg-update-certs-cron\n is enabled using\n\n\n[root@client ~]$ /sbin/service osg-update-certs-cron  status\n\n\n\n\nIf the service is disabled, enable it using\n\n\n[root@client ~]$ /sbin/service osg-update-certs-cron  start\n\n\n\n\nIf for some extraordinary reason you need to manually update the CA you could run \nosg-ca-manage [--force] refreshCA\n.\n\n\nWhere are the configuration files for fetch-crl?\n\n\n/etc/fetch-crl.conf\n or \n/etc/fetch-crl3.conf\n for fetch-crl 2.x or 3.x respectively\n\n\nReferences\n\n\nSome guides on x509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nRelated software:\n\n\n\n\nDescription, manual and examples of OsgCaManage\n\n\nOsgCaCertsUpdater\n\n\nUpgrading Fetch-Crl 2 to Fetch-Crl 3 on EL5", 
            "title": "CA Certificates"
        }, 
        {
            "location": "/Common/ca/#installing-certificate-authorities-certificates-and-related-rpms", 
            "text": "This document provides you with details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation list (CRL).  When installing software with RPMs, you need to decide how you want to install the Certificate Authority (CA) certificates. You might ask \"why do I care? Can\u2019t you just give them to me?\" We can, but you have a few things to consider:   What set of CA certificates do you want? How much control do you want over the set of CA certificates? (Some sites might not want to install specific CAs for policy or security reasons.)  How do you want to update them?  Do you want to centrally manage the CA certificates or install them on each computer at your site?   You have four options for installing CA certificates:   Install an RPM for a specific set of CA certificates.  Install  osg-update-certs , a program that lets you install/update a predefined set of CA certificates, then adjust the set by adding or deleting specific CAs.  Install an RPM that installs  no  CAs. This is useful when you want your RPM installations to succeed (because our RPMs require CA certificates, and this RPM satisfies that dependency) but you want to manage them with your own technique.  Make no choice, let  yum  decide for you.   Additionally this page also provides instruction on installation of a tool (fetch-crl) to ensure your site has up-to-date certificate revocation list (CRL) from the CA.  Prior to following the instructions on this page, you must enable our  yum repositories", 
            "title": "Installing Certificate Authorities Certificates and related RPMs"
        }, 
        {
            "location": "/Common/ca/#install-ca-certificates-options", 
            "text": "Please choose one of the four options to install the CA certificates.", 
            "title": "Install CA certificates: Options"
        }, 
        {
            "location": "/Common/ca/#option-1-install-an-rpm-for-a-specific-set-of-ca-certificates", 
            "text": "If you want to install an RPM for one of our predefined CA certificates, you have two choices to make:", 
            "title": "Option 1: Install an RPM for a specific set of CA certificates"
        }, 
        {
            "location": "/Common/ca/#which-set-of-cas", 
            "text": "( recommended ) The OSG CA certificates. This is similar to the IGTF set, but may have a small number of additions or deletions. (See  here  for details)  The default  IGTF  CA certificates.   Depending on your choice, you select one of two RPMs:     Set of CAs  Format  --  RPM name  Installation command (as root)      OSG  OpenSSL-both  osg-ca-certs  yum install osg-ca-certs    IGTF  OpenSSL-both  igtf-ca-certs  yum install igtf-ca-certs", 
            "title": "Which set of CAs?"
        }, 
        {
            "location": "/Common/ca/#how-do-i-keep-cas-updated", 
            "text": "Please follow the  update instructions  to make sure that the CAs are kept updated.", 
            "title": "How do I keep CAs updated?"
        }, 
        {
            "location": "/Common/ca/#option-2-install-osg-update-certs", 
            "text": "Install this with:  [root@client ~]$ yum install osg-ca-scripts  You have the same choices for CA certificates as above. In order to choose, you will run  osg-ca-manage , which will install the CA certificates. Then (if desired) you need to enable periodic updating of the CA certificates.     Set of CAs  Format  CA certs name  Installation command (as root)      OSG  OpenSSL-both  osg  /usr/sbin/osg-ca-manage setupCA --location root --url osg    IGTF  OpenSSL-both  igtf  /usr/sbin/osg-ca-manage setupCA --location root --url igtf     Here is an example:  [root@client ~]$ /usr/sbin/osg-ca-manage setupCA --location root --url osg \nSetting up CA Certificates for OSG installation\nCA Certificates will be installed into /etc/grid-security/certificates\nosg-update-certs\n  Log file: /var/log/osg-update-certs.log\n  Updates from: http://software.grid.iu.edu/pacman/cadist/ca-certs-version-new\n\nWill update CA certificates from version unknown to version 1.21NEW.\nUpdate successful.\n\nSetup completed successfully.  Initially the CA certificates will not be updated. You can tell by looking at:  [root@client ~]$ /sbin/service osg-update-certs-cron  status\nPeriodic osg-update-certs is disabled.  You can enable the  cron  job that updates the CA certs with:  [root@client ~]$ /sbin/service osg-update-certs-cron  start\nEnabling periodic osg-update-certs:                        [   OK   ]  A complete set of options available though  osg-ca-manage  command, including your interface to adding and removing CAs, could be found at  osg-ca-manage documentation", 
            "title": "Option 2: Install osg-update-certs"
        }, 
        {
            "location": "/Common/ca/#option-3-install-an-rpm-that-installs-no-cas", 
            "text": "Install this with:  yum install empty-ca-certs \u2013-enablerepo=osg-empty   Warning  If you choose this option, you are responsible for installing the CA certificates yourself. You must install them in  /etc/grid-security/certificates , or make a symlink from that location to the directory that contains the CA certificates.", 
            "title": "Option 3: Install an RPM that installs no CAs"
        }, 
        {
            "location": "/Common/ca/#option-4-make-no-choice-let-yum-decide-for-you", 
            "text": "If you use  yum  to install software that requires CA certificates but you haven\u2019t made one of these choices, yum will choose a default. Right now, it is Option #1 from above ( Install an RPM for a specific set of CA certificates ), and the osg-ca-certs RPM is chosen.", 
            "title": "Option 4: Make no choice, let yum decide for you"
        }, 
        {
            "location": "/Common/ca/#install-other-cas", 
            "text": "In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is,  osg-ca-certs  and the like, but not  osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs.     Set of CAs  Format  RPM name  Installation command (as root)      cilogon-basic   cilogon-openid  OpenSSL-both  cilogon-ca-certs  yum install cilogon-ca-certs", 
            "title": "Install other CAs"
        }, 
        {
            "location": "/Common/ca/#managing-certificate-revocation-lists", 
            "text": "In addition to CA certificates, you normally need to have updated Certificate Revocation Lists (CRLs) which are are lists of certificates that have been revoked for any reason. Software in the OSG Software Stack use these to ensure that you are talking to valid clients or servers. We use a tool named  fetch-crl  that periodically updates the CRLs. Fetch CRL is a utility that updates Certificate Authority (CA) Certificate Revocation Lists (CRLs). These are lists of certificates that were granted by the CA, but have since been revoked. It is good practice to regularly update the CRL list for each CA to ensure that you do not authenticate any certificate that has been revoked.  fetch-crl  is installed as two different system services. The fetch-crl-boot service runs only\nat boot time. The  fetch-crl-cron  service runs  fetch-crl  every 6 hours (with a random sleep\ntime included) by default. Both services are disabled by default. At the very minimum, the fetch-crl-cron  service needs to be enabled otherwise services will begin to fail as the\nexisting CRLs expire.", 
            "title": "Managing Certificate Revocation Lists"
        }, 
        {
            "location": "/Common/ca/#install-fetch-crl", 
            "text": "Normally  fetch-crl  is installed when you install the rest of the software and you do not need\nto specifically install it. If you do wish to install it, you can install it as:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ yum install fetch-crl3 # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7  \n[root@client ~]$ yum install fetch-crl", 
            "title": "Install fetch-crl"
        }, 
        {
            "location": "/Common/ca/#enable-and-start-fetch-crl", 
            "text": "To enable fetch-crl (fetch Certificate Revocation Lists) services by default on the node:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron on # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   \n[root@client ~]$ /sbin/chkconfig fetch-crl-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron on  To start fetch-crl:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /sbin/service fetch-crl3-boot start\n[root@client ~]$ /sbin/service fetch-crl3-cron start # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7   \n[root@client ~]$ /sbin/service fetch-crl-boot start\n[root@client ~]$ /sbin/service fetch-crl-cron start   Note  While it is necessary to start  fetch-crl-cron  in order to have it active,  fetch-crl-boot  is started automatically at boot time if enabled. The start command will run  fetch-crl-boot  at the moment when it is invoked and it may take some time to complete.", 
            "title": "Enable and Start fetch-crl"
        }, 
        {
            "location": "/Common/ca/#configure-fetch-crl", 
            "text": "To modify the times that fetch-crl-cron runs, edit  /etc/cron.d/fetch-crl  (or  /etc/cron.d/fetch-crl3  depending on the version you have).  By default,  fetch-crl  connects directly to the remote CA; this is\ninefficient and potentially harmful if done simultaneously by many nodes\n(e.g. all the worker nodes of a big cluster). We recommend you provide a\nHTTP proxy (such as  squid ) the worker nodes can utilize; OSG provides packaging of squid .  To configure fetch-crl to use an HTTP proxy server:    If using  fetch-crl  version 2 (the  fetch-crl  package on RHEL5 only), then create the file  /etc/sysconfig/fetch-crl  and add the following line:  export http_proxy= http://your.squid.fqdn:port  Adjust the URL appropriately for your proxy server.    If using  fetch-crl  version 3 on RHEL5 via the  fetch-crl3  package\n    or on RHEL6/RHEL7 via the  fetch-crl  package, then create or edit the\n    file  /etc/fetch-crl3.conf  (RHEL5) or  /etc/fetch-crl.conf \n    (RHEL6/RHEL7) and add the following line:  http_proxy= http://your.squid.fqdn:port  Again, adjust the URL appropriately for your proxy server.    Note that the  nosymlinks  option in the configuration files refers\nto ignoring links within the certificates directory (e.g. two different\nnames for the same file). It is perfectly fine if the path of the CA\ncertificates directory itself ( infodir ) is a link to a directory.  Any modifications to the configuration file will be preserved during an RPM update.  Current versions of  fetch-crl  and  fetch-crl3  produce more output.\nIt is possible to send the output to syslog instead of the default email system. To do so:    Change the configuration file to enable syslog:  logmode = syslog\n  syslogfacility = daemon\\ /pre\\    Make sure the file  /var/log/daemon  exists, e.g. touching the file   Change  /etc/logrotate.d  files to rotate it", 
            "title": "Configure fetch-crl"
        }, 
        {
            "location": "/Common/ca/#startstop-fetch-crl-a-quick-guide", 
            "text": "You need to fetch the latest CA Certificate Revocation Lists (CRLs) and you should enable the fetch-crl service to keep the CRLs up to date:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /usr/sbin/fetch-crl3 # This fetches the CRLs\n[root@client ~]$ /sbin/service fetch-crl3-boot start\n[root@client ~]$ /sbin/service fetch-crl3-cron start # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7 \n[root@client ~]$ /usr/sbin/fetch-crl # This fetches the CRLs\n[root@client ~]$ /sbin/service fetch-crl-boot start\n[root@client ~]$ /sbin/service fetch-crl-cron start  To enable the  fetch-crl  service to keep the CRLs up to date after reboots:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron on # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7  \n[root@client ~]$ /sbin/chkconfig fetch-crl-boot on\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron on  To stop  fetch-crl :  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /sbin/service fetch-crl3-boot stop\n[root@client ~]$ /sbin/service fetch-crl3-cron stop # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7  \n[root@client ~]$ /sbin/service fetch-crl-boot stop\n[root@client ~]$ /sbin/service fetch-crl-cron stop  To disable the fetch-crl service:  # For RHEL 5, CentOS 5, and SL5  \n[root@client ~]$ /sbin/chkconfig fetch-crl3-boot off\n[root@client ~]$ /sbin/chkconfig fetch-crl3-cron off # For RHEL 6 or 7, CentOS 6 or 7, and SL6 or SL7  \n[root@client ~]$ /sbin/chkconfig fetch-crl-boot off\n[root@client ~]$ /sbin/chkconfig fetch-crl-cron off", 
            "title": "Start/Stop fetch-crl: A quick guide"
        }, 
        {
            "location": "/Common/ca/#updating-cascrls", 
            "text": "", 
            "title": "Updating CAs/CRLs"
        }, 
        {
            "location": "/Common/ca/#why-maintain-up-to-date-trusted-ca-crl-information", 
            "text": "The Trusted Certificate Authority (CA) certificates, and their associated Certificate Revocation Lists (CRLs), are used for every transaction on a resource that establishes an authenticated network connection based on end user\u2019s certificate. In order for the authentication to succeed, the user\u2019s certificate must have been issued by one of the CAs in the Trusted CA directory, and the user\u2019s certificate must not be listed in the CRL for that CA. CRLs can be thought of as a black list of certificates. CAs are the trust authorities, similar to DMV that issues you the driving license. (Another way of thinking CRLs is the do-not-fly lists at the airports. if your certificate shows up in CRLs, you are not allowed access.) This is handled at the certificate validation stage even before the authorization check (which will provide the mapping of an authenticated user to a local account UID/GID). So you do not need to do worry about it; the grid software will do this for you. However, you should make sure that your site has the most up-to-date list of Trusted CAs. There are multiple trust authorities in OSG (think of it as a different DMV for each state). If you do not have an up-to-date list of CAs it is possible that some of your users transactions at your site will start to fail. A current CRL list for each CA is also necessary, since without one transactions for users of that CA will fail.", 
            "title": "Why maintain up-to-date Trusted CA /CRL information"
        }, 
        {
            "location": "/Common/ca/#how-to-ensure-you-are-get-up-to-date-cacrl-information", 
            "text": "If you installed CAs using rpm packages ( osg-ca-certs , igtf-ca-certs ) (Options 1, 4), you will need to install the software described in  the CA update document , and enable  osg-ca-certs-updater  service to keep the CAs automatically updated. If you do not install the updater, you will have to regularly run yum update to keep the CAs updated.  If you use Option 2 (i.e.  osg-update-certs ) then make sure that you have the corresponding service enabled.   [root@client ~]$ /sbin/service osg-update-certs-cron  status\n   Periodic osg-update-certs is enabled.   Ensure that fetch-crl cron is enabled\\   [root@client ~]$ /sbin/service fetch-crl-cron  status\n  Periodic fetch-crl is enabled.", 
            "title": "How to ensure you are get up-to-date CA/CRL information"
        }, 
        {
            "location": "/Common/ca/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Common/ca/#useful-configuration-and-log-files", 
            "text": "Configuration files:     Package  File Description  Location  Comment      All CA Packages  CA File Location  /etc/grid-security/certificates     All CA Packages  Index files  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt  Latest version also available at  http://software.grid.iu.edu/pacman/cadist/    All CA Packages  Change Log  /etc/grid-security/certificates/CHANGES  Latest version also available at  http://software.grid.iu.edu/pacman/cadist/CHANGES    osg-ca-certs or igtf-ca-certs  contain only CA files      osg-ca-scripts  Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    fetch-crl-2.x  Configuration file  /etc/fetch-crl.conf     fetch-crl-3.x  Configuration file  /etc/fetch-crl3.conf      The index and change log files contain a summary of all the CA distributed and their version.  Logs files:     Package  File Description  Location      osg-ca-scripts  Log file of osg-update-certs  /var/log/osg-update-certs.log    osg-ca-scripts  Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out    osg-ca-scripts  Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out    osg-ca-scripts  Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/Common/ca/#tests", 
            "text": "To test the host certificate of a server  openssl s_client  can be used. Here is an example with the gatekeeper:  UCL_PROMPT  openssl s_client -showcerts -cert /etc/grid-security/hostcert.pem -key /etc/grid-security/hostkey.pem -CApath /etc/grid-security/certificates/ -debug -connect osg-gk.mwt2.org:2119", 
            "title": "Tests"
        }, 
        {
            "location": "/Common/ca/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/Common/ca/#location-of-certificates", 
            "text": "/etc/grid-security/certificates", 
            "title": "Location of Certificates?"
        }, 
        {
            "location": "/Common/ca/#what-is-the-version-of-osg-ca-package-i-have-installed-and-what-are-its-contents", 
            "text": "The version of the CA package ca be found at  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt . The changes file can be found at  /etc/grid-security/certificates/CHANGES .", 
            "title": "What is the version of OSG CA package I have installed and what are its contents?"
        }, 
        {
            "location": "/Common/ca/#contents-of-osg-ca-package", 
            "text": "The OSG CA Distribution contains:   IGTF Distribution of Authority Root Certificates  (CAs accredited by the  International Grid Trust Federation )  Purdue TeraGrid CA   Details of CAs in OSG distribution can be found  here . For additional details what is in the current release, see the  distribution site  and  change log .", 
            "title": "Contents of OSG CA package?"
        }, 
        {
            "location": "/Common/ca/#how-can-i-add-or-remove-a-particular-ca-file", 
            "text": "Add and remove of CA files are supported only if you CA files are being installed using  osg-update-certs , which is included in the  osg-ca-scripts  package (option 2), for all other options no support for adding and removing a particular CA file is provided by OSG. The preferred approach to add or remove a CA is to use  osg-ca-manage . For adding a new CA  osg-ca-manage add [--dir  local_dir ] --hash  CA_hash  may be used, while a CA is removed using  osg-ca-manage remove --hash  CA_hash .", 
            "title": "How can I add or remove a particular CA file?"
        }, 
        {
            "location": "/Common/ca/#are-there-any-log-files-or-configuration-files-associated-with-ca-certificate-package", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4) no log or configuration files are present.  Log and configuration files are however present for  osg-ca-scripts  rpm package (option 2).  Config files:  /etc/osg/osg-update-certs.conf  Log files:  /var/log/osg-update-certs.log ,  /var/log/osg-ca-certs-status.system.out ,  /var/log/osg-ca-manage.system.out ,  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Are there any log files or configuration files associated with CA certificate package?"
        }, 
        {
            "location": "/Common/ca/#are-ca-packages-automatically-updated", 
            "text": "If CA files are installed using  osg-ca-certs  or  igtf-ca-certs  rpms (i.e. options 1, 4), you will need to install the software described in OsgCaCertsUpdater, and enable osg-ca-certs-updater service to keep the CAs automatically updated.  If CA files are being installed using  osg-ca-scripts  rpm package (option 2), CA files are kept up-to-date as long as  osg-update-certs-cron  service the package provides has been started.", 
            "title": "Are CA packages automatically updated?"
        }, 
        {
            "location": "/Common/ca/#how-do-i-manually-update-my-ca-package", 
            "text": "For Option 1: run one of the following  yum update osg-ca-certs  or  yum update igtf-ca-certs  depending on the rpm package you installed.  For Option 4: run  yum update osg-ca-certs  For Option 2: You do not need to do a manual update, make sure  osg-update-certs-cron  is enabled using  [root@client ~]$ /sbin/service osg-update-certs-cron  status  If the service is disabled, enable it using  [root@client ~]$ /sbin/service osg-update-certs-cron  start  If for some extraordinary reason you need to manually update the CA you could run  osg-ca-manage [--force] refreshCA .", 
            "title": "How do I manually update my CA package?"
        }, 
        {
            "location": "/Common/ca/#where-are-the-configuration-files-for-fetch-crl", 
            "text": "/etc/fetch-crl.conf  or  /etc/fetch-crl3.conf  for fetch-crl 2.x or 3.x respectively", 
            "title": "Where are the configuration files for fetch-crl?"
        }, 
        {
            "location": "/Common/ca/#references", 
            "text": "Some guides on x509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html   Related software:   Description, manual and examples of OsgCaManage  OsgCaCertsUpdater  Upgrading Fetch-Crl 2 to Fetch-Crl 3 on EL5", 
            "title": "References"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/", 
            "text": "Installing and Maintaining HTCondor-CE\n\n\nAbout This Guide\n\n\nThe HTCondor-CE software is a \njob gateway\n for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept \npilot jobs\n from a factory,\nwhich in turn are able to accept and run end-user jobs.\n\n\nUse this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points\n(consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will\n    create the Linux user IDs \ncondor\n (UID 4716), \ntomcat\n (UID 91) and\n    \ngratia\n (UID 42401)\n\n\nService certificate:\n The HTCondor-CE service uses a host\n    certificate at \n/etc/grid-security/host*.pem\n\n\nNetwork ports:\n The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)\n\n\nHost choice:\n HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster\n\n\n\n\nAs with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating\n    system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.\n\n\n\n\nIf your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nPBS\n\n\nyum install empty-torque --enablerepo=osg-empty\n\n\n\n\n\n\nSGE\n\n\nyum install empty-gridengine --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM(s):\n\n\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen use the following package(s)\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\nInstall the CE software:\n\n\n\n\nyum install *PACKAGE(S)*\n\n\nTo ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).\n\n\n\n\nNote\n\n\nHTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.\n\n\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on \noptional\nconfigurations\n.\n\n\nEnabling HTCondor-CE\n\n\nIf you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can \nskip\n this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.\n\n\n\n\nDecide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs\n\n\n\n\nEdit the gateway configuration file\n    \n/etc/osg/config.d/10-gateway.ini\n to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:\n\n\ngram_gateway_enabled = False\nhtcondor_gateway_enabled = True\n\n\n\nTo enable both HTCondor and GRAM CEs:\n\n\ngram_gateway_enabled = True\nhtcondor_gateway_enabled = True\n\n\n\n\n\n\n\nMore information about the Globus GRAM CE can be found \nhere\n.\n\n\nBatch System\n\n\nConfiguring the batch system\n\n\nEnable your batch system by editing the \nenabled\n field in the\n\n/etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini\n\nfile:\n\n\nenabled = True\n\n\n\n\nBatch systems other than HTCondor\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in\naddition to your HTCondor CE), skip to the \nconfiguring\nauthorization\n section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nSharing the spool directory\n\n\nTo transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the \nCE host\n. In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.\n\n\nNOTE\n: If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.\n\n\nBy default, the spool directory is \n/var/lib/condor-ce\n but you can\ncontrol this by setting \nSPOOL\n in\n\n/etc/condor-ce/config.d/99-local.conf\n. For example, the following sets\nthe \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL=/home/condor\n\n\n\n\nNOTE\n: The shared spool directory must\nbe readable and writeable by the \ncondor\n user for HTCondor CE to\nfunction correctly.\n\n\nDisable worker node proxy renewal\n\n\nWorker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit \n/etc/blah.config\n on the\nHTCondor CE host and set the following two values:\n\n\nblah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no\n\n\n\n\nNOTE\n: This configuration file uses bash syntax rules; there should be no whitespace around the \n=\n.\n\n\nConfiguring authorization\n\n\nThere are two methods to manage authorization for incoming jobs,\n\nedg-mkgridmap\n and GUMS. \nedg-mkgridmap\n is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using\n\nedg-mkgridmap\n unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:\n\n\n\n\nYou want to map users based on rules\n\n\nYou need to support multiple VO roles\n\n\nYou need to support gLExec for pilot jobs\n\n\n\n\nAuthorization with edg-mkgridmap\n\n\nTo configure your CE to use \nedg-mkgridmap\n:\n\n\n\n\nFollow the configuration instructions in \nthe edg-mkgridmap\n    document\n to define the VOs that your site\n    accepts\n\n\n\n\nSet some critical gridmap attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = gridmap\n\n\n\n\n\n\nEnable edg-mkgridmap and disable GUMS in the \n/etc/lcmaps.db\n\n    file.\n\n\nIn the \nauthorize_only\n section, comment out the\n\ngumsclient\n line and uncomment the \ngridmapfile\n line. The result\nshould be as follows:\n\n\nauthorize_only:\n # gumsclient -\n good | bad\n gridmapfile -\n good | bad\n\n\n\n\n\n\nSpecify the location of your grid mapfile in\n    \n/etc/condor-ce/config.d/01-common-auth.conf\n:\n\n\nGRIDMAP = /etc/grid-security/grid-mapfile\n\n\nNote:\n The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.\n\n\n\n\n\n\nAuthorization with GUMS\n\n\n\n\nFollow the instructions in \nthe GUMS installation and\n    configuration document\n to prepare GUMS\n\n\n\n\nSet some critical GUMS attributes by editing the\n    \n/etc/osg/config.d/10-misc.ini\n file on the HTCondor CE\n    host:\n\n\nauthorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME\n\n\n\n\n\n\n\n\nNote\n\n\nOnce \ngsi-authz.conf\n is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the \ncondor_mapfile\n. If this is not the desired behavior, set\n\nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nConfiguring information systems\n\n\nTo split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.\n\n\nGeneric Information Provider (GIP)\n\n\nThe \nGIP\n is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the \nGIP configuration\ndocument\n for instructions on\nhow to set up your \nGIP\n service.\n\n\nNOTE\n: If you have \ngip-1.3.11-4\n\ninstalled, manual intervention is required for correct reporting to\nBDII. See \n3.2.20 known\nissues\n.\n\n\nOSG Info Services\n\n\nosg-info-services\n takes the information collected from \nGIP\n and\nuploads it to OSG's central collector. For \nosg-info-services\n to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at \n/etc/grid-security/http/httpcert.pem\n and\n\n/etc/grid-security/http/httpkey.pem\n, respectively. Additionally, the\nservice runs as either the \ntomcat\n user or the account specified by the\n\nuser\n option in \n/etc/osg/config.d/30-gip.ini\n, thus your service\ncertificates need to be owned by the appropriate user.\n\n\n\n\n\n\nEnable osg-info-services in\n    \n/etc/osg/config.d/30-infoservices.ini\n:\n\n\nenabled = *True*\n\n2.  Generate a \nuser-vo-map\n file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:\n\n\nedg-mkgridmap\n\nii. If you're using GUMS, run the following:\n\n\ngums-host-cron\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n\ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the \nosg-configure\n tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The \nosg-configure\n software is included\nautomatically in an HTCondor CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n\n    directory\n\n\n\n\nNote\n\n\nThis document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings\n\n\nosg-configure -v\n\n\nFix any errors (at least) that \nosg-configure\n reports.\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the\n    configuration settings:\n\n\nosg-configure -c\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to \nthe section on using\nHTCondor CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nHTCondor accounting groups\n\n\nInstalling the HTCondor-CE View\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in\nthe \nJob Router Recipes\n\ndocument.\n\n\nNOTE\n: If you need to assign jobs to\nHTCondor accounting groups, refer to \nthis\n section.\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set\n\nNETWORK_HOSTNAME\n to the hostname of your public interface in\n\n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME=condorce.example.com\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public\ninterface\u2019s hostname.\n\n\nLimiting or disabling locally jobs running on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs\n    (the current default is 20), add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nLOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning \n $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n(updating \nLOCAL_JOB_LIMIT\n as appropriate.)\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the\n    following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to\n    \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)\n\n\n\n\n\n\nNOTE\n: RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the \nrsv\n user.\n\n\nHTCondor accounting groups\n\n\nNOTE\n: For HTCondor batch systems only\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are \ndocumented in the HTCondor manual\n.\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.\n\n\nMapping by UID\n\n\nTo map UID\u2019s to an accounting group, use \n/etc/osg/uid_table.txt\n. It is\nconsulted first and contains lines of the form:\n\n\nuid GroupName\n\n\n\n\nExample \nuid_table.txt\n:\n\n\nuscms02 TestGroup\nosg     other.osgedu\n\n\n\n\nMapping by DN or VOMS attribute\n\n\nTo map DN\u2019s or VOMS attributes to an accounting group, use\n\n/etc/osg/extattr_table.txt\n. This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:\n\n\nSubjectOrAttribute GroupName\n\n\n\n\nThe SubjectOrAttribute can be a Perl regular\nexpression.\n\n\nExample \nextattr_table.txt\n:\n\n\ncmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other\n\n\n\n\nInstall and run the HTCondor-CE-View\n\n\nThe HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nyum install htcondor-ce-view\n\n\n\n\n\n\nNext, uncomment the \nDAEMON_LIST\n configuration located at\n    \n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD\n]]]]\n\n\n\n\n\n\nRestart the CE service.\n\n\nservice condor-ce restart\n\n\n\n\n\n\nBy default, the website is served from port 80. This may be configured\nin \n/etc/condor-ce/config.d/05-ce-view.conf\n as well.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways in which you might use the\nHTCondor CE:\n\n\n\n\nManaging the HTCondor CE and associated services\n\n\nUsing HTCondor CE administrative tools to monitor and maintain the\n    job gateway\n\n\nUsing HTCondor CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor CE and associated services\n\n\nIn addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nOn EL 6: \nfetch-crl-boot\n and \nfetch-crl-cron\n  \n   On EL 5: \nfetch-crl3-boot\n and \nfetch-crl3-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nOSG Info Services\n\n\nosg-info-services\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nem\nSERVICE-NAME\n/em\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nem\nSERVICE-NAME\n/em\n off\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor CE administrative and user tools are documented in\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nThere are different ways to make sure that your HTCondor CE host is\nworking well:\n\n\n\n\nPerform automated validation by running \nRSV\n\n\nManually verify your HTCondor CE using \nthe HTCondor CE\n    troubleshooting guide\n; useful tools\n    include:\n\n\ncondor_ce_run\n\n\ncondor_ce_trace\n\n\ncondor_submit\n\n\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor CE, please refer to\n\nthe HTCondor CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be registered in the\n\nhttps://oim.grid.iu.edu/ OSG Information Management System\n\n(OIM). To register your resource:\n\n\n\n\nObtain, install, and verify your user\n    certificate\n (which you may have\n    done already)\n\n\nRegister your site and CE in\n    OIM\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis\npage\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting Jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the\nfollowing command:\n\n\n# condor_ce_config_val -config\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\ntomcat\n\n\nDefault user that runs GIP\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \nbr>  \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-and-maintaining-htcondor-ce", 
            "text": "", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#about-this-guide", 
            "text": "The HTCondor-CE software is a  job gateway  for an OSG Compute Element\n(CE). As such, HTCondor-CE is the entry point for jobs coming from the\nOSG \u2014 it handles authorization and delegation of jobs to your local\nbatch system. In OSG today, most CEs accept  pilot jobs  from a factory,\nwhich in turn are able to accept and run end-user jobs.  Use this page to learn how to install, configure, run, test, and\ntroubleshoot HTCondor-CE from the OSG software repositories.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#before-starting", 
            "text": "Before starting the installation process, consider the following points\n(consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will\n    create the Linux user IDs  condor  (UID 4716),  tomcat  (UID 91) and\n     gratia  (UID 42401)  Service certificate:  The HTCondor-CE service uses a host\n    certificate at  /etc/grid-security/host*.pem  Network ports:  The pilot factories must be able to contact your\n    HTCondor-CE service on ports 9619 and 9620 (TCP)  Host choice:  HTCondor-CE should be installed on a host that\n    already has the ability to submit jobs into your local cluster   As with all OSG software installations, there are some one-time (per\nhost) steps to prepare in advance:   Ensure the host has  a supported operating\n    system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the\nHTCondor CE job router) and other support software (e.g., GridFTP, a\nGratia probe, authorization software). To simplify installation, OSG\nprovides convenience RPMs that install all required software with a\nsingle command.   If your batch system is already installed via non-RPM means and is\n   in the following list, install the appropriate 'empty' RPM. Otherwise,\n   skip to the next step.      If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    PBS  yum install empty-torque --enablerepo=osg-empty    SGE  yum install empty-gridengine --enablerepo=osg-empty      Select the appropriate convenience RPM(s):      If your batch system is\u2026  Then use the following package(s)\u2026      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm      Install the CE software:   yum install *PACKAGE(S)*  To ease the transition from GRAM\nto HTCondor-CEs, the convenience RPMs install both types of job gateway\nsoftware. By default, the HTCondor gateway is enabled and the GRAM\ngateway is disabled, which is the correct configuration for most\nHTCondor-CE-based sites (but see the gateway configuration section below\nfor more options).   Note  HTCondor CE version 1.6 or later\nis required to send site resource information to OSG for matching jobs\nto resources.", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor CE with\nyour batch system and authorization method. For more advanced\nconfiguration, see the section on  optional\nconfigurations .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#enabling-htcondor-ce", 
            "text": "If you are installing HTCondor CE on a new host, the default\nconfiguration is correct and you can  skip  this step!\nHowever, if you are updating a host that used a Globus GRAM job gateway\n(aka the Globus gatekeeper), you must enable the HTCondor job gateway.   Decide whether to disable GRAM (the preferred option) or run\n    both HTCondor and GRAM CEs   Edit the gateway configuration file\n     /etc/osg/config.d/10-gateway.ini  to reflect your choice\n    To enable HTCondor CE and disable GRAM CE:  gram_gateway_enabled = False\nhtcondor_gateway_enabled = True  To enable both HTCondor and GRAM CEs:  gram_gateway_enabled = True\nhtcondor_gateway_enabled = True    More information about the Globus GRAM CE can be found  here .", 
            "title": "Enabling HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#batch-system", 
            "text": "", 
            "title": "Batch System"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-the-batch-system", 
            "text": "Enable your batch system by editing the  enabled  field in the /etc/osg/config.d/20-YOUR-BATCH-SYSTEM.ini \nfile:  enabled = True", 
            "title": "Configuring the batch system"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#batch-systems-other-than-htcondor", 
            "text": "If you are using HTCondor as your  local batch system  (i.e., in\naddition to your HTCondor CE), skip to the  configuring\nauthorization  section. For other batch\nsystems (e.g., PBS, LSF, SGE, SLURM), keep reading.", 
            "title": "Batch systems other than HTCondor"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#sharing-the-spool-directory", 
            "text": "To transfer files between the CE and the batch system, HTCondor CE\nrequires a shared file system. The current recommendation is to run a\ndedicated NFS server (whose installation is beyond the scope of this\ndocument) on the  CE host . In this setup, HTCondor-CE writes to the\nlocal spool directory, the NFS server exports the it, and the NFS server\nshares the it with all of the worker nodes.  NOTE : If you choose not to host the NFS\nserver on your CE, you will need to turn off root squash so that the\nHTCondor-CE daemons can write to the spool directory.  By default, the spool directory is  /var/lib/condor-ce  but you can\ncontrol this by setting  SPOOL  in /etc/condor-ce/config.d/99-local.conf . For example, the following sets\nthe  SPOOL  directory to  /home/condor :  SPOOL=/home/condor  NOTE : The shared spool directory must\nbe readable and writeable by the  condor  user for HTCondor CE to\nfunction correctly.", 
            "title": "Sharing the spool directory"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#disable-worker-node-proxy-renewal", 
            "text": "Worker node proxy renewal is not used by HTCondor-CE and leaving it on\nwill cause some jobs to be held. Edit  /etc/blah.config  on the\nHTCondor CE host and set the following two values:  blah_disable_wn_proxy_renewal=yes\nblah_delegate_renewed_proxies=no  NOTE : This configuration file uses bash syntax rules; there should be no whitespace around the  = .", 
            "title": "Disable worker node proxy renewal"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-authorization", 
            "text": "There are two methods to manage authorization for incoming jobs, edg-mkgridmap  and GUMS.  edg-mkgridmap  is easy to set up and maintain,\nand GUMS has more features and capabilities. We recommend using edg-mkgridmap  unless you have specific needs that require the use of\nGUMS. Some examples of these specific requirements are:   You want to map users based on rules  You need to support multiple VO roles  You need to support gLExec for pilot jobs", 
            "title": "Configuring authorization"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#authorization-with-edg-mkgridmap", 
            "text": "To configure your CE to use  edg-mkgridmap :   Follow the configuration instructions in  the edg-mkgridmap\n    document  to define the VOs that your site\n    accepts   Set some critical gridmap attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = gridmap    Enable edg-mkgridmap and disable GUMS in the  /etc/lcmaps.db \n    file.  In the  authorize_only  section, comment out the gumsclient  line and uncomment the  gridmapfile  line. The result\nshould be as follows:  authorize_only:\n # gumsclient -  good | bad\n gridmapfile -  good | bad    Specify the location of your grid mapfile in\n     /etc/condor-ce/config.d/01-common-auth.conf :  GRIDMAP = /etc/grid-security/grid-mapfile  Note:  The standard location for the grid mapfile is shown\nabove. Use that location unless you have specific reasons to put the\nfile somewhere else.", 
            "title": "Authorization with edg-mkgridmap"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#authorization-with-gums", 
            "text": "Follow the instructions in  the GUMS installation and\n    configuration document  to prepare GUMS   Set some critical GUMS attributes by editing the\n     /etc/osg/config.d/10-misc.ini  file on the HTCondor CE\n    host:  authorization_method = xacml\ngums_host = YOUR GUMS HOSTNAME     Note  Once  gsi-authz.conf  is in place,\nyour local HTCondor will attempt to utilize the LCMAPS callouts if\nenabled in the  condor_mapfile . If this is not the desired behavior, set GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Authorization with GUMS"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-information-systems", 
            "text": "To split jobs between the various sites of the OSG, information about\neach site\u2019s availability is uploaded to a central collector. The job\nfactories then query the central collector for idle resources and submit\npilot jobs to the available sites. To advertise your site, you will need\nto run the Generic Information Provider and OSG Info Services.", 
            "title": "Configuring information systems"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#generic-information-provider-gip", 
            "text": "The  GIP  is a service that discovers information about your site\nresources like the number of available cores and what VO's are allowed\nto run on your site. Consult the  GIP configuration\ndocument  for instructions on\nhow to set up your  GIP  service.  NOTE : If you have  gip-1.3.11-4 \ninstalled, manual intervention is required for correct reporting to\nBDII. See  3.2.20 known\nissues .", 
            "title": "Generic Information Provider (GIP)"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#osg-info-services", 
            "text": "osg-info-services  takes the information collected from  GIP  and\nuploads it to OSG's central collector. For  osg-info-services  to\ncommunicate with the appropriate servers, it needs a service certificate\nand key located at  /etc/grid-security/http/httpcert.pem  and /etc/grid-security/http/httpkey.pem , respectively. Additionally, the\nservice runs as either the  tomcat  user or the account specified by the user  option in  /etc/osg/config.d/30-gip.ini , thus your service\ncertificates need to be owned by the appropriate user.    Enable osg-info-services in\n     /etc/osg/config.d/30-infoservices.ini :  enabled = *True* \n2.  Generate a  user-vo-map  file with your authorization set\nup:\ni.  If you're using edg-mkgridmap, run the following:  edg-mkgridmap \nii. If you're using GUMS, run the following:  gums-host-cron", 
            "title": "OSG Info Services"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d \ndirectory does not apply those settings to software automatically.\nSettings that are made outside of the OSG directory take effect\nimmediately or at least when the relevant service is restarted. For the\nOSG settings, use the  osg-configure  tool to\nvalidate (to a limited extent) and apply the settings to the relevant\nsoftware components. The  osg-configure  software is included\nautomatically in an HTCondor CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d \n    directory   Note  This document describes the\ncritical settings for HTCondor CE and related software. You may need\nto configure other software that is installed on your HTCondor CE\nhost, too.     Validate the configuration settings  osg-configure -v  Fix any errors (at least) that  osg-configure  reports.    Once the validation command succeeds without errors, apply the\n    configuration settings:  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be\nrequired for setting up a small site. If you do not need any of the\nfollowing special configurations, skip to  the section on using\nHTCondor CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  HTCondor accounting groups  Installing the HTCondor-CE View", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#transforming-and-filtering-jobs", 
            "text": "If you need to modify or filter jobs, more information can be found in\nthe  Job Router Recipes \ndocument.  NOTE : If you need to assign jobs to\nHTCondor accounting groups, refer to  this  section.", 
            "title": "Transforming and filtering jobs"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuring-for-multiple-network-interfaces", 
            "text": "If you have multiple network interfaces with different hostnames, the\nHTCondor CE daemons need to know which hostname to use when\ncommunicating to each other. Generally, you will want to set NETWORK_HOSTNAME  to the hostname of your public interface in /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME=condorce.example.com  Replacing  condorce.example.com  text with your public\ninterface\u2019s hostname.", 
            "title": "Configuring for multiple network interfaces"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#limiting-or-disabling-locally-jobs-running-on-the-ce", 
            "text": "If you want to limit or disable jobs running locally on your CE, you\nwill need to configure HTCondor-CE's local and scheduler universes.\nLocal and scheduler universes are HTCondor CE\u2019s analogue to GRAM\u2019s\nmanaged fork: they allow jobs to be run on the CE itself. The two\nuniverses are effectively the same (scheduler universe launches a\nstarter process for each job), so we will be configuring them in unison.    To change the default limit  on the number of locally run jobs\n    (the current default is 20), add the following to\n     /etc/condor-ce/config.d/99-local.conf :  LOCAL_JOB_LIMIT = 20\nSTART_LOCAL_UNIVERSE = TotalLocalJobsRunning + TotalSchedulerJobsRunning   $(LOCAL_JOB_LIMIT)\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)  (updating  LOCAL_JOB_LIMIT  as appropriate.)    To only allow a specific user  to start locally run jobs, add the\n    following to  /etc/condor-ce/config.d/99-local.conf :  ALLOWED_LOCAL_USER=alice\nSTART_LOCAL_UNIVERSE = target.Owner =?= \"$(ALLOWED_LOCAL_USER)\"\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    To disable  locally run jobs, add the following to\n     /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE = False\nSTART_SCHEDULER_UNIVERSE = $(START_LOCAL_UNIVERSE)    NOTE : RSV requires the ability to start\nlocal universe jobs so if you are using RSV, you need to allow local\nuniverse jobs from the  rsv  user.", 
            "title": "Limiting or disabling locally jobs running on the CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#htcondor-accounting-groups", 
            "text": "NOTE : For HTCondor batch systems only  If you want to provide fairshare on a group basis, as opposed to a Unix\nuser basis, you can use HTCondor accounting groups. They are independent\nof the Unix groups the user may already be in, and are  documented in the HTCondor manual .\nIf you are using HTCondor accounting groups, you can map jobs from the\nCE into HTCondor accounting groups based on their numeric user id, their\nDN, or their VOMS attributes.", 
            "title": "HTCondor accounting groups"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#mapping-by-uid", 
            "text": "To map UID\u2019s to an accounting group, use  /etc/osg/uid_table.txt . It is\nconsulted first and contains lines of the form:  uid GroupName  Example  uid_table.txt :  uscms02 TestGroup\nosg     other.osgedu", 
            "title": "Mapping by UID"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#mapping-by-dn-or-voms-attribute", 
            "text": "To map DN\u2019s or VOMS attributes to an accounting group, use /etc/osg/extattr_table.txt . This file is only consulted if the user is\nnot found in the UID file and it contains lines of the form:  SubjectOrAttribute GroupName  The SubjectOrAttribute can be a Perl regular\nexpression.  Example  extattr_table.txt :  cmsprio cms.other.prio\ncms\\/Role=production cms.prod\n.* other", 
            "title": "Mapping by DN or VOMS attribute"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#install-and-run-the-htcondor-ce-view", 
            "text": "The HTCondor-CE-View is an optional web interface to the status of your\nCE. To run the View,    Begin by installing the package htcondor-ce-view:  yum install htcondor-ce-view    Next, uncomment the  DAEMON_LIST  configuration located at\n     /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST = $(DAEMON_LIST), CEVIEW, GANGLIAD\n]]]]    Restart the CE service.  service condor-ce restart    By default, the website is served from port 80. This may be configured\nin  /etc/condor-ce/config.d/05-ce-view.conf  as well.", 
            "title": "Install and run the HTCondor-CE-View"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways in which you might use the\nHTCondor CE:   Managing the HTCondor CE and associated services  Using HTCondor CE administrative tools to monitor and maintain the\n    job gateway  Using HTCondor CE user tools to test gateway operations", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#managing-htcondor-ce-and-associated-services", 
            "text": "In addition to the HTCondor CE job gateway service itself, there are a\nnumber of supporting services in your installation. The specific\nservices are:     Software  Service name  Notes      Fetch CRL  On EL 6:  fetch-crl-boot  and  fetch-crl-cron       On EL 5:  fetch-crl3-boot  and  fetch-crl3-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     OSG Info Services  osg-info-services     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  em SERVICE-NAME /em  start    Stop a service  service  em SERVICE-NAME /em  stop    Enable a service to start during boot  chkconfig  em SERVICE-NAME /em  on    Disable a service from starting during boot  chkconfig  em SERVICE-NAME /em  off", 
            "title": "Managing HTCondor CE and associated services"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#using-htcondor-ce-tools", 
            "text": "Some of the HTCondor CE administrative and user tools are documented in the HTCondor CE troubleshooting guide .", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#validating-htcondor-ce", 
            "text": "There are different ways to make sure that your HTCondor CE host is\nworking well:   Perform automated validation by running  RSV  Manually verify your HTCondor CE using  the HTCondor CE\n    troubleshooting guide ; useful tools\n    include:  condor_ce_run  condor_ce_trace  condor_submit", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor CE, please refer to the HTCondor CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered in the https://oim.grid.iu.edu/ OSG Information Management System \n(OIM). To register your resource:   Obtain, install, and verify your user\n    certificate  (which you may have\n    done already)  Register your site and CE in\n    OIM", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#getting-help", 
            "text": "To get assistance, please use the  this\npage .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting Jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#configuration", 
            "text": "The following directories contain the configuration for HTCondor-CE. The\ndirectories are parsed in the order presented and thus configuration\nwithin the final directory will override configuration specified in the\nprevious directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the\nfollowing command:  # condor_ce_config_val -config", 
            "title": "Configuration"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#users", 
            "text": "The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data    tomcat  Default user that runs GIP", 
            "title": "Users"
        }, 
        {
            "location": "/Computing_Element/HTCondor_CE/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem   br>   /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/Frontier_Squid/squid/", 
            "text": "Frontier Squid Caching Proxy Installation Guide\n\n\n\n\nAbout This Document\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\nApplicable Versions\n\n\nThe applicable software versions for this document are OSG Version \n=\n3.2.16. The version of frontier-squid installed should be \n= 2.7.STABLE9-19.1\n\n\nAbout Frontier Squid\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has \nmany\nadvantages\n\nover regular squid for common grid applications, especially Frontier and\nCVMFS.\n\n\nThe OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nFrontier Squid is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.\n\n\nThe \nosg-configure\n configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.\n\n\nEngineering Considerations\n\n\nIf you will be supporting the Frontier application at your site, review\nthe \nupstream documentation Hardware considerations\nsection\n\nto determine how to size your equipment.\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 5, 6, 7, and variants\n\n\nRoot access\n\n\n\n\nUsers The frontier-squid installation will create one user account\n\n\nunless it already exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation\nsection\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the \nupstream documentation Enabling monitoring\nsection\n.\n\n\nInstall Instructions\n\n\nPrior to installing squid, make sure the host's \nyum\n repositories are \nconfigured correctly\n for OSG.\n\n\nOnce configured, install \nfrontier-squid\n:\n\n\n[root@client ~]$ yum install frontier-squid\n\n\n\n\nThen enable it to start at boot:\n\n\n[root@client ~]$ chkconfig frontier-squid on\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the \noriginal Frontier Squid\n    documentation\n,\n    in \nthe Configuration\n    section\n\\\n\n\nEnable, start, and test the service (as described below)\n\n\nEnable WLCG monitoring as described in the \nupstream documentation\n    on enabling\n    monitoring\n\n    and \nregister the squid in\n    OIM\n.\n\n\n\n\n\n\nNote\n\n\nAn important difference between the standard Squid\nsoftware and the Frontier Squid variant is that Frontier Squid\nchanges are in \n/etc/squid/customize.sh\n instead of\n\n/etc/squid/squid.conf\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:\n\n\n\n\n\n\nOn your CE host, edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure\n to propagate the changes on your CE\n\n\n\n\n\n\n\n\nNote\n\n\nYou may want to finish other CE configuration tasks before running\n\nosg-configure\n. Just be sure to run it once before starting CE services.\n\n\n\n\nStarting and Stopping the Frontier Squid Service\n\n\nStarting frontier-squid:\n\n\nUCL_ROOT_PROMPT\n service frontier-squid start\n\n\n\n\nStopping frontier-squid:\n\n\nUCL_ROOT_PROMPT\n service frontier-squid stop\n\n\n\n\nTesting Frontier Squid\n\n\nAs any user on another computer, do the following (where\n\nyoursquid.your.domain\n is\nthe fully qualified domain name of your squid server):\n\n\n[user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2\n1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain\n\n\n\n\nIf the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says \nMISS\n again, something\nis probably wrong with the squid cache writes.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the \nupstream documentation Testing the\ninstallation\nsection\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the \nupstream documentation Log file\ncontents section\n.\n\n\nGetting Help\n\n\nTo get assistance please use \nHelp Procedure\n.", 
            "title": "HTTP Cache"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-caching-proxy-installation-guide", 
            "text": "", 
            "title": "Frontier Squid Caching Proxy Installation Guide"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-this-document", 
            "text": "This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Frontier_Squid/squid/#applicable-versions", 
            "text": "The applicable software versions for this document are OSG Version  =\n3.2.16. The version of frontier-squid installed should be  = 2.7.STABLE9-19.1", 
            "title": "Applicable Versions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#about-frontier-squid", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has  many\nadvantages \nover regular squid for common grid applications, especially Frontier and\nCVMFS.  The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.", 
            "title": "About Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS to\nhelp reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it may be best to\nrun the proxy on its own host. In that case, the Frontier Squid software\nstill will be installed on the CE, but it need not be enabled. Instead,\ninstall your proxy service on the separate host and then configure the\nCE host to refer to the proxy on that host.  The  osg-configure  configuration tool (version 1.0.45 and later) warns\nusers who have not added the proxy location to their CE configuration.\nIn the future, a proxy will be required and osg-configure will fail if\nthe proxy location is not set.", 
            "title": "Frontier Squid is Recommended"
        }, 
        {
            "location": "/Frontier_Squid/squid/#engineering-considerations", 
            "text": "If you will be supporting the Frontier application at your site, review\nthe  upstream documentation Hardware considerations\nsection \nto determine how to size your equipment.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/Frontier_Squid/squid/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Frontier_Squid/squid/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 5, 6, 7, and variants  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Frontier_Squid/squid/#users-the-frontier-squid-installation-will-create-one-user-account", 
            "text": "unless it already exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \u201csquid\u201d user to be a group that is also called \u201csquid\u201d.     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation\nsection .", 
            "title": "Users The frontier-squid installation will create one user account"
        }, 
        {
            "location": "/Frontier_Squid/squid/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the  upstream documentation Enabling monitoring\nsection .", 
            "title": "Networking"
        }, 
        {
            "location": "/Frontier_Squid/squid/#install-instructions", 
            "text": "Prior to installing squid, make sure the host's  yum  repositories are  configured correctly  for OSG.  Once configured, install  frontier-squid :  [root@client ~]$ yum install frontier-squid  Then enable it to start at boot:  [root@client ~]$ chkconfig frontier-squid on", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-frontier-squid", 
            "text": "", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-the-frontier-squid-service", 
            "text": "To configure the Frontier Squid service itself:   Follow the  original Frontier Squid\n    documentation ,\n    in  the Configuration\n    section \\  Enable, start, and test the service (as described below)  Enable WLCG monitoring as described in the  upstream documentation\n    on enabling\n    monitoring \n    and  register the squid in\n    OIM .    Note  An important difference between the standard Squid\nsoftware and the Frontier Squid variant is that Frontier Squid\nchanges are in  /etc/squid/customize.sh  instead of /etc/squid/squid.conf .", 
            "title": "Configuring the Frontier Squid Service"
        }, 
        {
            "location": "/Frontier_Squid/squid/#configuring-the-osg-ce", 
            "text": "To configure the OSG Compute Element (CE) to know about your Frontier\nSquid service:    On your CE host, edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure  to propagate the changes on your CE     Note  You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE services.", 
            "title": "Configuring the OSG CE"
        }, 
        {
            "location": "/Frontier_Squid/squid/#starting-and-stopping-the-frontier-squid-service", 
            "text": "Starting frontier-squid:  UCL_ROOT_PROMPT  service frontier-squid start  Stopping frontier-squid:  UCL_ROOT_PROMPT  service frontier-squid stop", 
            "title": "Starting and Stopping the Frontier Squid Service"
        }, 
        {
            "location": "/Frontier_Squid/squid/#testing-frontier-squid", 
            "text": "As any user on another computer, do the following (where yoursquid.your.domain  is\nthe fully qualified domain name of your squid server):  [user@client ~]$ export http_proxy=http://yoursquid.your.domain:3128\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: MISS from yoursquid.your.domain\n[user@client ~]$ wget -qdO/dev/null http://frontier.cern.ch 2 1|grep X-Cache\nX-Cache: HIT from yoursquid.your.domain  If the grep doesn\u2019t print anything, try removing it from the pipeline to\nsee if errors are obvious. If the second try says  MISS  again, something\nis probably wrong with the squid cache writes.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the  upstream documentation Testing the\ninstallation\nsection .", 
            "title": "Testing Frontier Squid"
        }, 
        {
            "location": "/Frontier_Squid/squid/#frontier-squid-log-files", 
            "text": "Log file contents are explained in the  upstream documentation Log file\ncontents section .", 
            "title": "Frontier Squid Log Files"
        }, 
        {
            "location": "/Frontier_Squid/squid/#getting-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/Other/cvmfs/", 
            "text": "Install CVMFS\n\n\nHere we describe how to install the \nCVMFS\n (Cern-VM file system) client.\nThis document is intended for system administrators who wish to install this client to have access to files distributed\nby cvmfs servers via HTTP.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.2.22.\nThe version of cvmfs installed should be \n= 2.1.20-1.osg or greater.\n\n\n\n\nRequirements\n\n\nHost and OS\n\n\n\n\nOS is RedHat 5, 6, 7 or variants\n\n\nroot\n access\n\n\nautofs\n should be installed\n\n\nfuse\n should be installed (or will be as part of the installation)\n\n\nSufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details \nbelow\n)\n\n\n\n\nUsers and Groups\n\n\nThis installation will create one user unless it already exists:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nThe installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse rpm is not for some reason already installed, installing cvmfs will also install fuse and that will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs\n\n\n\n\n\n\n\n\nNetworking\n\n\nYou will need network access to a local squid server such as the \nsquid distributed by OSG\n. The squid will need out-bound access to cvmfs stratum 1 servers.\n\n\nUpgrading\n\n\nWhen upgrading to cvmfs 2.1.20, delete the setting of \nCVMFS_SERVER_URL\n in \n/etc/cvmfs/domain.d/cern.ch.local\n. If that's the only thing in the file (which is likely) then delete the whole file.\n\n\nWhen upgrading from a cvmfs 2.0.X version, all \n/cvmfs\n repositories must be unmounted in order to upgrade. When upgrading between 2.1.X versions, repositories may be mounted.\n\n\nNote that version 2.1 removed the \n/etc/init.d/cvmfs\n script. Starting it didn't actually do anything anyway (it is automatically starts when mounted), and the stop function has been moved to \ncvmfs_config umount\n.\nThe \nrestartautofs\n function is instead done by \nservice autofs restart\n. The \nprobe\n function has also been moved to \ncvmfs_config probe\n. Since 2.1.X enables shared cache by default, so in order to reclaim the previous cache space when upgrading from 2.0.X you must manually remove the old caches. For example\n\n\nrm -rf /var/cache/cvmfs/*.*\n\n\n\n\nInstall Instructions\n\n\nPrior to installing CVMFS, make sure the \nyum repositories\n are correctly configured for OSG.\n\n\nThe following will install cvmfs from the OSG repository. It will also install cern public keys as well as fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution, OASIS.\n\n\nyum install osg-oasis\n\n\n\n\nCreate or edit \n/etc/fuse.conf\n. It should contain the following in order to allow fuse to do proper file ownership:\n\n\nuser_allow_other\n\n\n\n\nCreate or edit \n/etc/auto.master\n. It should contain the following in order to allow cvmfs to automount:\n\n\n/cvmfs /etc/auto.cvmfs\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nservice autofs restart\nStopping automount:                       [  OK  ]\nStarting automount:                       [  OK  ]\n\n\n\n\nCreate or edit \n/etc/cvmfs/default.local\n, a file that controls the cvmfs configuration.\nBelow is a sample configuration, \nbut please note\n that you will need to customize this for your site. (In particular, the \nCVMFS_HTTP_PROXY\n line below only works within the .fnal.gov domain.)\n\n\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\n\nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY=\nhttp://squid.fnal.gov:3128\n\n\n\n\n\nCVMFS 2.1 by default allows any repository to be mounted. The recommended CVMFS_REPOSITORIES setting is what it is above so that tools such as \ncvmfs_config\n and \ncvmfs_talk\n that use known repositories will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. A full list of cern.ch repositories is found at \nhttp://cernvm.cern.ch/portal/cvmfs/examples\n. The only opensciencegrid.org repository is currently oasis.\n\n\nSet up a list of cvmfs HTTP proxies to retrieve from in CVMFS_HTTP_PROXY. Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. A special proxy called DIRECT can be placed last in the list to indicate directly connecting to servers if all other proxies fail. This is acceptable for small sites but discouraged for large sites because of the potential load that could be put upon the stratum one servers.\n\n\nSet up the cache limit in \nCVMFS_QUOTA_LIMIT\n (in MB). Recommended for most applications is 20GB. This is the combined limit for all repositories. This cache will be stored in \n$CVMFS_CACHE_BASE\n. Make sure that at least 20% more than that amount of space stays available for cvmfs in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem.\n\n\nVerifying cvmfs\n\n\nAfter CVMFS is installed, you should be able to see the \n/cvmfs\n directory. But note that it will initially appear to be empty:\n\n\n$ ls /cvmfs\n\n\n\n\n\nDirectories within \n/cvmfs\n will not be mounted until you examine them. For instance:\n\n\n# ls -l /cvmfs/atlas.cern.ch\n/cvmfs/atlas.cern.ch:\ntotal 5\ndrwxr-xr-x 1 cvmfs cvmfs 4096 Mar  5  2012 repo\n# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n/cvmfs/oasis.opensciencegrid.org/cmssoft:\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 28 10:33 cms -\n /cvmfs/cms.cern.ch\n# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n\n\n\n\nTroubleshooting problems\n\n\nIf no directories exist under \n/cvmfs/\n, you can try the following steps to debug:\n\n\n\n\nMount it manually \nmkdir /mnt/cvmfs\n then \nmount -t cvmfs REPOSITORYNAME /mnt/cvmfs\n where \nREPOSITORYNAME\n is the repository, for example \noasis.opensciencegrid.org\n. If this works, then cvmfs is working, but there is a problem with automount.\n\n\nIf that doesn't work and doesn't give any explanatory errors, try \ncvmfs_config chksetup\n or \ncvmfs_config showconfig REPOSITORYNAME\n to verify your setup.\n\n\nIf chksetup reports access problems to proxies, it may be caused by access control settings in the squids.\n\n\nIf you have changed settings in \n/etc/cvmfs/default.local\n, and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of \n/etc/cvmfs/default.conf\n regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation.\n\n\nMore things to try are in the \nupstream documentation\n.\n\n\n\n\nStarting and Stopping services\n\n\nOnce it is set up, cvmfs is always automatically started when one of the repositories are accessed.\n\n\ncvmfs can be stopped via:\n\n\n# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n\n\n\n\nScreendump of Install\n\n\n[root@fermicloud044 ~]# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nRetrieving http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nPreparing...                ########################################### [100%]\n   1:epel-release           ########################################### [100%]\n[root@fermicloud044 ~]# yum install yum-priorities\nLoaded plugins: security\nepel/metalink                                            |  15 kB     00:00     \nepel                                                     | 4.4 kB     00:00     \nepel/primary_db                                          | 6.5 MB     00:02     \nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package yum-plugin-priorities.noarch 0:1.1.30-14.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch         Version               Repository\n                                                                           Size\n================================================================================\nInstalling:\n yum-plugin-priorities       noarch       1.1.30-14.el6         slf        21 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 21 k\nInstalled size: 28 k\nIs this ok [y/N]: y\nDownloading Packages:\nyum-plugin-priorities-1.1.30-14.el6.noarch.rpm           |  21 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n  Verifying  : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n\nInstalled:\n  yum-plugin-priorities.noarch 0:1.1.30-14.el6                                  \n\nComplete!\n[root@fermicloud044 ~]# grep plugins /etc/yum.conf\nplugins=1\n[root@fermicloud044 ~]# rpm -Uvh http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nwarning: /var/tmp/rpm-tmp.C1YSbQ: Header V3 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud044 ~]# yum install osg-oasis\nLoaded plugins: priorities, security\nosg                                                      | 1.9 kB     00:00     \nosg/primary_db                                           | 1.9 MB     00:00     \n342 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package osg-oasis.noarch 0:5-1.osg32.el6 will be installed\n--\n Processing Dependency: cvmfs-config-osg \n= 1.1 for package: osg-oasis-5-1.osg32.el6.noarch\n--\n Processing Dependency: cvmfs \n= 2.1.20 for package: osg-oasis-5-1.osg32.el6.noarch\n--\n Running transaction check\n---\n Package cvmfs.x86_64 0:2.1.20-1.osg32.el6 will be installed\n--\n Processing Dependency: libfuse.so.2(FUSE_2.4)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: /usr/sbin/semanage for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2(FUSE_2.6)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: fuse-libs for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: gdb for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2(FUSE_2.5)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: fuse for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--\n Processing Dependency: libfuse.so.2()(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n---\n Package cvmfs-config-osg.noarch 0:1.1-5.osg32.el6 will be installed\n--\n Running transaction check\n---\n Package fuse.x86_64 0:2.8.3-4.el6 will be installed\n---\n Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed\n---\n Package gdb.x86_64 0:7.2-60.el6 will be installed\n---\n Package policycoreutils-python.x86_64 0:2.0.83-19.30.el6 will be installed\n--\n Processing Dependency: libsemanage-python \n= 2.0.43-4 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: audit-libs-python \n= 1.4.2-1 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: setools-libs-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: libselinux-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Processing Dependency: libcgroup for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--\n Running transaction check\n---\n Package audit-libs-python.x86_64 0:2.2-2.el6 will be installed\n---\n Package libcgroup.x86_64 0:0.37-7.el6 will be installed\n---\n Package libselinux-python.x86_64 0:2.0.94-5.3.el6 will be installed\n---\n Package libsemanage-python.x86_64 0:2.0.43-4.2.el6 will be installed\n---\n Package setools-libs-python.x86_64 0:3.3.7-4.el6 will be installed\n--\n Processing Dependency: setools-libs = 3.3.7-4.el6 for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4(VERS_4.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libsefs.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libseaudit.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libqpol.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libapol.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libpoldiff.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Processing Dependency: libsefs.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--\n Running transaction check\n---\n Package setools-libs.x86_64 0:3.3.7-4.el6 will be installed\n--\n Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch        Version                 Repository\n                                                                           Size\n================================================================================\nInstalling:\n osg-oasis                   noarch      5-1.osg32.el6           osg      2.4 k\nInstalling for dependencies:\n audit-libs-python           x86_64      2.2-2.el6               slf       58 k\n cvmfs                       x86_64      2.1.20-1.osg32.el6      osg      5.9 M\n cvmfs-config-osg            noarch      1.1-5.osg32.el6         osg      8.0 k\n fuse                        x86_64      2.8.3-4.el6             slf       70 k\n fuse-libs                   x86_64      2.8.3-4.el6             slf       73 k\n gdb                         x86_64      7.2-60.el6              slf      2.3 M\n libcgroup                   x86_64      0.37-7.el6              slf      110 k\n libselinux-python           x86_64      2.0.94-5.3.el6          slf      201 k\n libsemanage-python          x86_64      2.0.43-4.2.el6          slf       80 k\n policycoreutils-python      x86_64      2.0.83-19.30.el6        slf      341 k\n setools-libs                x86_64      3.3.7-4.el6             slf      399 k\n setools-libs-python         x86_64      3.3.7-4.el6             slf      221 k\n\nTransaction Summary\n================================================================================\nInstall      13 Package(s)\n\nTotal download size: 9.8 M\nInstalled size: 35 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/13): audit-libs-python-2.2-2.el6.x86_64.rpm           |  58 kB     00:00     \n(2/13): cvmfs-2.1.20-1.osg32.el6.x86_64.rpm              | 5.9 MB     00:00     \n(3/13): cvmfs-config-osg-1.1-5.osg32.el6.noarch.rpm      | 8.0 kB     00:00     \n(4/13): fuse-2.8.3-4.el6.x86_64.rpm                      |  70 kB     00:00     \n(5/13): fuse-libs-2.8.3-4.el6.x86_64.rpm                 |  73 kB     00:00     \n(6/13): gdb-7.2-60.el6.x86_64.rpm                        | 2.3 MB     00:00     \n(7/13): libcgroup-0.37-7.el6.x86_64.rpm                  | 110 kB     00:00     \n(8/13): libselinux-python-2.0.94-5.3.el6.x86_64.rpm      | 201 kB     00:00     \n(9/13): libsemanage-python-2.0.43-4.2.el6.x86_64.rpm     |  80 kB     00:00     \n(10/13): osg-oasis-5-1.osg32.el6.noarch.rpm              | 2.4 kB     00:00     \n(11/13): policycoreutils-python-2.0.83-19.30.el6.x86_64. | 341 kB     00:00     \n(12/13): setools-libs-3.3.7-4.el6.x86_64.rpm             | 399 kB     00:00     \n(13/13): setools-libs-python-3.3.7-4.el6.x86_64.rpm      | 221 kB     00:00     \n--------------------------------------------------------------------------------\nTotal                                           9.7 MB/s | 9.8 MB     00:01     \nwarning: rpmts_HdrFromFdno: Header V4 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nRetrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nImporting GPG key 0x824B8603:\n Userid : OSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n\n Package: osg-release-3.2-7.osg32.el6.noarch (installed)\n From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\nWarning: RPMDB altered outside of yum.\n  Installing : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     1/13 \n  Installing : setools-libs-3.3.7-4.el6.x86_64                             2/13 \n  Installing : setools-libs-python-3.3.7-4.el6.x86_64                      3/13 \n  Installing : fuse-libs-2.8.3-4.el6.x86_64                                4/13 \n  Installing : libsemanage-python-2.0.43-4.2.el6.x86_64                    5/13 \n  Installing : gdb-7.2-60.el6.x86_64                                       6/13 \n  Installing : fuse-2.8.3-4.el6.x86_64                                     7/13 \n  Installing : audit-libs-python-2.2-2.el6.x86_64                          8/13 \n  Installing : libselinux-python-2.0.94-5.3.el6.x86_64                     9/13 \n  Installing : libcgroup-0.37-7.el6.x86_64                                10/13 \n  Installing : policycoreutils-python-2.0.83-19.30.el6.x86_64             11/13 \n  Installing : cvmfs-2.1.20-1.osg32.el6.x86_64                            12/13 \n  Installing : osg-oasis-5-1.osg32.el6.noarch                             13/13 \n  Verifying  : libcgroup-0.37-7.el6.x86_64                                 1/13 \n  Verifying  : libselinux-python-2.0.94-5.3.el6.x86_64                     2/13 \n  Verifying  : audit-libs-python-2.2-2.el6.x86_64                          3/13 \n  Verifying  : policycoreutils-python-2.0.83-19.30.el6.x86_64              4/13 \n  Verifying  : fuse-2.8.3-4.el6.x86_64                                     5/13 \n  Verifying  : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     6/13 \n  Verifying  : setools-libs-python-3.3.7-4.el6.x86_64                      7/13 \n  Verifying  : gdb-7.2-60.el6.x86_64                                       8/13 \n  Verifying  : osg-oasis-5-1.osg32.el6.noarch                              9/13 \n  Verifying  : libsemanage-python-2.0.43-4.2.el6.x86_64                   10/13 \n  Verifying  : cvmfs-2.1.20-1.osg32.el6.x86_64                            11/13 \n  Verifying  : fuse-libs-2.8.3-4.el6.x86_64                               12/13 \n  Verifying  : setools-libs-3.3.7-4.el6.x86_64                            13/13 \n\nInstalled:\n  osg-oasis.noarch 0:5-1.osg32.el6                                              \n\nDependency Installed:\n  audit-libs-python.x86_64 0:2.2-2.el6                                          \n  cvmfs.x86_64 0:2.1.20-1.osg32.el6                                             \n  cvmfs-config-osg.noarch 0:1.1-5.osg32.el6                                     \n  fuse.x86_64 0:2.8.3-4.el6                                                     \n  fuse-libs.x86_64 0:2.8.3-4.el6                                                \n  gdb.x86_64 0:7.2-60.el6                                                       \n  libcgroup.x86_64 0:0.37-7.el6                                                 \n  libselinux-python.x86_64 0:2.0.94-5.3.el6                                     \n  libsemanage-python.x86_64 0:2.0.43-4.2.el6                                    \n  policycoreutils-python.x86_64 0:2.0.83-19.30.el6                              \n  setools-libs.x86_64 0:3.3.7-4.el6                                             \n  setools-libs-python.x86_64 0:3.3.7-4.el6                                      \n\nComplete!\n[root@fermicloud044 ~]# echo user_allow_other \n/etc/fuse.conf\n[root@fermicloud044 ~]# echo \n/cvmfs /etc/auto.cvmfs\n \n/etc/auto.master\n[root@fermicloud044 ~]# service autofs restart\nStopping automount:                                        [  OK  ]\nStarting automount:                                        [  OK  ]\n[root@fermicloud044 ~]# cat \n/etc/cvmfs/default.local\nCVMFS_REPOSITORIES=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,`\n\nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY=\nhttp://squid.fnal.gov:3128\n\n[root@fermicloud044 ~]# ls /cvmfs\n[root@fermicloud044 ~]# ls -l /cvmfs/atlas.cern.ch\ntotal 5\ndrwxr-xr-x 6 cvmfs cvmfs 4096 Sep 12  2014 repo\n[root@fermicloud044 ~]# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 Mar 11  2014 cms -\n /cvmfs/cms.cern.ch\n[root@fermicloud044 ~]# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n[root@fermicloud044 ~]# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n[root@fermicloud044 ~]# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n[root@fermicloud044 ~]# \n\n\n\n\nFile Locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\n/etc/cvmfs/default.local\n\n\ncvmfs environment settings and repository setup\n\n\n\n\n\n\nfuse\n\n\n/etc/fuse.conf\n\n\nfuse settings\n\n\n\n\n\n\nautomount\n\n\n/etc/auto.master\n\n\nautomount settings\n\n\n\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug reporting and OSG-specific issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\nFor general CERN VM FileSystem support contact \n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.\n\n\nReferences\n\n\n\n\nhttp://cernvm.cern.ch/portal/filesystem/techinformation\n\n\nhttps://ecsft.cern.ch/dist/cvmfs/cvmfstech-2.1-6.pdf", 
            "title": "CVMFS"
        }, 
        {
            "location": "/Other/cvmfs/#install-cvmfs", 
            "text": "Here we describe how to install the  CVMFS  (Cern-VM file system) client.\nThis document is intended for system administrators who wish to install this client to have access to files distributed\nby cvmfs servers via HTTP.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.2.22.\nThe version of cvmfs installed should be  = 2.1.20-1.osg or greater.", 
            "title": "Install CVMFS"
        }, 
        {
            "location": "/Other/cvmfs/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/cvmfs/#host-and-os", 
            "text": "OS is RedHat 5, 6, 7 or variants  root  access  autofs  should be installed  fuse  should be installed (or will be as part of the installation)  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details  below )", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/cvmfs/#users-and-groups", 
            "text": "This installation will create one user unless it already exists:     User  Comment      cvmfs  CernVM-FS service account     The installation will also create a cvmfs group and default the cvmfs user to that group. In addition, if the fuse rpm is not for some reason already installed, installing cvmfs will also install fuse and that will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/Other/cvmfs/#networking", 
            "text": "You will need network access to a local squid server such as the  squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers.", 
            "title": "Networking"
        }, 
        {
            "location": "/Other/cvmfs/#upgrading", 
            "text": "When upgrading to cvmfs 2.1.20, delete the setting of  CVMFS_SERVER_URL  in  /etc/cvmfs/domain.d/cern.ch.local . If that's the only thing in the file (which is likely) then delete the whole file.  When upgrading from a cvmfs 2.0.X version, all  /cvmfs  repositories must be unmounted in order to upgrade. When upgrading between 2.1.X versions, repositories may be mounted.  Note that version 2.1 removed the  /etc/init.d/cvmfs  script. Starting it didn't actually do anything anyway (it is automatically starts when mounted), and the stop function has been moved to  cvmfs_config umount .\nThe  restartautofs  function is instead done by  service autofs restart . The  probe  function has also been moved to  cvmfs_config probe . Since 2.1.X enables shared cache by default, so in order to reclaim the previous cache space when upgrading from 2.0.X you must manually remove the old caches. For example  rm -rf /var/cache/cvmfs/*.*", 
            "title": "Upgrading"
        }, 
        {
            "location": "/Other/cvmfs/#install-instructions", 
            "text": "Prior to installing CVMFS, make sure the  yum repositories  are correctly configured for OSG.  The following will install cvmfs from the OSG repository. It will also install cern public keys as well as fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution, OASIS.  yum install osg-oasis  Create or edit  /etc/fuse.conf . It should contain the following in order to allow fuse to do proper file ownership:  user_allow_other  Create or edit  /etc/auto.master . It should contain the following in order to allow cvmfs to automount:  /cvmfs /etc/auto.cvmfs  Restart autofs to make the change take effect:  service autofs restart\nStopping automount:                       [  OK  ]\nStarting automount:                       [  OK  ]  Create or edit  /etc/cvmfs/default.local , a file that controls the cvmfs configuration.\nBelow is a sample configuration,  but please note  that you will need to customize this for your site. (In particular, the  CVMFS_HTTP_PROXY  line below only works within the .fnal.gov domain.)  CVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,` \nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY= http://squid.fnal.gov:3128   CVMFS 2.1 by default allows any repository to be mounted. The recommended CVMFS_REPOSITORIES setting is what it is above so that tools such as  cvmfs_config  and  cvmfs_talk  that use known repositories will use two common repositories plus any additional that have been mounted. You may want to choose a different set of always-known repositories. A full list of cern.ch repositories is found at  http://cernvm.cern.ch/portal/cvmfs/examples . The only opensciencegrid.org repository is currently oasis.  Set up a list of cvmfs HTTP proxies to retrieve from in CVMFS_HTTP_PROXY. Vertical bars separating proxies means to load balance between them and try them all before continuing. A semicolon between proxies means to try that one only after the previous ones have failed. A special proxy called DIRECT can be placed last in the list to indicate directly connecting to servers if all other proxies fail. This is acceptable for small sites but discouraged for large sites because of the potential load that could be put upon the stratum one servers.  Set up the cache limit in  CVMFS_QUOTA_LIMIT  (in MB). Recommended for most applications is 20GB. This is the combined limit for all repositories. This cache will be stored in  $CVMFS_CACHE_BASE . Make sure that at least 20% more than that amount of space stays available for cvmfs in that filesystem. This is very important, since if that space is not available it can cause many I/O errors and application crashes. Many system administrators choose to put the cache space in a separate filesystem.", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Other/cvmfs/#verifying-cvmfs", 
            "text": "After CVMFS is installed, you should be able to see the  /cvmfs  directory. But note that it will initially appear to be empty:  $ ls /cvmfs  Directories within  /cvmfs  will not be mounted until you examine them. For instance:  # ls -l /cvmfs/atlas.cern.ch\n/cvmfs/atlas.cern.ch:\ntotal 5\ndrwxr-xr-x 1 cvmfs cvmfs 4096 Mar  5  2012 repo\n# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n/cvmfs/oasis.opensciencegrid.org/cmssoft:\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 28 10:33 cms -  /cvmfs/cms.cern.ch\n# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org", 
            "title": "Verifying cvmfs"
        }, 
        {
            "location": "/Other/cvmfs/#troubleshooting-problems", 
            "text": "If no directories exist under  /cvmfs/ , you can try the following steps to debug:   Mount it manually  mkdir /mnt/cvmfs  then  mount -t cvmfs REPOSITORYNAME /mnt/cvmfs  where  REPOSITORYNAME  is the repository, for example  oasis.opensciencegrid.org . If this works, then cvmfs is working, but there is a problem with automount.  If that doesn't work and doesn't give any explanatory errors, try  cvmfs_config chksetup  or  cvmfs_config showconfig REPOSITORYNAME  to verify your setup.  If chksetup reports access problems to proxies, it may be caused by access control settings in the squids.  If you have changed settings in  /etc/cvmfs/default.local , and they do not seem to be taking effect, note that there are other configuration files that can override the settings. See the comments at the beginning of  /etc/cvmfs/default.conf  regarding the order in which configuration files are evaluated and look for old files that may have been left from a previous installation.  More things to try are in the  upstream documentation .", 
            "title": "Troubleshooting problems"
        }, 
        {
            "location": "/Other/cvmfs/#starting-and-stopping-services", 
            "text": "Once it is set up, cvmfs is always automatically started when one of the repositories are accessed.  cvmfs can be stopped via:  # cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/Other/cvmfs/#screendump-of-install", 
            "text": "[root@fermicloud044 ~]# rpm -Uvh http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nRetrieving http://download.fedoraproject.org/pub/epel/6/i386/epel-release-6-8.noarch.rpm\nPreparing...                ########################################### [100%]\n   1:epel-release           ########################################### [100%]\n[root@fermicloud044 ~]# yum install yum-priorities\nLoaded plugins: security\nepel/metalink                                            |  15 kB     00:00     \nepel                                                     | 4.4 kB     00:00     \nepel/primary_db                                          | 6.5 MB     00:02     \nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package yum-plugin-priorities.noarch 0:1.1.30-14.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch         Version               Repository\n                                                                           Size\n================================================================================\nInstalling:\n yum-plugin-priorities       noarch       1.1.30-14.el6         slf        21 k\n\nTransaction Summary\n================================================================================\nInstall       1 Package(s)\n\nTotal download size: 21 k\nInstalled size: 28 k\nIs this ok [y/N]: y\nDownloading Packages:\nyum-plugin-priorities-1.1.30-14.el6.noarch.rpm           |  21 kB     00:00     \nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n  Verifying  : yum-plugin-priorities-1.1.30-14.el6.noarch                   1/1 \n\nInstalled:\n  yum-plugin-priorities.noarch 0:1.1.30-14.el6                                  \n\nComplete!\n[root@fermicloud044 ~]# grep plugins /etc/yum.conf\nplugins=1\n[root@fermicloud044 ~]# rpm -Uvh http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg/3.2/osg-3.2-el6-release-latest.rpm\nwarning: /var/tmp/rpm-tmp.C1YSbQ: Header V3 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud044 ~]# yum install osg-oasis\nLoaded plugins: priorities, security\nosg                                                      | 1.9 kB     00:00     \nosg/primary_db                                           | 1.9 MB     00:00     \n342 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package osg-oasis.noarch 0:5-1.osg32.el6 will be installed\n--  Processing Dependency: cvmfs-config-osg  = 1.1 for package: osg-oasis-5-1.osg32.el6.noarch\n--  Processing Dependency: cvmfs  = 2.1.20 for package: osg-oasis-5-1.osg32.el6.noarch\n--  Running transaction check\n---  Package cvmfs.x86_64 0:2.1.20-1.osg32.el6 will be installed\n--  Processing Dependency: libfuse.so.2(FUSE_2.4)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: /usr/sbin/semanage for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2(FUSE_2.6)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: fuse-libs for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: gdb for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2(FUSE_2.5)(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: fuse for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n--  Processing Dependency: libfuse.so.2()(64bit) for package: cvmfs-2.1.20-1.osg32.el6.x86_64\n---  Package cvmfs-config-osg.noarch 0:1.1-5.osg32.el6 will be installed\n--  Running transaction check\n---  Package fuse.x86_64 0:2.8.3-4.el6 will be installed\n---  Package fuse-libs.x86_64 0:2.8.3-4.el6 will be installed\n---  Package gdb.x86_64 0:7.2-60.el6 will be installed\n---  Package policycoreutils-python.x86_64 0:2.0.83-19.30.el6 will be installed\n--  Processing Dependency: libsemanage-python  = 2.0.43-4 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: audit-libs-python  = 1.4.2-1 for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: setools-libs-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: libselinux-python for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Processing Dependency: libcgroup for package: policycoreutils-python-2.0.83-19.30.el6.x86_64\n--  Running transaction check\n---  Package audit-libs-python.x86_64 0:2.2-2.el6 will be installed\n---  Package libcgroup.x86_64 0:0.37-7.el6 will be installed\n---  Package libselinux-python.x86_64 0:2.0.94-5.3.el6 will be installed\n---  Package libsemanage-python.x86_64 0:2.0.43-4.2.el6 will be installed\n---  Package setools-libs-python.x86_64 0:3.3.7-4.el6 will be installed\n--  Processing Dependency: setools-libs = 3.3.7-4.el6 for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1(VERS_1.3)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1(VERS_1.4)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4(VERS_4.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1(VERS_1.2)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libsefs.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4(VERS_4.0)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4(VERS_4.1)(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libseaudit.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libqpol.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libapol.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libpoldiff.so.1()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Processing Dependency: libsefs.so.4()(64bit) for package: setools-libs-python-3.3.7-4.el6.x86_64\n--  Running transaction check\n---  Package setools-libs.x86_64 0:3.3.7-4.el6 will be installed\n--  Finished Dependency Resolution\n\nDependencies Resolved\n\n================================================================================\n Package                     Arch        Version                 Repository\n                                                                           Size\n================================================================================\nInstalling:\n osg-oasis                   noarch      5-1.osg32.el6           osg      2.4 k\nInstalling for dependencies:\n audit-libs-python           x86_64      2.2-2.el6               slf       58 k\n cvmfs                       x86_64      2.1.20-1.osg32.el6      osg      5.9 M\n cvmfs-config-osg            noarch      1.1-5.osg32.el6         osg      8.0 k\n fuse                        x86_64      2.8.3-4.el6             slf       70 k\n fuse-libs                   x86_64      2.8.3-4.el6             slf       73 k\n gdb                         x86_64      7.2-60.el6              slf      2.3 M\n libcgroup                   x86_64      0.37-7.el6              slf      110 k\n libselinux-python           x86_64      2.0.94-5.3.el6          slf      201 k\n libsemanage-python          x86_64      2.0.43-4.2.el6          slf       80 k\n policycoreutils-python      x86_64      2.0.83-19.30.el6        slf      341 k\n setools-libs                x86_64      3.3.7-4.el6             slf      399 k\n setools-libs-python         x86_64      3.3.7-4.el6             slf      221 k\n\nTransaction Summary\n================================================================================\nInstall      13 Package(s)\n\nTotal download size: 9.8 M\nInstalled size: 35 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/13): audit-libs-python-2.2-2.el6.x86_64.rpm           |  58 kB     00:00     \n(2/13): cvmfs-2.1.20-1.osg32.el6.x86_64.rpm              | 5.9 MB     00:00     \n(3/13): cvmfs-config-osg-1.1-5.osg32.el6.noarch.rpm      | 8.0 kB     00:00     \n(4/13): fuse-2.8.3-4.el6.x86_64.rpm                      |  70 kB     00:00     \n(5/13): fuse-libs-2.8.3-4.el6.x86_64.rpm                 |  73 kB     00:00     \n(6/13): gdb-7.2-60.el6.x86_64.rpm                        | 2.3 MB     00:00     \n(7/13): libcgroup-0.37-7.el6.x86_64.rpm                  | 110 kB     00:00     \n(8/13): libselinux-python-2.0.94-5.3.el6.x86_64.rpm      | 201 kB     00:00     \n(9/13): libsemanage-python-2.0.43-4.2.el6.x86_64.rpm     |  80 kB     00:00     \n(10/13): osg-oasis-5-1.osg32.el6.noarch.rpm              | 2.4 kB     00:00     \n(11/13): policycoreutils-python-2.0.83-19.30.el6.x86_64. | 341 kB     00:00     \n(12/13): setools-libs-3.3.7-4.el6.x86_64.rpm             | 399 kB     00:00     \n(13/13): setools-libs-python-3.3.7-4.el6.x86_64.rpm      | 221 kB     00:00     \n--------------------------------------------------------------------------------\nTotal                                           9.7 MB/s | 9.8 MB     00:01     \nwarning: rpmts_HdrFromFdno: Header V4 DSA/SHA1 Signature, key ID 824b8603: NOKEY\nRetrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nImporting GPG key 0x824B8603:\n Userid : OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org \n Package: osg-release-3.2-7.osg32.el6.noarch (installed)\n From   : /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nTransaction Test Succeeded\nRunning Transaction\nWarning: RPMDB altered outside of yum.\n  Installing : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     1/13 \n  Installing : setools-libs-3.3.7-4.el6.x86_64                             2/13 \n  Installing : setools-libs-python-3.3.7-4.el6.x86_64                      3/13 \n  Installing : fuse-libs-2.8.3-4.el6.x86_64                                4/13 \n  Installing : libsemanage-python-2.0.43-4.2.el6.x86_64                    5/13 \n  Installing : gdb-7.2-60.el6.x86_64                                       6/13 \n  Installing : fuse-2.8.3-4.el6.x86_64                                     7/13 \n  Installing : audit-libs-python-2.2-2.el6.x86_64                          8/13 \n  Installing : libselinux-python-2.0.94-5.3.el6.x86_64                     9/13 \n  Installing : libcgroup-0.37-7.el6.x86_64                                10/13 \n  Installing : policycoreutils-python-2.0.83-19.30.el6.x86_64             11/13 \n  Installing : cvmfs-2.1.20-1.osg32.el6.x86_64                            12/13 \n  Installing : osg-oasis-5-1.osg32.el6.noarch                             13/13 \n  Verifying  : libcgroup-0.37-7.el6.x86_64                                 1/13 \n  Verifying  : libselinux-python-2.0.94-5.3.el6.x86_64                     2/13 \n  Verifying  : audit-libs-python-2.2-2.el6.x86_64                          3/13 \n  Verifying  : policycoreutils-python-2.0.83-19.30.el6.x86_64              4/13 \n  Verifying  : fuse-2.8.3-4.el6.x86_64                                     5/13 \n  Verifying  : cvmfs-config-osg-1.1-5.osg32.el6.noarch                     6/13 \n  Verifying  : setools-libs-python-3.3.7-4.el6.x86_64                      7/13 \n  Verifying  : gdb-7.2-60.el6.x86_64                                       8/13 \n  Verifying  : osg-oasis-5-1.osg32.el6.noarch                              9/13 \n  Verifying  : libsemanage-python-2.0.43-4.2.el6.x86_64                   10/13 \n  Verifying  : cvmfs-2.1.20-1.osg32.el6.x86_64                            11/13 \n  Verifying  : fuse-libs-2.8.3-4.el6.x86_64                               12/13 \n  Verifying  : setools-libs-3.3.7-4.el6.x86_64                            13/13 \n\nInstalled:\n  osg-oasis.noarch 0:5-1.osg32.el6                                              \n\nDependency Installed:\n  audit-libs-python.x86_64 0:2.2-2.el6                                          \n  cvmfs.x86_64 0:2.1.20-1.osg32.el6                                             \n  cvmfs-config-osg.noarch 0:1.1-5.osg32.el6                                     \n  fuse.x86_64 0:2.8.3-4.el6                                                     \n  fuse-libs.x86_64 0:2.8.3-4.el6                                                \n  gdb.x86_64 0:7.2-60.el6                                                       \n  libcgroup.x86_64 0:0.37-7.el6                                                 \n  libselinux-python.x86_64 0:2.0.94-5.3.el6                                     \n  libsemanage-python.x86_64 0:2.0.43-4.2.el6                                    \n  policycoreutils-python.x86_64 0:2.0.83-19.30.el6                              \n  setools-libs.x86_64 0:3.3.7-4.el6                                             \n  setools-libs-python.x86_64 0:3.3.7-4.el6                                      \n\nComplete!\n[root@fermicloud044 ~]# echo user_allow_other  /etc/fuse.conf\n[root@fermicloud044 ~]# echo  /cvmfs /etc/auto.cvmfs   /etc/auto.master\n[root@fermicloud044 ~]# service autofs restart\nStopping automount:                                        [  OK  ]\nStarting automount:                                        [  OK  ]\n[root@fermicloud044 ~]# cat  /etc/cvmfs/default.local\nCVMFS_REPOSITORIES= `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr ' ' ,` \nCVMFS_CACHE_BASE=/var/cache/cvmfs\nCVMFS_QUOTA_LIMIT=20000\nCVMFS_HTTP_PROXY= http://squid.fnal.gov:3128 \n[root@fermicloud044 ~]# ls /cvmfs\n[root@fermicloud044 ~]# ls -l /cvmfs/atlas.cern.ch\ntotal 5\ndrwxr-xr-x 6 cvmfs cvmfs 4096 Sep 12  2014 repo\n[root@fermicloud044 ~]# ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\ntotal 1\nlrwxrwxrwx 1 cvmfs cvmfs 18 Mar 11  2014 cms -  /cvmfs/cms.cern.ch\n[root@fermicloud044 ~]# ls -l /cvmfs/glast.egi.eu\ntotal 5\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n[root@fermicloud044 ~]# ls /cvmfs\natlas.cern.ch  cms.cern.ch  glast.egi.eu  oasis.opensciencegrid.org\n[root@fermicloud044 ~]# cvmfs_config umount\nUnmounting /cvmfs/atlas.cern.ch: OK\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\nUnmounting /cvmfs/cms.cern.ch: OK\nUnmounting /cvmfs/glast.egi.eu: OK\n[root@fermicloud044 ~]#", 
            "title": "Screendump of Install"
        }, 
        {
            "location": "/Other/cvmfs/#file-locations", 
            "text": "Service/Process  Configuration File  Description      cvmfs  /etc/cvmfs/default.local  cvmfs environment settings and repository setup    fuse  /etc/fuse.conf  fuse settings    automount  /etc/auto.master  automount settings", 
            "title": "File Locations"
        }, 
        {
            "location": "/Other/cvmfs/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug reporting and OSG-specific issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .  For general CERN VM FileSystem support contact  .   For a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/cvmfs/#references", 
            "text": "http://cernvm.cern.ch/portal/filesystem/techinformation  https://ecsft.cern.ch/dist/cvmfs/cvmfstech-2.1-6.pdf", 
            "title": "References"
        }, 
        {
            "location": "/Other/gsissh/", 
            "text": "Installing GSI OpenSSH\n\n\nThis document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.\n\n\nRequirements\n\n\nHost and OS\n\n\nThe GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.\n\n\nUsers and Groups\n\n\nThe RPM installation will try to create the \ngsisshd\n user and group and the \n/var/empty/gsisshd\n directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to \n/var/empty/gsisshd\n and belongs to the \ngsisshd\n user and group. You may change it if needed to something else as long as the ownerships remain the same.\n\n\nNetworking\n\n\nYou'll find more client specific details also in the \nFirewall section\n of this document.\n\n\nInstallation procedure\n\n\nPrior to install, make sure you have:\n\n \nYum repositories correctly configured\n for OSG.\n\n \nCA certificates installed\n\n\nGSI OpenSSH Installation\n\n\nStart with installing GSI OpenSSH from the repository\n\n\nyum install gsi-openssh-server gsi-openssh-clients\n\n\n\n\nIn addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:\n\n\nConfiguration and Operations\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/sshd_config\n\n\nConfiguration file\n\n\n\n\n\n\ngsisshd\n\n\n/etc/sysconfig/gsisshd\n\n\nEnvironment variables for gsisshd\n\n\n\n\n\n\ngsisshd\n\n\n/etc/lcmaps.db\n\n\nLCMAPS configuration\n\n\n\n\n\n\n\n\nLog Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/var/log/messages\n\n\nAll log messages\n\n\n\n\n\n\n\n\nOther Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nHost certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nKey certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/ssh_host_rsa_key\n\n\nRSA Host key\n\n\n\n\n\n\n\n\nConfiguration\n\n\nConfiguration\n\n\nIn order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink \n/etc/gsissh/ssh_host_dsa_key\n and \n/etc/gsissh/ssh_host_rsa_key\n \nto \n/etc/ssh/ssh_host_dsa_key\n and \n/etc/ssh/ssh_host_rsa_key\n respectively. \n\n\n\n\nNote\n\n\nRegardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of \n/etc/shadow\n).\n\n\n\n\nUsing a gridmap file for authorization\n\n\nIn order to use gsissh, you'll need to create mappings in your \n\n/etc/grid-security/grid-mapfile\n for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the \n\n/etc/grid-security/gsi-authz.conf\n file is empty or that all \nof the lines in the file are commented out using a \n#\n at the beginning of the line.\n\n\n\n\nNote\n\n\nThe mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy\n\n\n\n\nAn example of the \n/etc/grid-security/grid-mapfile\n follows:\n\n\n/DC=org/DC=doegrids/OU=People/CN=USER NAME 123456\n useraccount\n\n\n\n\nUsing LCMAPS and GUMS for authorization\n\n\nIn order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit \n/etc/grid-security/gsi-authz.conf\n to indicate that Globus should do a GSI callout for authorization. The file should contain the following:\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\nso that LCMAPS is used. Next, install the lcmaps rpms:\n\n\nyum install lcmaps lcas-lcmaps-gt4-interface\n\n\n\n\nFinally, you'll need to modify \n/etc/lcmaps.db\n so that the \ngumsclient\n entry has the correct endpoint for your gums server.\n\n\nStarting and Enabling Services\n\n\nTo start the services:\n\n\n\n\n\n\nTo start GSI OpenSSH you can use the service command, e.g.:\n\n\nservice gsisshd start\n\n\n\n\n\n\nYou should also enable the appropriate services so that they are automatically started when your system is powered on:\n\n\n\n\n\n\nTo enable OpenSSH by default on the node:\n\n\nchkconfig gsisshd on\n\n\n\n\n\n\nStopping and Disabling Services\n\n\nTo stop the services:\n\n\n\n\n\n\nTo stop OpenSSH you can use: \\\npre class=\u201crootscreen\u201d>\n\n\nservice gsisshd stop\n\n\n\n\n\n\nIn addition, you can disable services by running the following commands. However, you don't need to do this normally.\n\n\n\n\n\n\nOptionally, to disable OpenSSH:\n\n\nchkconfig gsisshd off\n\n\n\n\n\n\nTroubleshooting\n\n\nYou can get information on troubleshooting errors on the \nNCSA page\n.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/gsisshd\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\nOutput goes to \n/var/log/messages\n by default.\n\n\nTest GSI OpenSSH\n\n\nAfter starting the \ngsisshd\n service you can check if it is running correctly\n\n\n$ grid-proxy-init\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\nEnter GRID pass phrase for this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n$ gsissh localhost -p 2222\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n$\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "GSI-enabled SSH"
        }, 
        {
            "location": "/Other/gsissh/#installing-gsi-openssh", 
            "text": "This document gives instructions on installing and using the GSI OpenSSH server available in the OSG repository and configuring it so that you can use on your cluster.", 
            "title": "Installing GSI OpenSSH"
        }, 
        {
            "location": "/Other/gsissh/#requirements", 
            "text": "", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/gsissh/#host-and-os", 
            "text": "The GSI OpenSSH rpms will require an user account and group in order for the privilege separation to work.", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/gsissh/#users-and-groups", 
            "text": "The RPM installation will try to create the  gsisshd  user and group and the  /var/empty/gsisshd  directory with the correct ownership if they are not present. If you are using a configuration management system or ROCKS, you should make sure that these users and groups are created before installing the RPMs to avoid potential issues. The gsisshd user should have an empty home directory. By default, this is home directory set to  /var/empty/gsisshd  and belongs to the  gsisshd  user and group. You may change it if needed to something else as long as the ownerships remain the same.", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/Other/gsissh/#networking", 
            "text": "You'll find more client specific details also in the  Firewall section  of this document.", 
            "title": "Networking"
        }, 
        {
            "location": "/Other/gsissh/#installation-procedure", 
            "text": "Prior to install, make sure you have:   Yum repositories correctly configured  for OSG.   CA certificates installed", 
            "title": "Installation procedure"
        }, 
        {
            "location": "/Other/gsissh/#gsi-openssh-installation", 
            "text": "Start with installing GSI OpenSSH from the repository  yum install gsi-openssh-server gsi-openssh-clients  In addition, you'll need to install CA certificates in order for GSIOpenSSH to work. You can follow the instructions below in order to install them:", 
            "title": "GSI OpenSSH Installation"
        }, 
        {
            "location": "/Other/gsissh/#configuration-and-operations", 
            "text": "", 
            "title": "Configuration and Operations"
        }, 
        {
            "location": "/Other/gsissh/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      gsisshd  /etc/gsissh/sshd_config  Configuration file    gsisshd  /etc/sysconfig/gsisshd  Environment variables for gsisshd    gsisshd  /etc/lcmaps.db  LCMAPS configuration     Log Files     Service or Process  Log File  Description      gsisshd  /var/log/messages  All log messages     Other Files     Service or Process  File  Description      gsisshd  /etc/grid-security/hostcert.pem  Host certificate    gsisshd  /etc/grid-security/hostcert.pem  Key certificate    gsisshd  /etc/gsissh/ssh_host_rsa_key  RSA Host key", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/Other/gsissh/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/Other/gsissh/#configuration_1", 
            "text": "In order to get a running instance of the GSI OpenSSH server, you'll\nneed to change the default configuration. However, before you go any\nfurther, you'll need to decide whether you want GSI OpenSSH to be your \nprimary ssh service or not (e.g. whether the GSI OpenSSH service will \nreplace your existing SSH service). If you choose not to replace your \nexisting service, you'll need to change the port setting in the GSI \nOpenSSH configuration to another port (e.g. 2222) so that you can run \nboth SSH services at the same time. Regardless of your choice, you \nshould probably have both services use the same host key. In order \nto do this, symlink  /etc/gsissh/ssh_host_dsa_key  and  /etc/gsissh/ssh_host_rsa_key  \nto  /etc/ssh/ssh_host_dsa_key  and  /etc/ssh/ssh_host_rsa_key  respectively.    Note  Regardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (have ! in the password field of  /etc/shadow ).", 
            "title": "Configuration"
        }, 
        {
            "location": "/Other/gsissh/#using-a-gridmap-file-for-authorization", 
            "text": "In order to use gsissh, you'll need to create mappings in your  /etc/grid-security/grid-mapfile  for the DNs that you will \nallow to login. The mappings should be entered one to a line, \nwith each line consisting of DN followed by the account the DN \nshould map to. Also, you should ensure that the  /etc/grid-security/gsi-authz.conf  file is empty or that all \nof the lines in the file are commented out using a  #  at the beginning of the line.   Note  The mappings will not consider VOMS extensions so the first mapping that matches will be used regardless of the VO role or VO present in the users proxy   An example of the  /etc/grid-security/grid-mapfile  follows:  /DC=org/DC=doegrids/OU=People/CN=USER NAME 123456  useraccount", 
            "title": "Using a gridmap file for authorization"
        }, 
        {
            "location": "/Other/gsissh/#using-lcmaps-and-gums-for-authorization", 
            "text": "In order to use LCMAPS callouts with GSI OpenSSH, you'll first need to edit  /etc/grid-security/gsi-authz.conf  to indicate that Globus should do a GSI callout for authorization. The file should contain the following:  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  so that LCMAPS is used. Next, install the lcmaps rpms:  yum install lcmaps lcas-lcmaps-gt4-interface  Finally, you'll need to modify  /etc/lcmaps.db  so that the  gumsclient  entry has the correct endpoint for your gums server.", 
            "title": "Using LCMAPS and GUMS for authorization"
        }, 
        {
            "location": "/Other/gsissh/#starting-and-enabling-services", 
            "text": "To start the services:    To start GSI OpenSSH you can use the service command, e.g.:  service gsisshd start    You should also enable the appropriate services so that they are automatically started when your system is powered on:    To enable OpenSSH by default on the node:  chkconfig gsisshd on", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/Other/gsissh/#stopping-and-disabling-services", 
            "text": "To stop the services:    To stop OpenSSH you can use: \\ pre class=\u201crootscreen\u201d>  service gsisshd stop    In addition, you can disable services by running the following commands. However, you don't need to do this normally.    Optionally, to disable OpenSSH:  chkconfig gsisshd off", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/Other/gsissh/#troubleshooting", 
            "text": "You can get information on troubleshooting errors on the  NCSA page .  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/gsisshd  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Other/gsissh/#test-gsi-openssh", 
            "text": "After starting the  gsisshd  service you can check if it is running correctly  $ grid-proxy-init\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\nEnter GRID pass phrase for this identity:\nCreating proxy ............................................................................................... Done\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n$ gsissh localhost -p 2222\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n$", 
            "title": "Test GSI OpenSSH"
        }, 
        {
            "location": "/Other/gsissh/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/wn/", 
            "text": "Installing and Using the Worker Node Client From RPMs\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the \nreference section\n below for contents of the Worker Node Client.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nInstall using RPMs and \nyum\n (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nUse from OASIS\n - useful when worker nodes already mount \nOASIS\n on your worker nodes\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall the Worker Node Client\n\n\nInstall the Worker Node Client RPM:\n\n\nyum install osg-wn-client\n\n\n\n\nServices\n\n\nFetch-CRL is the only service required to support the WN Client.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nOn EL6 and EL7: \nfetch-crl-boot\n and \nfetch-crl-cron\n\n\n\n\n\n\n\n\nOn EL5: \nfetch-crl3-boot\n and \nfetch-crl3-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nfetch-crl-boot\n will begin fetching CRLS, which can take a few minutes\nand fail on transient errors. You can add configuration to ignore these\ntransient errors in \n/etc/fetch-crl.conf\n for EL6 or EL7 machines or\n\n/etc/fetch-crl3.conf\n for EL5 machines:\n\n\nnoerrors\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValiding the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to \n/etc/osg/wn-client\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nyum and RPM\n, \nthe best practices\n for using yum to install software, and using \nyum repositories\n.\n\n\nTo see the currently installed version of the worker node package, run the following command:\n\n\nrpm -q --requires osg-wn-client\n\n\n\n\nSee \nour yum basics guide\n for more details on using RPM to see what was installed.", 
            "title": "RPM-based Worker Node"
        }, 
        {
            "location": "/Other/wn/#installing-and-using-the-worker-node-client-from-rpms", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the  reference section  below for contents of the Worker Node Client.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Install using RPMs and  yum  (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes  Use from OASIS  - useful when worker nodes already mount  OASIS  on your worker nodes   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from an RPM.", 
            "title": "Installing and Using the Worker Node Client From RPMs"
        }, 
        {
            "location": "/Other/wn/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Other/wn/#install-the-worker-node-client", 
            "text": "Install the Worker Node Client RPM:  yum install osg-wn-client", 
            "title": "Install the Worker Node Client"
        }, 
        {
            "location": "/Other/wn/#services", 
            "text": "Fetch-CRL is the only service required to support the WN Client.     Software  Service name  Notes      Fetch CRL  On EL6 and EL7:  fetch-crl-boot  and  fetch-crl-cron     On EL5:  fetch-crl3-boot  and  fetch-crl3-cron  See  CA documentation  for more info       Note  fetch-crl-boot  will begin fetching CRLS, which can take a few minutes\nand fail on transient errors. You can add configuration to ignore these\ntransient errors in  /etc/fetch-crl.conf  for EL6 or EL7 machines or /etc/fetch-crl3.conf  for EL5 machines:  noerrors   As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service SERVICE-NAME start    Stop a service  service SERVICE-NAME stop    Enable a service to start during boot  chkconfig SERVICE-NAME on    Disable a service from starting during boot  chkconfig SERVICE-NAME off", 
            "title": "Services"
        }, 
        {
            "location": "/Other/wn/#validing-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to  /etc/osg/wn-client", 
            "title": "Validing the Worker Node Client"
        }, 
        {
            "location": "/Other/wn/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/wn/#reference", 
            "text": "Please see the documentation on using  yum and RPM ,  the best practices  for using yum to install software, and using  yum repositories .  To see the currently installed version of the worker node package, run the following command:  rpm -q --requires osg-wn-client  See  our yum basics guide  for more details on using RPM to see what was installed.", 
            "title": "Reference"
        }, 
        {
            "location": "/Other/glexec/", 
            "text": "Glexec Installation Guide\n\n\nThis document is intended for System Administrators that are installing the OSG version of glexec.\n\n\nGlexec is commonly used for what are referred to as \u201cpilot\u201d or \u201cglidein\u201d jobs.\n\n\nTraditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.\n\n\nIn a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \u201cpull\u201d down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.\n\n\nGlexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.\n\n\nThe pilot job will \u201cpull\u201d user jobs down from the central queue and invoke glexec which will then\n\n\n\n\nauthenticate the user job\u2019s proxy,\n\n\nperform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,\n\n\nand then run the user job under the local account assigned by the authorization service for that user.\n\n\n\n\nIn effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.\n\n\nMany worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.\n\n\nFor more information regarding pilot-based systems and glexec:\n\n\n\n\nglideinWMS - The glidein based WMS\n\n\nAddressing the pilot security problem with gLExec (pdf)\n\n\n\n\nEngineering Considerations\n\n\nThis section describes any prerequisite software/considerations that must be taken into account before the glexec software installation is performed. It should be reviewed completely before starting the installation process.\n\n\nA large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \u2018net.core.somaxconn\u2019 on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time. (For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots). At the same time, the Apache parameter \u2018ListenBacklog\u2019 must be changed to the same value. Also note that Fermilab determined that for best performance, the Apache parameter \u2018MaxClients\u2019 on GUMS servers (at least on their dual-core Virtual Machines) should be set to a value of 32. For details on these and other parameters on the GUMS server see GumsScalability.\n\n\nRequirements\n\n\nThese are the requirements that must be met to install glexec.\n\n\n\n\nNote\n\n\nNormally you will install the \nOSG worker node\n first. Technically, installing the \nosg-wn-client-glexec\n package will also install the worker node, but we do not duplicate instructions specific to the worker node here, so refer to the \nOSG worker node\n for details about the worker node installation.\n\n\n\n\nHost and OS\n\n\n\n\nOS is Red Hat Enterprise Linux 5, 6, 7, and variants.\n\n\nRoot access\n\n\n\n\nUsers\n\n\nThe glexec installation will create two users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nglexec\n\n\nReduced privilege separate id used to improve security. Set the default gid of the \u201cglexec\u201d user to be a group that is also called \u201cglexec\u201d.\n\n\n\n\n\n\ngratia\n\n\nNeeded for the glexec gratia probe which is also automatically installed.\n\n\n\n\n\n\n\n\nIn addition, OSG glexec requires a range of \ngroup ids\n for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the \n/usr/bin/id\n command. The recommended names are \u2019glexecNN\u2019 where NN is a number starting from 00.\n\n\n\n\nDefine at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.\n\n\nThey must be consecutive and in any range (default range is 65000-65049, configured in the \nConfiguring glexec\n section below).\n\n\nThe same group ids can be used on every worker node.\n\n\n\n\n\n\n\nInstall Instructions\n\n\nPrior to installing \nglexec\n, verify the \nyum repositories\n are correctly configured.\n\n\nSome of the worker node client software verifies proxies or certificates. In order to do this, they will need the \nCA certificates\n used to sign the proxies.\n\n\nInstall glexec\n\n\n\n\nNote\n\n\nThe glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see \nthese instructions\n. If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:\n\n\nyum install condor-procd\n\n\n\n\nAfter meeting all the requirements in the previous section, install glexec with this command:\n\n\nyum install osg-wn-client-glexec\n\n\n\n\nConfiguring glexec\n\n\nThe following steps need to be done after the glexec installation is complete.\n\n\n\n\nFirst, review the contents of \n/etc/glexec.conf\n. All of the defaults should be fine, but if you want to change the behavior, the parameters are described in \nman glexec.conf\n.\n\n\n\n\nNext, review all of the contents of \n/etc/lcmaps.db\n and in particular update the following pieces.\n\n\n\n\n\n\nIf you have GUMS, change the yourgums.yourdomain in the following line to the fully qualified domain name of your GUMS server:\n\n\n\"\u2013endpoint \nhttps://yourgums.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort\n\"\n\n\n\n\n\n\nIf you want to use a range of tracking group ids other than the default as described in the \nRequirements\n section above, uncomment and change the \n-min-gid\n and \n-max-gid\n lines to your chosen values:\n\n\n\"-min-gid 65000\u201d \u201c-max-gid 65049\"\n\n\n\n\n\n\nUncomment the following two lines:\n\n\nglexectracking = \"lcmaps_glexec_tracking.mod\"\n                   \"-exec /usr/sbin/glexec_monitor\"\n\n\n\n\n\n\nIf you have GUMS, uncomment the following policy toward the end of the file:\n\n\nverifyproxy -\n gumsclient\n gumsclient -\n glexectracking\n\n\nor if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:\n\n\nverifyproxy -\n gridmapfile\n gridmapfile -\n glexectracking\n\n\n\n\n\n\nTesting the Installation of glexec\n\n\nNow, \nas a non-privileged user (not root)\n , do the following (where \nyourvo\n is your VO, and \nNNN\n is your uid as reported by \n/usr/bin/id\n):\n\n\nvoms-proxy-init -voms yourvo:/yourvo\nexport GLEXEC_CLIENT_CERT=/tmp/x509up_u\nNNN\n\n/usr/sbin/glexec /usr/bin/id\nuid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)\n\n\n\n\nIf your \nlcmaps.db\n is set up to not use a host certificate as described in GlexecPilotCert, you should also set\n\n\nexport X509_USER_PROXY=/tmp/x509up_u\nNNN\n\n\n\n\n\n(substitute \nNNN\n for your UID) before running glexec.\n\n\nIf \nglexec\n is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)\n\n\nIf you have problems, please read about \ntroubleshooting glexec\n.\n\n\nGlexec log files\n\n\nGlexec\n sends all its log information by default to syslog. Where it goes from there depends on your syslog configuration, but by default they go to \n/var/log/messages\n. Here are some sample messages:\n\n\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.\n\n\n\n\nThese particular messages are pretty common, caused by forgetting to uncomment the beginning of the glexectracking rule in \n/etc/lcmaps.db\n.\n\n\nIt is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the \nLOG_LOCAL[0-7]\n log facilities that are unused, for example \nLOG_LOCAL1\n. Then set the following in \n/etc/glexec.conf\n:\n\n\nsyslog_facility = LOG_LOCAL1\n\n\n\n\nand add a corresponding parameter to the \nlcmaps_glexec_tracking.mod\n entry in \n/etc/lcmaps.db\n:\n\n\n  \n-log-facility LOG_LOCAL1\n\n\n\n\n\nThen in \n/etc/syslog.conf\n on el5 or \n/etc/rsyslog.conf\n on el6 add a line like this\n\n\nlocal1.* /var/log/glexec.log\n\n\n\n\nand also exclude those messages from \n/var/log/messages\n by adding \nlocal1.none\n after other wildcards on the existing \n/var/log/messages\n line, for example:\n\n\n*.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages\n\n\n\n\nBe sure to notify the system logger to re-read the configuration file with \nservice syslog reload\n on el5, or with \nservice rsyslog restart\n on el6.\n\n\nrsyslog\n, by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to \n/etc/rsyslog.conf\n after the line \"$ModLoad imuxsock.so\":\n\n\n$SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0\n\n\n\n\nand of course do \nservice rsyslog restart\n.\n\n\nAlternatively, \nsyslog-ng\n (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in \n/etc/syslog-ng/syslog-ng.conf\n will separate the glexec messages into \n/var/log/glexec.log\n:\n\n\ndestination d_glexec { file(\n/var/log/glexec.log\n); };\nfilter f_glexec { program(\n^glexec\n); };\nfilter f_notglexec { not program(\n^glexec\n); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };\n\n\n\n\nThen later, in the log rule writing sending to \nd_mesg\n, add a \nfilter(f_notglexec);\n before the destination rule to keep glexec messages out of \n/var/log/messages\n:\n\n\nlog { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.", 
            "title": "Worker node glexec"
        }, 
        {
            "location": "/Other/glexec/#glexec-installation-guide", 
            "text": "This document is intended for System Administrators that are installing the OSG version of glexec.  Glexec is commonly used for what are referred to as \u201cpilot\u201d or \u201cglidein\u201d jobs.  Traditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper). The user job was authenticated/authorized to run at that site based on the user\u2019s proxy credentials and run under the local unix account assigned.  In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue). The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources. It will then submit what is called a pilot/glidein job to a remote site. This pilot job gets authenticated/authorized to run on a worker node in that site\u2019s cluster. It will then \u201cpull\u201d down user jobs from the centralized queue and execute them. Both the pilot and the user job are run under the pilot job\u2019s proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user\u2019s proxy credentials and, thus, the user\u2019s jobs do not run using it\u2019s own local unix account.  Glexec is a security tool that can be used to resolve this problem. It is meant to be used by VOs that run these pilot-based jobs. It has a number of authentication plugins and can be used both by European grid and by OSG.  The pilot job will \u201cpull\u201d user jobs down from the central queue and invoke glexec which will then   authenticate the user job\u2019s proxy,  perform an authorization callout (to GUMS in the case of OSG, or possibly a gridmapfile) similar to that done by the gatekeeper,  and then run the user job under the local account assigned by the authorization service for that user.   In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node. The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.  Many worker node clusters use shared file systems like NFS for much of their software and user home accounts. Since glexec is an suid program, it must be installed on every single worker node individually. Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.  For more information regarding pilot-based systems and glexec:   glideinWMS - The glidein based WMS  Addressing the pilot security problem with gLExec (pdf)", 
            "title": "Glexec Installation Guide"
        }, 
        {
            "location": "/Other/glexec/#engineering-considerations", 
            "text": "This section describes any prerequisite software/considerations that must be taken into account before the glexec software installation is performed. It should be reviewed completely before starting the installation process.  A large number of batch slots using glexec can occasionally put an enormous strain on GUMS servers and cause overloading and client timeouts. In order to survive peak loads, the sysctl parameter \u2018net.core.somaxconn\u2019 on a GUMS server machine should be set at least as high as the maximum number of job slots that might attempt to contact the server at about the same time. (For example, Fermilab set the value to 4096 on each of two servers and tested with a continuous load from 5000 job slots). At the same time, the Apache parameter \u2018ListenBacklog\u2019 must be changed to the same value. Also note that Fermilab determined that for best performance, the Apache parameter \u2018MaxClients\u2019 on GUMS servers (at least on their dual-core Virtual Machines) should be set to a value of 32. For details on these and other parameters on the GUMS server see GumsScalability.", 
            "title": "Engineering Considerations"
        }, 
        {
            "location": "/Other/glexec/#requirements", 
            "text": "These are the requirements that must be met to install glexec.   Note  Normally you will install the  OSG worker node  first. Technically, installing the  osg-wn-client-glexec  package will also install the worker node, but we do not duplicate instructions specific to the worker node here, so refer to the  OSG worker node  for details about the worker node installation.", 
            "title": "Requirements"
        }, 
        {
            "location": "/Other/glexec/#host-and-os", 
            "text": "OS is Red Hat Enterprise Linux 5, 6, 7, and variants.  Root access", 
            "title": "Host and OS"
        }, 
        {
            "location": "/Other/glexec/#users", 
            "text": "The glexec installation will create two users unless they are already created.     User  Comment      glexec  Reduced privilege separate id used to improve security. Set the default gid of the \u201cglexec\u201d user to be a group that is also called \u201cglexec\u201d.    gratia  Needed for the glexec gratia probe which is also automatically installed.     In addition, OSG glexec requires a range of  group ids  for tracking purposes. You don\u2018t actually have to create the group entries but it is recommended to do so in order to reserve the gids and so they can be associated with names in the  /usr/bin/id  command. The recommended names are \u2019glexecNN\u2019 where NN is a number starting from 00.   Define at least 4 group ids per batch slot per worker node. A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.  They must be consecutive and in any range (default range is 65000-65049, configured in the  Configuring glexec  section below).  The same group ids can be used on every worker node.", 
            "title": "Users"
        }, 
        {
            "location": "/Other/glexec/#install-instructions", 
            "text": "Prior to installing  glexec , verify the  yum repositories  are correctly configured.  Some of the worker node client software verifies proxies or certificates. In order to do this, they will need the  CA certificates  used to sign the proxies.", 
            "title": "Install Instructions"
        }, 
        {
            "location": "/Other/glexec/#install-glexec", 
            "text": "Note  The glexec tracking function requires a part of HTCondor. There are multiple ways to install HTCondor, for details see  these instructions . If you want a minimal install, you can run just this command to install the needed piece from the OSG distribution:  yum install condor-procd   After meeting all the requirements in the previous section, install glexec with this command:  yum install osg-wn-client-glexec", 
            "title": "Install glexec"
        }, 
        {
            "location": "/Other/glexec/#configuring-glexec", 
            "text": "The following steps need to be done after the glexec installation is complete.   First, review the contents of  /etc/glexec.conf . All of the defaults should be fine, but if you want to change the behavior, the parameters are described in  man glexec.conf .   Next, review all of the contents of  /etc/lcmaps.db  and in particular update the following pieces.    If you have GUMS, change the yourgums.yourdomain in the following line to the fully qualified domain name of your GUMS server:  \"\u2013endpoint  https://yourgums.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort \"    If you want to use a range of tracking group ids other than the default as described in the  Requirements  section above, uncomment and change the  -min-gid  and  -max-gid  lines to your chosen values:  \"-min-gid 65000\u201d \u201c-max-gid 65049\"    Uncomment the following two lines:  glexectracking = \"lcmaps_glexec_tracking.mod\"\n                   \"-exec /usr/sbin/glexec_monitor\"    If you have GUMS, uncomment the following policy toward the end of the file:  verifyproxy -  gumsclient\n gumsclient -  glexectracking  or if you have do not have GUMS and want to use a gridmapfile, uncomment the following policy:  verifyproxy -  gridmapfile\n gridmapfile -  glexectracking", 
            "title": "Configuring glexec"
        }, 
        {
            "location": "/Other/glexec/#testing-the-installation-of-glexec", 
            "text": "Now,  as a non-privileged user (not root)  , do the following (where  yourvo  is your VO, and  NNN  is your uid as reported by  /usr/bin/id ):  voms-proxy-init -voms yourvo:/yourvo\nexport GLEXEC_CLIENT_CERT=/tmp/x509up_u NNN \n/usr/sbin/glexec /usr/bin/id\nuid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)  If your  lcmaps.db  is set up to not use a host certificate as described in GlexecPilotCert, you should also set  export X509_USER_PROXY=/tmp/x509up_u NNN   (substitute  NNN  for your UID) before running glexec.  If  glexec  is successful, it will print out the uid and gid that your proxy would normally be mapped to by your GUMS server, plus a supplementary tracking group. (The actual names and numbers will be different from what you see above.)  If you have problems, please read about  troubleshooting glexec .", 
            "title": "Testing the Installation of glexec"
        }, 
        {
            "location": "/Other/glexec/#glexec-log-files", 
            "text": "Glexec  sends all its log information by default to syslog. Where it goes from there depends on your syslog configuration, but by default they go to  /var/log/messages . Here are some sample messages:  Apr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-PluginInit(): plugin glexectracking not found (arguments: )\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps.mod-lcmaps_startPluginManager(): error initializing plugin: glexectracking\nApr 25 16:36:16 fermicloud053 glexec[2867]: lcmaps: lcmaps_init() error: could not start plugin manager\nApr 25 16:36:16 fermicloud053 glexec[2867]: Initialisation of LCMAPS failed.  These particular messages are pretty common, caused by forgetting to uncomment the beginning of the glexectracking rule in  /etc/lcmaps.db .  It is possible to redirect glexec log messages to a different file with standard syslog. To do that, choose one of the  LOG_LOCAL[0-7]  log facilities that are unused, for example  LOG_LOCAL1 . Then set the following in  /etc/glexec.conf :  syslog_facility = LOG_LOCAL1  and add a corresponding parameter to the  lcmaps_glexec_tracking.mod  entry in  /etc/lcmaps.db :     -log-facility LOG_LOCAL1   Then in  /etc/syslog.conf  on el5 or  /etc/rsyslog.conf  on el6 add a line like this  local1.* /var/log/glexec.log  and also exclude those messages from  /var/log/messages  by adding  local1.none  after other wildcards on the existing  /var/log/messages  line, for example:  *.info;local1.none;mail.none;authpriv.none;cron.none /var/log/messages  Be sure to notify the system logger to re-read the configuration file with  service syslog reload  on el5, or with  service rsyslog restart  on el6.  rsyslog , by default, limits the rate at which messages may be logged, and if maximum debugging is enabled in glexec this limit is reached. To avoid that, you can add the following to  /etc/rsyslog.conf  after the line \"$ModLoad imuxsock.so\":  $SystemLogRateLimitInterval 0\n$SystemLogRateLimitBurst 0  and of course do  service rsyslog restart .  Alternatively,  syslog-ng  (available in the EPEL repository) can do the same job by matching all the messages that have the string \"glexec\" in the name.\nThese rules in  /etc/syslog-ng/syslog-ng.conf  will separate the glexec messages into  /var/log/glexec.log :  destination d_glexec { file( /var/log/glexec.log ); };\nfilter f_glexec { program( ^glexec ); };\nfilter f_notglexec { not program( ^glexec ); };\nlog { source(s_sys); filter(f_glexec); destination(d_glexec); };  Then later, in the log rule writing sending to  d_mesg , add a  filter(f_notglexec);  before the destination rule to keep glexec messages out of  /var/log/messages :  log { source(s_sys); filter(f_filter1); filter(f_notglexec); destination(d_mesg); };", 
            "title": "Glexec log files"
        }, 
        {
            "location": "/Other/glexec/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Common/ca_updater/", 
            "text": "OSG CA Certificates Updater\n\n\nThis document explains the installation and use of \nosg-ca-certs-updater\n, a package in the OSG Software 3.x distribution that provides automatic updates of CA certificates.\n\n\nRequirements\n\n\n\n\nOS must be Red Hat Enterprise Linux 5 or 6 or variants.\n\n\nThe OSG repositories must be installed and enabled. See the \nYum Repositories\n page for instructions.\n\n\nOne grid-certificates package from the OSG repositories must be installed as described \nhere\n. Currently, these are: \nigtf-ca-certs\n, \nosg-ca-certs\n.\n\n\n\n\nInstall instructions\n\n\nRun the following command to install the latest version of the updater.\n\n\n[root@client ~]$ yum install osg-ca-certs-updater\n\n\n\n\nServices\n\n\nStarting and Enabling Services\n\n\nRun the following to enable the updater. This will persist until the machine is rebooted.\n\n\n[root@client ~]$ service osg-ca-certs-updater-cron start\n\n\n\n\nRun the following to enable the updater when the machine is rebooted.\n\n\n[root@client ~]$ chkconfig osg-ca-certs-updater-cron on\n\n\n\n\nRun both commands if you wish for the service to activate immediately and remain active throughout reboots.\n\n\nStopping and Disabling Services\n\n\nEnter the following to disable the updater. This will persist until the machine is rebooted.\n\n\n[root@client ~]$ service osg-ca-certs-updater-cron stop\n\n\n\n\nEnter the following to disable the updater when the machine is rebooted.\n\n\n[root@client ~]$ chkconfig osg-ca-certs-updater-cron off\n\n\n\n\nRun both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.\n\n\nConfiguration\n\n\nWhile there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the \ncron\n entry of the service. This entry is located in the file \n/etc/cron.d/osg-ca-certs-updater\n. Please see the Unix manual page for \ncrontab\n in section 5 for an explanation of the format. The manual page can be accessed by the command \nman 5 crontab\n. The valid command-line arguments can be listed by running \nosg-ca-certs-updater --help\n. Reasonable defaults have been provided, namely:\n\n\n\n\nAttempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.\n\n\nRun the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.\n\n\nDelay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.\n\n\nDo not warn the administrator about update failures that have happened less than 72 hours since the last successful update.\n\n\nLog errors only.\n\n\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration file\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nosg-ca-certs-updater\n\n\nCron entry for periodically launching the updater\n\n\n/etc/cron.d/osg-ca-certs-updater\n\n\nCommand-line arguments to the updater can be specified here\n\n\n\n\n\n\nosg-release\n\n\nRepo definition files for production OSG repositories\n\n\n/etc/yum.repos.d/osg.repo\n or \n/etc/yum.repos.d/osg-el6.repo\n\n\nMake sure these repositories are enabled and reachable from the host you are trying to update\n\n\n\n\n\n\n\n\nLog files\n\n\nLogging is performed to the console by default. Please see the manual for your \ncron\n daemon to find out how it handles console output.\n\n\nA logfile can be specified via the \n-l\n / \n--logfile\n command-line option.\n\n\nIf logging to syslog via the \n-s\n / \n--log-to-syslog\n option, the updater will write to the \nuser\n section of the syslog. The file \n/etc/syslog.conf\n determines where syslog messages are saved.\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.\n\n\nReferences\n\n\nSome guides on X.509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nRelated software:\n\n\n\n\nOSG CA certificates\n\n\nDescription, manual and examples of OsgCaManage\n\n\nOsgCaCertsUpdater", 
            "title": "CA Updater"
        }, 
        {
            "location": "/Common/ca_updater/#osg-ca-certificates-updater", 
            "text": "This document explains the installation and use of  osg-ca-certs-updater , a package in the OSG Software 3.x distribution that provides automatic updates of CA certificates.", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/Common/ca_updater/#requirements", 
            "text": "OS must be Red Hat Enterprise Linux 5 or 6 or variants.  The OSG repositories must be installed and enabled. See the  Yum Repositories  page for instructions.  One grid-certificates package from the OSG repositories must be installed as described  here . Currently, these are:  igtf-ca-certs ,  osg-ca-certs .", 
            "title": "Requirements"
        }, 
        {
            "location": "/Common/ca_updater/#install-instructions", 
            "text": "Run the following command to install the latest version of the updater.  [root@client ~]$ yum install osg-ca-certs-updater", 
            "title": "Install instructions"
        }, 
        {
            "location": "/Common/ca_updater/#services", 
            "text": "", 
            "title": "Services"
        }, 
        {
            "location": "/Common/ca_updater/#starting-and-enabling-services", 
            "text": "Run the following to enable the updater. This will persist until the machine is rebooted.  [root@client ~]$ service osg-ca-certs-updater-cron start  Run the following to enable the updater when the machine is rebooted.  [root@client ~]$ chkconfig osg-ca-certs-updater-cron on  Run both commands if you wish for the service to activate immediately and remain active throughout reboots.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/Common/ca_updater/#stopping-and-disabling-services", 
            "text": "Enter the following to disable the updater. This will persist until the machine is rebooted.  [root@client ~]$ service osg-ca-certs-updater-cron stop  Enter the following to disable the updater when the machine is rebooted.  [root@client ~]$ chkconfig osg-ca-certs-updater-cron off  Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/Common/ca_updater/#configuration", 
            "text": "While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the  cron  entry of the service. This entry is located in the file  /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for  crontab  in section 5 for an explanation of the format. The manual page can be accessed by the command  man 5 crontab . The valid command-line arguments can be listed by running  osg-ca-certs-updater --help . Reasonable defaults have been provided, namely:   Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.  Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.  Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.  Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update.  Log errors only.", 
            "title": "Configuration"
        }, 
        {
            "location": "/Common/ca_updater/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Common/ca_updater/#useful-configuration-and-log-files", 
            "text": "", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/Common/ca_updater/#configuration-file", 
            "text": "Package  File Description  Location  Comment      osg-ca-certs-updater  Cron entry for periodically launching the updater  /etc/cron.d/osg-ca-certs-updater  Command-line arguments to the updater can be specified here    osg-release  Repo definition files for production OSG repositories  /etc/yum.repos.d/osg.repo  or  /etc/yum.repos.d/osg-el6.repo  Make sure these repositories are enabled and reachable from the host you are trying to update", 
            "title": "Configuration file"
        }, 
        {
            "location": "/Common/ca_updater/#log-files", 
            "text": "Logging is performed to the console by default. Please see the manual for your  cron  daemon to find out how it handles console output.  A logfile can be specified via the  -l  /  --logfile  command-line option.  If logging to syslog via the  -s  /  --log-to-syslog  option, the updater will write to the  user  section of the syslog. The file  /etc/syslog.conf  determines where syslog messages are saved.", 
            "title": "Log files"
        }, 
        {
            "location": "/Common/ca_updater/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Common/ca_updater/#references", 
            "text": "Some guides on X.509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html   Related software:   OSG CA certificates  Description, manual and examples of OsgCaManage  OsgCaCertsUpdater", 
            "title": "References"
        }, 
        {
            "location": "/Common/cert_scripts/", 
            "text": "Certificate Scripts package\n\n\n \n\n\nAbout This Document\n\n\nThis the home page for documenting the cert-scripts package that provides a command-line interface to the DOEGrids CA website and some additional utilities for dealing with X509 certificates. This package was developed originally by the PPDG project and is now maintained by the OSG RA.\n\n\nAs an alternative to the web browser interface, these scripts are contributed to the DOEGrids PKI to allow a command-line interface to the certificate authority for submitting certificate requests, retrieving signed certificates, renewing certificates, directory lookup of existing certificates, and checking the remaining lifetime of certificates and certificate revocation lists. They work directly with the PEM format files used by Globus. These are perl scripts and bash shell scripts (some awk), depend upon openssl, ldapsearch and the perl LWP:: module with \nSSL support\n. Click on the File link below for the usage description of the script, or to download the tar file package containing the scripts.\n\n\nHow to get Help?\n\n\nTo get assistance please use \nHelp Procedure\n.\n\n\nRequirements\n\n\n\n\nA host to install the Cert Scripts package. It is normally included in the CE.\n\n\nOS is \n. Currently most of our testing has been done on Scientific Linux 5.\n\n\nRoot access\n\n\nAllow outbound network connection to the CA\n\n\n\n\nInstallation Procedure \nOSGAllInstallCertScripts\n\n\nYumRepositories\n\n\nInstallCertAuth\n\n\nOSGBriefInstallCertScripts\n\n\nInstall the certificate scripts package The Cert Scripts package can be installed with the following command:\n\n\n[root@client ~]$ yum install osg-cert-scripts\n\n\n\n\nOSGBriefInstallCertScripts\n \nOSGAllInstallCertScripts\n\n\nUsage of certificate scripts package This package is mainly used to request certificates via the command line: either host and service certificates or user certificates.\n\n\nGet host and service certificates using command line\n\n\nExample usage of check-cert-time The cert-check-time script is helpful in setting up and monitoring the CA certificates and CRL\u2019s that get installed in your trusted certificates directory. This section describes using these scripts to check the CA and CRL status. \u2014+++! Checking CA certificates\n\n\nThere are numerous CA certificates installed with VDT and you may not want to allow all of them on your site. The \ncert-check-time\n is a helpful command for reviewing them. This must be run in a directory where you have write access even though it does not create any permanent files. You may want to redirect stdout to a file you can then review.\n\n\nUCL_PROMPT\n \nb\ncert-check-time -cR -s /usr/share/osg-cert-scripts/\n/b\n\n\n\n\n\nFor each CA, the output shows:\n\n\n\n\nremaining lifetime of the CA certificate (in days),\n\n\nthe human readable name of the CA,\n\n\nand the location of the actual certificate file.\n\n\n\n\nTWISTY_OPTS_OUTPUT\n\n\n         days  name       CA certificate file\n       6712.9 subject= /DC=HK/DC=HKU/DC=GRID/CN=HKU Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/4798da47.0\n       6674.9 subject= /DC=cz/DC=cesnet-ca/O=CESNET CA/CN=CESNET CA Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/edca0fc0.0\n       6640.8 subject= /C=FR/O=CNRS/CN=CNRS2  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/163af95c.0\n       6639.8 subject= /C=FR/O=CNRS/CN=CNRS2-Projets  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/09ff08b7.0\n       6638.8 subject= /C=FR/O=CNRS/CN=GRID2-FR  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d11f973e.0\n       6620.4 subject= /C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Client Authentication and Email  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9ec3a561.0\n       6620.4 subject= /C=NL/O=TERENA/CN=TERENA eScience Personal CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/169d7f9c.0\n       6620.4 subject= /C=GB/ST=Greater Manchester/L=Salford/O=Comodo CA Limited/CN=AAA Certificate Services  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/75680d2e.0\n       6542.2 subject= /DC=by/DC=grid/O=uiip.bas-net.by/CN=Belarusian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/709bed08.0\n       6274.0 subject= /C=MK/O=MARGI/CN=MARGI-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7d0d064a.0\n       6191.7 subject= /C=UK/O=eScienceRoot/OU=Authority/CN=UK e-Science Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/98ef0ee5.0\n       6165.7 subject= /C=TW/O=AS/CN=Academia Sinica Grid Computing Certification Authority Mercury  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9cd75e87.0\n       5929.1 subject= /C=CH/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCHgrid Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d0b701c0.0\n       5799.7 subject= /DC=ch/DC=cern/CN=CERN Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d254cc30.0\n       5719.6 subject= /C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid Root CA 2006  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/28a58577.0\n       5690.1 subject= /C=HR/O=edu/OU=srce/CN=SRCE CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ff94d436.0\n       5581.6 subject= /DC=CN/DC=Grid/CN=Root Certificate Authority at CNIC  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b2771d44.0\n       5536.3 subject= /C=CA/O=Grid/CN=Grid Canada Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/bffbd7d0.0\n       5437.9 subject= /C=TR/O=TRGrid/CN=TR-Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1691b9ba.0\n       5327.0 subject= /DC=cz/DC=cesnet-ca/CN=CESNET CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9b59ecad.0\n       5084.8 subject= /C=JP/O=AIST/OU=GRID/CN=Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a317c467.0\n       4947.1 subject= /C=PT/O=LIPCA/CN=LIP Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/11b4a5a2.0\n       4361.6 subject= /DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d1b603c3.0\n       3774.1 subject= /C=BM/O=QuoVadis Limited/OU=Root Certification Authority/CN=QuoVadis Root Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/5cf9d536.0\n       3737.4 subject= /C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/16da7552.0\n       3572.8 subject= /C=PL/O=GRID/CN=Polish Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/8a661490.0\n       3482.8 subject= /C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Hardware  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ff783690.0\n       3482.8 subject= /C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3c58f906.0\n       3482.8 subject= /C=NL/O=TERENA/CN=TERENA eScience SSL CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/20ce830e.0\n       3319.1 subject= /DC=cz/DC=cesnet-ca/O=CESNET CA/CN=CESNET CA 3  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/712ae4cc.0\n       3089.1 subject= /DC=gov/DC=fnal/O=Fermilab/OU=Certificate Authorities/CN=Kerberized CA HSM  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/99f9f5a3.0\n       3087.2 subject= /C=BM/O=QuoVadis Limited/OU=Issuing Certification Authority/CN=QuoVadis Grid ICA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e72045ce.0\n       3039.3 subject= /DC=MD/DC=MD-Grid/O=RENAM/OU=Certification Authority/CN=MD-Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9ff26ea4.0\n       2939.0 subject= /C=BE/OU=BEGRID/O=BELNET/CN=BEgrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e8d818e6.0\n       2906.9 subject= /C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN SLCS-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a02131f7.0\n       2904.9 subject= /C=VE/O=Grid/O=Universidad de Los Andes/OU=CeCalCULA/CN=ULAGrid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3f0f4285.0\n       2879.6 subject= /DC=IN/DC=GARUDAINDIA/CN=Indian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/da75f6a8.0\n       2874.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/684261aa.0\n       2850.4 subject= /DC=TW/DC=ORG/DC=NCHC/CN=NCHC CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/71a89a47.0\n       2769.2 subject= /DC=NET/DC=PRAGMA-GRID/CN=PRAGMA-UCSD CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7721d4d3.0\n       2763.2 subject= /DC=LV/DC=latgrid/CN=Certification Authority for Latvian Grid  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/742edd45.0\n       2756.7 subject= /DC=me/DC=ac/DC=MREN/CN=MREN-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3232b9bc.0\n       2581.9 subject= /C=PK/O=NCP/CN=PK-GRID-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/f5ead794.0\n       2560.4 subject= /C=MX/O=UNAMgrid/OU=UNAM/CN=CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/24c3ccde.0\n       2549.2 subject= /C=MA/O=MaGrid/CN=MaGrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7b54708e.0\n       2510.1 subject= /DC=RO/DC=RomanianGRID/O=ROSA/OU=Certification Authority/CN=RomanianGRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1f3834d0.0\n       2485.1 subject= /C=AR/O=e-Ciencia/OU=UNLP/L=CeSPI/CN=PKIGrid  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b7bcb7b2.0\n       2449.6 subject= /C=KR/O=KISTI/O=GRID/CN=KISTI Grid Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/722e5071.0\n       2446.1 subject= /DC=BR/DC=UFF/DC=IC/O=UFF LACGrid CA/CN=UFF Latin American and Caribbean Catch-all Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a9082267.0\n       2367.8 subject= /C=CL/O=REUNACA/CN=REUNA Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/295adc19.0\n       2343.9 subject= /C=RS/O=AEGIS/CN=AEGIS-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/393f7863.0\n       2279.2 subject= /DC=bg/DC=acad/CN=BG.ACAD CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2418a3f3.0\n       2238.5 subject= /C=TH/O=NECTEC/OU=GOC/CN=NECTEC GOC CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/8a047de1.0\n       2147.9 subject= /C=IT/O=INFN/CN=INFN CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2f3fadf6.0\n       2147.8 subject= /DC=ch/DC=cern/CN=CERN Trusted Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1d879c6c.0\n       2068.6 subject= /C=JP/O=National Research Grid Initiative/OU=CGRD/CN=NAREGI CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a87d9192.0\n       2067.7 subject= /C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/82b36fca.0\n       2064.1 subject= /C=BR/O=ICPEDU/O=UFF BrGrid CA/CN=UFF Brazilian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/0a2bac92.0\n       1928.6 subject= /DC=CN/DC=Grid/DC=SDG/CN=Scientific Data Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/c48c63f3.0\n       1877.5 subject= /C=AU/O=APACGrid/OU=CA/CN=APACGrid/emailAddress=camanager@vpac.org  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1e12d831.0\n       1769.8 subject= /DC=org/DC=balticgrid/CN=Baltic Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2a237f16.0\n       1725.0 subject= /C=RU/O=RDIG/CN=Russian Data-Intensive Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/55994d72.0\n       1682.6 subject= /DC=es/DC=irisgrid/CN=IRISGridCA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9dd23746.0\n       1649.8 subject= /C=CN/O=HEP/CN=gridca-cn/emailAddress=gridca@ihep.ac.cn  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ba2f39ca.0\n       1575.2 subject= /C=AT/O=AustrianGrid/OU=Certification Authority/CN=Certificate Issuer  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/6e3b436b.0\n       1571.4 subject= /C=HU/O=NIIF/OU=Certificate Authorities/CN=NIIF Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/cc800af0.0\n       1389.9 subject= /CN=Purdue TeraGrid RA/OU=Purdue TeraGrid/O=Purdue University/ST=Indiana/C=US  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/67e8acfa.0\n       1378.2 subject= /CN=PurdueCA/O=Purdue University/ST=Indiana/C=US  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/95009ddc.0\n       1374.7 subject= /DC=ORG/DC=SEE-GRID/CN=SEE-GRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/468d15b3.0\n       1301.9 subject= /C=DE/O=GermanGrid/CN=GridKa-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/dd4b34ea.0\n       1211.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=GridShib CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e8ac4b61.0\n       1141.9 subject= /C=IL/O=IUCC/CN=IUCC/emailAddress=ca@mail.iucc.ac.il  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/6fee79b0.0\n       1111.4 subject= /C=AM/O=ArmeSFo/CN=ArmeSFo CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d0c2a341.0\n       1049.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC MICS CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2ac09305.0\n       1049.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC Classic CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e5cc84c2.0\n       1025.9 subject= /C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein PCA Grid - G01  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1149214e.0\n        943.2 subject= /DC=net/DC=ES/OU=Certificate Authorities/CN=NERSC Online CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b93d6240.0\n        910.1 subject= /C=IR/O=IPM/O=IRAN-GRID/CN=IRAN-GRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ce33db76.0\n        815.9 subject= /C=CY/O=CyGrid/O=HPCL/CN=CyGridCA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/afe55e66.0\n        800.7 subject= /DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1c3f2ca8.0\n        795.3 subject= /DC=org/DC=ugrid/CN=UGRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/0a12b607.0\n        761.9 subject= /C=SK/O=SlovakGrid/CN=SlovakGrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e13e0fcf.0\n        713.7 subject= /C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/367b75c3.0\n        619.1 subject= /C=IE/O=Grid-Ireland/CN=Grid-Ireland Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1e43b9cc.0\n        523.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=MyProxy  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/f2e89fe3.0\n        523.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=CACL  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9b95bbf2.0\n        450.1 subject= /C=CH/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCHslcs CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/304cf809.0\n        279.7 subject= /C=SI/O=SiGNET/CN=SiGNET CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3d5be7bc.0\n        179.9 subject= /O=Grid/O=NorduGrid/CN=NorduGrid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1f0e8352.0\n        135.0 subject= /C=JP/O=KEK/OU=CRC/CN=KEK GRID Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/617ff41b.0\nnearest CA certificate expiration 134.984 days\n\n\n\n\n\n\nIn addition, at the bottom of the listing it points out which CA\u2019s do not have a CRL. This is useful AFTER the edg-crl-upgraded daemon has run at least once because then it shows those CA\u2019s which have not published a certificate revocation list. Note that two of the CAs, Kerberos CAs from PSC (85ca9edc.0) and FNAL (e1fce4e9.0) don\u2019t really need CRLs since they only generate short lived certificates.\n\n\nChecking CRL\u2019s\n\n\nCertificate revocation lists contain the list of certificates (by serial number) that have been issued by a CA but were then revoked, meaning you should not accept them. CRL\u2019s are updated frequently and typically have a lifetime limited to a month or less. When a CRL has expired, the CRL file will still exist in the trusted certificates directory, but Globus will fail \nall\n authentication attempts for \nall\n certificates issued by the corresponding CA.\n\n\nFor this reason, and others, it is important that CRL files are current and not expired. Another variation of the cert-check-time script will list the remaining lifetime of CRL\u2019s in the trusted certificates directory. This must be run in a directory where you have write access even though it does not create any files. You may want to redirect stdout to a file you can then review.\n\n\nUCL_PROMPT\n \nb\ncert-check-time -r -s /usr/share/osg-cert-scripts/ \n/b\n\n\n\n\n\nFor each CRL, the sample output below shows:\n\n\n\n\nthe remaining lifetime,\n\n\nthe name of the CA that issued the CRL\n\n\nand the actual CRL file.\n\n\n\n\nTWISTY_OPTS_OUTPUT\n\n\n         days  name       CRL file\n        365.1 issuer=/C=CA/O=Grid/CN=Grid Canada CA  crl:/opt/osg036/globus/TRUSTED_CA/5f54f417.r0\n        340.8 issuer=/CN=SWITCH CA/emailAddress=switch.ca@switch.ch/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/c4435d12.r0\n        317.1 issuer=/C=FR/O=CNRS/CN=CNRS-Projets  crl:/opt/osg036/globus/TRUSTED_CA/34a509c3.r0\n        317.1 issuer=/C=FR/O=CNRS/CN=CNRS  crl:/opt/osg036/globus/TRUSTED_CA/cf4ba8c8.r0\n        257.1 issuer=/CN=SwissSign Silver CA/emailAddress=silver@swisssign.com/O=SwissSign/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/e9d08b40.r0\n        257.1 issuer=/CN=SwissSign Bronze CA/emailAddress=bronze@swisssign.com/O=SwissSign/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/e36e7a72.r0\n        204.4 issuer=/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  crl:/opt/osg036/globus/TRUSTED_CA/d1b603c3.r0\n        149.4 issuer=/C=CH/O=SwissSign/CN=SwissSign CA (RSA IK May 6 1999 18:00:58)/emailAddress=ca@SwissSign.com  crl:/opt/osg036/globus/TRUSTED_CA/7b2d086c.r0\n         31.0 issuer=/C=RU/O=RDIG/CN=Russian Data-Intensive Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/55994d72.r0\n         30.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein Server CA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/fe102e03.r0\n         30.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein User CA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/34f8e29c.r0\n         29.9 issuer=/C=IT/O=INFN/CN=INFN Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/49f18420.r0\n         29.9 issuer=/C=CY/O=CyGrid/O=HPCL/CN=CyGridCA  crl:/opt/osg036/globus/TRUSTED_CA/afe55e66.r0\n         29.9 issuer=/C=IE/O=Grid-Ireland/CN=Grid-Ireland Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/1e43b9cc.r0\n         29.7 issuer=/DC=es/DC=irisgrid/CN=IRISGridCA  crl:/opt/osg036/globus/TRUSTED_CA/9dd23746.r0\n         29.7 issuer=/C=IL/O=IUCC/CN=IUCC/emailAddress=ca@mail.iucc.ac.il  crl:/opt/osg036/globus/TRUSTED_CA/6fee79b0.r0\n         29.6 issuer=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1  crl:/opt/osg036/globus/TRUSTED_CA/1c3f2ca8.r0\n         29.5 issuer=/C=TW/O=AS/CN=Academia Sinica Grid Computing Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/a692434d.r0\n         29.3 issuer=/C=FR/O=CNRS/CN=GRID-FR  crl:/opt/osg036/globus/TRUSTED_CA/12a1d8c2.r0\n         29.1 issuer=/C=DE/O=GermanGrid/CN=GridKa-CA  crl:/opt/osg036/globus/TRUSTED_CA/dd4b34ea.r0\n         28.8 issuer=/C=JP/O=National Research Grid Initiative/OU=GRID/CN=NAREGI CA  crl:/opt/osg036/globus/TRUSTED_CA/0cb5fc2c.r0\n         28.3 issuer=/C=AU/O=APACGrid/OU=CA/CN=APACGrid/emailAddress=camanager@vpac.org  crl:/opt/osg036/globus/TRUSTED_CA/1e12d831.r0\n         27.9 issuer=/DC=ORG/DC=SEE-GRID/CN=SEE-GRID CA  crl:/opt/osg036/globus/TRUSTED_CA/468d15b3.r0\n         27.9 issuer=/C=GR/O=HellasGrid/CN=HellasGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/ede78092.r0\n         27.1 issuer=/C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth  crl:/opt/osg036/globus/TRUSTED_CA/16da7552.r0\n         27.0 issuer=/C=PL/O=GRID/CN=Polish Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/8a661490.r0\n         26.8 issuer=/O=Grid/O=NorduGrid/CN=NorduGrid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/1f0e8352.r0\n         25.9 issuer=/C=AM/O=ArmeSFo/CN=ArmeSFo CA  crl:/opt/osg036/globus/TRUSTED_CA/d0c2a341.r0\n         25.9 issuer=/C=BE/O=BELNET/OU=BEGrid/CN=BEGrid CA/emailAddress=gridca@belnet.be  crl:/opt/osg036/globus/TRUSTED_CA/03aa0ecb.r0\n         24.0 issuer=/C=UK/O=eScience/OU=Authority/CN=CA/emailAddress=ca-operator@grid-support.ac.uk  crl:/opt/osg036/globus/TRUSTED_CA/01621954.r0\n         23.9 issuer=/C=HU/O=KFKI RMKI CA/CN=KFKI RMKI CA  crl:/opt/osg036/globus/TRUSTED_CA/5e5501f3.r0\n         23.9 issuer=/C=SK/O=SlovakGrid/CN=SlovakGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/e13e0fcf.r0\n         23.1 issuer=/C=PT/O=LIPCA/CN=LIP Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/11b4a5a2.r0\n         23.0 issuer=/C=PT/O=LIP/OU=LISCC/CN=LIP Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/41380387.r0\n         23.0 issuer=/C=AT/O=AustrianGrid/OU=Certification Authority/CN=Certificate Issuer  crl:/opt/osg036/globus/TRUSTED_CA/6e3b436b.r0\n         22.9 issuer=/C=CZ/O=CESNET/CN=CESNET CA  crl:/opt/osg036/globus/TRUSTED_CA/ed99a497.r0\n         21.9 issuer=/C=KR/O=KISTI/CN=KISTI GRID ROOT CA  crl:/opt/osg036/globus/TRUSTED_CA/47183fda.r0\n         21.7 issuer=/C=CN/O=HEP/CN=gridca-cn/emailAddress=gridca@ihep.ac.cn  crl:/opt/osg036/globus/TRUSTED_CA/ba2f39ca.r0\n         21.1 issuer=/C=CH/O=CERN/OU=GRID/CN=CERN CA  crl:/opt/osg036/globus/TRUSTED_CA/fa3af1d7.r0\n         19.1 issuer=/C=SI/O=SiGNET/CN=SiGNET CA/emailAddress=signet-ca@ijs.si  crl:/opt/osg036/globus/TRUSTED_CA/747183a5.r0\n         16.0 issuer=/DC=org/DC=balticgrid/CN=Baltic Grid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/2a237f16.r0\n         15.9 issuer=/C=EE/O=Grid/CN=Estonian Grid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/566bf40f.r0\n         12.9 issuer=/C=PK/O=NCP/CN=ncp.edu.pk  crl:/opt/osg036/globus/TRUSTED_CA/d2a353a5.r0\n         11.3 issuer=/C=US/ST=California/L=Los Angeles/O=University of Southern California/CN=University of Southern California PKI-Lite CA, release 1/emailAddress=nmiadmin@usc.edu  crl:/opt/osg036/globus/TRUSTED_CA/2ca73e82.r0\n         10.8 issuer=/C=RU/O=DataGrid/CN=Russian DataGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/d64ccb53.r0\n          9.7 issuer=/C=TR/O=TRGrid/CN=TR-Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/1691b9ba.r0\n          9.3 issuer=/C=US/O=Pittsburgh Supercomputing Center/CN=PSC Root Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/aa99c057.r0\n          9.2 issuer=/C=ES/O=DATAGRID-ES/CN=DATAGRID-ES CA  crl:/opt/osg036/globus/TRUSTED_CA/13eab55e.r0\n          8.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein PCA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/1149214e.r0\n          7.7 issuer=/C=JP/O=AIST/OU=GRID/CN=Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/a317c467.r0\n          7.2 issuer=/DC=cz/DC=cesnet-ca/CN=CESNET CA  crl:/opt/osg036/globus/TRUSTED_CA/9b59ecad.r0\n ***      6.8 issuer=/C=US/O=SDSC/OU=SDSC-CA/CN=Certificate Authority/UID=certman  crl:/opt/osg036/globus/TRUSTED_CA/3deda549.r0\n ***      6.2 issuer=/C=CA/O=Grid/CN=Grid Canada Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/bffbd7d0.r0\n ***      4.4 issuer=/C=HU/O=NIIF/OU=Certificate Authorities/CN=NIIF Root CA  crl:/opt/osg036/globus/TRUSTED_CA/cc800af0.r0\n ***      4.2 issuer=/C=US/O=UTAustin/OU=TACC/CN=TACC Certification Authority/UID=caman  crl:/opt/osg036/globus/TRUSTED_CA/9a1da9f9.r0\n ***      1.1 issuer=/CN=PurdueCA/O=Purdue University/ST=Indiana/C=US  crl:/opt/osg036/globus/TRUSTED_CA/95009ddc.r0\n ***      1.0 issuer=/C=CH/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCH Server CA/emailAddress=switch.server.ca@switch.ch  crl:/opt/osg036/globus/TRUSTED_CA/072fe468.r0\n ***      1.0 issuer=/C=CH/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCH Personal CA/emailAddress=switch.personal.ca@switch.ch  crl:/opt/osg036/globus/TRUSTED_CA/4aa5ef7d.r0\n ***      1.0 issuer=/CN=SWITCH Personal CA/emailAddress=switch.personal.ca@switch.ch/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/7c0f6d74.r0\n ***      1.0 issuer=/CN=SWITCH Server CA/emailAddress=switch.server.ca@switch.ch/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/f8b4299c.r0\n ***      1.0 issuer=/CN=Purdue TeraGrid RA/OU=Purdue TeraGrid/O=Purdue University/ST=Indiana/C=US  crl:/opt/osg036/globus/TRUSTED_CA/67e8acfa.r0\nnearest CRL expiration 0.951088 days\n\n\n\n\n\n\nSite administrators may find it useful to run this command in a daily cron job following the edg-crl-upgraded daemon as a way to monitor the status of the CRL\u2019s.\n\n\nReferences\n\n\nFor additional information on the functionality of a script execute it with the -help option.\n\n\nFiles in the package:\n\n\n\n\n\n\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nREADME\n\n\ndescribes the package, includes release notes\n\n\n\n\n\n\ncert-check-time\n\n\nchecks lifetime of certificates and revocation lists\n\n\n\n\n\n\ncert-gridadmin\n\n\nimmediate issuance of service certificates for authorized requestors\n\n\n\n\n\n\ncert-lookup\n\n\nqueries directory based on DN of certificates\n\n\n\n\n\n\ncert-request\n\n\ngenerates and submits a certificate signing request\n\n\n\n\n\n\ncert-retrieve\n\n\nretrieves signed certificate previously requested\n\n\n\n\n\n\ncert-renew\n\n\nrenews existing person certificate (not host or service)\n\n\n\n\n\n\nmulti-cert-gridadmin\n\n\nimmediate issuance of multiple service certificates for authorized administrators (new with V2-3)\n\n\n\n\n\n\nInstallationNotes.txt\n\n\nextra installation requirements for multi-cert-gridadmin (new with V2-3)\n\n\n\n\n\n\n\n\nFAQ\n\n\nHow to perform common tasks. In \nred\n the items you have to change.\n\n\nRequest a certificate for myself (personal certificate)\n\n\nUCL_PROMPT\n cert-request -ou p\n\n\n\n\nFull details in the \ncommand line document\n or in the \nWeb interface document\n (for a browser based alt.).\n\n\nRequest a certificate for my computer (host certificate)\n\n\nUCL_PROMPT\n cert-request -ou s\n\n\n\n\nFull details in the \nhost and service certificates document\n.\n\n\nRequest a certificate for the http service on my computer (service certificate)\n\n\nUCL_PROMPT\n cert-request -ou s -service http -host \nmy-computer.some.domain\n -label \nhttp-my-computer\n\n\n\n\n\nFull details in the \nhost and service certificates document\n.\n\n\nRetrieve a certificate\n\n\n\n\nCheck the email notice you got when the certificate was granted for the serial number (\n0xNNNN\n)\n\n\n\n\nUCL_PROMPT\n cert-retrieve -serial \n0xNNNN\n [-label \nlabel-matching-cert-request\n]\n\n\n\n\nUse the \n-p12\n option to create the PKCS12 format file useful for importing your certificate into a web browser or email program.\n\n\nIf you need to get lots of service certificates\n\n\n\n\nAsk your RA to grant you the \ngridadmin\n privilege. 2. Use \ncert-gridadmin\n and you can get service certificates issued immediately without using the web interface.\n\n\n\n\nMy personal certificate is about to expire, how do I get another with the same DN?\n\n\n\n\nUse \ncert-renew\n\n\n\n\nFull details in the \ncommand line document\n or in the \nWeb interface document\n (for a browser based alt.).", 
            "title": "Certificate Request Scripts"
        }, 
        {
            "location": "/Common/cert_scripts/#certificate-scripts-package", 
            "text": "", 
            "title": "Certificate Scripts package"
        }, 
        {
            "location": "/Common/cert_scripts/#about-this-document", 
            "text": "This the home page for documenting the cert-scripts package that provides a command-line interface to the DOEGrids CA website and some additional utilities for dealing with X509 certificates. This package was developed originally by the PPDG project and is now maintained by the OSG RA.  As an alternative to the web browser interface, these scripts are contributed to the DOEGrids PKI to allow a command-line interface to the certificate authority for submitting certificate requests, retrieving signed certificates, renewing certificates, directory lookup of existing certificates, and checking the remaining lifetime of certificates and certificate revocation lists. They work directly with the PEM format files used by Globus. These are perl scripts and bash shell scripts (some awk), depend upon openssl, ldapsearch and the perl LWP:: module with  SSL support . Click on the File link below for the usage description of the script, or to download the tar file package containing the scripts.", 
            "title": "About This Document"
        }, 
        {
            "location": "/Common/cert_scripts/#how-to-get-help", 
            "text": "To get assistance please use  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Common/cert_scripts/#requirements", 
            "text": "A host to install the Cert Scripts package. It is normally included in the CE.  OS is  . Currently most of our testing has been done on Scientific Linux 5.  Root access  Allow outbound network connection to the CA", 
            "title": "Requirements"
        }, 
        {
            "location": "/Common/cert_scripts/#installation-procedure-osgallinstallcertscripts", 
            "text": "YumRepositories  InstallCertAuth  OSGBriefInstallCertScripts", 
            "title": "Installation Procedure OSGAllInstallCertScripts"
        }, 
        {
            "location": "/Common/cert_scripts/#install-the-certificate-scripts-package-the-cert-scripts-package-can-be-installed-with-the-following-command", 
            "text": "[root@client ~]$ yum install osg-cert-scripts  OSGBriefInstallCertScripts   OSGAllInstallCertScripts", 
            "title": "Install the certificate scripts package The Cert Scripts package can be installed with the following command:"
        }, 
        {
            "location": "/Common/cert_scripts/#usage-of-certificate-scripts-package-this-package-is-mainly-used-to-request-certificates-via-the-command-line-either-host-and-service-certificates-or-user-certificates", 
            "text": "", 
            "title": "Usage of certificate scripts package This package is mainly used to request certificates via the command line: either host and service certificates or user certificates."
        }, 
        {
            "location": "/Common/cert_scripts/#get-host-and-service-certificates-using-command-line", 
            "text": "", 
            "title": "Get host and service certificates using command line"
        }, 
        {
            "location": "/Common/cert_scripts/#example-usage-of-check-cert-time-the-cert-check-time-script-is-helpful-in-setting-up-and-monitoring-the-ca-certificates-and-crls-that-get-installed-in-your-trusted-certificates-directory-this-section-describes-using-these-scripts-to-check-the-ca-and-crl-status-checking-ca-certificates", 
            "text": "There are numerous CA certificates installed with VDT and you may not want to allow all of them on your site. The  cert-check-time  is a helpful command for reviewing them. This must be run in a directory where you have write access even though it does not create any permanent files. You may want to redirect stdout to a file you can then review.  UCL_PROMPT   b cert-check-time -cR -s /usr/share/osg-cert-scripts/ /b   For each CA, the output shows:   remaining lifetime of the CA certificate (in days),  the human readable name of the CA,  and the location of the actual certificate file.   TWISTY_OPTS_OUTPUT           days  name       CA certificate file\n       6712.9 subject= /DC=HK/DC=HKU/DC=GRID/CN=HKU Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/4798da47.0\n       6674.9 subject= /DC=cz/DC=cesnet-ca/O=CESNET CA/CN=CESNET CA Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/edca0fc0.0\n       6640.8 subject= /C=FR/O=CNRS/CN=CNRS2  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/163af95c.0\n       6639.8 subject= /C=FR/O=CNRS/CN=CNRS2-Projets  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/09ff08b7.0\n       6638.8 subject= /C=FR/O=CNRS/CN=GRID2-FR  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d11f973e.0\n       6620.4 subject= /C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Client Authentication and Email  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9ec3a561.0\n       6620.4 subject= /C=NL/O=TERENA/CN=TERENA eScience Personal CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/169d7f9c.0\n       6620.4 subject= /C=GB/ST=Greater Manchester/L=Salford/O=Comodo CA Limited/CN=AAA Certificate Services  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/75680d2e.0\n       6542.2 subject= /DC=by/DC=grid/O=uiip.bas-net.by/CN=Belarusian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/709bed08.0\n       6274.0 subject= /C=MK/O=MARGI/CN=MARGI-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7d0d064a.0\n       6191.7 subject= /C=UK/O=eScienceRoot/OU=Authority/CN=UK e-Science Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/98ef0ee5.0\n       6165.7 subject= /C=TW/O=AS/CN=Academia Sinica Grid Computing Certification Authority Mercury  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9cd75e87.0\n       5929.1 subject= /C=CH/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCHgrid Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d0b701c0.0\n       5799.7 subject= /DC=ch/DC=cern/CN=CERN Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d254cc30.0\n       5719.6 subject= /C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid Root CA 2006  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/28a58577.0\n       5690.1 subject= /C=HR/O=edu/OU=srce/CN=SRCE CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ff94d436.0\n       5581.6 subject= /DC=CN/DC=Grid/CN=Root Certificate Authority at CNIC  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b2771d44.0\n       5536.3 subject= /C=CA/O=Grid/CN=Grid Canada Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/bffbd7d0.0\n       5437.9 subject= /C=TR/O=TRGrid/CN=TR-Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1691b9ba.0\n       5327.0 subject= /DC=cz/DC=cesnet-ca/CN=CESNET CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9b59ecad.0\n       5084.8 subject= /C=JP/O=AIST/OU=GRID/CN=Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a317c467.0\n       4947.1 subject= /C=PT/O=LIPCA/CN=LIP Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/11b4a5a2.0\n       4361.6 subject= /DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d1b603c3.0\n       3774.1 subject= /C=BM/O=QuoVadis Limited/OU=Root Certification Authority/CN=QuoVadis Root Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/5cf9d536.0\n       3737.4 subject= /C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/16da7552.0\n       3572.8 subject= /C=PL/O=GRID/CN=Polish Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/8a661490.0\n       3482.8 subject= /C=US/ST=UT/L=Salt Lake City/O=The USERTRUST Network/OU=http://www.usertrust.com/CN=UTN-USERFirst-Hardware  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ff783690.0\n       3482.8 subject= /C=SE/O=AddTrust AB/OU=AddTrust External TTP Network/CN=AddTrust External CA Root  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3c58f906.0\n       3482.8 subject= /C=NL/O=TERENA/CN=TERENA eScience SSL CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/20ce830e.0\n       3319.1 subject= /DC=cz/DC=cesnet-ca/O=CESNET CA/CN=CESNET CA 3  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/712ae4cc.0\n       3089.1 subject= /DC=gov/DC=fnal/O=Fermilab/OU=Certificate Authorities/CN=Kerberized CA HSM  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/99f9f5a3.0\n       3087.2 subject= /C=BM/O=QuoVadis Limited/OU=Issuing Certification Authority/CN=QuoVadis Grid ICA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e72045ce.0\n       3039.3 subject= /DC=MD/DC=MD-Grid/O=RENAM/OU=Certification Authority/CN=MD-Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9ff26ea4.0\n       2939.0 subject= /C=BE/OU=BEGRID/O=BELNET/CN=BEgrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e8d818e6.0\n       2906.9 subject= /C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN SLCS-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a02131f7.0\n       2904.9 subject= /C=VE/O=Grid/O=Universidad de Los Andes/OU=CeCalCULA/CN=ULAGrid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3f0f4285.0\n       2879.6 subject= /DC=IN/DC=GARUDAINDIA/CN=Indian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/da75f6a8.0\n       2874.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/684261aa.0\n       2850.4 subject= /DC=TW/DC=ORG/DC=NCHC/CN=NCHC CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/71a89a47.0\n       2769.2 subject= /DC=NET/DC=PRAGMA-GRID/CN=PRAGMA-UCSD CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7721d4d3.0\n       2763.2 subject= /DC=LV/DC=latgrid/CN=Certification Authority for Latvian Grid  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/742edd45.0\n       2756.7 subject= /DC=me/DC=ac/DC=MREN/CN=MREN-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3232b9bc.0\n       2581.9 subject= /C=PK/O=NCP/CN=PK-GRID-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/f5ead794.0\n       2560.4 subject= /C=MX/O=UNAMgrid/OU=UNAM/CN=CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/24c3ccde.0\n       2549.2 subject= /C=MA/O=MaGrid/CN=MaGrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/7b54708e.0\n       2510.1 subject= /DC=RO/DC=RomanianGRID/O=ROSA/OU=Certification Authority/CN=RomanianGRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1f3834d0.0\n       2485.1 subject= /C=AR/O=e-Ciencia/OU=UNLP/L=CeSPI/CN=PKIGrid  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b7bcb7b2.0\n       2449.6 subject= /C=KR/O=KISTI/O=GRID/CN=KISTI Grid Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/722e5071.0\n       2446.1 subject= /DC=BR/DC=UFF/DC=IC/O=UFF LACGrid CA/CN=UFF Latin American and Caribbean Catch-all Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a9082267.0\n       2367.8 subject= /C=CL/O=REUNACA/CN=REUNA Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/295adc19.0\n       2343.9 subject= /C=RS/O=AEGIS/CN=AEGIS-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/393f7863.0\n       2279.2 subject= /DC=bg/DC=acad/CN=BG.ACAD CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2418a3f3.0\n       2238.5 subject= /C=TH/O=NECTEC/OU=GOC/CN=NECTEC GOC CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/8a047de1.0\n       2147.9 subject= /C=IT/O=INFN/CN=INFN CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2f3fadf6.0\n       2147.8 subject= /DC=ch/DC=cern/CN=CERN Trusted Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1d879c6c.0\n       2068.6 subject= /C=JP/O=National Research Grid Initiative/OU=CGRD/CN=NAREGI CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/a87d9192.0\n       2067.7 subject= /C=GR/O=HellasGrid/OU=Certification Authorities/CN=HellasGrid CA 2006  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/82b36fca.0\n       2064.1 subject= /C=BR/O=ICPEDU/O=UFF BrGrid CA/CN=UFF Brazilian Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/0a2bac92.0\n       1928.6 subject= /DC=CN/DC=Grid/DC=SDG/CN=Scientific Data Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/c48c63f3.0\n       1877.5 subject= /C=AU/O=APACGrid/OU=CA/CN=APACGrid/emailAddress=camanager@vpac.org  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1e12d831.0\n       1769.8 subject= /DC=org/DC=balticgrid/CN=Baltic Grid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2a237f16.0\n       1725.0 subject= /C=RU/O=RDIG/CN=Russian Data-Intensive Grid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/55994d72.0\n       1682.6 subject= /DC=es/DC=irisgrid/CN=IRISGridCA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9dd23746.0\n       1649.8 subject= /C=CN/O=HEP/CN=gridca-cn/emailAddress=gridca@ihep.ac.cn  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ba2f39ca.0\n       1575.2 subject= /C=AT/O=AustrianGrid/OU=Certification Authority/CN=Certificate Issuer  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/6e3b436b.0\n       1571.4 subject= /C=HU/O=NIIF/OU=Certificate Authorities/CN=NIIF Root CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/cc800af0.0\n       1389.9 subject= /CN=Purdue TeraGrid RA/OU=Purdue TeraGrid/O=Purdue University/ST=Indiana/C=US  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/67e8acfa.0\n       1378.2 subject= /CN=PurdueCA/O=Purdue University/ST=Indiana/C=US  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/95009ddc.0\n       1374.7 subject= /DC=ORG/DC=SEE-GRID/CN=SEE-GRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/468d15b3.0\n       1301.9 subject= /C=DE/O=GermanGrid/CN=GridKa-CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/dd4b34ea.0\n       1211.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=GridShib CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e8ac4b61.0\n       1141.9 subject= /C=IL/O=IUCC/CN=IUCC/emailAddress=ca@mail.iucc.ac.il  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/6fee79b0.0\n       1111.4 subject= /C=AM/O=ArmeSFo/CN=ArmeSFo CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/d0c2a341.0\n       1049.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC MICS CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/2ac09305.0\n       1049.5 subject= /DC=EDU/DC=UTEXAS/DC=TACC/O=UT-AUSTIN/CN=TACC Classic CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e5cc84c2.0\n       1025.9 subject= /C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein PCA Grid - G01  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1149214e.0\n        943.2 subject= /DC=net/DC=ES/OU=Certificate Authorities/CN=NERSC Online CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/b93d6240.0\n        910.1 subject= /C=IR/O=IPM/O=IRAN-GRID/CN=IRAN-GRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/ce33db76.0\n        815.9 subject= /C=CY/O=CyGrid/O=HPCL/CN=CyGridCA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/afe55e66.0\n        800.7 subject= /DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1c3f2ca8.0\n        795.3 subject= /DC=org/DC=ugrid/CN=UGRID CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/0a12b607.0\n        761.9 subject= /C=SK/O=SlovakGrid/CN=SlovakGrid CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/e13e0fcf.0\n        713.7 subject= /C=UK/O=eScienceCA/OU=Authority/CN=UK e-Science CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/367b75c3.0\n        619.1 subject= /C=IE/O=Grid-Ireland/CN=Grid-Ireland Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1e43b9cc.0\n        523.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=MyProxy  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/f2e89fe3.0\n        523.2 subject= /C=US/O=National Center for Supercomputing Applications/OU=Certificate Authorities/CN=CACL  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/9b95bbf2.0\n        450.1 subject= /C=CH/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCHslcs CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/304cf809.0\n        279.7 subject= /C=SI/O=SiGNET/CN=SiGNET CA  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/3d5be7bc.0\n        179.9 subject= /O=Grid/O=NorduGrid/CN=NorduGrid Certification Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/1f0e8352.0\n        135.0 subject= /C=JP/O=KEK/OU=CRC/CN=KEK GRID Certificate Authority  cert:/ncsa/apps-local/itb-1.2/globus/TRUSTED_CA/617ff41b.0\nnearest CA certificate expiration 134.984 days   In addition, at the bottom of the listing it points out which CA\u2019s do not have a CRL. This is useful AFTER the edg-crl-upgraded daemon has run at least once because then it shows those CA\u2019s which have not published a certificate revocation list. Note that two of the CAs, Kerberos CAs from PSC (85ca9edc.0) and FNAL (e1fce4e9.0) don\u2019t really need CRLs since they only generate short lived certificates.", 
            "title": "Example usage of check-cert-time The cert-check-time script is helpful in setting up and monitoring the CA certificates and CRL\u2019s that get installed in your trusted certificates directory. This section describes using these scripts to check the CA and CRL status. \u2014+++! Checking CA certificates"
        }, 
        {
            "location": "/Common/cert_scripts/#checking-crls", 
            "text": "Certificate revocation lists contain the list of certificates (by serial number) that have been issued by a CA but were then revoked, meaning you should not accept them. CRL\u2019s are updated frequently and typically have a lifetime limited to a month or less. When a CRL has expired, the CRL file will still exist in the trusted certificates directory, but Globus will fail  all  authentication attempts for  all  certificates issued by the corresponding CA.  For this reason, and others, it is important that CRL files are current and not expired. Another variation of the cert-check-time script will list the remaining lifetime of CRL\u2019s in the trusted certificates directory. This must be run in a directory where you have write access even though it does not create any files. You may want to redirect stdout to a file you can then review.  UCL_PROMPT   b cert-check-time -r -s /usr/share/osg-cert-scripts/  /b   For each CRL, the sample output below shows:   the remaining lifetime,  the name of the CA that issued the CRL  and the actual CRL file.   TWISTY_OPTS_OUTPUT           days  name       CRL file\n        365.1 issuer=/C=CA/O=Grid/CN=Grid Canada CA  crl:/opt/osg036/globus/TRUSTED_CA/5f54f417.r0\n        340.8 issuer=/CN=SWITCH CA/emailAddress=switch.ca@switch.ch/O=Switch - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/c4435d12.r0\n        317.1 issuer=/C=FR/O=CNRS/CN=CNRS-Projets  crl:/opt/osg036/globus/TRUSTED_CA/34a509c3.r0\n        317.1 issuer=/C=FR/O=CNRS/CN=CNRS  crl:/opt/osg036/globus/TRUSTED_CA/cf4ba8c8.r0\n        257.1 issuer=/CN=SwissSign Silver CA/emailAddress=silver@swisssign.com/O=SwissSign/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/e9d08b40.r0\n        257.1 issuer=/CN=SwissSign Bronze CA/emailAddress=bronze@swisssign.com/O=SwissSign/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/e36e7a72.r0\n        204.4 issuer=/DC=net/DC=ES/O=ESnet/OU=Certificate Authorities/CN=ESnet Root CA 1  crl:/opt/osg036/globus/TRUSTED_CA/d1b603c3.r0\n        149.4 issuer=/C=CH/O=SwissSign/CN=SwissSign CA (RSA IK May 6 1999 18:00:58)/emailAddress=ca@SwissSign.com  crl:/opt/osg036/globus/TRUSTED_CA/7b2d086c.r0\n         31.0 issuer=/C=RU/O=RDIG/CN=Russian Data-Intensive Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/55994d72.r0\n         30.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein Server CA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/fe102e03.r0\n         30.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein User CA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/34f8e29c.r0\n         29.9 issuer=/C=IT/O=INFN/CN=INFN Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/49f18420.r0\n         29.9 issuer=/C=CY/O=CyGrid/O=HPCL/CN=CyGridCA  crl:/opt/osg036/globus/TRUSTED_CA/afe55e66.r0\n         29.9 issuer=/C=IE/O=Grid-Ireland/CN=Grid-Ireland Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/1e43b9cc.r0\n         29.7 issuer=/DC=es/DC=irisgrid/CN=IRISGridCA  crl:/opt/osg036/globus/TRUSTED_CA/9dd23746.r0\n         29.7 issuer=/C=IL/O=IUCC/CN=IUCC/emailAddress=ca@mail.iucc.ac.il  crl:/opt/osg036/globus/TRUSTED_CA/6fee79b0.r0\n         29.6 issuer=/DC=org/DC=DOEGrids/OU=Certificate Authorities/CN=DOEGrids CA 1  crl:/opt/osg036/globus/TRUSTED_CA/1c3f2ca8.r0\n         29.5 issuer=/C=TW/O=AS/CN=Academia Sinica Grid Computing Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/a692434d.r0\n         29.3 issuer=/C=FR/O=CNRS/CN=GRID-FR  crl:/opt/osg036/globus/TRUSTED_CA/12a1d8c2.r0\n         29.1 issuer=/C=DE/O=GermanGrid/CN=GridKa-CA  crl:/opt/osg036/globus/TRUSTED_CA/dd4b34ea.r0\n         28.8 issuer=/C=JP/O=National Research Grid Initiative/OU=GRID/CN=NAREGI CA  crl:/opt/osg036/globus/TRUSTED_CA/0cb5fc2c.r0\n         28.3 issuer=/C=AU/O=APACGrid/OU=CA/CN=APACGrid/emailAddress=camanager@vpac.org  crl:/opt/osg036/globus/TRUSTED_CA/1e12d831.r0\n         27.9 issuer=/DC=ORG/DC=SEE-GRID/CN=SEE-GRID CA  crl:/opt/osg036/globus/TRUSTED_CA/468d15b3.r0\n         27.9 issuer=/C=GR/O=HellasGrid/CN=HellasGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/ede78092.r0\n         27.1 issuer=/C=NL/O=NIKHEF/CN=NIKHEF medium-security certification auth  crl:/opt/osg036/globus/TRUSTED_CA/16da7552.r0\n         27.0 issuer=/C=PL/O=GRID/CN=Polish Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/8a661490.r0\n         26.8 issuer=/O=Grid/O=NorduGrid/CN=NorduGrid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/1f0e8352.r0\n         25.9 issuer=/C=AM/O=ArmeSFo/CN=ArmeSFo CA  crl:/opt/osg036/globus/TRUSTED_CA/d0c2a341.r0\n         25.9 issuer=/C=BE/O=BELNET/OU=BEGrid/CN=BEGrid CA/emailAddress=gridca@belnet.be  crl:/opt/osg036/globus/TRUSTED_CA/03aa0ecb.r0\n         24.0 issuer=/C=UK/O=eScience/OU=Authority/CN=CA/emailAddress=ca-operator@grid-support.ac.uk  crl:/opt/osg036/globus/TRUSTED_CA/01621954.r0\n         23.9 issuer=/C=HU/O=KFKI RMKI CA/CN=KFKI RMKI CA  crl:/opt/osg036/globus/TRUSTED_CA/5e5501f3.r0\n         23.9 issuer=/C=SK/O=SlovakGrid/CN=SlovakGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/e13e0fcf.r0\n         23.1 issuer=/C=PT/O=LIPCA/CN=LIP Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/11b4a5a2.r0\n         23.0 issuer=/C=PT/O=LIP/OU=LISCC/CN=LIP Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/41380387.r0\n         23.0 issuer=/C=AT/O=AustrianGrid/OU=Certification Authority/CN=Certificate Issuer  crl:/opt/osg036/globus/TRUSTED_CA/6e3b436b.r0\n         22.9 issuer=/C=CZ/O=CESNET/CN=CESNET CA  crl:/opt/osg036/globus/TRUSTED_CA/ed99a497.r0\n         21.9 issuer=/C=KR/O=KISTI/CN=KISTI GRID ROOT CA  crl:/opt/osg036/globus/TRUSTED_CA/47183fda.r0\n         21.7 issuer=/C=CN/O=HEP/CN=gridca-cn/emailAddress=gridca@ihep.ac.cn  crl:/opt/osg036/globus/TRUSTED_CA/ba2f39ca.r0\n         21.1 issuer=/C=CH/O=CERN/OU=GRID/CN=CERN CA  crl:/opt/osg036/globus/TRUSTED_CA/fa3af1d7.r0\n         19.1 issuer=/C=SI/O=SiGNET/CN=SiGNET CA/emailAddress=signet-ca@ijs.si  crl:/opt/osg036/globus/TRUSTED_CA/747183a5.r0\n         16.0 issuer=/DC=org/DC=balticgrid/CN=Baltic Grid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/2a237f16.r0\n         15.9 issuer=/C=EE/O=Grid/CN=Estonian Grid Certification Authority  crl:/opt/osg036/globus/TRUSTED_CA/566bf40f.r0\n         12.9 issuer=/C=PK/O=NCP/CN=ncp.edu.pk  crl:/opt/osg036/globus/TRUSTED_CA/d2a353a5.r0\n         11.3 issuer=/C=US/ST=California/L=Los Angeles/O=University of Southern California/CN=University of Southern California PKI-Lite CA, release 1/emailAddress=nmiadmin@usc.edu  crl:/opt/osg036/globus/TRUSTED_CA/2ca73e82.r0\n         10.8 issuer=/C=RU/O=DataGrid/CN=Russian DataGrid CA  crl:/opt/osg036/globus/TRUSTED_CA/d64ccb53.r0\n          9.7 issuer=/C=TR/O=TRGrid/CN=TR-Grid CA  crl:/opt/osg036/globus/TRUSTED_CA/1691b9ba.r0\n          9.3 issuer=/C=US/O=Pittsburgh Supercomputing Center/CN=PSC Root Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/aa99c057.r0\n          9.2 issuer=/C=ES/O=DATAGRID-ES/CN=DATAGRID-ES CA  crl:/opt/osg036/globus/TRUSTED_CA/13eab55e.r0\n          8.0 issuer=/C=DE/O=DFN-Verein/OU=DFN-PKI/CN=DFN-Verein PCA Grid - G01  crl:/opt/osg036/globus/TRUSTED_CA/1149214e.r0\n          7.7 issuer=/C=JP/O=AIST/OU=GRID/CN=Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/a317c467.r0\n          7.2 issuer=/DC=cz/DC=cesnet-ca/CN=CESNET CA  crl:/opt/osg036/globus/TRUSTED_CA/9b59ecad.r0\n ***      6.8 issuer=/C=US/O=SDSC/OU=SDSC-CA/CN=Certificate Authority/UID=certman  crl:/opt/osg036/globus/TRUSTED_CA/3deda549.r0\n ***      6.2 issuer=/C=CA/O=Grid/CN=Grid Canada Certificate Authority  crl:/opt/osg036/globus/TRUSTED_CA/bffbd7d0.r0\n ***      4.4 issuer=/C=HU/O=NIIF/OU=Certificate Authorities/CN=NIIF Root CA  crl:/opt/osg036/globus/TRUSTED_CA/cc800af0.r0\n ***      4.2 issuer=/C=US/O=UTAustin/OU=TACC/CN=TACC Certification Authority/UID=caman  crl:/opt/osg036/globus/TRUSTED_CA/9a1da9f9.r0\n ***      1.1 issuer=/CN=PurdueCA/O=Purdue University/ST=Indiana/C=US  crl:/opt/osg036/globus/TRUSTED_CA/95009ddc.r0\n ***      1.0 issuer=/C=CH/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCH Server CA/emailAddress=switch.server.ca@switch.ch  crl:/opt/osg036/globus/TRUSTED_CA/072fe468.r0\n ***      1.0 issuer=/C=CH/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/CN=SWITCH Personal CA/emailAddress=switch.personal.ca@switch.ch  crl:/opt/osg036/globus/TRUSTED_CA/4aa5ef7d.r0\n ***      1.0 issuer=/CN=SWITCH Personal CA/emailAddress=switch.personal.ca@switch.ch/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/7c0f6d74.r0\n ***      1.0 issuer=/CN=SWITCH Server CA/emailAddress=switch.server.ca@switch.ch/O=SWITCH - Teleinformatikdienste fuer Lehre und Forschung/C=CH  crl:/opt/osg036/globus/TRUSTED_CA/f8b4299c.r0\n ***      1.0 issuer=/CN=Purdue TeraGrid RA/OU=Purdue TeraGrid/O=Purdue University/ST=Indiana/C=US  crl:/opt/osg036/globus/TRUSTED_CA/67e8acfa.r0\nnearest CRL expiration 0.951088 days   Site administrators may find it useful to run this command in a daily cron job following the edg-crl-upgraded daemon as a way to monitor the status of the CRL\u2019s.", 
            "title": "Checking CRL\u2019s"
        }, 
        {
            "location": "/Common/cert_scripts/#references", 
            "text": "For additional information on the functionality of a script execute it with the -help option.  Files in the package:     File  Description      README  describes the package, includes release notes    cert-check-time  checks lifetime of certificates and revocation lists    cert-gridadmin  immediate issuance of service certificates for authorized requestors    cert-lookup  queries directory based on DN of certificates    cert-request  generates and submits a certificate signing request    cert-retrieve  retrieves signed certificate previously requested    cert-renew  renews existing person certificate (not host or service)    multi-cert-gridadmin  immediate issuance of multiple service certificates for authorized administrators (new with V2-3)    InstallationNotes.txt  extra installation requirements for multi-cert-gridadmin (new with V2-3)", 
            "title": "References"
        }, 
        {
            "location": "/Common/cert_scripts/#faq", 
            "text": "How to perform common tasks. In  red  the items you have to change.", 
            "title": "FAQ"
        }, 
        {
            "location": "/Common/cert_scripts/#request-a-certificate-for-myself-personal-certificate", 
            "text": "UCL_PROMPT  cert-request -ou p  Full details in the  command line document  or in the  Web interface document  (for a browser based alt.).", 
            "title": "Request a certificate for myself (personal certificate)"
        }, 
        {
            "location": "/Common/cert_scripts/#request-a-certificate-for-my-computer-host-certificate", 
            "text": "UCL_PROMPT  cert-request -ou s  Full details in the  host and service certificates document .", 
            "title": "Request a certificate for my computer (host certificate)"
        }, 
        {
            "location": "/Common/cert_scripts/#request-a-certificate-for-the-http-service-on-my-computer-service-certificate", 
            "text": "UCL_PROMPT  cert-request -ou s -service http -host  my-computer.some.domain  -label  http-my-computer   Full details in the  host and service certificates document .", 
            "title": "Request a certificate for the http service on my computer (service certificate)"
        }, 
        {
            "location": "/Common/cert_scripts/#retrieve-a-certificate", 
            "text": "Check the email notice you got when the certificate was granted for the serial number ( 0xNNNN )   UCL_PROMPT  cert-retrieve -serial  0xNNNN  [-label  label-matching-cert-request ]  Use the  -p12  option to create the PKCS12 format file useful for importing your certificate into a web browser or email program.", 
            "title": "Retrieve a certificate"
        }, 
        {
            "location": "/Common/cert_scripts/#if-you-need-to-get-lots-of-service-certificates", 
            "text": "Ask your RA to grant you the  gridadmin  privilege. 2. Use  cert-gridadmin  and you can get service certificates issued immediately without using the web interface.", 
            "title": "If you need to get lots of service certificates"
        }, 
        {
            "location": "/Common/cert_scripts/#my-personal-certificate-is-about-to-expire-how-do-i-get-another-with-the-same-dn", 
            "text": "Use  cert-renew   Full details in the  command line document  or in the  Web interface document  (for a browser based alt.).", 
            "title": "My personal certificate is about to expire, how do I get another with the same DN?"
        }, 
        {
            "location": "/Computing_Element/job_router/", 
            "text": "Writing Routes For HTCondor-CE\n\n\n\n\nAbout This Guide\n\n\nThe \nJobRouter\n is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route\u2018s requirements, the route creates a transformed job (referred to as the \u2019routed job\u2019) that is then submitted to the batch system. The CE package comes with default routes located in \n/etc/condor-ce/config.d/02-ce-*.conf\n that provide enough basic functionality for a small site.\n\n\nIf you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.\n\n\nRoutePitfalls\n\n\nQuirks and Pitfalls\n\n\n\n\nThe JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\nMake sure to run \ncondor_ce_reconfig\n after changing your routes, otherwise they will not take effect.\n\n\nBefore the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with \ncondor_ce_config_val JOB_ROUTER_ENTRIES\n to see if HTCondor-CE has ingested them properly.\n\n\nDo \nnot\n set the \nJOB_ROUTER_DEFAULTS\n configuration variable yourself. This will cause the CE to stop functioning.\n\n\nDo \nnot\n set the job environment through the JobRouter. Instead, add any changes to the /etc/osg/config.d/ \n[Local Settings]\n section and run osg-configure, as documented \nhere\n.\n\n\nHTCondor batch system only: Local universe jobs are excluded from any routing.\n\n\n\n\n\n\n#JobRouteConstruction\n\n\nHow Job Routes are Constructed\n\n\nEach job route\u2019s \nClassAd\n is constructed by combining each entry from the \nJOB_ROUTER_ENTRIES\n with the \nJOB_ROUTER_DEFAULTS\n. Attributes that are set in \nJOB_ROUTER_ENTRIES\n will override those set in \nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_ENTRIES\n\n\nJOB_ROUTER_ENTRIES\n is a configuration variable whose default is set in \n/etc/condor-ce/config.d/02-ce-*.conf\n but may be overriden by the administrator in \n/etc/condor-ce/config.d/99-local.conf\n. This document outlines the many changes you can make to \nJOB_ROUTER_ENTRIES\n to fit your site\u2019s needs.\n\n\n#JobRouterDefaults\n\n\nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_DEFAULTS\n is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE\u2019s functionality. To view its contents, run the following command:\n\n\nUCL_PROMPT\n condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g'\n\n\n\n\nTWISTY_OPTS_OUTPUT\n showlink=\"Show sample output\"\n\n\n[ MaxIdleJobs = 2000;\n MaxJobs = 10000;\n /* by default, accept all jobs */ Requirements = True;\n /* now modify routed job attributes */ /* remove routed job if the client disappears for 48 hours or it is idle for 6 */ /*set_PeriodicRemove = (LastClientContact - time() \n 48*60*60) || (JobStatus == 1 \n (time() - QDate) \n 6*60);\n*/ delete_PeriodicRemove = true;\n delete_CondorCE = true;\n set_RoutedJob = true;\n copy_environment = \norig_environment\n;\n set_osg_environment = \nOSG_GRID='/etc/osg/wn-client/' OSG_SQUID_LOCATION='fermicloud133.fnal.gov:3128' OSG_SITE_READ='None' OSG_APP='/share/osg/app' OSG_GLEXEC_LOCATION='None' OSG_DATA='UNAVAILABLE' OSG_HOSTNAME='fermicloud136.fnal.gov' OSG_STORAGE_ELEMENT='False' OSG_SITE_NAME='herp' GLOBUS_LOCATION='/usr' OSG_WN_TMP='None' OSG_DEFAULT_SE='None' OSG_SITE_WRITE='None'\n;\n eval_set_environment = debug(strcat(\nHOME=\n, userHome(Owner, \n/\n), \n \n, ifThenElse(orig_environment is undefined, osg_environment, strcat(osg_environment, \n \n, orig_environment) )));\n /* Set new requirements */ /* set_requirements = LastClientContact - time() \n 30*60;\n */ set_requirements = True;\n set_InputRSL = ifThenElse(GlobusRSL is null, [], eval_rsl(GlobusRSL));\n /* Note default memory request of 2GB */ /* Note yet another nested condition allow pass attributes (maxMemory,xcount,jobtype,queue) via gWMS Factory described within ClassAd if undefined via RSL */ eval_set_RequestMemory = ifThenElse(InputRSL.maxMemory isnt null, InputRSL.maxMemory, ifThenElse(maxMemory isnt null, maxMemory, ifThenElse(default_maxMemory isnt null, default_maxMemory, 2000)));\n eval_set_remote_queue = ifThenElse(InputRSL.queue isnt null, InputRSL.queue, ifThenElse(queue isnt null, queue, ifThenElse(default_queue isnt null, default_queue, \n)));\n /* HTCondor uses RequestCpus;\n blahp uses SMPGranularity and NodeNumber.  Default is 1 core. */ eval_set_RequestCpus = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n eval_set_remote_SMPGranularity = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n eval_set_remote_NodeNumber = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n /* If remote_cerequirements is a string, BLAH will parse it as an expression before examining it */ eval_set_remote_cerequirements = ifThenElse(InputRSL.maxWalTlime isnt null, strcat(\nWalltime == \n, string(60*InputRSL.maxWallTime), \n \n CondorCE == 1\n), ifThenElse(maxWallTime isnt null, strcat(\nWalltime == \n, string(60*maxWallTime), \n \n CondorCE == 1\n), ifThenElse(default_maxWallTime isnt null, strcat(\nWalltime == \n, string(60*default_maxWallTime), \n \n CondorCE == 1\n), \nCondorCE == 1\n)));\n ]\n\n\n\n\n\n\n If a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break. \n Do \nnot\n set the \nJOB_ROUTER_DEFAULTS\n configuration variable yourself. This will cause the CE to stop functioning.\n\n\nGeneric Routes\n\n\nNew routes should be placed in \n/etc/condor-ce/config.d/99-local.conf\n, not the original \n02-ce-*.conf\n.\n\n\nRequired fields\n\n\nThe minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in \n/usr/share/condor-ce/config.d/02-ce-\nbatch system\n-defaults.conf\n, provided by the \nosg-ce-\nbatch system\n packages.\n\n\nBatch system\n\n\nEach route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the \nTargetUniverse\n attribute needs to be set to \n5\n or \n\"vanilla\"\n. For all other batch systems, the \nTargetUniverse\n attribute needs to be set to \n9\n or \n\"grid\"\n and the \nGridResource\n attribute needs to be set to \n\"batch \nbatch system\n\"\n (where \nbatch system\n can be one of \npbs\n (for both users of \npbs\n and \nSLURM\n), \nlsf\n, or \nsge\n).\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n# Route to an HTCondor-CE batch system \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nTargetUniverse = 5; \\\n/b\n/span\n\n     name = \nRoute jobs to HTCondor\n; \\\n] \\\n[ \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n# Route to a PBS batch system \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nGridResource = \nbatch pbs\n; \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nTargetUniverse = 9; \\\n/b\n/span\n\n     name = \nRoute jobs to PBS\n; \\\n]\n\n\n\n\nRoute name\n\n\nTo identify routes, you will need to assign a name to the route with the \nname\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nname = \nRoute jobs to HTCondor\n; \\\n/b\n/span\n\n]\n\n\n\n\nThe name of the route will be useful in debugging since it shows up in the output of \ncondor_ce_job_router_info\n, the \nJobRouterLog\n, and in the ClassAd of the routed job, which can be viewed with \ncondor_ce_q\n or \ncondor_ce_history\n.\n\n\nWriting multiple routes\n\n\nIf your batch system needs incoming jobs to be sorted (e.g. if different VO\u2019s need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they\u2019re the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a \nqueue\n attribute set to \n\"analy\"\n and routes them to the site\u2019s HTCondor batch system. Any other jobs will be sent to that site\u2019s PBS batch system.\n\n\n The JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\nJOB_ROUTER_ENTRIES = \nspan style=\nbackground-color: #FFCCFF;\nb\n[ \\\n/b\n/span\n\n     TargetUniverse = 5; \\\n     name = \nRoute jobs to HTCondor\n; \\\n     Requirements = (TARGET.queue =?= \nanaly\n); \\\n\nspan style=\nbackground-color: #FFCCFF;\nb\n] \\\n/b\n/span\n\n\nspan style=\nbackground-color: #FFCCFF;\nb\n[ \\\n/b\n/span\n\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRoute jobs to PBS\n; \\\n     Requirements = (TARGET.queue =!= \nanaly\n); \\\n\nspan style=\nbackground-color: #FFCCFF;\nb\n]\n/b\n/span\n\n\n\n\n\nWriting comments\n\n\nTo write comments you can use C-style comments, text enclosed by \n/* */\n. If the comment is at the end of a line, it still has to be followed by the line continuation character.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nC-style comments\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* This is a comment */ \\\n/b\n/span\n\n] \n\n\n\n\nFor \ncondor_ce_version\n 8.2.x or greater, you can also use \n#\n to comment out single lines:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nHash comments\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n# BrokenAttribute = \ncommented out\n; \\\n/b\n/span\n \n] \n\n\n\n\nSetting attributes for all routes\n\n\nTo set an attribute that will be applied to all routes, you will need to use the \n[[#SettingAttributes][set_]]\n function for each route.\n\n\n Do \nnot\n try to do this by setting the \nJOB_ROUTER_DEFAULTS\n configuration variable, as this will cause the CE to stop functioning.\n\n\nThe following routes set the \nPeriodic_Hold\n attribute for both routes:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nRoute jobs to HTCondor\n; \\\n     Requirements = (TARGET.queue =?= \nanaly\n); \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\ \n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n/b\n/span\n\n] \\\n[ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRoute jobs to PBS\n; \\\n     Requirements = (TARGET.queue =!= \nanaly\n); \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\ \n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n/b\n/span\n\n] \n\n\n\n\nFiltering jobs based on\u2026\n\n\nTo filter jobs, use the \nRequirements\n attribute. Jobs will evaluate against the ClassAd expression set in the \nRequirements\n and if the expression evaluates to \nTRUE\n, the route will match. More information on the syntax of ClassAd's can be found in the \nHTCondor manual\n. For an example on how incoming jobs interact with filtering in job routes, consult \nthis document\n.\n\n\nWhen setting requirements, you need to prefix job attributes that you are filtering with \nTARGET.\n so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a =queue = \u201canaly\u201d= attribute, then the following job route will not match:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     queue = \nnot-analy\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nRequirements = (TARGET.queue =?= \nanaly\n); \\\n/b\n/span\n\n] \n\n\n\n\nThis is because when evaluating the route requirement, the job route will compare its own \nqueue\n attribute to \u201canaly\u201d and see that it does not match. You can read more about comparing two ClassAds in the \nHTCondor manual\n.\n\n\n If you have an HTCondor batch system, note the difference with \nset_requirements\n. \n The JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.\n\n\n#GlideIn\n\n\nGlidein queue\n\n\nTo filter jobs based on their glidein queue attribute, your routes will need a \nRequirements\n expression using the incoming job\u2019s \nqueue\n attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by \nTARGET\n) is an \nanaly\n (Analysis) glidein:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by queue\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nRequirements = (TARGET.queue =?= \nanaly\n); \\\n/b\n/span\n\n] \n\n\n\n\nJob submitter\n\n\nTo filter jobs based on who submitted it, your routes will need a \nRequirements\n expression using the incoming job\u2019s \nOwner\n attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is \nusatlas2\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nFiltering by job submitter\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nRequirements = (TARGET.Owner =?= \nusatlas2\n); \\\n/b\n/span\n\n] \n\n\n\n\nAlternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter\u2019s name begins with \nusatlas\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by job submitter (regular expression)\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nRequirements = regexp(\n^usatlas\n, TARGET.Owner); \\\n/b\n/span\n\n] \n\n\n\n\nVOMS attribute\n\n\nTo filter jobs based on the subject of the job\u2019s proxy, your routes will need a \nRequirements\n expression using the incoming job\u2019s \nx509UserProxyFirstFQAN\n attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains \n/cms/Role=Pilot\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nFiltering by VOMS attribute (regex)\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nRequirements = regexp(\n\\/cms\\/Role\\=pilot\n, TARGET.x509UserProxyFirstFQAN); \\\n/b\n/span\n\n] \n\n\n\n\nSetting a default\u2026\n\n\nThis section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult \nthis document\n.\n\n\nMaximum number of jobs\n\n\nTo set a default limit to the maximum number of jobs per route, you can edit the configuration variable \nCONDORCE_MAX_JOBS\n in \n/etc/condor-ce/config.d/01-ce-router.conf\n:\n\n\nspan style=\nbackground-color: #FFCCFF;\nb\nCONDORCE_MAX_JOBS = 10000\n/b\n/span\n\n\n\n\n\nNote that this is to be placed directly into the HTCondor-CE configuration, not into a job route.\n\n\nMaximum memory\n\n\nTo set a default maximum memory for routed jobs, set the attribute \ndefault_maxMemory\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest memory\n; \\\n     /* Set the requested memory to 1 GB */ \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_default_maxMemory = 1000; \\\n/b\n/span\n\n] \n\n\n\n\nNumber of cores to request\n\n\nTo set a default number of cores for routed jobs, set the attribute \ndefault_xcount\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nRequest CPU\n; \\\n     /* Set the requested cores to 8 */ \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_default_xcount = 8; \\\n/b\n/span\n\n] \n\n\n\n\nMaximum walltime\n\n\nTo set a default maximum walltime (in minutes) for routed jobs, set the attribute \ndefault_maxWallTime\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting WallTime\n; \\\n     /* Set the max walltime to 1 hr */ \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_default_maxWallTime = 60; \\\n/b\n/span\n\n] \n\n\n\n\nEditing attributes\u2026\n\n\nThe following functions are operations that affect job attributes and are evaluated in the following order:\n\n\n\n\ncopy_*\n\n\ndelete_*\n\n\nset_*\n\n\neval_set_*\n\n\n\n\nAfter each job route\u2019s ClassAd is \nconstructed\n, the above operations are evaluated in order. For example, if the attribute \nfoo\n is set using \neval_set_foo\n in the \nJOB_ROUTER_DEFAULTS\n, you\u2018ll be unable to use \ndelete_foo\n to remote it from your jobs since the attribute is set using \neval_set_foo\n after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in \nJOB_ROUTER_DEFAULTS\n get overriden by the same operation in \nJOB_ROUTER_ENTRIES\n. So to \u2019delete\u2019 \nfoo\n, we would add =eval_set_foo = \u201c\u201d= to the route in the \nJOB_ROUTER_ENTRIES\n, resulting in \nfoo\n being absent from the routed job.\n\n\nMore documentation can be found in the \nHTCondor manual\n.\n\n\nCopying attributes\n\n\nTo copy the value of an attribute of the incoming job to an attribute of the routed job, use \ncopy_\n. The following route copies the \nenvironment\n attribute of the incoming job and sets the attribute \nOriginal_Environment\n on the routed job to the same value:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\ncopy_environment = \nOriginal_Environment\n; \\\n/b\n/span\n\n] \n\n\n\n\nRemoving attributes\n\n\nTo remove an attribute of the incoming job from the routed job, use \ndelete_\n. The following route removes the \nenvironment\n attribute from the routed job:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nCopying attributes\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\ndelete_environment = True; \\\n/b\n/span\n\n] \n\n\n\n\n#SettingAttributes\n\n\nSetting attributes\n\n\nTo set an attribute on the routed job, use \nset_\n. The following route sets the Job\u2019s \nRank\n attribute to 5:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Rank = 5; \\\n/b\n/span\n\n] \n\n\n\n\nSetting attributes with ClassAd expressions\n\n\nTo set an attribute to a ClassAd expression to be evaluated, use \nset_eval\n. The following route sets the \nExperiment\n attribute to \natlas.osguser\n if the Owner of the incoming job is \nosguser\n:\n\n\n If a value is set in JOB_ROUTER_DEFAULTS with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting an attribute with a !ClassAd expression\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\neval_set_Experiment = strcat(\natlas.\n, Owner); \\\n/b\n/span\n\n] \n\n\n\n\nLimiting the number of\u2026\n\n\nThis section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).\n\n\n If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via \nuser priorities and group accounting\n.\n\n\nTotal jobs\n\n\nTo set a limit on the number of jobs for a specific route, set the \nMaxJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 100\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nMaxJobs = 100; \\\n/b\n/span\n\n] \\\n[ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nLimit the total number of jobs to 75\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nMaxJobs = 75; \\\n/b\n/span\n\n]\n\n\n\n\nIdle jobs\n\n\nTo set a limit on the number of idle jobs for a specific route, set the \nMaxIdleJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 100\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nMaxIdleJobs = 100; \\\n/b\n/span\n\n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name = \nLimit the total number of idle jobs to 75\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nMaxIdleJobs = 75; \\\n/b\n/span\n\n]\n\n\n\n\nDebugRoutes\n\n\nDebugging routes\n\n\nTo help debug expressions in your routes, you can use the \ndebug()\n function. First, set the debug mode for the JobRouter by editing a file in \n/etc/condor-ce/config.d/\n to read\n\n\nJOB_ROUTER_DEBUG = D_FULLDEBUG\n\n\n\n\nThen wrap the problematic attribute in \ndebug()\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nDebugging a difficult !ClassAd expression\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\neval_set_Experiment = debug(strcat(\natlas\n, Name)); \\\n/b\n/span\n\n] \n\n\n\n\n\n\nYou will find the debugging output in \n/var/log/condor-ce/JobRouterLog\n.\n\n\nRoutes for HTCondor Batch Systems\n\n\nSetting periodic hold, release or remove\n\n\nTo release, remove or put a job on hold if it meets certain criteria, use the \nPERIODIC_*\n family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE\u2019s configuration.\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name = \nSetting periodic statements\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Periodic_Remove = ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ); \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\n/* Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason */ \\\n/b\n/span\n\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Periodic_Release = HoldReasonCode == 6 \n regexp(\nVMGAHP_ERR_INTERNAL\n, HoldReason); \\\n/b\n/span\n\n] \n\n\n\n\n#RoutedReq\n\n\nSetting routed job requirements\n\n\nIf you need to set requirements on your routed job, you will need to use \nset_Requirements\n instead of \nRequirements\n. The \nRequirements\n attribute filters jobs coming into your CE into different job routes whereas \nset_requirements\n will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the \nHTCondor manual\n.\n\n\nTo ensure that your job lands on a Linux machine in your pool:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_Requirements =  OpSys == \nLINUX\n \\\n/b\n/span\n\n]\n\n\n\n\nSetting accounting groups\n\n\nTo assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using \nUID and ExtAttr tables\n.\n\n\nRoutes for non-HTCondor Batch Systems\n\n\nSetting a default batch queue\n\n\nTo set a default queue for routed jobs, set the attribute \ndefault_queue\n:\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting batch system queues\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_default_queue = \nosg_queue\n; \\\n/b\n/span\n\n] \n\n\n\n\nSetting batch system directives\n\n\nTo write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in \n/etc/blahp/\n (e.g., if your local batch system is PBS, edit \n/etc/blahp/pbs_local_submit_attributes.sh\n). This file is sourced during submit time and anything printed to stdout is appended to the job submit script that gets submitted to your batch system. ClassAd attributes can be passed from the routed job to the local submit attributes script via the \ndefault_remote_cerequirements\n attribute, which can take the following form:\n\n\ndefault_remote_cerequirements = foo == X \n bar == Y \n ...\n\n\n\n\nThis sets \nfoo\n to value \nX\n and \nbar\n to \nY\n in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the \nx509UserProxyFirstFQAN\n attribute of the job submitted to a PBS batch system\n\n\nJOB_ROUTER_ENTRIES = [ \\\n     GridResource = \nbatch pbs\n; \\\n     TargetUniverse = 9; \\\n     name = \nSetting job submit variables\n; \\\n     \nspan style=\nbackground-color: #FFCCFF;\nb\nset_default_remote_cerequirements = strcat(\nWalltime == 3600 \n AccountingGroup == \\\n, x509UserProxyFirstFQAN, \n\\\n); \\\n/b\n/span\n\n] \n\n\n\n\nWith \n/etc/blahp/pbs_local_submit_attributes.sh\n containing.\n\n\n#!/bin/bash\necho \n#PBS -l walltime=$Walltime\n\necho \n#PBS -A $AccountingGroup\n\n\n\n\n\nThe result is that the following will be appended to the script that gets submitted to your batch system:\n\n\n#PBS -l walltime=3600\n#PBS -A \nspan style=\nbackground-color: #FFCCFF;\nlt;CE job's x509UserProxyFirstFQAN attribute\ngt;\n/span\n\n\n\n\n\nExample Configurations\n\n\nAGLT2\u2019s job routes\n\n\nAtlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.\n\n\n\n\nSetting various HTCondor-specific attributes like \nRank\n, \nAccountingGroup\n, \nJobPrio\n and \nPeriodic_Remove\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nLastandFrac\n, \nIdleMP8Pressure\n, \nlocalQue\n, \nIsAnalyJob\n and \nJobMemoryLimit\n.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttps://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content\n\n\nTWISTY_OPTS_OUTPUT\n showlink=\"Click to expand full job route\u2026\"\n\n\nJOB_ROUTER_ENTRIES = \\\n/* Still to do on all routes, get job requirements and add them here */ \\\n/* ***** Route no 1 ***** */ \\\n/* ***** Analysis queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue==\nanaly\n; \\\n    Name = \nAnalysis Queue\n; \\\n    TargetUniverse = 5; \\\n    eval_set_IdleMP8Pressure = $(IdleMP8Pressure); \\\n    eval_set_LastAndFrac = $(LastAndFrac); \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ) \n (IfThenElse((Owner == \natlasconnect\n || Owner == \nmuoncal\n),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.prod.analy.\n,Owner); \\\n    set_localQue = \nAnalysis\n; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 5; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 2 ***** */ \\\n/* ***** splitterNT queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue == \nsplitterNT\n; \\\n    Name = \nSplitter ntuple queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = \ngroup_calibrate.muoncal\n; \\\n    set_localQue = \nSplitter\n; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 10; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 3 ***** */ \\\n/* ***** splitter queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue == \nsplitter\n; \\\n    Name = \nSplitter queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = \ngroup_calibrate.muoncal\n; \\\n    set_localQue = \nSplitter\n; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 15; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 4 ***** */ \\\n/* ***** xrootd queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue == \nxrootd\n; \\\n    Name = \nXrootd queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.prod.analy.\n,Owner); \\\n    set_localQue = \nAnalysis\n; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 35; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 5 ***** */ \\\n/* ***** Tier3Test queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue == \nTier3Test\n; \\\n    Name = \nTier3 Test Queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ) \n ( IS_TIER3_TEST_QUEUE =?= True ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.prod.analy.\n,Owner); \\\n    set_localQue = \nTier3Test\n; \\\n    set_IsTier3TestJob = True; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 20; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 6 ***** */ \\\n/* ***** mp8 queue ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue==\nmp8\n; \\\n    Name = \nMCORE Queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ) \n (( TARGET.Cpus == 8 \n TARGET.CPU_TYPE =?= \nmp8\n ) || TARGET.PARTITIONED =?= True ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.prod.mcore.\n,Owner); \\\n    set_localQue = \nMP8\n; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 25; \\\n    set_Rank = 0.0; \\\n    eval_set_RequestCpus = 8; \\\n    set_JobMemoryLimit = 33552000; \\\n    set_Slot_Type = \nmp8\n; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 7 ***** */ \\\n/* ***** Installation queue, triggered by usatlas2 user ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue is undefined \n target.Owner == \nusatlas2\n; \\\n    Name = \nInstall Queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ) \n ( TARGET.IS_INSTALL_QUE =?= True ) \n (TARGET.AGLT2_SITE == \nUM\n ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.other.\n,Owner); \\\n    set_localQue = \nDefault\n; \\\n    set_IsAnalyJob = False; \\\n    set_IsInstallJob = True; \\\n    set_JobPrio = 15; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 8 ***** */ \\\n/* ***** Default queue for usatlas1 user ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue is undefined \n regexp(\nusatlas1\n,target.Owner); \\\n    Name = \nATLAS Production Queue\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.prod.prod.\n,Owner); \\\n    set_localQue = \nDefault\n; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 9 ***** */ \\\n/* ***** Default queue for any other usatlas account ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue is undefined \n (regexp(\nusatlas2\n,target.Owner) || regexp(\nusatlas3\n,target.Owner)); \\\n    Name = \nOther ATLAS Production\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_gatekpr.other.\n,Owner); \\\n    set_localQue = \nDefault\n; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 10 ***** */ \\\n/* ***** Anything else. Set queue as Default and assign to other VOs  ***** */ \\\n  [ \\\n    GridResource = \ncondor localhost localhost\n; \\\n    eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n); \\\n    Requirements = target.queue is undefined \n ifThenElse(regexp(\nusatlas\n,target.Owner),false,true); \\\n    Name = \nOther Jobs\n; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk \n= 21000000 ) ) \n ( TARGET.Arch == \nX86_64\n ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat(\ngroup_VOgener.\n,Owner); \\\n    set_localQue = \nDefault\n; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime \n (3*24*60*60 + 5*60) ) || (ImageSize \n JobMemoryLimit) ); \\\n  ]\n\n\n\n\n\n\nBNL\u2019s job routes\n\n\nAtlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:\n\n\n\n\nSetting various HTCondor-specific attributes like \nJobLeaseDuration\n, \nRequirements\n and \nPeriodic_Hold\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nRACF_Group\n, \nExperiment\n, \nJob_Type\n and \nVO\n.\n\n\nJobs are split into different routes based on the \nGlideIn\n queue that they\u2019re in.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches \nincoming\n jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttp://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html\n\n\nTWISTY_OPTS_OUTPUT\n showlink=\"Click to expand full job route\u2026\"\n\n\n###############################################################################\n#\n# HTCondor-CE HTCondor batch system configuration file.\n#\n###############################################################################\n\n# Submit the job to the site Condor\n\nJOB_ROUTER_ENTRIES = \\\n   [ \\\n     GridResource = \ncondor localhost localhost\n; \\\n     eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n$(FULL_HOSTNAME)\n); \\\n     TargetUniverse = 5; \\\n     name = \nBNL_Condor_Pool_long\n; \\\n     Requirements = target.queue==\nanalysis.long\n; \\\n     eval_set_RACF_Group = \nlong\n; \\\n     set_Experiment = \natlas\n; \\\n     set_requirements = ( ( Arch == \nINTEL\n || Arch == \nX86_64\n ) \n ( CPU_Experiment == \natlas\n ) ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n     set_Job_Type = \ncas\n; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource = \ncondor localhost localhost\n; \\\n     eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n$(FULL_HOSTNAME)\n); \\\n     TargetUniverse = 5; \\\n     name = \nBNL_Condor_Pool_short\n; \\\n     Requirements = target.queue==\nanalysis.short\n; \\\n     eval_set_RACF_Group = \nshort\n; \\\n     set_Experiment = \natlas\n; \\\n     set_requirements = ( ( Arch == \nINTEL\n || Arch == \nX86_64\n ) \n ( CPU_Experiment == \natlas\n ) ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n     set_Job_Type = \ncas\n; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource = \ncondor localhost localhost\n; \\\n     eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n$(FULL_HOSTNAME)\n); \\\n     TargetUniverse = 5; \\\n     name = \nBNL_Condor_Pool_grid\n; \\\n     Requirements = target.queue==\ngrid\n; \\\n     eval_set_RACF_Group = \ngrid\n; \\\n     set_Experiment = \natlas\n; \\\n     set_requirements = ( ( Arch == \nINTEL\n || Arch == \nX86_64\n ) \n ( CPU_Experiment == \natlas\n ) ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n     set_Job_Type = \ncas\n; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource = \ncondor localhost localhost\n; \\\n     eval_set_GridResource = strcat(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n$(FULL_HOSTNAME)\n); \\\n     TargetUniverse = 5; \\\n     name = \nBNL_Condor_Pool\n; \\\n     Requirements = target.queue is undefined; \\\n     eval_set_RACF_Group = \ngrid\n; \\\n     set_requirements = ( ( Arch == \nINTEL\n || Arch == \nX86_64\n ) \n ( CPU_Experiment == \nrcf\n ) ) \n ( TARGET.OpSys == \nLINUX\n ) \n ( TARGET.Disk \n= RequestDisk ) \n ( TARGET.Memory \n= RequestMemory ) \n ( TARGET.HasFileTransfer ); \\\n     set_Experiment = \natlas\n; \\\n     set_Job_Type = \ncas\n; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts \n= 1 \n JobStatus == 1) || NumJobStarts \n 1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ]\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting Jobs to HTCondor-CE", 
            "title": "Job Router Recipe"
        }, 
        {
            "location": "/Computing_Element/job_router/#writing-routes-for-htcondor-ce", 
            "text": "", 
            "title": "Writing Routes For HTCondor-CE"
        }, 
        {
            "location": "/Computing_Element/job_router/#about-this-guide", 
            "text": "The  JobRouter  is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route\u2018s requirements, the route creates a transformed job (referred to as the \u2019routed job\u2019) that is then submitted to the batch system. The CE package comes with default routes located in  /etc/condor-ce/config.d/02-ce-*.conf  that provide enough basic functionality for a small site.  If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.  RoutePitfalls", 
            "title": "About This Guide"
        }, 
        {
            "location": "/Computing_Element/job_router/#quirks-and-pitfalls", 
            "text": "The JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.  Make sure to run  condor_ce_reconfig  after changing your routes, otherwise they will not take effect.  Before the last square bracket, make sure all lines end in a line continuation character (backslash). You can inspect the syntax of your routes with  condor_ce_config_val JOB_ROUTER_ENTRIES  to see if HTCondor-CE has ingested them properly.  Do  not  set the  JOB_ROUTER_DEFAULTS  configuration variable yourself. This will cause the CE to stop functioning.  Do  not  set the job environment through the JobRouter. Instead, add any changes to the /etc/osg/config.d/  [Local Settings]  section and run osg-configure, as documented  here .  HTCondor batch system only: Local universe jobs are excluded from any routing.    #JobRouteConstruction", 
            "title": "Quirks and Pitfalls"
        }, 
        {
            "location": "/Computing_Element/job_router/#how-job-routes-are-constructed", 
            "text": "Each job route\u2019s  ClassAd  is constructed by combining each entry from the  JOB_ROUTER_ENTRIES  with the  JOB_ROUTER_DEFAULTS . Attributes that are set in  JOB_ROUTER_ENTRIES  will override those set in  JOB_ROUTER_DEFAULTS", 
            "title": "How Job Routes are Constructed"
        }, 
        {
            "location": "/Computing_Element/job_router/#job95router95entries", 
            "text": "JOB_ROUTER_ENTRIES  is a configuration variable whose default is set in  /etc/condor-ce/config.d/02-ce-*.conf  but may be overriden by the administrator in  /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to  JOB_ROUTER_ENTRIES  to fit your site\u2019s needs.  #JobRouterDefaults", 
            "title": "JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/Computing_Element/job_router/#job95router95defaults", 
            "text": "JOB_ROUTER_DEFAULTS  is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE\u2019s functionality. To view its contents, run the following command:  UCL_PROMPT  condor_ce_config_val JOB_ROUTER_DEFAULTS | sed 's/;/;\\n/g'  TWISTY_OPTS_OUTPUT  showlink=\"Show sample output\"  [ MaxIdleJobs = 2000;\n MaxJobs = 10000;\n /* by default, accept all jobs */ Requirements = True;\n /* now modify routed job attributes */ /* remove routed job if the client disappears for 48 hours or it is idle for 6 */ /*set_PeriodicRemove = (LastClientContact - time()   48*60*60) || (JobStatus == 1   (time() - QDate)   6*60);\n*/ delete_PeriodicRemove = true;\n delete_CondorCE = true;\n set_RoutedJob = true;\n copy_environment =  orig_environment ;\n set_osg_environment =  OSG_GRID='/etc/osg/wn-client/' OSG_SQUID_LOCATION='fermicloud133.fnal.gov:3128' OSG_SITE_READ='None' OSG_APP='/share/osg/app' OSG_GLEXEC_LOCATION='None' OSG_DATA='UNAVAILABLE' OSG_HOSTNAME='fermicloud136.fnal.gov' OSG_STORAGE_ELEMENT='False' OSG_SITE_NAME='herp' GLOBUS_LOCATION='/usr' OSG_WN_TMP='None' OSG_DEFAULT_SE='None' OSG_SITE_WRITE='None' ;\n eval_set_environment = debug(strcat( HOME= , userHome(Owner,  / ),    , ifThenElse(orig_environment is undefined, osg_environment, strcat(osg_environment,    , orig_environment) )));\n /* Set new requirements */ /* set_requirements = LastClientContact - time()   30*60;\n */ set_requirements = True;\n set_InputRSL = ifThenElse(GlobusRSL is null, [], eval_rsl(GlobusRSL));\n /* Note default memory request of 2GB */ /* Note yet another nested condition allow pass attributes (maxMemory,xcount,jobtype,queue) via gWMS Factory described within ClassAd if undefined via RSL */ eval_set_RequestMemory = ifThenElse(InputRSL.maxMemory isnt null, InputRSL.maxMemory, ifThenElse(maxMemory isnt null, maxMemory, ifThenElse(default_maxMemory isnt null, default_maxMemory, 2000)));\n eval_set_remote_queue = ifThenElse(InputRSL.queue isnt null, InputRSL.queue, ifThenElse(queue isnt null, queue, ifThenElse(default_queue isnt null, default_queue,  )));\n /* HTCondor uses RequestCpus;\n blahp uses SMPGranularity and NodeNumber.  Default is 1 core. */ eval_set_RequestCpus = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n eval_set_remote_SMPGranularity = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n eval_set_remote_NodeNumber = ifThenElse(InputRSL.xcount isnt null, InputRSL.xcount, ifThenElse(xcount isnt null, xcount, ifThenElse(default_xcount isnt null, default_xcount, 1)));\n /* If remote_cerequirements is a string, BLAH will parse it as an expression before examining it */ eval_set_remote_cerequirements = ifThenElse(InputRSL.maxWalTlime isnt null, strcat( Walltime ==  , string(60*InputRSL.maxWallTime),     CondorCE == 1 ), ifThenElse(maxWallTime isnt null, strcat( Walltime ==  , string(60*maxWallTime),     CondorCE == 1 ), ifThenElse(default_maxWallTime isnt null, strcat( Walltime ==  , string(60*default_maxWallTime),     CondorCE == 1 ),  CondorCE == 1 )));\n ]    If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.   Do  not  set the  JOB_ROUTER_DEFAULTS  configuration variable yourself. This will cause the CE to stop functioning.", 
            "title": "JOB_ROUTER_DEFAULTS"
        }, 
        {
            "location": "/Computing_Element/job_router/#generic-routes", 
            "text": "New routes should be placed in  /etc/condor-ce/config.d/99-local.conf , not the original  02-ce-*.conf .", 
            "title": "Generic Routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#required-fields", 
            "text": "The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in  /usr/share/condor-ce/config.d/02-ce- batch system -defaults.conf , provided by the  osg-ce- batch system  packages.", 
            "title": "Required fields"
        }, 
        {
            "location": "/Computing_Element/job_router/#batch-system", 
            "text": "Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the  TargetUniverse  attribute needs to be set to  5  or  \"vanilla\" . For all other batch systems, the  TargetUniverse  attribute needs to be set to  9  or  \"grid\"  and the  GridResource  attribute needs to be set to  \"batch  batch system \"  (where  batch system  can be one of  pbs  (for both users of  pbs  and  SLURM ),  lsf , or  sge ).  JOB_ROUTER_ENTRIES = [ \\\n      span style= background-color: #FFCCFF; b # Route to an HTCondor-CE batch system \\ /b /span \n      span style= background-color: #FFCCFF; b TargetUniverse = 5; \\ /b /span \n     name =  Route jobs to HTCondor ; \\\n] \\\n[ \\\n      span style= background-color: #FFCCFF; b # Route to a PBS batch system \\ /b /span \n      span style= background-color: #FFCCFF; b GridResource =  batch pbs ; \\ /b /span \n      span style= background-color: #FFCCFF; b TargetUniverse = 9; \\ /b /span \n     name =  Route jobs to PBS ; \\\n]", 
            "title": "Batch system"
        }, 
        {
            "location": "/Computing_Element/job_router/#route-name", 
            "text": "To identify routes, you will need to assign a name to the route with the  name  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      span style= background-color: #FFCCFF; b name =  Route jobs to HTCondor ; \\ /b /span \n]  The name of the route will be useful in debugging since it shows up in the output of  condor_ce_job_router_info , the  JobRouterLog , and in the ClassAd of the routed job, which can be viewed with  condor_ce_q  or  condor_ce_history .", 
            "title": "Route name"
        }, 
        {
            "location": "/Computing_Element/job_router/#writing-multiple-routes", 
            "text": "If your batch system needs incoming jobs to be sorted (e.g. if different VO\u2019s need to go to separate queues), you will need to write multiple job routes. Each route is enclosed by square brackets and unless they\u2019re the last closing bracket, they need to be followed by the line continuation character. The following routes takes incoming jobs that have a  queue  attribute set to  \"analy\"  and routes them to the site\u2019s HTCondor batch system. Any other jobs will be sent to that site\u2019s PBS batch system.   The JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.  JOB_ROUTER_ENTRIES =  span style= background-color: #FFCCFF; b [ \\ /b /span \n     TargetUniverse = 5; \\\n     name =  Route jobs to HTCondor ; \\\n     Requirements = (TARGET.queue =?=  analy ); \\ span style= background-color: #FFCCFF; b ] \\ /b /span  span style= background-color: #FFCCFF; b [ \\ /b /span \n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Route jobs to PBS ; \\\n     Requirements = (TARGET.queue =!=  analy ); \\ span style= background-color: #FFCCFF; b ] /b /span", 
            "title": "Writing multiple routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#writing-comments", 
            "text": "To write comments you can use C-style comments, text enclosed by  /* */ . If the comment is at the end of a line, it still has to be followed by the line continuation character.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  C-style comments ; \\\n      span style= background-color: #FFCCFF; b /* This is a comment */ \\ /b /span \n]   For  condor_ce_version  8.2.x or greater, you can also use  #  to comment out single lines:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Hash comments ; \\\n      span style= background-color: #FFCCFF; b # BrokenAttribute =  commented out ; \\ /b /span  \n]", 
            "title": "Writing comments"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-attributes-for-all-routes", 
            "text": "To set an attribute that will be applied to all routes, you will need to use the  [[#SettingAttributes][set_]]  function for each route.   Do  not  try to do this by setting the  JOB_ROUTER_DEFAULTS  configuration variable, as this will cause the CE to stop functioning.  The following routes set the  Periodic_Hold  attribute for both routes:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Route jobs to HTCondor ; \\\n     Requirements = (TARGET.queue =?=  analy ); \\\n      span style= background-color: #FFCCFF; b /* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\  /b /span \n      span style= background-color: #FFCCFF; b set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\ /b /span \n] \\\n[ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Route jobs to PBS ; \\\n     Requirements = (TARGET.queue =!=  analy ); \\\n      span style= background-color: #FFCCFF; b /* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\  /b /span \n      span style= background-color: #FFCCFF; b set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\ /b /span \n]", 
            "title": "Setting attributes for all routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#filtering-jobs-based-on", 
            "text": "To filter jobs, use the  Requirements  attribute. Jobs will evaluate against the ClassAd expression set in the  Requirements  and if the expression evaluates to  TRUE , the route will match. More information on the syntax of ClassAd's can be found in the  HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult  this document .  When setting requirements, you need to prefix job attributes that you are filtering with  TARGET.  so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a =queue = \u201canaly\u201d= attribute, then the following job route will not match:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n     queue =  not-analy ; \\\n      span style= background-color: #FFCCFF; b Requirements = (TARGET.queue =?=  analy ); \\ /b /span \n]   This is because when evaluating the route requirement, the job route will compare its own  queue  attribute to \u201canaly\u201d and see that it does not match. You can read more about comparing two ClassAds in the  HTCondor manual .   If you have an HTCondor batch system, note the difference with  set_requirements .   The JobRouter matches jobs to routes in a round-robin fashion. This means that if a job can match to multiple routes, it can be routed by any of them! So when writing job routes, make sure that they are exclusive to each other and that your jobs can only match to a single route.  #GlideIn", 
            "title": "Filtering jobs based on\u2026"
        }, 
        {
            "location": "/Computing_Element/job_router/#glidein-queue", 
            "text": "To filter jobs based on their glidein queue attribute, your routes will need a  Requirements  expression using the incoming job\u2019s  queue  attribute. The following entry routes jobs to the PBS queue if the incoming job (specified by  TARGET ) is an  analy  (Analysis) glidein:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by queue ; \\\n      span style= background-color: #FFCCFF; b Requirements = (TARGET.queue =?=  analy ); \\ /b /span \n]", 
            "title": "Glidein queue"
        }, 
        {
            "location": "/Computing_Element/job_router/#job-submitter", 
            "text": "To filter jobs based on who submitted it, your routes will need a  Requirements  expression using the incoming job\u2019s  Owner  attribute. The following entry routes jobs to the HTCondor batch system iff the submitter is  usatlas2 :  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Filtering by job submitter ; \\\n      span style= background-color: #FFCCFF; b Requirements = (TARGET.Owner =?=  usatlas2 ); \\ /b /span \n]   Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system iff the submitter\u2019s name begins with  usatlas :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by job submitter (regular expression) ; \\\n      span style= background-color: #FFCCFF; b Requirements = regexp( ^usatlas , TARGET.Owner); \\ /b /span \n]", 
            "title": "Job submitter"
        }, 
        {
            "location": "/Computing_Element/job_router/#voms-attribute", 
            "text": "To filter jobs based on the subject of the job\u2019s proxy, your routes will need a  Requirements  expression using the incoming job\u2019s  x509UserProxyFirstFQAN  attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains  /cms/Role=Pilot :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Filtering by VOMS attribute (regex) ; \\\n      span style= background-color: #FFCCFF; b Requirements = regexp( \\/cms\\/Role\\=pilot , TARGET.x509UserProxyFirstFQAN); \\ /b /span \n]", 
            "title": "VOMS attribute"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-a-default", 
            "text": "This section outlines how to set default job limits, memory, cores, queue, and maximum walltime. For an example on how users can override these defaults, consult  this document .", 
            "title": "Setting a default\u2026"
        }, 
        {
            "location": "/Computing_Element/job_router/#maximum-number-of-jobs", 
            "text": "To set a default limit to the maximum number of jobs per route, you can edit the configuration variable  CONDORCE_MAX_JOBS  in  /etc/condor-ce/config.d/01-ce-router.conf :  span style= background-color: #FFCCFF; b CONDORCE_MAX_JOBS = 10000 /b /span   Note that this is to be placed directly into the HTCondor-CE configuration, not into a job route.", 
            "title": "Maximum number of jobs"
        }, 
        {
            "location": "/Computing_Element/job_router/#maximum-memory", 
            "text": "To set a default maximum memory for routed jobs, set the attribute  default_maxMemory :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request memory ; \\\n     /* Set the requested memory to 1 GB */ \\\n      span style= background-color: #FFCCFF; b set_default_maxMemory = 1000; \\ /b /span \n]", 
            "title": "Maximum memory"
        }, 
        {
            "location": "/Computing_Element/job_router/#number-of-cores-to-request", 
            "text": "To set a default number of cores for routed jobs, set the attribute  default_xcount :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Request CPU ; \\\n     /* Set the requested cores to 8 */ \\\n      span style= background-color: #FFCCFF; b set_default_xcount = 8; \\ /b /span \n]", 
            "title": "Number of cores to request"
        }, 
        {
            "location": "/Computing_Element/job_router/#maximum-walltime", 
            "text": "To set a default maximum walltime (in minutes) for routed jobs, set the attribute  default_maxWallTime :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting WallTime ; \\\n     /* Set the max walltime to 1 hr */ \\\n      span style= background-color: #FFCCFF; b set_default_maxWallTime = 60; \\ /b /span \n]", 
            "title": "Maximum walltime"
        }, 
        {
            "location": "/Computing_Element/job_router/#editing-attributes", 
            "text": "The following functions are operations that affect job attributes and are evaluated in the following order:   copy_*  delete_*  set_*  eval_set_*   After each job route\u2019s ClassAd is  constructed , the above operations are evaluated in order. For example, if the attribute  foo  is set using  eval_set_foo  in the  JOB_ROUTER_DEFAULTS , you\u2018ll be unable to use  delete_foo  to remote it from your jobs since the attribute is set using  eval_set_foo  after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in  JOB_ROUTER_DEFAULTS  get overriden by the same operation in  JOB_ROUTER_ENTRIES . So to \u2019delete\u2019  foo , we would add =eval_set_foo = \u201c\u201d= to the route in the  JOB_ROUTER_ENTRIES , resulting in  foo  being absent from the routed job.  More documentation can be found in the  HTCondor manual .", 
            "title": "Editing attributes\u2026"
        }, 
        {
            "location": "/Computing_Element/job_router/#copying-attributes", 
            "text": "To copy the value of an attribute of the incoming job to an attribute of the routed job, use  copy_ . The following route copies the  environment  attribute of the incoming job and sets the attribute  Original_Environment  on the routed job to the same value:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      span style= background-color: #FFCCFF; b copy_environment =  Original_Environment ; \\ /b /span \n]", 
            "title": "Copying attributes"
        }, 
        {
            "location": "/Computing_Element/job_router/#removing-attributes", 
            "text": "To remove an attribute of the incoming job from the routed job, use  delete_ . The following route removes the  environment  attribute from the routed job:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Copying attributes ; \\\n      span style= background-color: #FFCCFF; b delete_environment = True; \\ /b /span \n]   #SettingAttributes", 
            "title": "Removing attributes"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-attributes", 
            "text": "To set an attribute on the routed job, use  set_ . The following route sets the Job\u2019s  Rank  attribute to 5:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute ; \\\n      span style= background-color: #FFCCFF; b set_Rank = 5; \\ /b /span \n]", 
            "title": "Setting attributes"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-attributes-with-classad-expressions", 
            "text": "To set an attribute to a ClassAd expression to be evaluated, use  set_eval . The following route sets the  Experiment  attribute to  atlas.osguser  if the Owner of the incoming job is  osguser :   If a value is set in JOB_ROUTER_DEFAULTS with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES .  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting an attribute with a !ClassAd expression ; \\\n      span style= background-color: #FFCCFF; b eval_set_Experiment = strcat( atlas. , Owner); \\ /b /span \n]", 
            "title": "Setting attributes with ClassAd expressions"
        }, 
        {
            "location": "/Computing_Element/job_router/#limiting-the-number-of", 
            "text": "This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).   If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via  user priorities and group accounting .", 
            "title": "Limiting the number of\u2026"
        }, 
        {
            "location": "/Computing_Element/job_router/#total-jobs", 
            "text": "To set a limit on the number of jobs for a specific route, set the  MaxJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 100 ; \\\n      span style= background-color: #FFCCFF; b MaxJobs = 100; \\ /b /span \n] \\\n[ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Limit the total number of jobs to 75 ; \\\n      span style= background-color: #FFCCFF; b MaxJobs = 75; \\ /b /span \n]", 
            "title": "Total jobs"
        }, 
        {
            "location": "/Computing_Element/job_router/#idle-jobs", 
            "text": "To set a limit on the number of idle jobs for a specific route, set the  MaxIdleJobs  attribute:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 100 ; \\\n      span style= background-color: #FFCCFF; b MaxIdleJobs = 100; \\ /b /span \n] \\\n[ \\\n     TargetUniverse = 5; \\\n     name =  Limit the total number of idle jobs to 75 ; \\\n      span style= background-color: #FFCCFF; b MaxIdleJobs = 75; \\ /b /span \n]  DebugRoutes", 
            "title": "Idle jobs"
        }, 
        {
            "location": "/Computing_Element/job_router/#debugging-routes", 
            "text": "To help debug expressions in your routes, you can use the  debug()  function. First, set the debug mode for the JobRouter by editing a file in  /etc/condor-ce/config.d/  to read  JOB_ROUTER_DEBUG = D_FULLDEBUG  Then wrap the problematic attribute in  debug() :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Debugging a difficult !ClassAd expression ; \\\n      span style= background-color: #FFCCFF; b eval_set_Experiment = debug(strcat( atlas , Name)); \\ /b /span \n]    You will find the debugging output in  /var/log/condor-ce/JobRouterLog .", 
            "title": "Debugging routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#routes-for-htcondor-batch-systems", 
            "text": "", 
            "title": "Routes for HTCondor Batch Systems"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-periodic-hold-release-or-remove", 
            "text": "To release, remove or put a job on hold if it meets certain criteria, use the  PERIODIC_*  family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting PERIODIC_EXPR_INTERVAL in your CE\u2019s configuration.  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n     name =  Setting periodic statements ; \\\n      span style= background-color: #FFCCFF; b /* Puts the routed job on hold if the job's been idle and has been started at least once or if the job has tried to start more than once */ \\ /b /span \n      span style= background-color: #FFCCFF; b set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\ /b /span \n      span style= background-color: #FFCCFF; b /* Remove routed jobs if their walltime is longer than 3 days and 5 minutes */ \\ /b /span \n      span style= background-color: #FFCCFF; b set_Periodic_Remove = ( RemoteWallClockTime   (3*24*60*60 + 5*60) ); \\ /b /span \n      span style= background-color: #FFCCFF; b /* Release routed jobs if the condor_starter couldn't start the executable and 'VMGAHP_ERR_INTERNAL' is in the HoldReason */ \\ /b /span \n      span style= background-color: #FFCCFF; b set_Periodic_Release = HoldReasonCode == 6   regexp( VMGAHP_ERR_INTERNAL , HoldReason); \\ /b /span \n]   #RoutedReq", 
            "title": "Setting periodic hold, release or remove"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-routed-job-requirements", 
            "text": "If you need to set requirements on your routed job, you will need to use  set_Requirements  instead of  Requirements . The  Requirements  attribute filters jobs coming into your CE into different job routes whereas  set_requirements  will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the  HTCondor manual .  To ensure that your job lands on a Linux machine in your pool:  JOB_ROUTER_ENTRIES = [ \\\n     TargetUniverse = 5; \\\n      span style= background-color: #FFCCFF; b set_Requirements =  OpSys ==  LINUX  \\ /b /span \n]", 
            "title": "Setting routed job requirements"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-accounting-groups", 
            "text": "To assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using  UID and ExtAttr tables .", 
            "title": "Setting accounting groups"
        }, 
        {
            "location": "/Computing_Element/job_router/#routes-for-non-htcondor-batch-systems", 
            "text": "", 
            "title": "Routes for non-HTCondor Batch Systems"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-a-default-batch-queue", 
            "text": "To set a default queue for routed jobs, set the attribute  default_queue :  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting batch system queues ; \\\n      span style= background-color: #FFCCFF; b set_default_queue =  osg_queue ; \\ /b /span \n]", 
            "title": "Setting a default batch queue"
        }, 
        {
            "location": "/Computing_Element/job_router/#setting-batch-system-directives", 
            "text": "To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in  /etc/blahp/  (e.g., if your local batch system is PBS, edit  /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the job submit script that gets submitted to your batch system. ClassAd attributes can be passed from the routed job to the local submit attributes script via the  default_remote_cerequirements  attribute, which can take the following form:  default_remote_cerequirements = foo == X   bar == Y   ...  This sets  foo  to value  X  and  bar  to  Y  in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the  x509UserProxyFirstFQAN  attribute of the job submitted to a PBS batch system  JOB_ROUTER_ENTRIES = [ \\\n     GridResource =  batch pbs ; \\\n     TargetUniverse = 9; \\\n     name =  Setting job submit variables ; \\\n      span style= background-color: #FFCCFF; b set_default_remote_cerequirements = strcat( Walltime == 3600   AccountingGroup == \\ , x509UserProxyFirstFQAN,  \\ ); \\ /b /span \n]   With  /etc/blahp/pbs_local_submit_attributes.sh  containing.  #!/bin/bash\necho  #PBS -l walltime=$Walltime \necho  #PBS -A $AccountingGroup   The result is that the following will be appended to the script that gets submitted to your batch system:  #PBS -l walltime=3600\n#PBS -A  span style= background-color: #FFCCFF; lt;CE job's x509UserProxyFirstFQAN attribute gt; /span", 
            "title": "Setting batch system directives"
        }, 
        {
            "location": "/Computing_Element/job_router/#example-configurations", 
            "text": "", 
            "title": "Example Configurations"
        }, 
        {
            "location": "/Computing_Element/job_router/#aglt2s-job-routes", 
            "text": "Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.   Setting various HTCondor-specific attributes like  Rank ,  AccountingGroup ,  JobPrio  and  Periodic_Remove  (see the  HTCondor manual  for more). Some of these are site-specific like  LastandFrac ,  IdleMP8Pressure ,  localQue ,  IsAnalyJob  and  JobMemoryLimit .  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content  TWISTY_OPTS_OUTPUT  showlink=\"Click to expand full job route\u2026\"  JOB_ROUTER_ENTRIES = \\\n/* Still to do on all routes, get job requirements and add them here */ \\\n/* ***** Route no 1 ***** */ \\\n/* ***** Analysis queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue== analy ; \\\n    Name =  Analysis Queue ; \\\n    TargetUniverse = 5; \\\n    eval_set_IdleMP8Pressure = $(IdleMP8Pressure); \\\n    eval_set_LastAndFrac = $(LastAndFrac); \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer )   (IfThenElse((Owner ==  atlasconnect  || Owner ==  muoncal ),IfThenElse(IdleMP8Pressure,(TARGET.PARTITIONED =!= TRUE),True),IfThenElse(LastAndFrac,(TARGET.PARTITIONED =!= TRUE),True))); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.prod.analy. ,Owner); \\\n    set_localQue =  Analysis ; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 5; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 2 ***** */ \\\n/* ***** splitterNT queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue ==  splitterNT ; \\\n    Name =  Splitter ntuple queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup =  group_calibrate.muoncal ; \\\n    set_localQue =  Splitter ; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 10; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 3 ***** */ \\\n/* ***** splitter queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue ==  splitter ; \\\n    Name =  Splitter queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup =  group_calibrate.muoncal ; \\\n    set_localQue =  Splitter ; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 15; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 4 ***** */ \\\n/* ***** xrootd queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue ==  xrootd ; \\\n    Name =  Xrootd queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.prod.analy. ,Owner); \\\n    set_localQue =  Analysis ; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 35; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 5 ***** */ \\\n/* ***** Tier3Test queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue ==  Tier3Test ; \\\n    Name =  Tier3 Test Queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer )   ( IS_TIER3_TEST_QUEUE =?= True ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.prod.analy. ,Owner); \\\n    set_localQue =  Tier3Test ; \\\n    set_IsTier3TestJob = True; \\\n    set_IsAnalyJob = True; \\\n    set_JobPrio = 20; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 6 ***** */ \\\n/* ***** mp8 queue ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue== mp8 ; \\\n    Name =  MCORE Queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer )   (( TARGET.Cpus == 8   TARGET.CPU_TYPE =?=  mp8  ) || TARGET.PARTITIONED =?= True ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.prod.mcore. ,Owner); \\\n    set_localQue =  MP8 ; \\\n    set_IsAnalyJob = False; \\\n    set_JobPrio = 25; \\\n    set_Rank = 0.0; \\\n    eval_set_RequestCpus = 8; \\\n    set_JobMemoryLimit = 33552000; \\\n    set_Slot_Type =  mp8 ; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 7 ***** */ \\\n/* ***** Installation queue, triggered by usatlas2 user ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue is undefined   target.Owner ==  usatlas2 ; \\\n    Name =  Install Queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer )   ( TARGET.IS_INSTALL_QUE =?= True )   (TARGET.AGLT2_SITE ==  UM  ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.other. ,Owner); \\\n    set_localQue =  Default ; \\\n    set_IsAnalyJob = False; \\\n    set_IsInstallJob = True; \\\n    set_JobPrio = 15; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 8 ***** */ \\\n/* ***** Default queue for usatlas1 user ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue is undefined   regexp( usatlas1 ,target.Owner); \\\n    Name =  ATLAS Production Queue ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.prod.prod. ,Owner); \\\n    set_localQue =  Default ; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 9 ***** */ \\\n/* ***** Default queue for any other usatlas account ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue is undefined   (regexp( usatlas2 ,target.Owner) || regexp( usatlas3 ,target.Owner)); \\\n    Name =  Other ATLAS Production ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat( group_gatekpr.other. ,Owner); \\\n    set_localQue =  Default ; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ] \\\n/* ***** Route no 10 ***** */ \\\n/* ***** Anything else. Set queue as Default and assign to other VOs  ***** */ \\\n  [ \\\n    GridResource =  condor localhost localhost ; \\\n    eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ); \\\n    Requirements = target.queue is undefined   ifThenElse(regexp( usatlas ,target.Owner),false,true); \\\n    Name =  Other Jobs ; \\\n    TargetUniverse = 5; \\\n    set_requirements = ( ( TARGET.TotalDisk =?= undefined ) || ( TARGET.TotalDisk  = 21000000 ) )   ( TARGET.Arch ==  X86_64  )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n    eval_set_AccountingGroup = strcat( group_VOgener. ,Owner); \\\n    set_localQue =  Default ; \\\n    set_IsAnalyJob = False; \\\n    set_Rank = (SlotID + (64-TARGET.DETECTED_CORES))*1.0; \\\n    set_JobMemoryLimit = 4194000; \\\n    set_Periodic_Remove = ( ( RemoteWallClockTime   (3*24*60*60 + 5*60) ) || (ImageSize   JobMemoryLimit) ); \\\n  ]", 
            "title": "AGLT2\u2019s job routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#bnls-job-routes", 
            "text": "Atlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:   Setting various HTCondor-specific attributes like  JobLeaseDuration ,  Requirements  and  Periodic_Hold  (see the  HTCondor manual  for more). Some of these are site-specific like  RACF_Group ,  Experiment ,  Job_Type  and  VO .  Jobs are split into different routes based on the  GlideIn  queue that they\u2019re in.  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches  incoming  jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  http://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html  TWISTY_OPTS_OUTPUT  showlink=\"Click to expand full job route\u2026\"  ###############################################################################\n#\n# HTCondor-CE HTCondor batch system configuration file.\n#\n###############################################################################\n\n# Submit the job to the site Condor\n\nJOB_ROUTER_ENTRIES = \\\n   [ \\\n     GridResource =  condor localhost localhost ; \\\n     eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,  $(FULL_HOSTNAME) ); \\\n     TargetUniverse = 5; \\\n     name =  BNL_Condor_Pool_long ; \\\n     Requirements = target.queue== analysis.long ; \\\n     eval_set_RACF_Group =  long ; \\\n     set_Experiment =  atlas ; \\\n     set_requirements = ( ( Arch ==  INTEL  || Arch ==  X86_64  )   ( CPU_Experiment ==  atlas  ) )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n     set_Job_Type =  cas ; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource =  condor localhost localhost ; \\\n     eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,  $(FULL_HOSTNAME) ); \\\n     TargetUniverse = 5; \\\n     name =  BNL_Condor_Pool_short ; \\\n     Requirements = target.queue== analysis.short ; \\\n     eval_set_RACF_Group =  short ; \\\n     set_Experiment =  atlas ; \\\n     set_requirements = ( ( Arch ==  INTEL  || Arch ==  X86_64  )   ( CPU_Experiment ==  atlas  ) )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n     set_Job_Type =  cas ; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource =  condor localhost localhost ; \\\n     eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,  $(FULL_HOSTNAME) ); \\\n     TargetUniverse = 5; \\\n     name =  BNL_Condor_Pool_grid ; \\\n     Requirements = target.queue== grid ; \\\n     eval_set_RACF_Group =  grid ; \\\n     set_Experiment =  atlas ; \\\n     set_requirements = ( ( Arch ==  INTEL  || Arch ==  X86_64  )   ( CPU_Experiment ==  atlas  ) )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n     set_Job_Type =  cas ; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ] \\\n   [ \\\n     GridResource =  condor localhost localhost ; \\\n     eval_set_GridResource = strcat( condor  ,  $(FULL_HOSTNAME) ,  $(FULL_HOSTNAME) ); \\\n     TargetUniverse = 5; \\\n     name =  BNL_Condor_Pool ; \\\n     Requirements = target.queue is undefined; \\\n     eval_set_RACF_Group =  grid ; \\\n     set_requirements = ( ( Arch ==  INTEL  || Arch ==  X86_64  )   ( CPU_Experiment ==  rcf  ) )   ( TARGET.OpSys ==  LINUX  )   ( TARGET.Disk  = RequestDisk )   ( TARGET.Memory  = RequestMemory )   ( TARGET.HasFileTransfer ); \\\n     set_Experiment =  atlas ; \\\n     set_Job_Type =  cas ; \\\n     set_JobLeaseDuration = 3600; \\\n     set_Periodic_Hold = (NumJobStarts  = 1   JobStatus == 1) || NumJobStarts   1; \\\n     eval_set_VO = x509UserProxyVOName; \\\n   ]", 
            "title": "BNL\u2019s job routes"
        }, 
        {
            "location": "/Computing_Element/job_router/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  The HTCondor-CE troubleshooting guide  Submitting Jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/Other/wn_oasis/", 
            "text": "Configuring a Site to Use the Worker Node Client Software From OASIS\n\n\n\n\nAbout This Guide\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the \nreference section\n below for contents of the Worker Node Client.\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:\n\n\n\n\nUse the Worker Node Client software from OASIS (this guide) - useful when \nOASIS\n is already mounted on your worker nodes\n\n\nInstall using RPMs and Yum\n - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\n\n\nAlso note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.\n\n\nConfiguring Your Site to Use the Worker Node Client From OASIS\n\n\nBelow are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.\n\n\n\n\nOn every worker node, \ninstall and configure OASIS\n\n\nDetermine the OASIS path to the Worker Node Client software for your worker nodes: \n\n\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nUse\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a05\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el5-x86_64\n\n\n\n\n\n\nEL\u00a06\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el6-x86_64\n\n\n\n\n\n\n\n\n\n\nOn the CE, in the \n/etc/osg/config.d/10-storage.ini\n file, set the \ngrid_dir\n configuration setting to the path from the previous step\n\n\n\n\nFor more information, see the \nOSG environment variables reference page\n and the \nCE configuration instructions\n.\n4. Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration: \n\n\nosg-configure -v  \nosg-configure -c\n\n\n\n\nValiding the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to the directory of your WN Client installation\n\n\n\n\nManually Using the Worker Node Client From OASIS\n\n\nIf you must log onto a worker node and use the Worker Node Client software directly during your login session, use the steps below to set up access to the software:\n\n\n\n\nDetermine the OASIS path to the Worker Node Client software for your worker nodes:\n\n\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nUse\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a05\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el5-x86_64/setup.sh\n\n\n\n\n\n\nEL\u00a06\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el6-x86_64/setup.sh\n\n\n\n\n\n\n\n\n\n\nSet up access to the Worker Node Client software for the duration of your login session:\n\n\n\n\nsource **PATH**\n\n\n\n\nwhere \nPATH\n is from the previous step.\n\n\nTroubleshooting\n\n\nSome possible issues that may come up:\n\n\n\n\nA missing softlink to the CA certs directory. To check this, run:\n\n\n\n\nls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.2/current/el5-x86_64/etc/grid-security\n\n\n\n\nand check that \ncertificates\n is linked to somewhere. The fix is to yum update the oasis-config package to version 4. A known workaround is to run:\n\n\nexport X509\\_CERT\\_DIR=/cvmfs/oasis.opensciencegrid.org/mis/certificates\n\n\n\n\nbefore any commands.\n- OpenJDK 7 must be installed from RPMs before using software that needs Java (e.g., \nsrm-ls\n) from OASIS. There are some instructions \nhere\n. If you are running under an EL5 distribution, java programs may produce an error like this:\n\n\nException in thread \u201dmain\n java.lang.NoClassDefFoundError: org/globus/gsi/OpenSSLKey\n\n\n\n\nThis shouldn\u2018t happen under EL6 distributions. The (unofficial) workaround is to run the following as root:\n\n\nmkdir /usr/share/java-1.7.0\n/usr/lib/java-1.7.0\n\n\n\n\nThis problem is also mentioned \nhere\n.\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nyum and RPM\n, \nthe best practices\n for using yum to install software, and using \nyum repositories\n.", 
            "title": "OASIS-based Worker Node"
        }, 
        {
            "location": "/Other/wn_oasis/#configuring-a-site-to-use-the-worker-node-client-software-from-oasis", 
            "text": "", 
            "title": "Configuring a Site to Use the Worker Node Client Software From OASIS"
        }, 
        {
            "location": "/Other/wn_oasis/#about-this-guide", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node that can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect to use. See the  reference section  below for contents of the Worker Node Client.  It is possible to install the Worker Node Client software in a variety of ways, depending on what works best for distributing and managing software at your site:   Use the Worker Node Client software from OASIS (this guide) - useful when  OASIS  is already mounted on your worker nodes  Install using RPMs and Yum  - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes   This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/Other/wn_oasis/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system   Also note that, once configured to use OASIS, each use of Worker Node Client software will cause that software and its associated files to be downloaded from a CVMFS server or your local cache thereof. This may result in extra network activity, especially if Worker Node Client tools are used heavily.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/Other/wn_oasis/#configuring-your-site-to-use-the-worker-node-client-from-oasis", 
            "text": "Below are the one-time steps that you must perform to configure your site to use the Worker Node Client software from OASIS.   On every worker node,  install and configure OASIS  Determine the OASIS path to the Worker Node Client software for your worker nodes:       Worker Node OS  Use\u2026      EL\u00a05  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el5-x86_64    EL\u00a06  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el6-x86_64      On the CE, in the  /etc/osg/config.d/10-storage.ini  file, set the  grid_dir  configuration setting to the path from the previous step   For more information, see the  OSG environment variables reference page  and the  CE configuration instructions .\n4. Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:   osg-configure -v  \nosg-configure -c", 
            "title": "Configuring Your Site to Use the Worker Node Client From OASIS"
        }, 
        {
            "location": "/Other/wn_oasis/#validing-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job\u2019s output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to the directory of your WN Client installation", 
            "title": "Validing the Worker Node Client"
        }, 
        {
            "location": "/Other/wn_oasis/#manually-using-the-worker-node-client-from-oasis", 
            "text": "If you must log onto a worker node and use the Worker Node Client software directly during your login session, use the steps below to set up access to the software:   Determine the OASIS path to the Worker Node Client software for your worker nodes:      Worker Node OS  Use\u2026      EL\u00a05  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el5-x86_64/setup.sh    EL\u00a06  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.2/current/el6-x86_64/setup.sh      Set up access to the Worker Node Client software for the duration of your login session:   source **PATH**  where  PATH  is from the previous step.", 
            "title": "Manually Using the Worker Node Client From OASIS"
        }, 
        {
            "location": "/Other/wn_oasis/#troubleshooting", 
            "text": "Some possible issues that may come up:   A missing softlink to the CA certs directory. To check this, run:   ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.2/current/el5-x86_64/etc/grid-security  and check that  certificates  is linked to somewhere. The fix is to yum update the oasis-config package to version 4. A known workaround is to run:  export X509\\_CERT\\_DIR=/cvmfs/oasis.opensciencegrid.org/mis/certificates  before any commands.\n- OpenJDK 7 must be installed from RPMs before using software that needs Java (e.g.,  srm-ls ) from OASIS. There are some instructions  here . If you are running under an EL5 distribution, java programs may produce an error like this:  Exception in thread \u201dmain  java.lang.NoClassDefFoundError: org/globus/gsi/OpenSSLKey  This shouldn\u2018t happen under EL6 distributions. The (unofficial) workaround is to run the following as root:  mkdir /usr/share/java-1.7.0\n/usr/lib/java-1.7.0  This problem is also mentioned  here .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/Other/wn_oasis/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/Other/wn_oasis/#reference", 
            "text": "Please see the documentation on using  yum and RPM ,  the best practices  for using yum to install software, and using  yum repositories .", 
            "title": "Reference"
        }, 
        {
            "location": "/Storage/gridftp/", 
            "text": "Install a GridFTP Server\n\n\n \n\n\n\u2014# About this Document This page explains how to install the stand-alone Globus GridFTP server.\n\n\nThe GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:\n\n\n\n\nA simple front-end to a filesystem allowing access over WAN - for example NFS.\n\n\nBeStMan is capable of distributing its workload among several gsiftp servers so if you expect large movements of data into/out of your site, multiple gsiftp servers can be set up.\n\n\n\n\n\u2014# Requirements\n\n\n\u2014## Host and OS\n\n\n\n\nOS must be \n.\n\n\nEPEL\n repos enabled.\n\n\nRoot access\n\n\n\n\n\u2014## Certificates\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n \\\nbr> \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nCertificates\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates (see below).\n\n\n\u2014## Users\n\n\nGridUsers\n For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nFor gums users, this means that each user that can be authenticated by gums should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here. \nGridUsers\n\n\n\u2014## Networking\n\n\nFirewalls\n \nDocumentation/Release3.FirewallInformation\n \\ \nFirewalls\n\n\nIf you have a multi-homed host you may be interested in reading \nthis section\n.\n\n\n\u2014## Engineering Considerations\n\n\nIt is recommended that the GridFTP package be installed on its own server if:\n\n\n\n\nYou are serving the VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and have more than 250 cores\n\n\nYour site will be managing more than 50 TB of disk space\n\n\n\n\nIf you are planning to have a Storage Element with BeStMan and have more than 1Gbps bandwidth, then you should plan on at least one GridFTP server per 4Gbps of available bandwidth (assuming you have 10Gbps interfaces on the server) if you want to maximize throughput.\n\n\nAlso, you have to decide what authorization mechanism you prefer. You may use either grid-mapfile or a GUMS server for users\u2019 authentication and authorization. We currently recommend using GUMS as it provides superior flexibility and allows a site to manage all of its mappings in one central location; most large sites use GUMS.\n\n\n OSG does not support launching the GridFTP server with xinetd \u2014 only launching with init is supported.\n\n\n\u2014# Install Instructions\n\n\nYumRepositories\n \nInstallCertAuth\n\n\nFull\n\n\nInstall\n GridFTP requires a certificate package to run. If you require a specific certificate package, follow the Documentation/Release3.InstallCertAuth instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).\n\n\n\u2014## Installing the GridFTP Server\n\n\nFirst, you will need to install the GridFTP meta-package:\n\n\n[root@client ~]$ yum install osg-gridftp\n\n\n\n\nInstall\n\n\n\u2014# Configuration\n\n\n\u2014## Authorization\n\n\nAuthorization\n There are two authorization options:\n\n\n\n\nGridmap file\n\n\nGUMS authentication server\n\n\n\n\nPlease choose one of these and follow the instructions in one of the two following sections.\n\n\n\u2014### Configuring Gridmap Support\n\n\nGridmap\n By default, GridFTP uses a gridmap file, found in \n/etc/grid-security/grid-mapfile\n. This file is not generated by default. There are two ways you can generate this file. You can generate this file manually, by including DN/username combinations. This is most useful for debugging. Otherwise, you can install edg-mkgridmap, which will periodically contact a list of VOMS servers that you specify. It assembles a list of users from those servers and creates a grid-mapfile. This grid-mapfile serves both as a list of authorized users and provides a mapping from user dns to local user ids.\n\n\nTo install edg_mkgridmap, perform the following steps\n\n\nyum install edg-mkgridmap\n\n\n\n\nReview \n/etc/edg-mkgridmap.conf\n to make sure that it has all VOs that you are interested in and also to comment out any VOs that you do not wish to support.\n\n\nvi /etc/edg-mkgridmap.conf\n\n\n\n\nThis utility \nedg-mkgridmap\n runs as a cronjob \n/etc/cron.d/edg-mkgridmap-cron\n (by default every 6 hours). You can also run \nedg-mkgridmap\n manually to see that it generates \n/etc/grid-security/grid-mapfile\n.\n\n\nedg-mkgridmap\n\n\n\n\nThen, you can enable/start the service.\n\n\n/sbin/service edg-mkgridmap start\n/sbin/chkconfig edg-mkgridmap on\n\n\n\n\nYou can read more on this page: \nedg_mkgridmap (on the CE)\n \nGridmap\n\n\n\u2014### Configuring GUMS support\n\n\nGums\n By default, GridFTP uses a gridmap file, found in \n/etc/grid-security/gridmap-file\n. If you want to use GUMS security (recommended), you will need to enable it using the following steps:\n\n\nFirst, edit \n/etc/grid-security/gsi-authz.conf\n and uncomment the globus callout.\n\n\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n\n\n\n\nNote that this used to be the full path to the library (\n/usr/lib64\n or \n/usr/lib\n), but now we rely on the linker for proper resolution in this file.\n\n\nNext edit \n/etc/lcmaps.db\n to edit your gums information:\n\n\n...\ngumsclient = \nlcmaps_gums_client.mod\n\n             \n-resourcetype ce\n\n             \n-actiontype execute-now\n\n             \n-capath /etc/grid-security/certificates\n\n             \n-cert   /etc/grid-security/hostcert.pem\n\n             \n-key    /etc/grid-security/hostkey.pem\n\n             \n--cert-owner root\n\n# Change this URL to your GUMS server\n             \n--endpoint https://\ngums.fnal.gov:8443\n/gums/services/GUMSXACMLAuthorizationServicePort\n\n\n\n\n\nIf you would like to run SAZ, you will need to enable the relevant lines in the above file as well (more documentation to be added later). \nGums\n\n\nAuthorization\n\n\n\u2014## (Optional) Modifying the Environment\n\n\nEnvironment\n Environment variables are stored in \n/etc/sysconfig/globus-gridftp-server\n which is sourced on service startup. If you want to change LCMAPS log levels, or globus port ranges, you can edit them there.\n\n\n#Uncomment and modify for firewalls\n#export GLOBUS_TCP_PORT_RANGE=min,max\n#export GLOBUS_TCP_SOURCE_RANGE=min,max\n\n\n\n\nNote that the variables \nGLOBUS_TCP_PORT_RANGE\n and \nGLOBUS_TCP_SOURCE_RANGE\n can be set here to allow globus to navigate around firewall rules.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/globus-gridftp-server\n and choose a higher debug level:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2\n\n\n\n\nOutput goes to \n/var/log/messages\n by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.\n\n\nEnvironment\n\n\n#ConfigMultiHomed \u2014## Configuring a multi-homed server\n\n\nMultiHomed\n The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file \n/etc/gridftp.conf\n.\n\n\nTo use a single interface you can set \nhostname\n to the Hostname or IP address to use:\\\nhostname IP-TO-USE\\\n You can also set separately the \ncontrol_interface\n, \ndata_interface\n and \nipc_interface\n. E.g. on systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting \ndata_interface\n: \\\n control\\_interface IP-TO-USE data\\_interface IP-TO-USE ipc\\_interface IP-TO-USE\\\n \nMultiHomed\n\n\nFor more options available for the GridFTP server, read the comments in the configuration file (\n/etc/gridftp.conf\n) or see the \nGlobus manual\n mentioned in the \nReference\n section below.\n\n\n\u2014# Starting GridFTP\n\n\nStarting\n\n\nStarting GridFTP:\n\n\n[root@client ~]$ service globus-gridftp-server start\n\n\n\n\nStarting\n To start Gridftp automatically at boot time\n\n\n[root@client ~]$ chkconfig globus-gridftp-server on\n\n\n\n\n\u2014# Stopping GridFTP \nStopping\n Stopping GridFTP:\n\n\n[root@client ~]$ service globus-gridftp-server stop\n\n\n\n\nStopping\n\n\n\u2014# Validation of services\n\n\nValidation\n\n\nThe GridFTP service can be validated by using globus-url-copy. You will need to run \ngrid-proxy-init\n or \nvoms-proxy-init\n in order to get a valid user proxy in order to communicate with the GridFTP server.\n\n\nUCL_PROMPT\n globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero\n\nUCL_PROMPT\n echo $?\n0\n\n\n\n\nNote that you should preferably not try to run validation as root, as globus-url-copy will sometimes attempt to use the host certificate instead of your user certificate, with confusing results. \nValidation\n\n\n\u2014# Gratia GridFTP Transfer Probe The \nGratia GridFTP probe\n collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, make sure following is set in file /etc/gratia/gridftp-transfer/ProbeConfig\n\n\nEnableProbe=\n1\n\n\n\n\n\nAll other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.\n\n\n\u2014# Useful Configuration and Log Files\n\n\nLocations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP and LCMAPS\n\n\n\n\n\n\n\n\n/usr/share/osg/sysconfig/globus-gridftp-server-plugin\n\n\nWhere environment variables for GridFTP plugin are included\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nGridFTP transfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\nGratia probe\n\n\n/var/logs/gratia\n\n\n\n\n\n\n\n\n\n\nLocations\n\n\nFull\n\n\n\u2014# How to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug support and issues, submit a ticket to the \nGrid Operations Center\n.\n\n\nFor community support and best-effort software team support contact \n.\n\n\n\n\nFor a full set of help options, see \nHelp Procedure\n.\n\n\n\u2014# Screen Dump of Install Procedure\n\n\n\n\n[root@fermicloud108 ~]# wget http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm\n--2011-10-18 11:07:32--  http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm\nResolving download.fedoraproject.org... 140.211.169.197, 152.19.134.146, 209.132.181.16, ...\nConnecting to download.fedoraproject.org|140.211.169.197|:80... connected.\nHTTP request sent, awaiting response... 302 FOUND\nLocation: http://kdeforge.unl.edu/mirrors/epel/5/i386/epel-release-5-4.noarch.rpm [following]\n--2011-10-18 11:07:33--  http://kdeforge.unl.edu/mirrors/epel/5/i386/epel-release-5-4.noarch.rpm\nResolving kdeforge.unl.edu... 129.93.181.6\nConnecting to kdeforge.unl.edu|129.93.181.6|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12232 (12K) [application/x-rpm]\nSaving to: `epel-release-5-4.noarch.rpm'\n\n100%[==================================================================\n] 12,232      --.-K/s   in 0.03s\n\n2011-10-18 11:07:33 (349 KB/s) - `epel-release-5-4.noarch.rpm' saved [12232/12232]\n\n[root@fermicloud108 ~]# rpm -i epel-release-5-4.noarch.rpm\nwarning: epel-release-5-4.noarch.rpm: Header V3 DSA signature: NOKEY, key ID 217521f6\n[root@fermicloud108 ~]# yum -y install yum-priorities\nLoaded plugins: kernel-module\nepel                                                                                 | 3.7 kB     00:00\nepel/primary_db                                                                      | 3.8 MB     00:02\nfermi-base                                                                           | 2.1 kB     00:00\nfermi-security                                                                       | 1.9 kB     00:00\nfermi-security/primary_db                                                            | 1.7 MB     00:00\nsl-base                                                                              | 2.1 kB     00:00\nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package yum-priorities.noarch 0:1.1.16-14.el5 set to be updated\n--\n Finished Dependency Resolution\nBeginning Kernel Module Plugin\nFinished Kernel Module Plugin\n\nDependencies Resolved\n\n============================================================================================================\n Package                      Arch                 Version                      Repository             Size\n============================================================================================================\nInstalling:\n yum-priorities               noarch               1.1.16-14.el5                sl-base                14 k\n\nTransaction Summary\n============================================================================================================\nInstall       1 Package(s)\nUpgrade       0 Package(s)\n\nTotal download size: 14 k\nDownloading Packages:\nyum-priorities-1.1.16-14.el5.noarch.rpm                                              |  14 kB     00:00\nRunning rpm_check_debug\nRunning Transaction Test\nFinished Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing     : yum-priorities                                                                       1/1\n\nInstalled:\n  yum-priorities.noarch 0:1.1.16-14.el5\n\nComplete!\n[root@fermicloud108 ~]# rpm -Uvh http://repo.grid.iu.edu/osg-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg-release-latest.rpm\nwarning: /var/tmp/rpm-xfer.tQF1ZU: Header V3 DSA signature: NOKEY, key ID 824b8603\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud108 ~]# yum --enablerepo=osg-testing install osg-gridftp\nLoaded plugins: kernel-module, priorities\nosg                                                                                  | 1.9 kB     00:00\nosg/primary_db                                                                       |  65 kB     00:00\nosg-testing                                                                          | 1.9 kB     00:00\nosg-testing/primary_db                                                               | 319 kB     00:00\n1232 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--\n Running transaction check\n---\n Package osg-gridftp.x86_64 0:3.0.0-5 set to be updated\n--\n Processing Dependency: globus-gridftp-server-progs for package: osg-gridftp\n--\n Processing Dependency: gratia-probe-gridftp-transfer for package: osg-gridftp\n--\n Processing Dependency: vo-client for package: osg-gridftp\n--\n Processing Dependency: grid-certificates for package: osg-gridftp\n--\n Processing Dependency: gums-client for package: osg-gridftp\n--\n Processing Dependency: liblcas_lcmaps_gt4_mapping.so.0()(64bit) for package: osg-gridftp\n--\n Running transaction check\n---\n Package globus-gridftp-server-progs.x86_64 0:6.1-5.osg set to be updated\n--\n Processing Dependency: globus-gridftp-server = 6.1-5.osg for package: globus-gridftp-server-progs\n--\n Processing Dependency: globus-xio-gsi-driver \n= 2 for package: globus-gridftp-server-progs\n--\n Processing Dependency: perl(Globus::Core::Paths) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gssapi_gsi.so.9(globus_gssapi_gsi)(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_credential.so.5()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gssapi_error.so.4()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_io.so.8()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_cert_utils.so.8()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_callout.so.2()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_openssl.so.3()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_authz.so.2()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_authz_callout_error.so.2()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_common.so.14()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_proxy_ssl.so.4()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_xio.so.3()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_proxy_core.so.6()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gfork.so.3()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gridftp_server_control.so.2()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_ftp_control.so.4()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gss_assist.so.8()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_openssl_error.so.2()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gridftp_server.so.6()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_sysconfig.so.5()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_usage.so.3()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gsi_callback.so.4()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_gssapi_gsi.so.9()(64bit) for package: globus-gridftp-server-progs\n--\n Processing Dependency: libglobus_oldgaa.so.4()(64bit) for package: globus-gridftp-server-progs\n---\n Package gratia-probe-gridftp-transfer.noarch 0:1.09-0.4.1.pre set to be updated\n--\n Processing Dependency: gratia-probe-common \n= 1.09-0.4.1.pre for package: gratia-probe-gridftp-transfer\n--\n Processing Dependency: netlogger for package: gratia-probe-gridftp-transfer\n---\n Package gums-client.noarch 0:1.3.18.002-3 set to be updated\n--\n Processing Dependency: gums = 1.3.18.002 for package: gums-client\n--\n Processing Dependency: osg-vo-map for package: gums-client\n---\n Package lcas-lcmaps-gt4-interface.x86_64 0:0.1.4-6.osg set to be updated\n--\n Processing Dependency: liblcas.so.0()(64bit) for package: lcas-lcmaps-gt4-interface\n--\n Processing Dependency: liblcmaps.so.0()(64bit) for package: lcas-lcmaps-gt4-interface\n--\n Processing Dependency: libglobus_gridmap_callout_error.so.1()(64bit) for package: lcas-lcmaps-gt4-interface\n---\n Package osg-ca-certs.noarch 0:1.24-1 set to be updated\n---\n Package vo-client.noarch 0:38-9.osg set to be updated\n--\n Running transaction check\n---\n Package globus-authz.x86_64 0:2.0-2.osg set to be updated\n---\n Package globus-authz-callout-error.x86_64 0:2.0-2.osg set to be updated\n---\n Package globus-callout.x86_64 0:2.0-2.osg set to be updated\n--\n Processing Dependency: libltdl.so.3()(64bit) for package: globus-callout\n---\n Package globus-common.x86_64 0:14.0-3.osg set to be updated\n---\n Package globus-ftp-control.x86_64 0:4.0-2.osg set to be updated\n---\n Package globus-gfork.x86_64 0:3.0-2.osg set to be updated\n---\n Package globus-gridftp-server.x86_64 0:6.1-5.osg set to be updated\n---\n Package globus-gridftp-server-control.x86_64 0:2.0-3.osg set to be updated\n--\n Processing Dependency: globus-xio-pipe-driver \n= 2 for package: globus-gridftp-server-control\n---\n Package globus-gridmap-callout-error.x86_64 0:1.1-1.osg set to be updated\n---\n Package globus-gsi-callback.x86_64 0:4.0-2.osg set to be updated\n---\n Package globus-gsi-cert-utils.x86_64 0:8.0-2.osg set to be updated\n---\n Package globus-gsi-credential.x86_64 0:5.0-3.osg set to be updated\n---\n Package globus-gsi-openssl-error.x86_64 0:2.0-2.osg set to be updated\n---\n Package globus-gsi-proxy-core.x86_64 0:6.0-2.osg set to be updated\n---\n Package globus-gsi-proxy-ssl.x86_64 0:4.0-2.osg set to be updated\n---\n Package globus-gsi-sysconfig.x86_64 0:5.0-3.osg set to be updated\n---\n Package globus-gss-assist.x86_64 0:8.0-2.osg set to be updated\n---\n Package globus-gssapi-error.x86_64 0:4.0-2.osg set to be updated\n---\n Package globus-gssapi-gsi.x86_64 0:10.0-1.osg set to be updated\n---\n Package globus-io.x86_64 0:9.0-2.osg set to be updated\n---\n Package globus-openssl-module.x86_64 0:3.0-2.osg set to be updated\n---\n Package globus-usage.x86_64 0:3.0-2.osg set to be updated\n---\n Package globus-xio.x86_64 0:3.0-3.osg set to be updated\n---\n Package globus-xio-gsi-driver.x86_64 0:2.0-2.osg set to be updated\n---\n Package gratia-probe-common.noarch 0:1.09-0.4.1.pre set to be updated\n--\n Processing Dependency: pyOpenSSL for package: gratia-probe-common\n---\n Package gums.noarch 0:1.3.18.002-3 set to be updated\n--\n Processing Dependency: java for package: gums\n---\n Package lcas.x86_64 0:1.3.13-8.osg set to be updated\n--\n Processing Dependency: liblcas_userban.so()(64bit) for package: lcas\n---\n Package lcmaps.x86_64 0:1.4.28-14.osg set to be updated\n--\n Processing Dependency: lcmaps-plugins-saz-client for package: lcmaps\n--\n Processing Dependency: lcmaps-plugins-gums-client for package: lcmaps\n--\n Processing Dependency: liblcmaps_scas_client.so.0()(64bit) for package: lcmaps\n--\n Processing Dependency: liblcmaps_verify_proxy.so.0()(64bit) for package: lcmaps\n--\n Processing Dependency: libvomsapi.so.1()(64bit) for package: lcmaps\n--\n Processing Dependency: liblcmaps_posix_enf.so.0()(64bit) for package: lcmaps\n---\n Package netlogger.noarch 0:4.2.0-1 set to be updated\n---\n Package osg-vo-map.noarch 0:0.0.1-1.osg set to be updated\n--\n Running transaction check\n---\n Package globus-xio-pipe-driver.x86_64 0:2.0-2.osg set to be updated\n---\n Package java-1.6.0-openjdk.x86_64 1:1.6.0.0-1.22.1.9.8.el5_6 set to be updated\n--\n Processing Dependency: jpackage-utils \n= 1.7.3-1jpp.2 for package: java-1.6.0-openjdk\n--\n Processing Dependency: libasound.so.2(ALSA_0.9)(64bit) for package: java-1.6.0-openjdk\n--\n Processing Dependency: libasound.so.2(ALSA_0.9.0rc4)(64bit) for package: java-1.6.0-openjdk\n--\n Processing Dependency: tzdata-java for package: java-1.6.0-openjdk\n--\n Processing Dependency: libXtst.so.6()(64bit) for package: java-1.6.0-openjdk\n--\n Processing Dependency: libasound.so.2()(64bit) for package: java-1.6.0-openjdk\n--\n Processing Dependency: libgif.so.4()(64bit) for package: java-1.6.0-openjdk\n---\n Package lcas-plugins-basic.x86_64 0:1.3.5-5.osg set to be updated\n---\n Package lcmaps-plugins-basic.x86_64 0:1.4.5-1.osg set to be updated\n---\n Package lcmaps-plugins-gums-client.x86_64 0:0.0.2-2.osg set to be updated\n--\n Processing Dependency: lcmaps-plugins-scas-client for package: lcmaps-plugins-gums-client\n---\n Package lcmaps-plugins-saz-client.x86_64 0:0.2.22-7.osg set to be updated\n--\n Processing Dependency: saml2-xacml2-c-lib for package: lcmaps-plugins-saz-client\n--\n Processing Dependency: libxacml.so.0()(64bit) for package: lcmaps-plugins-saz-client\n---\n Package lcmaps-plugins-verify-proxy.x86_64 0:1.4.9-2.osg set to be updated\n---\n Package libtool-ltdl.x86_64 0:1.5.22-7.el5_4 set to be updated\n---\n Package pyOpenSSL.x86_64 0:0.6-1.p24.7.2.2 set to be updated\n---\n Package voms.x86_64 0:2.0.6-3.osg set to be updated\n--\n Running transaction check\n---\n Package alsa-lib.x86_64 0:1.0.17-1.el5 set to be updated\n---\n Package giflib.x86_64 0:4.1.3-7.1.el5_3.1 set to be updated\n---\n Package jpackage-utils.noarch 0:1.7.3-1jpp.2.el5 set to be updated\n---\n Package lcmaps-plugins-scas-client.x86_64 0:0.2.22-7.osg set to be updated\n---\n Package libXtst.x86_64 0:1.0.1-3.1 set to be updated\n---\n Package saml2-xacml2-c-lib.x86_64 0:1.0.1-6.osg set to be updated\n---\n Package tzdata-java.x86_64 0:2011h-2.el5 set to be updated\n--\n Finished Dependency Resolution\nBeginning Kernel Module Plugin\nFinished Kernel Module Plugin\n\nDependencies Resolved\n\n============================================================================================================\n Package                            Arch        Version                           Repository           Size\n============================================================================================================\nInstalling:\n osg-gridftp                        x86_64      3.0.0-5                           osg-testing         2.1 k\nInstalling for dependencies:\n alsa-lib                           x86_64      1.0.17-1.el5                      sl-base             414 k\n giflib                             x86_64      4.1.3-7.1.el5_3.1                 sl-base              39 k\n globus-authz                       x86_64      2.0-2.osg                         osg-testing          14 k\n globus-authz-callout-error         x86_64      2.0-2.osg                         osg-testing         9.9 k\n globus-callout                     x86_64      2.0-2.osg                         osg-testing          16 k\n globus-common                      x86_64      14.0-3.osg                        osg-testing         128 k\n globus-ftp-control                 x86_64      4.0-2.osg                         osg-testing          73 k\n globus-gfork                       x86_64      3.0-2.osg                         osg-testing          19 k\n globus-gridftp-server              x86_64      6.1-5.osg                         osg-testing         163 k\n globus-gridftp-server-control      x86_64      2.0-3.osg                         osg-testing          77 k\n globus-gridftp-server-progs        x86_64      6.1-5.osg                         osg-testing          40 k\n globus-gridmap-callout-error       x86_64      1.1-1.osg                         osg-testing         6.7 k\n globus-gsi-callback                x86_64      4.0-2.osg                         osg-testing          41 k\n globus-gsi-cert-utils              x86_64      8.0-2.osg                         osg-testing          18 k\n globus-gsi-credential              x86_64      5.0-3.osg                         osg-testing          35 k\n globus-gsi-openssl-error           x86_64      2.0-2.osg                         osg-testing          16 k\n globus-gsi-proxy-core              x86_64      6.0-2.osg                         osg-testing          36 k\n globus-gsi-proxy-ssl               x86_64      4.0-2.osg                         osg-testing          17 k\n globus-gsi-sysconfig               x86_64      5.0-3.osg                         osg-testing          29 k\n globus-gss-assist                  x86_64      8.0-2.osg                         osg-testing          34 k\n globus-gssapi-error                x86_64      4.0-2.osg                         osg-testing          13 k\n globus-gssapi-gsi                  x86_64      10.0-1.osg                        osg-testing          60 k\n globus-io                          x86_64      9.0-2.osg                         osg-testing          44 k\n globus-openssl-module              x86_64      3.0-2.osg                         osg-testing          14 k\n globus-usage                       x86_64      3.0-2.osg                         osg-testing          16 k\n globus-xio                         x86_64      3.0-3.osg                         osg-testing         178 k\n globus-xio-gsi-driver              x86_64      2.0-2.osg                         osg-testing          37 k\n globus-xio-pipe-driver             x86_64      2.0-2.osg                         osg-testing          16 k\n gratia-probe-common                noarch      1.09-0.4.1.pre                    osg-testing         132 k\n gratia-probe-gridftp-transfer      noarch      1.09-0.4.1.pre                    osg-testing          22 k\n gums                               noarch      1.3.18.002-3                      osg-testing          25 M\n gums-client                        noarch      1.3.18.002-3                      osg-testing          13 k\n java-1.6.0-openjdk                 x86_64      1:1.6.0.0-1.22.1.9.8.el5_6        fermi-security       37 M\n jpackage-utils                     noarch      1.7.3-1jpp.2.el5                  sl-base              61 k\n lcas                               x86_64      1.3.13-8.osg                      osg-testing          28 k\n lcas-lcmaps-gt4-interface          x86_64      0.1.4-6.osg                       osg-testing          17 k\n lcas-plugins-basic                 x86_64      1.3.5-5.osg                       osg-testing          23 k\n lcmaps                             x86_64      1.4.28-14.osg                     osg-testing          89 k\n lcmaps-plugins-basic               x86_64      1.4.5-1.osg                       osg-testing          38 k\n lcmaps-plugins-gums-client         x86_64      0.0.2-2.osg                       osg-testing         2.6 k\n lcmaps-plugins-saz-client          x86_64      0.2.22-7.osg                      osg-testing          32 k\n lcmaps-plugins-scas-client         x86_64      0.2.22-7.osg                      osg-testing          39 k\n lcmaps-plugins-verify-proxy        x86_64      1.4.9-2.osg                       osg-testing          23 k\n libXtst                            x86_64      1.0.1-3.1                         sl-base              16 k\n libtool-ltdl                       x86_64      1.5.22-7.el5_4                    fermi-security       38 k\n netlogger                          noarch      4.2.0-1                           osg-testing         624 k\n osg-ca-certs                       noarch      1.24-1                            osg-testing         450 k\n osg-vo-map                         noarch      0.0.1-1.osg                       osg-testing         7.3 k\n pyOpenSSL                          x86_64      0.6-1.p24.7.2.2                   sl-base             120 k\n saml2-xacml2-c-lib                 x86_64      1.0.1-6.osg                       osg-testing         581 k\n tzdata-java                        x86_64      2011h-2.el5                       fermi-security      178 k\n vo-client                          noarch      38-9.osg                          osg-testing          15 k\n voms                               x86_64      2.0.6-3.osg                       osg-testing         171 k\n\nTransaction Summary\n============================================================================================================\nInstall      54 Package(s)\nUpgrade       0 Package(s)\n\nTotal download size: 66 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/54): osg-gridftp-3.0.0-5.x86_64.rpm                                               | 2.1 kB     00:00\n(2/54): lcmaps-plugins-gums-client-0.0.2-2.osg.x86_64.rpm                            | 2.6 kB     00:00\n(3/54): globus-gridmap-callout-error-1.1-1.osg.x86_64.rpm                            | 6.7 kB     00:00\n(4/54): osg-vo-map-0.0.1-1.osg.noarch.rpm                                            | 7.3 kB     00:00\n(5/54): globus-authz-callout-error-2.0-2.osg.x86_64.rpm                              | 9.9 kB     00:00\n(6/54): gums-client-1.3.18.002-3.noarch.rpm                                          |  13 kB     00:00\n(7/54): globus-gssapi-error-4.0-2.osg.x86_64.rpm                                     |  13 kB     00:00\n(8/54): globus-authz-2.0-2.osg.x86_64.rpm                                            |  14 kB     00:00\n(9/54): globus-openssl-module-3.0-2.osg.x86_64.rpm                                   |  14 kB     00:00\n(10/54): vo-client-38-9.osg.noarch.rpm                                               |  15 kB     00:00\n(11/54): globus-gsi-openssl-error-2.0-2.osg.x86_64.rpm                               |  16 kB     00:00\n(12/54): libXtst-1.0.1-3.1.x86_64.rpm                                                |  16 kB     00:00\n(13/54): globus-usage-3.0-2.osg.x86_64.rpm                                           |  16 kB     00:00\n(14/54): globus-callout-2.0-2.osg.x86_64.rpm                                         |  16 kB     00:00\n(15/54): globus-xio-pipe-driver-2.0-2.osg.x86_64.rpm                                 |  16 kB     00:00\n(16/54): globus-gsi-proxy-ssl-4.0-2.osg.x86_64.rpm                                   |  17 kB     00:00\n(17/54): lcas-lcmaps-gt4-interface-0.1.4-6.osg.x86_64.rpm                            |  17 kB     00:00\n(18/54): globus-gsi-cert-utils-8.0-2.osg.x86_64.rpm                                  |  18 kB     00:00\n(19/54): globus-gfork-3.0-2.osg.x86_64.rpm                                           |  19 kB     00:00\n(20/54): gratia-probe-gridftp-transfer-1.09-0.4.1.pre.noarch.rpm                     |  22 kB     00:00\n(21/54): lcas-plugins-basic-1.3.5-5.osg.x86_64.rpm                                   |  23 kB     00:00\n(22/54): lcmaps-plugins-verify-proxy-1.4.9-2.osg.x86_64.rpm                          |  23 kB     00:00\n(23/54): lcas-1.3.13-8.osg.x86_64.rpm                                                |  28 kB     00:00\n(24/54): globus-gsi-sysconfig-5.0-3.osg.x86_64.rpm                                   |  29 kB     00:00\n(25/54): lcmaps-plugins-saz-client-0.2.22-7.osg.x86_64.rpm                           |  32 kB     00:00\n(26/54): globus-gss-assist-8.0-2.osg.x86_64.rpm                                      |  34 kB     00:00\n(27/54): globus-gsi-credential-5.0-3.osg.x86_64.rpm                                  |  35 kB     00:00\n(28/54): globus-gsi-proxy-core-6.0-2.osg.x86_64.rpm                                  |  36 kB     00:00\n(29/54): globus-xio-gsi-driver-2.0-2.osg.x86_64.rpm                                  |  37 kB     00:00\n(30/54): libtool-ltdl-1.5.22-7.el5_4.x86_64.rpm                                      |  38 kB     00:00\n(31/54): lcmaps-plugins-basic-1.4.5-1.osg.x86_64.rpm                                 |  38 kB     00:00\n(32/54): lcmaps-plugins-scas-client-0.2.22-7.osg.x86_64.rpm                          |  39 kB     00:00\n(33/54): giflib-4.1.3-7.1.el5_3.1.x86_64.rpm                                         |  39 kB     00:00\n(34/54): globus-gridftp-server-progs-6.1-5.osg.x86_64.rpm                            |  40 kB     00:00\n(35/54): globus-gsi-callback-4.0-2.osg.x86_64.rpm                                    |  41 kB     00:00\n(36/54): globus-io-9.0-2.osg.x86_64.rpm                                              |  44 kB     00:00\n(37/54): globus-gssapi-gsi-10.0-1.osg.x86_64.rpm                                     |  60 kB     00:00\n(38/54): jpackage-utils-1.7.3-1jpp.2.el5.noarch.rpm                                  |  61 kB     00:00\n(39/54): globus-ftp-control-4.0-2.osg.x86_64.rpm                                     |  73 kB     00:00\n(40/54): globus-gridftp-server-control-2.0-3.osg.x86_64.rpm                          |  77 kB     00:00\n(41/54): lcmaps-1.4.28-14.osg.x86_64.rpm                                             |  89 kB     00:00\n(42/54): pyOpenSSL-0.6-1.p24.7.2.2.x86_64.rpm                                        | 120 kB     00:00\n(43/54): globus-common-14.0-3.osg.x86_64.rpm                                         | 128 kB     00:00\n(44/54): gratia-probe-common-1.09-0.4.1.pre.noarch.rpm                               | 132 kB     00:00\n(45/54): globus-gridftp-server-6.1-5.osg.x86_64.rpm                                  | 163 kB     00:00\n(46/54): voms-2.0.6-3.osg.x86_64.rpm                                                 | 171 kB     00:00\n(47/54): globus-xio-3.0-3.osg.x86_64.rpm                                             | 178 kB     00:00\n(48/54): tzdata-java-2011h-2.el5.x86_64.rpm                                          | 178 kB     00:00\n(49/54): alsa-lib-1.0.17-1.el5.x86_64.rpm                                            | 414 kB     00:00\n(50/54): osg-ca-certs-1.24-1.noarch.rpm                                              | 450 kB     00:00\n(51/54): saml2-xacml2-c-lib-1.0.1-6.osg.x86_64.rpm                                   | 581 kB     00:00\n(52/54): netlogger-4.2.0-1.noarch.rpm                                                | 624 kB     00:00\n(53/54): gums-1.3.18.002-3.noarch.rpm                                                |  25 MB     00:02\n(54/54): java-1.6.0-openjdk-1.6.0.0-1.22.1.9.8.el5_6.x86_64.rpm                      |  37 MB     00:00\n------------------------------------------------------------------------------------------------------------\nTotal                                                                       5.3 MB/s |  66 MB     00:12\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603\nosg-testing/gpgkey                                                                   | 1.7 kB     00:00\nImporting GPG key 0x824B8603 \nOSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nFinished Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing     : globus-gsi-proxy-ssl                                                                1/54\n  Installing     : saml2-xacml2-c-lib                                                                  2/54\n  Installing     : lcmaps-plugins-saz-client                                                           3/54\n  Installing     : libtool-ltdl                                                                        4/54\n  Installing     : globus-common                                                                       5/54\n  Installing     : globus-gsi-openssl-error                                                            6/54\n  Installing     : globus-openssl-module                                                               7/54\n  Installing     : globus-gsi-sysconfig                                                                8/54\n  Installing     : globus-gsi-cert-utils                                                               9/54\n  Installing     : globus-gsi-callback                                                                10/54\n  Installing     : globus-gsi-credential                                                              11/54\n  Installing     : globus-gsi-proxy-core                                                              12/54\n  Installing     : globus-gssapi-gsi                                                                  13/54\n  Installing     : globus-callout                                                                     14/54\n  Installing     : globus-gss-assist                                                                  15/54\n  Installing     : globus-xio                                                                         16/54\n  Installing     : globus-gssapi-error                                                                17/54\n  Installing     : globus-xio-gsi-driver                                                              18/54\n  Installing     : globus-io                                                                          19/54\n  Installing     : globus-authz-callout-error                                                         20/54\n  Installing     : globus-authz                                                                       21/54\n  Installing     : globus-ftp-control                                                                 22/54\n  Installing     : globus-usage                                                                       23/54\n  Installing     : globus-gfork                                                                       24/54\n  Installing     : globus-gridmap-callout-error                                                       25/54\n  Installing     : globus-xio-pipe-driver                                                             26/54\n  Installing     : globus-gridftp-server-control                                                      27/54\n  Installing     : globus-gridftp-server                                                              28/54\n  Installing     : globus-gridftp-server-progs                                                        29/54\n  Installing     : lcmaps-plugins-scas-client                                                         30/54\n  Installing     : voms                                                                               31/54\n  Installing     : giflib                                                                             32/54\n  Installing     : lcmaps-plugins-basic                                                               33/54\n  Installing     : pyOpenSSL                                                                          34/54\n  Installing     : alsa-lib                                                                           35/54\n  Installing     : lcmaps-plugins-verify-proxy                                                        36/54\n  Installing     : libXtst                                                                            37/54\n  Installing     : osg-ca-certs                                                                       38/54\n  Installing     : vo-client                                                                          39/54\n  Installing     : gratia-probe-common                                                                40/54\n  Installing     : lcmaps-plugins-gums-client                                                         41/54\n  Installing     : lcmaps                                                                             42/54\n  Installing     : jpackage-utils                                                                     43/54\n  Installing     : osg-vo-map                                                                         44/54\n  Installing     : netlogger                                                                          45/54\n  Installing     : gratia-probe-gridftp-transfer                                                      46/54\n  Installing     : tzdata-java                                                                        47/54\n  Installing     : java-1.6.0-openjdk                                                                 48/54\n  Installing     : gums                                                                               49/54\n  Installing     : gums-client                                                                        50/54\n  Installing     : lcas                                                                               51/54\n  Installing     : lcas-lcmaps-gt4-interface                                                          52/54\n  Installing     : lcas-plugins-basic                                                                 53/54\n  Installing     : osg-gridftp                                                                        54/54\n\nInstalled:\n  osg-gridftp.x86_64 0:3.0.0-5\n\nDependency Installed:\n  alsa-lib.x86_64 0:1.0.17-1.el5\n  giflib.x86_64 0:4.1.3-7.1.el5_3.1\n  globus-authz.x86_64 0:2.0-2.osg\n  globus-authz-callout-error.x86_64 0:2.0-2.osg\n  globus-callout.x86_64 0:2.0-2.osg\n  globus-common.x86_64 0:14.0-3.osg\n  globus-ftp-control.x86_64 0:4.0-2.osg\n  globus-gfork.x86_64 0:3.0-2.osg\n  globus-gridftp-server.x86_64 0:6.1-5.osg\n  globus-gridftp-server-control.x86_64 0:2.0-3.osg\n  globus-gridftp-server-progs.x86_64 0:6.1-5.osg\n  globus-gridmap-callout-error.x86_64 0:1.1-1.osg\n  globus-gsi-callback.x86_64 0:4.0-2.osg\n  globus-gsi-cert-utils.x86_64 0:8.0-2.osg\n  globus-gsi-credential.x86_64 0:5.0-3.osg\n  globus-gsi-openssl-error.x86_64 0:2.0-2.osg\n  globus-gsi-proxy-core.x86_64 0:6.0-2.osg\n  globus-gsi-proxy-ssl.x86_64 0:4.0-2.osg\n  globus-gsi-sysconfig.x86_64 0:5.0-3.osg\n  globus-gss-assist.x86_64 0:8.0-2.osg\n  globus-gssapi-error.x86_64 0:4.0-2.osg\n  globus-gssapi-gsi.x86_64 0:10.0-1.osg\n  globus-io.x86_64 0:9.0-2.osg\n  globus-openssl-module.x86_64 0:3.0-2.osg\n  globus-usage.x86_64 0:3.0-2.osg\n  globus-xio.x86_64 0:3.0-3.osg\n  globus-xio-gsi-driver.x86_64 0:2.0-2.osg\n  globus-xio-pipe-driver.x86_64 0:2.0-2.osg\n  gratia-probe-common.noarch 0:1.09-0.4.1.pre\n  gratia-probe-gridftp-transfer.noarch 0:1.09-0.4.1.pre\n  gums.noarch 0:1.3.18.002-3\n  gums-client.noarch 0:1.3.18.002-3\n  java-1.6.0-openjdk.x86_64 1:1.6.0.0-1.22.1.9.8.el5_6\n  jpackage-utils.noarch 0:1.7.3-1jpp.2.el5\n  lcas.x86_64 0:1.3.13-8.osg\n  lcas-lcmaps-gt4-interface.x86_64 0:0.1.4-6.osg\n  lcas-plugins-basic.x86_64 0:1.3.5-5.osg\n  lcmaps.x86_64 0:1.4.28-14.osg\n  lcmaps-plugins-basic.x86_64 0:1.4.5-1.osg\n  lcmaps-plugins-gums-client.x86_64 0:0.0.2-2.osg\n  lcmaps-plugins-saz-client.x86_64 0:0.2.22-7.osg\n  lcmaps-plugins-scas-client.x86_64 0:0.2.22-7.osg\n  lcmaps-plugins-verify-proxy.x86_64 0:1.4.9-2.osg\n  libXtst.x86_64 0:1.0.1-3.1\n  libtool-ltdl.x86_64 0:1.5.22-7.el5_4\n  netlogger.noarch 0:4.2.0-1\n  osg-ca-certs.noarch 0:1.24-1\n  osg-vo-map.noarch 0:0.0.1-1.osg\n  pyOpenSSL.x86_64 0:0.6-1.p24.7.2.2\n  saml2-xacml2-c-lib.x86_64 0:1.0.1-6.osg\n  tzdata-java.x86_64 0:2011h-2.el5\n  vo-client.noarch 0:38-9.osg\n  voms.x86_64 0:2.0.6-3.osg\n\nComplete!\n[root@fermicloud108 ~]# cat /etc/grid-security/gsi-authz.conf\n#globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n[root@fermicloud108 ~]# sed -i 's/\\#globus_mapping/globus_mapping/' /etc/grid-security/gsi-authz.conf\n[root@fermicloud108 ~]# cat /etc/grid-security/gsi-authz.conf\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n[root@fermicloud108 ~]# vi /etc/lcmaps.db\n[root@fermicloud108 ~]# sed -i 's/yourgums.yourdomain/gums.fnal.gov/' /etc/lcmaps.db\n[root@fermicloud108 ~]# vi /etc/lcmaps.db\n[root@fermicloud108 ~]# service globus-gridftp-server start\nStarted GridFTP Server                                     [  OK  ]\n[root@fermicloud108 ~]# ps -ef | grep globus\nroot      2364     1  0 11:12 ?        00:00:00 /usr/sbin/globus-gridftp-server -c /etc/gridftp.conf -pidfile /var/run/globus-gridftp-server.pid -no-detach -config-base-path /\nroot      2371  2164  0 11:12 pts/0    00:00:00 grep globus\n[root@fermicloud108 ~]#  \n\n\n\n\n\n\n#DocReferences \u2014# References\n\n\n\n\nGlobus GridFTP administration manual\n\n\nGlobus GridFTP tutorial", 
            "title": "GridFTP Server"
        }, 
        {
            "location": "/Storage/gridftp/#install-a-gridftp-server", 
            "text": "\u2014# About this Document This page explains how to install the stand-alone Globus GridFTP server.  The GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:   A simple front-end to a filesystem allowing access over WAN - for example NFS.  BeStMan is capable of distributing its workload among several gsiftp servers so if you expect large movements of data into/out of your site, multiple gsiftp servers can be set up.   \u2014# Requirements  \u2014## Host and OS   OS must be  .  EPEL  repos enabled.  Root access   \u2014## Certificates  Certificates     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  \\ br>  /etc/grid-security/hostkey.pem     Certificates  Instructions  to request a service certificate.  You will also need a copy of CA certificates (see below).  \u2014## Users  GridUsers  For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  For gums users, this means that each user that can be authenticated by gums should be created on the server.  Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.  GridUsers  \u2014## Networking  Firewalls   Documentation/Release3.FirewallInformation  \\  Firewalls  If you have a multi-homed host you may be interested in reading  this section .  \u2014## Engineering Considerations  It is recommended that the GridFTP package be installed on its own server if:   You are serving the VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and have more than 250 cores  Your site will be managing more than 50 TB of disk space   If you are planning to have a Storage Element with BeStMan and have more than 1Gbps bandwidth, then you should plan on at least one GridFTP server per 4Gbps of available bandwidth (assuming you have 10Gbps interfaces on the server) if you want to maximize throughput.  Also, you have to decide what authorization mechanism you prefer. You may use either grid-mapfile or a GUMS server for users\u2019 authentication and authorization. We currently recommend using GUMS as it provides superior flexibility and allows a site to manage all of its mappings in one central location; most large sites use GUMS.   OSG does not support launching the GridFTP server with xinetd \u2014 only launching with init is supported.  \u2014# Install Instructions  YumRepositories   InstallCertAuth  Full  Install  GridFTP requires a certificate package to run. If you require a specific certificate package, follow the Documentation/Release3.InstallCertAuth instructions to install it. If you do not install a grid certificate package first, the install procedure will install one for you as part of its dependencies. (usually osg-ca-certs).  \u2014## Installing the GridFTP Server  First, you will need to install the GridFTP meta-package:  [root@client ~]$ yum install osg-gridftp  Install  \u2014# Configuration  \u2014## Authorization  Authorization  There are two authorization options:   Gridmap file  GUMS authentication server   Please choose one of these and follow the instructions in one of the two following sections.  \u2014### Configuring Gridmap Support  Gridmap  By default, GridFTP uses a gridmap file, found in  /etc/grid-security/grid-mapfile . This file is not generated by default. There are two ways you can generate this file. You can generate this file manually, by including DN/username combinations. This is most useful for debugging. Otherwise, you can install edg-mkgridmap, which will periodically contact a list of VOMS servers that you specify. It assembles a list of users from those servers and creates a grid-mapfile. This grid-mapfile serves both as a list of authorized users and provides a mapping from user dns to local user ids.  To install edg_mkgridmap, perform the following steps  yum install edg-mkgridmap  Review  /etc/edg-mkgridmap.conf  to make sure that it has all VOs that you are interested in and also to comment out any VOs that you do not wish to support.  vi /etc/edg-mkgridmap.conf  This utility  edg-mkgridmap  runs as a cronjob  /etc/cron.d/edg-mkgridmap-cron  (by default every 6 hours). You can also run  edg-mkgridmap  manually to see that it generates  /etc/grid-security/grid-mapfile .  edg-mkgridmap  Then, you can enable/start the service.  /sbin/service edg-mkgridmap start\n/sbin/chkconfig edg-mkgridmap on  You can read more on this page:  edg_mkgridmap (on the CE)   Gridmap  \u2014### Configuring GUMS support  Gums  By default, GridFTP uses a gridmap file, found in  /etc/grid-security/gridmap-file . If you want to use GUMS security (recommended), you will need to enable it using the following steps:  First, edit  /etc/grid-security/gsi-authz.conf  and uncomment the globus callout.  globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout  Note that this used to be the full path to the library ( /usr/lib64  or  /usr/lib ), but now we rely on the linker for proper resolution in this file.  Next edit  /etc/lcmaps.db  to edit your gums information:  ...\ngumsclient =  lcmaps_gums_client.mod \n              -resourcetype ce \n              -actiontype execute-now \n              -capath /etc/grid-security/certificates \n              -cert   /etc/grid-security/hostcert.pem \n              -key    /etc/grid-security/hostkey.pem \n              --cert-owner root \n# Change this URL to your GUMS server\n              --endpoint https:// gums.fnal.gov:8443 /gums/services/GUMSXACMLAuthorizationServicePort   If you would like to run SAZ, you will need to enable the relevant lines in the above file as well (more documentation to be added later).  Gums  Authorization  \u2014## (Optional) Modifying the Environment  Environment  Environment variables are stored in  /etc/sysconfig/globus-gridftp-server  which is sourced on service startup. If you want to change LCMAPS log levels, or globus port ranges, you can edit them there.  #Uncomment and modify for firewalls\n#export GLOBUS_TCP_PORT_RANGE=min,max\n#export GLOBUS_TCP_SOURCE_RANGE=min,max  Note that the variables  GLOBUS_TCP_PORT_RANGE  and  GLOBUS_TCP_SOURCE_RANGE  can be set here to allow globus to navigate around firewall rules.  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/globus-gridftp-server  and choose a higher debug level:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n#  4: also info, 5: maximum debug\nLCMAPS_DEBUG_LEVEL=2  Output goes to  /var/log/messages  by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.  Environment  #ConfigMultiHomed \u2014## Configuring a multi-homed server  MultiHomed  The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file  /etc/gridftp.conf .  To use a single interface you can set  hostname  to the Hostname or IP address to use:\\ hostname IP-TO-USE\\  You can also set separately the  control_interface ,  data_interface  and  ipc_interface . E.g. on systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting  data_interface : \\  control\\_interface IP-TO-USE data\\_interface IP-TO-USE ipc\\_interface IP-TO-USE\\   MultiHomed  For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or see the  Globus manual  mentioned in the  Reference  section below.  \u2014# Starting GridFTP  Starting  Starting GridFTP:  [root@client ~]$ service globus-gridftp-server start  Starting  To start Gridftp automatically at boot time  [root@client ~]$ chkconfig globus-gridftp-server on  \u2014# Stopping GridFTP  Stopping  Stopping GridFTP:  [root@client ~]$ service globus-gridftp-server stop  Stopping  \u2014# Validation of services  Validation  The GridFTP service can be validated by using globus-url-copy. You will need to run  grid-proxy-init  or  voms-proxy-init  in order to get a valid user proxy in order to communicate with the GridFTP server.  UCL_PROMPT  globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero UCL_PROMPT  echo $?\n0  Note that you should preferably not try to run validation as root, as globus-url-copy will sometimes attempt to use the host certificate instead of your user certificate, with confusing results.  Validation  \u2014# Gratia GridFTP Transfer Probe The  Gratia GridFTP probe  collects the information about the Gridftp transfers and forwards it to central Gratia collector. You need to enable the probe first. To do this, make sure following is set in file /etc/gratia/gridftp-transfer/ProbeConfig  EnableProbe= 1   All other configuration settings should be suitable for most purposes. However, you can edit them if needed. The probe runs every 30 minutes as a cron job.  \u2014# Useful Configuration and Log Files  Locations     Service/Process  Configuration File  Description      GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP and LCMAPS     /usr/share/osg/sysconfig/globus-gridftp-server-plugin  Where environment variables for GridFTP plugin are included    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration    Gratia Probe  /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      GridFTP  /var/log/gridftp.log  GridFTP transfer log     /var/log/gridftp-auth.log  GridFTP authorization log    Gratia probe  /var/logs/gratia      Locations  Full  \u2014# How to get Help?  If you cannot resolve the problem, there are several ways to receive help:   For bug support and issues, submit a ticket to the  Grid Operations Center .  For community support and best-effort software team support contact  .   For a full set of help options, see  Help Procedure .  \u2014# Screen Dump of Install Procedure   [root@fermicloud108 ~]# wget http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm\n--2011-10-18 11:07:32--  http://download.fedoraproject.org/pub/epel/5/i386/epel-release-5-4.noarch.rpm\nResolving download.fedoraproject.org... 140.211.169.197, 152.19.134.146, 209.132.181.16, ...\nConnecting to download.fedoraproject.org|140.211.169.197|:80... connected.\nHTTP request sent, awaiting response... 302 FOUND\nLocation: http://kdeforge.unl.edu/mirrors/epel/5/i386/epel-release-5-4.noarch.rpm [following]\n--2011-10-18 11:07:33--  http://kdeforge.unl.edu/mirrors/epel/5/i386/epel-release-5-4.noarch.rpm\nResolving kdeforge.unl.edu... 129.93.181.6\nConnecting to kdeforge.unl.edu|129.93.181.6|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 12232 (12K) [application/x-rpm]\nSaving to: `epel-release-5-4.noarch.rpm'\n\n100%[================================================================== ] 12,232      --.-K/s   in 0.03s\n\n2011-10-18 11:07:33 (349 KB/s) - `epel-release-5-4.noarch.rpm' saved [12232/12232]\n\n[root@fermicloud108 ~]# rpm -i epel-release-5-4.noarch.rpm\nwarning: epel-release-5-4.noarch.rpm: Header V3 DSA signature: NOKEY, key ID 217521f6\n[root@fermicloud108 ~]# yum -y install yum-priorities\nLoaded plugins: kernel-module\nepel                                                                                 | 3.7 kB     00:00\nepel/primary_db                                                                      | 3.8 MB     00:02\nfermi-base                                                                           | 2.1 kB     00:00\nfermi-security                                                                       | 1.9 kB     00:00\nfermi-security/primary_db                                                            | 1.7 MB     00:00\nsl-base                                                                              | 2.1 kB     00:00\nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package yum-priorities.noarch 0:1.1.16-14.el5 set to be updated\n--  Finished Dependency Resolution\nBeginning Kernel Module Plugin\nFinished Kernel Module Plugin\n\nDependencies Resolved\n\n============================================================================================================\n Package                      Arch                 Version                      Repository             Size\n============================================================================================================\nInstalling:\n yum-priorities               noarch               1.1.16-14.el5                sl-base                14 k\n\nTransaction Summary\n============================================================================================================\nInstall       1 Package(s)\nUpgrade       0 Package(s)\n\nTotal download size: 14 k\nDownloading Packages:\nyum-priorities-1.1.16-14.el5.noarch.rpm                                              |  14 kB     00:00\nRunning rpm_check_debug\nRunning Transaction Test\nFinished Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing     : yum-priorities                                                                       1/1\n\nInstalled:\n  yum-priorities.noarch 0:1.1.16-14.el5\n\nComplete!\n[root@fermicloud108 ~]# rpm -Uvh http://repo.grid.iu.edu/osg-release-latest.rpm\nRetrieving http://repo.grid.iu.edu/osg-release-latest.rpm\nwarning: /var/tmp/rpm-xfer.tQF1ZU: Header V3 DSA signature: NOKEY, key ID 824b8603\nPreparing...                ########################################### [100%]\n   1:osg-release            ########################################### [100%]\n[root@fermicloud108 ~]# yum --enablerepo=osg-testing install osg-gridftp\nLoaded plugins: kernel-module, priorities\nosg                                                                                  | 1.9 kB     00:00\nosg/primary_db                                                                       |  65 kB     00:00\nosg-testing                                                                          | 1.9 kB     00:00\nosg-testing/primary_db                                                               | 319 kB     00:00\n1232 packages excluded due to repository priority protections\nSetting up Install Process\nResolving Dependencies\n--  Running transaction check\n---  Package osg-gridftp.x86_64 0:3.0.0-5 set to be updated\n--  Processing Dependency: globus-gridftp-server-progs for package: osg-gridftp\n--  Processing Dependency: gratia-probe-gridftp-transfer for package: osg-gridftp\n--  Processing Dependency: vo-client for package: osg-gridftp\n--  Processing Dependency: grid-certificates for package: osg-gridftp\n--  Processing Dependency: gums-client for package: osg-gridftp\n--  Processing Dependency: liblcas_lcmaps_gt4_mapping.so.0()(64bit) for package: osg-gridftp\n--  Running transaction check\n---  Package globus-gridftp-server-progs.x86_64 0:6.1-5.osg set to be updated\n--  Processing Dependency: globus-gridftp-server = 6.1-5.osg for package: globus-gridftp-server-progs\n--  Processing Dependency: globus-xio-gsi-driver  = 2 for package: globus-gridftp-server-progs\n--  Processing Dependency: perl(Globus::Core::Paths) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gssapi_gsi.so.9(globus_gssapi_gsi)(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_credential.so.5()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gssapi_error.so.4()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_io.so.8()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_cert_utils.so.8()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_callout.so.2()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_openssl.so.3()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_authz.so.2()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_authz_callout_error.so.2()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_common.so.14()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_proxy_ssl.so.4()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_xio.so.3()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_proxy_core.so.6()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gfork.so.3()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gridftp_server_control.so.2()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_ftp_control.so.4()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gss_assist.so.8()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_openssl_error.so.2()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gridftp_server.so.6()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_sysconfig.so.5()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_usage.so.3()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gsi_callback.so.4()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_gssapi_gsi.so.9()(64bit) for package: globus-gridftp-server-progs\n--  Processing Dependency: libglobus_oldgaa.so.4()(64bit) for package: globus-gridftp-server-progs\n---  Package gratia-probe-gridftp-transfer.noarch 0:1.09-0.4.1.pre set to be updated\n--  Processing Dependency: gratia-probe-common  = 1.09-0.4.1.pre for package: gratia-probe-gridftp-transfer\n--  Processing Dependency: netlogger for package: gratia-probe-gridftp-transfer\n---  Package gums-client.noarch 0:1.3.18.002-3 set to be updated\n--  Processing Dependency: gums = 1.3.18.002 for package: gums-client\n--  Processing Dependency: osg-vo-map for package: gums-client\n---  Package lcas-lcmaps-gt4-interface.x86_64 0:0.1.4-6.osg set to be updated\n--  Processing Dependency: liblcas.so.0()(64bit) for package: lcas-lcmaps-gt4-interface\n--  Processing Dependency: liblcmaps.so.0()(64bit) for package: lcas-lcmaps-gt4-interface\n--  Processing Dependency: libglobus_gridmap_callout_error.so.1()(64bit) for package: lcas-lcmaps-gt4-interface\n---  Package osg-ca-certs.noarch 0:1.24-1 set to be updated\n---  Package vo-client.noarch 0:38-9.osg set to be updated\n--  Running transaction check\n---  Package globus-authz.x86_64 0:2.0-2.osg set to be updated\n---  Package globus-authz-callout-error.x86_64 0:2.0-2.osg set to be updated\n---  Package globus-callout.x86_64 0:2.0-2.osg set to be updated\n--  Processing Dependency: libltdl.so.3()(64bit) for package: globus-callout\n---  Package globus-common.x86_64 0:14.0-3.osg set to be updated\n---  Package globus-ftp-control.x86_64 0:4.0-2.osg set to be updated\n---  Package globus-gfork.x86_64 0:3.0-2.osg set to be updated\n---  Package globus-gridftp-server.x86_64 0:6.1-5.osg set to be updated\n---  Package globus-gridftp-server-control.x86_64 0:2.0-3.osg set to be updated\n--  Processing Dependency: globus-xio-pipe-driver  = 2 for package: globus-gridftp-server-control\n---  Package globus-gridmap-callout-error.x86_64 0:1.1-1.osg set to be updated\n---  Package globus-gsi-callback.x86_64 0:4.0-2.osg set to be updated\n---  Package globus-gsi-cert-utils.x86_64 0:8.0-2.osg set to be updated\n---  Package globus-gsi-credential.x86_64 0:5.0-3.osg set to be updated\n---  Package globus-gsi-openssl-error.x86_64 0:2.0-2.osg set to be updated\n---  Package globus-gsi-proxy-core.x86_64 0:6.0-2.osg set to be updated\n---  Package globus-gsi-proxy-ssl.x86_64 0:4.0-2.osg set to be updated\n---  Package globus-gsi-sysconfig.x86_64 0:5.0-3.osg set to be updated\n---  Package globus-gss-assist.x86_64 0:8.0-2.osg set to be updated\n---  Package globus-gssapi-error.x86_64 0:4.0-2.osg set to be updated\n---  Package globus-gssapi-gsi.x86_64 0:10.0-1.osg set to be updated\n---  Package globus-io.x86_64 0:9.0-2.osg set to be updated\n---  Package globus-openssl-module.x86_64 0:3.0-2.osg set to be updated\n---  Package globus-usage.x86_64 0:3.0-2.osg set to be updated\n---  Package globus-xio.x86_64 0:3.0-3.osg set to be updated\n---  Package globus-xio-gsi-driver.x86_64 0:2.0-2.osg set to be updated\n---  Package gratia-probe-common.noarch 0:1.09-0.4.1.pre set to be updated\n--  Processing Dependency: pyOpenSSL for package: gratia-probe-common\n---  Package gums.noarch 0:1.3.18.002-3 set to be updated\n--  Processing Dependency: java for package: gums\n---  Package lcas.x86_64 0:1.3.13-8.osg set to be updated\n--  Processing Dependency: liblcas_userban.so()(64bit) for package: lcas\n---  Package lcmaps.x86_64 0:1.4.28-14.osg set to be updated\n--  Processing Dependency: lcmaps-plugins-saz-client for package: lcmaps\n--  Processing Dependency: lcmaps-plugins-gums-client for package: lcmaps\n--  Processing Dependency: liblcmaps_scas_client.so.0()(64bit) for package: lcmaps\n--  Processing Dependency: liblcmaps_verify_proxy.so.0()(64bit) for package: lcmaps\n--  Processing Dependency: libvomsapi.so.1()(64bit) for package: lcmaps\n--  Processing Dependency: liblcmaps_posix_enf.so.0()(64bit) for package: lcmaps\n---  Package netlogger.noarch 0:4.2.0-1 set to be updated\n---  Package osg-vo-map.noarch 0:0.0.1-1.osg set to be updated\n--  Running transaction check\n---  Package globus-xio-pipe-driver.x86_64 0:2.0-2.osg set to be updated\n---  Package java-1.6.0-openjdk.x86_64 1:1.6.0.0-1.22.1.9.8.el5_6 set to be updated\n--  Processing Dependency: jpackage-utils  = 1.7.3-1jpp.2 for package: java-1.6.0-openjdk\n--  Processing Dependency: libasound.so.2(ALSA_0.9)(64bit) for package: java-1.6.0-openjdk\n--  Processing Dependency: libasound.so.2(ALSA_0.9.0rc4)(64bit) for package: java-1.6.0-openjdk\n--  Processing Dependency: tzdata-java for package: java-1.6.0-openjdk\n--  Processing Dependency: libXtst.so.6()(64bit) for package: java-1.6.0-openjdk\n--  Processing Dependency: libasound.so.2()(64bit) for package: java-1.6.0-openjdk\n--  Processing Dependency: libgif.so.4()(64bit) for package: java-1.6.0-openjdk\n---  Package lcas-plugins-basic.x86_64 0:1.3.5-5.osg set to be updated\n---  Package lcmaps-plugins-basic.x86_64 0:1.4.5-1.osg set to be updated\n---  Package lcmaps-plugins-gums-client.x86_64 0:0.0.2-2.osg set to be updated\n--  Processing Dependency: lcmaps-plugins-scas-client for package: lcmaps-plugins-gums-client\n---  Package lcmaps-plugins-saz-client.x86_64 0:0.2.22-7.osg set to be updated\n--  Processing Dependency: saml2-xacml2-c-lib for package: lcmaps-plugins-saz-client\n--  Processing Dependency: libxacml.so.0()(64bit) for package: lcmaps-plugins-saz-client\n---  Package lcmaps-plugins-verify-proxy.x86_64 0:1.4.9-2.osg set to be updated\n---  Package libtool-ltdl.x86_64 0:1.5.22-7.el5_4 set to be updated\n---  Package pyOpenSSL.x86_64 0:0.6-1.p24.7.2.2 set to be updated\n---  Package voms.x86_64 0:2.0.6-3.osg set to be updated\n--  Running transaction check\n---  Package alsa-lib.x86_64 0:1.0.17-1.el5 set to be updated\n---  Package giflib.x86_64 0:4.1.3-7.1.el5_3.1 set to be updated\n---  Package jpackage-utils.noarch 0:1.7.3-1jpp.2.el5 set to be updated\n---  Package lcmaps-plugins-scas-client.x86_64 0:0.2.22-7.osg set to be updated\n---  Package libXtst.x86_64 0:1.0.1-3.1 set to be updated\n---  Package saml2-xacml2-c-lib.x86_64 0:1.0.1-6.osg set to be updated\n---  Package tzdata-java.x86_64 0:2011h-2.el5 set to be updated\n--  Finished Dependency Resolution\nBeginning Kernel Module Plugin\nFinished Kernel Module Plugin\n\nDependencies Resolved\n\n============================================================================================================\n Package                            Arch        Version                           Repository           Size\n============================================================================================================\nInstalling:\n osg-gridftp                        x86_64      3.0.0-5                           osg-testing         2.1 k\nInstalling for dependencies:\n alsa-lib                           x86_64      1.0.17-1.el5                      sl-base             414 k\n giflib                             x86_64      4.1.3-7.1.el5_3.1                 sl-base              39 k\n globus-authz                       x86_64      2.0-2.osg                         osg-testing          14 k\n globus-authz-callout-error         x86_64      2.0-2.osg                         osg-testing         9.9 k\n globus-callout                     x86_64      2.0-2.osg                         osg-testing          16 k\n globus-common                      x86_64      14.0-3.osg                        osg-testing         128 k\n globus-ftp-control                 x86_64      4.0-2.osg                         osg-testing          73 k\n globus-gfork                       x86_64      3.0-2.osg                         osg-testing          19 k\n globus-gridftp-server              x86_64      6.1-5.osg                         osg-testing         163 k\n globus-gridftp-server-control      x86_64      2.0-3.osg                         osg-testing          77 k\n globus-gridftp-server-progs        x86_64      6.1-5.osg                         osg-testing          40 k\n globus-gridmap-callout-error       x86_64      1.1-1.osg                         osg-testing         6.7 k\n globus-gsi-callback                x86_64      4.0-2.osg                         osg-testing          41 k\n globus-gsi-cert-utils              x86_64      8.0-2.osg                         osg-testing          18 k\n globus-gsi-credential              x86_64      5.0-3.osg                         osg-testing          35 k\n globus-gsi-openssl-error           x86_64      2.0-2.osg                         osg-testing          16 k\n globus-gsi-proxy-core              x86_64      6.0-2.osg                         osg-testing          36 k\n globus-gsi-proxy-ssl               x86_64      4.0-2.osg                         osg-testing          17 k\n globus-gsi-sysconfig               x86_64      5.0-3.osg                         osg-testing          29 k\n globus-gss-assist                  x86_64      8.0-2.osg                         osg-testing          34 k\n globus-gssapi-error                x86_64      4.0-2.osg                         osg-testing          13 k\n globus-gssapi-gsi                  x86_64      10.0-1.osg                        osg-testing          60 k\n globus-io                          x86_64      9.0-2.osg                         osg-testing          44 k\n globus-openssl-module              x86_64      3.0-2.osg                         osg-testing          14 k\n globus-usage                       x86_64      3.0-2.osg                         osg-testing          16 k\n globus-xio                         x86_64      3.0-3.osg                         osg-testing         178 k\n globus-xio-gsi-driver              x86_64      2.0-2.osg                         osg-testing          37 k\n globus-xio-pipe-driver             x86_64      2.0-2.osg                         osg-testing          16 k\n gratia-probe-common                noarch      1.09-0.4.1.pre                    osg-testing         132 k\n gratia-probe-gridftp-transfer      noarch      1.09-0.4.1.pre                    osg-testing          22 k\n gums                               noarch      1.3.18.002-3                      osg-testing          25 M\n gums-client                        noarch      1.3.18.002-3                      osg-testing          13 k\n java-1.6.0-openjdk                 x86_64      1:1.6.0.0-1.22.1.9.8.el5_6        fermi-security       37 M\n jpackage-utils                     noarch      1.7.3-1jpp.2.el5                  sl-base              61 k\n lcas                               x86_64      1.3.13-8.osg                      osg-testing          28 k\n lcas-lcmaps-gt4-interface          x86_64      0.1.4-6.osg                       osg-testing          17 k\n lcas-plugins-basic                 x86_64      1.3.5-5.osg                       osg-testing          23 k\n lcmaps                             x86_64      1.4.28-14.osg                     osg-testing          89 k\n lcmaps-plugins-basic               x86_64      1.4.5-1.osg                       osg-testing          38 k\n lcmaps-plugins-gums-client         x86_64      0.0.2-2.osg                       osg-testing         2.6 k\n lcmaps-plugins-saz-client          x86_64      0.2.22-7.osg                      osg-testing          32 k\n lcmaps-plugins-scas-client         x86_64      0.2.22-7.osg                      osg-testing          39 k\n lcmaps-plugins-verify-proxy        x86_64      1.4.9-2.osg                       osg-testing          23 k\n libXtst                            x86_64      1.0.1-3.1                         sl-base              16 k\n libtool-ltdl                       x86_64      1.5.22-7.el5_4                    fermi-security       38 k\n netlogger                          noarch      4.2.0-1                           osg-testing         624 k\n osg-ca-certs                       noarch      1.24-1                            osg-testing         450 k\n osg-vo-map                         noarch      0.0.1-1.osg                       osg-testing         7.3 k\n pyOpenSSL                          x86_64      0.6-1.p24.7.2.2                   sl-base             120 k\n saml2-xacml2-c-lib                 x86_64      1.0.1-6.osg                       osg-testing         581 k\n tzdata-java                        x86_64      2011h-2.el5                       fermi-security      178 k\n vo-client                          noarch      38-9.osg                          osg-testing          15 k\n voms                               x86_64      2.0.6-3.osg                       osg-testing         171 k\n\nTransaction Summary\n============================================================================================================\nInstall      54 Package(s)\nUpgrade       0 Package(s)\n\nTotal download size: 66 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/54): osg-gridftp-3.0.0-5.x86_64.rpm                                               | 2.1 kB     00:00\n(2/54): lcmaps-plugins-gums-client-0.0.2-2.osg.x86_64.rpm                            | 2.6 kB     00:00\n(3/54): globus-gridmap-callout-error-1.1-1.osg.x86_64.rpm                            | 6.7 kB     00:00\n(4/54): osg-vo-map-0.0.1-1.osg.noarch.rpm                                            | 7.3 kB     00:00\n(5/54): globus-authz-callout-error-2.0-2.osg.x86_64.rpm                              | 9.9 kB     00:00\n(6/54): gums-client-1.3.18.002-3.noarch.rpm                                          |  13 kB     00:00\n(7/54): globus-gssapi-error-4.0-2.osg.x86_64.rpm                                     |  13 kB     00:00\n(8/54): globus-authz-2.0-2.osg.x86_64.rpm                                            |  14 kB     00:00\n(9/54): globus-openssl-module-3.0-2.osg.x86_64.rpm                                   |  14 kB     00:00\n(10/54): vo-client-38-9.osg.noarch.rpm                                               |  15 kB     00:00\n(11/54): globus-gsi-openssl-error-2.0-2.osg.x86_64.rpm                               |  16 kB     00:00\n(12/54): libXtst-1.0.1-3.1.x86_64.rpm                                                |  16 kB     00:00\n(13/54): globus-usage-3.0-2.osg.x86_64.rpm                                           |  16 kB     00:00\n(14/54): globus-callout-2.0-2.osg.x86_64.rpm                                         |  16 kB     00:00\n(15/54): globus-xio-pipe-driver-2.0-2.osg.x86_64.rpm                                 |  16 kB     00:00\n(16/54): globus-gsi-proxy-ssl-4.0-2.osg.x86_64.rpm                                   |  17 kB     00:00\n(17/54): lcas-lcmaps-gt4-interface-0.1.4-6.osg.x86_64.rpm                            |  17 kB     00:00\n(18/54): globus-gsi-cert-utils-8.0-2.osg.x86_64.rpm                                  |  18 kB     00:00\n(19/54): globus-gfork-3.0-2.osg.x86_64.rpm                                           |  19 kB     00:00\n(20/54): gratia-probe-gridftp-transfer-1.09-0.4.1.pre.noarch.rpm                     |  22 kB     00:00\n(21/54): lcas-plugins-basic-1.3.5-5.osg.x86_64.rpm                                   |  23 kB     00:00\n(22/54): lcmaps-plugins-verify-proxy-1.4.9-2.osg.x86_64.rpm                          |  23 kB     00:00\n(23/54): lcas-1.3.13-8.osg.x86_64.rpm                                                |  28 kB     00:00\n(24/54): globus-gsi-sysconfig-5.0-3.osg.x86_64.rpm                                   |  29 kB     00:00\n(25/54): lcmaps-plugins-saz-client-0.2.22-7.osg.x86_64.rpm                           |  32 kB     00:00\n(26/54): globus-gss-assist-8.0-2.osg.x86_64.rpm                                      |  34 kB     00:00\n(27/54): globus-gsi-credential-5.0-3.osg.x86_64.rpm                                  |  35 kB     00:00\n(28/54): globus-gsi-proxy-core-6.0-2.osg.x86_64.rpm                                  |  36 kB     00:00\n(29/54): globus-xio-gsi-driver-2.0-2.osg.x86_64.rpm                                  |  37 kB     00:00\n(30/54): libtool-ltdl-1.5.22-7.el5_4.x86_64.rpm                                      |  38 kB     00:00\n(31/54): lcmaps-plugins-basic-1.4.5-1.osg.x86_64.rpm                                 |  38 kB     00:00\n(32/54): lcmaps-plugins-scas-client-0.2.22-7.osg.x86_64.rpm                          |  39 kB     00:00\n(33/54): giflib-4.1.3-7.1.el5_3.1.x86_64.rpm                                         |  39 kB     00:00\n(34/54): globus-gridftp-server-progs-6.1-5.osg.x86_64.rpm                            |  40 kB     00:00\n(35/54): globus-gsi-callback-4.0-2.osg.x86_64.rpm                                    |  41 kB     00:00\n(36/54): globus-io-9.0-2.osg.x86_64.rpm                                              |  44 kB     00:00\n(37/54): globus-gssapi-gsi-10.0-1.osg.x86_64.rpm                                     |  60 kB     00:00\n(38/54): jpackage-utils-1.7.3-1jpp.2.el5.noarch.rpm                                  |  61 kB     00:00\n(39/54): globus-ftp-control-4.0-2.osg.x86_64.rpm                                     |  73 kB     00:00\n(40/54): globus-gridftp-server-control-2.0-3.osg.x86_64.rpm                          |  77 kB     00:00\n(41/54): lcmaps-1.4.28-14.osg.x86_64.rpm                                             |  89 kB     00:00\n(42/54): pyOpenSSL-0.6-1.p24.7.2.2.x86_64.rpm                                        | 120 kB     00:00\n(43/54): globus-common-14.0-3.osg.x86_64.rpm                                         | 128 kB     00:00\n(44/54): gratia-probe-common-1.09-0.4.1.pre.noarch.rpm                               | 132 kB     00:00\n(45/54): globus-gridftp-server-6.1-5.osg.x86_64.rpm                                  | 163 kB     00:00\n(46/54): voms-2.0.6-3.osg.x86_64.rpm                                                 | 171 kB     00:00\n(47/54): globus-xio-3.0-3.osg.x86_64.rpm                                             | 178 kB     00:00\n(48/54): tzdata-java-2011h-2.el5.x86_64.rpm                                          | 178 kB     00:00\n(49/54): alsa-lib-1.0.17-1.el5.x86_64.rpm                                            | 414 kB     00:00\n(50/54): osg-ca-certs-1.24-1.noarch.rpm                                              | 450 kB     00:00\n(51/54): saml2-xacml2-c-lib-1.0.1-6.osg.x86_64.rpm                                   | 581 kB     00:00\n(52/54): netlogger-4.2.0-1.noarch.rpm                                                | 624 kB     00:00\n(53/54): gums-1.3.18.002-3.noarch.rpm                                                |  25 MB     00:02\n(54/54): java-1.6.0-openjdk-1.6.0.0-1.22.1.9.8.el5_6.x86_64.rpm                      |  37 MB     00:00\n------------------------------------------------------------------------------------------------------------\nTotal                                                                       5.3 MB/s |  66 MB     00:12\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603\nosg-testing/gpgkey                                                                   | 1.7 kB     00:00\nImporting GPG key 0x824B8603  OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\nIs this ok [y/N]: y\nRunning rpm_check_debug\nRunning Transaction Test\nFinished Transaction Test\nTransaction Test Succeeded\nRunning Transaction\n  Installing     : globus-gsi-proxy-ssl                                                                1/54\n  Installing     : saml2-xacml2-c-lib                                                                  2/54\n  Installing     : lcmaps-plugins-saz-client                                                           3/54\n  Installing     : libtool-ltdl                                                                        4/54\n  Installing     : globus-common                                                                       5/54\n  Installing     : globus-gsi-openssl-error                                                            6/54\n  Installing     : globus-openssl-module                                                               7/54\n  Installing     : globus-gsi-sysconfig                                                                8/54\n  Installing     : globus-gsi-cert-utils                                                               9/54\n  Installing     : globus-gsi-callback                                                                10/54\n  Installing     : globus-gsi-credential                                                              11/54\n  Installing     : globus-gsi-proxy-core                                                              12/54\n  Installing     : globus-gssapi-gsi                                                                  13/54\n  Installing     : globus-callout                                                                     14/54\n  Installing     : globus-gss-assist                                                                  15/54\n  Installing     : globus-xio                                                                         16/54\n  Installing     : globus-gssapi-error                                                                17/54\n  Installing     : globus-xio-gsi-driver                                                              18/54\n  Installing     : globus-io                                                                          19/54\n  Installing     : globus-authz-callout-error                                                         20/54\n  Installing     : globus-authz                                                                       21/54\n  Installing     : globus-ftp-control                                                                 22/54\n  Installing     : globus-usage                                                                       23/54\n  Installing     : globus-gfork                                                                       24/54\n  Installing     : globus-gridmap-callout-error                                                       25/54\n  Installing     : globus-xio-pipe-driver                                                             26/54\n  Installing     : globus-gridftp-server-control                                                      27/54\n  Installing     : globus-gridftp-server                                                              28/54\n  Installing     : globus-gridftp-server-progs                                                        29/54\n  Installing     : lcmaps-plugins-scas-client                                                         30/54\n  Installing     : voms                                                                               31/54\n  Installing     : giflib                                                                             32/54\n  Installing     : lcmaps-plugins-basic                                                               33/54\n  Installing     : pyOpenSSL                                                                          34/54\n  Installing     : alsa-lib                                                                           35/54\n  Installing     : lcmaps-plugins-verify-proxy                                                        36/54\n  Installing     : libXtst                                                                            37/54\n  Installing     : osg-ca-certs                                                                       38/54\n  Installing     : vo-client                                                                          39/54\n  Installing     : gratia-probe-common                                                                40/54\n  Installing     : lcmaps-plugins-gums-client                                                         41/54\n  Installing     : lcmaps                                                                             42/54\n  Installing     : jpackage-utils                                                                     43/54\n  Installing     : osg-vo-map                                                                         44/54\n  Installing     : netlogger                                                                          45/54\n  Installing     : gratia-probe-gridftp-transfer                                                      46/54\n  Installing     : tzdata-java                                                                        47/54\n  Installing     : java-1.6.0-openjdk                                                                 48/54\n  Installing     : gums                                                                               49/54\n  Installing     : gums-client                                                                        50/54\n  Installing     : lcas                                                                               51/54\n  Installing     : lcas-lcmaps-gt4-interface                                                          52/54\n  Installing     : lcas-plugins-basic                                                                 53/54\n  Installing     : osg-gridftp                                                                        54/54\n\nInstalled:\n  osg-gridftp.x86_64 0:3.0.0-5\n\nDependency Installed:\n  alsa-lib.x86_64 0:1.0.17-1.el5\n  giflib.x86_64 0:4.1.3-7.1.el5_3.1\n  globus-authz.x86_64 0:2.0-2.osg\n  globus-authz-callout-error.x86_64 0:2.0-2.osg\n  globus-callout.x86_64 0:2.0-2.osg\n  globus-common.x86_64 0:14.0-3.osg\n  globus-ftp-control.x86_64 0:4.0-2.osg\n  globus-gfork.x86_64 0:3.0-2.osg\n  globus-gridftp-server.x86_64 0:6.1-5.osg\n  globus-gridftp-server-control.x86_64 0:2.0-3.osg\n  globus-gridftp-server-progs.x86_64 0:6.1-5.osg\n  globus-gridmap-callout-error.x86_64 0:1.1-1.osg\n  globus-gsi-callback.x86_64 0:4.0-2.osg\n  globus-gsi-cert-utils.x86_64 0:8.0-2.osg\n  globus-gsi-credential.x86_64 0:5.0-3.osg\n  globus-gsi-openssl-error.x86_64 0:2.0-2.osg\n  globus-gsi-proxy-core.x86_64 0:6.0-2.osg\n  globus-gsi-proxy-ssl.x86_64 0:4.0-2.osg\n  globus-gsi-sysconfig.x86_64 0:5.0-3.osg\n  globus-gss-assist.x86_64 0:8.0-2.osg\n  globus-gssapi-error.x86_64 0:4.0-2.osg\n  globus-gssapi-gsi.x86_64 0:10.0-1.osg\n  globus-io.x86_64 0:9.0-2.osg\n  globus-openssl-module.x86_64 0:3.0-2.osg\n  globus-usage.x86_64 0:3.0-2.osg\n  globus-xio.x86_64 0:3.0-3.osg\n  globus-xio-gsi-driver.x86_64 0:2.0-2.osg\n  globus-xio-pipe-driver.x86_64 0:2.0-2.osg\n  gratia-probe-common.noarch 0:1.09-0.4.1.pre\n  gratia-probe-gridftp-transfer.noarch 0:1.09-0.4.1.pre\n  gums.noarch 0:1.3.18.002-3\n  gums-client.noarch 0:1.3.18.002-3\n  java-1.6.0-openjdk.x86_64 1:1.6.0.0-1.22.1.9.8.el5_6\n  jpackage-utils.noarch 0:1.7.3-1jpp.2.el5\n  lcas.x86_64 0:1.3.13-8.osg\n  lcas-lcmaps-gt4-interface.x86_64 0:0.1.4-6.osg\n  lcas-plugins-basic.x86_64 0:1.3.5-5.osg\n  lcmaps.x86_64 0:1.4.28-14.osg\n  lcmaps-plugins-basic.x86_64 0:1.4.5-1.osg\n  lcmaps-plugins-gums-client.x86_64 0:0.0.2-2.osg\n  lcmaps-plugins-saz-client.x86_64 0:0.2.22-7.osg\n  lcmaps-plugins-scas-client.x86_64 0:0.2.22-7.osg\n  lcmaps-plugins-verify-proxy.x86_64 0:1.4.9-2.osg\n  libXtst.x86_64 0:1.0.1-3.1\n  libtool-ltdl.x86_64 0:1.5.22-7.el5_4\n  netlogger.noarch 0:4.2.0-1\n  osg-ca-certs.noarch 0:1.24-1\n  osg-vo-map.noarch 0:0.0.1-1.osg\n  pyOpenSSL.x86_64 0:0.6-1.p24.7.2.2\n  saml2-xacml2-c-lib.x86_64 0:1.0.1-6.osg\n  tzdata-java.x86_64 0:2011h-2.el5\n  vo-client.noarch 0:38-9.osg\n  voms.x86_64 0:2.0.6-3.osg\n\nComplete!\n[root@fermicloud108 ~]# cat /etc/grid-security/gsi-authz.conf\n#globus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n[root@fermicloud108 ~]# sed -i 's/\\#globus_mapping/globus_mapping/' /etc/grid-security/gsi-authz.conf\n[root@fermicloud108 ~]# cat /etc/grid-security/gsi-authz.conf\nglobus_mapping liblcas_lcmaps_gt4_mapping.so lcmaps_callout\n[root@fermicloud108 ~]# vi /etc/lcmaps.db\n[root@fermicloud108 ~]# sed -i 's/yourgums.yourdomain/gums.fnal.gov/' /etc/lcmaps.db\n[root@fermicloud108 ~]# vi /etc/lcmaps.db\n[root@fermicloud108 ~]# service globus-gridftp-server start\nStarted GridFTP Server                                     [  OK  ]\n[root@fermicloud108 ~]# ps -ef | grep globus\nroot      2364     1  0 11:12 ?        00:00:00 /usr/sbin/globus-gridftp-server -c /etc/gridftp.conf -pidfile /var/run/globus-gridftp-server.pid -no-detach -config-base-path /\nroot      2371  2164  0 11:12 pts/0    00:00:00 grep globus\n[root@fermicloud108 ~]#     #DocReferences \u2014# References   Globus GridFTP administration manual  Globus GridFTP tutorial", 
            "title": "Install a GridFTP Server"
        }
    ]
}