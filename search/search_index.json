{
    "docs": [
        {
            "location": "/", 
            "text": "OSG Site Administrator Documentation\n\n\nWelcome to the home of the Open Science Grid (OSG) Site Administrator documentation!  This documentation aims to\nprovide OSG site admins with the necessary information to install, configure, and operate site services.\n\n\nIf you are not a site adminstrator:\n\n\n\n\nIf you are a \nresearcher\n interested in using OSG resources, you may want to view our\n  \nuser documentation\n.\n\n\nIf you'd like to learn more about the OSG and our mission, visit the OSG consortium's\n  \nhomepage\n.\n\n\n\n\nThis document outlines the overall installation process for an OSG site and provides many links into detailed\ninstallation, configuration, troubleshooting, and similar pages. If you do not see software-related technical\ndocumentation listed here, try the search bar at the top or contacting us at\n\nhelp@opensciencegrid.org\n.\n\n\nPlan the Site\n\n\nIf you have not done so already, \nplan the overall architecture of your OSG site\n.\nIt is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software\ncomponents for each host.\nBe sure to consider \nthe operating systems that OSG supports\n. For example, a basic site might include:\n\n\n\n\n\n\n\n\nPurpose\n\n\nHost\n\n\nMajor Software\n\n\n\n\n\n\n\n\n\n\nCompute Element (CE)\n\n\nosg-ce.example.edu\n\n\nOSG CE, HTCondor Central Manager, etc. (\nosg-ce-condor\n)\n\n\n\n\n\n\nWorker Nodes\n\n\nwNNN.cluster.example.edu\n\n\nOSG worker node client (\nosg-wn-client\n)\n\n\n\n\n\n\n\n\nPrepare the Batch System\n\n\nThe assumption is that you have an existing batch system at your site.\nCurrently, we support \nHTCondor\n,\n\nLSF\n, \nPBS\n and\n\nTORQUE\n,\n\nSGE\n, and \nSlurm\n batch systems.\n\n\nFor smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute\nElement (CE) on the central host of your batch system.\nAt such a site - especially if you have minimal time to maintain a CE - you may want to contact\n\n to ask about using an OSG-hosted CE instead of running your own.\nBefore proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into\nyour batch system.\n\n\nAdd OSG Software\n\n\nIf necessary, provision all OSG hosts that are in your site plan that do not exist yet.\nThe general steps to installing an OSG site are:\n\n\n\n\nInstall \nOSG Yum Repos\n and the \nCompute Element software\n\n   on your CE host\n\n\nInstall the \nWorker Node client\n on your worker nodes.\n\n\nInstall \noptional software\n to increase the capabilities of your site.\n\n\n\n\n\n\nNote\n\n\nFor sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management\ntool to install, configure, and maintain your site.\nWhile beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular\nconfiguration management tools are \nPuppet\n, \nChef\n,\n\nAnsible\n, and \nCFEngine\n.\n\n\n\n\nGeneral Installation Instructions\n\n\n\n\nSecurity information for OSG signed RPMs\n\n\nUsing Yum and RPM\n\n\nInstall the OSG Yum repositories\n\n\nOSG Software release series\n - look here to upgrade between releases 3.1, 3.2, 3.3, and 3.4.\n\n\n\n\nInstalling and Managing Certificates for Site Security\n\n\n\n\nInstalling the grid certificate authorities (CAs)\n\n\nHow do I get X.509 host certificates?\n\n\nAutomatically updating the grid certificate authorities (CAs)\n\n\nOSG PKI command line client reference\n\n\n\n\nInstalling and Configuring the Compute Element\n\n\n\n\nInstall the compute element (HTCondor-CE and other software):\n\n\nOverview and architecture\n\n\nRequest a Hosted CE\n\n\nInstall HTCondor-CE\n\n\nConfigure the HTCondor-CE job router\n, including common recipes\n\n\nTroubleshooting HTCondor-CE installations\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\n\n\nosg-configure\n Reference\n\n\n\n\nAdding OSG Software to Worker Nodes\n\n\n\n\nWorker Node (WN) Client Overview\n\n\nInstall the WN client software on every worker node \u2013 pick a method:\n\n\nUsing RPMs\n \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)\n\n\nUsing a tarball\n \u2013 useful for installation onto a shared filesystem (does not\n    require root access)\n\n\nUsing OASIS\n \u2013 useful when \nCVMFS\n is already mounted\n    on your worker nodes\n\n\n\n\n\n\n(optional) \nInstall the CernVM-FS client\n to make it easy for user jobs to use needed\n    software from OSG's OASIS repositories\n\n\n(optional) \nInstall singularity on the OSG worker node\n, to allow pilot jobs to\n    isolate user jobs.\n\n\n\n\nInstalling and Configuring Other Services\n\n\nAll of these node types and their services are optional, although OSG requires an HTTP caching service if you have\ninstalled \nCVMFS\n on your worker nodes.\n\n\n\n\nInstall Frontier Squid\n, an HTTP caching proxy service.\n\n\nStorage element:\n\n\nExisting POSIX-based systems (such as NFS, Lustre, or GPFS):\n\n\nInstall standalone OSG GridFTP\n: GridFTP server\n\n\n(optional) \nInstall load-balanced OSG GridFTP\n: when a single GridFTP server\n    isn't enough\n\n\n\n\n\n\nHadoop Distributed File System (HDFS):\n\n\nHadoop Overview\n: HDFS information, planning, and guides\n\n\n\n\n\n\nXRootD:\n\n\nXRootd Overview\n: XRootD information, planning, and guides\n\n\nInstall XRootD Server\n: XRootD redirector installation\n\n\n\n\n\n\n\n\n\n\nRSV monitoring to monitor and report to OSG on the health of your site\n\n\nInstall RSV\n\n\n\n\n\n\nInstall the GlideinWMS VO Frontend\n if your want your users' jobs to run on the OSG\n\n\nInstall the RSV GlideinWMS Tester\n if you want to test your front-end's\n    ability to submit jobs to sites in the OSG\n\n\n\n\n\n\n\n\nVerify OSG Software\n\n\nBefore receiving real OSG work, your site needs to successfully run test jobs from our\n\nGlideinWMS\n factory and report usage to the \nGRACC\n.\n\n\nIf you haven't already, \nregister\n any publicly facing resources with OSG software installed,\nincluding HTCondor-CE, Frontier Squid, GridFTP, and/or XRootD.\n\n\nTest locally\n\n\nIt is useful to test \nmanual\n submission of jobs from inside and outside of your site through your CE to your batch\nsystem.\nIf this process does not work manually, it will probably not work for the GlideinWMS pilot factory either.\n\n\n\n\nTest job submission into an HTCondor-CE\n\n\n\n\nGet test jobs\n\n\nTo begin running pilots at your site, e-mail \n and ask for test pilots.\nPlease provide them with the following information:\n\n\n\n\nThe fully qualified domain name of the CE\n\n\nResource name\n\n\nSupported OS version of your worker nodes (e.g., EL6, EL7, or both)\n\n\nSupport for multicore jobs\n\n\nMaximum job walltime\n\n\nMaximum job memory usage\n\n\n\n\nOnce the factory team has enough information, they will start submitting pilots from the test factory to your CE.\nInitially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that\nnumber will be ramped up to 10, then 100.\n\n\nVerify reporting and monitoring\n\n\nTo verify that your site is correctly reporting to the OSG, check\n\nOSG's Accounting Portal\n for records of your site reports\n(select your site from the drop-down box). If you have enabled the OSG VO, you can also check\n\nhttp://flock.opensciencegrid.org/monitoring/condor/sites/all_1day.html\n.\n\n\nScale Up to Full Production\n\n\nAfter successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports,\nyour site will be deemed production ready.\nNo action is required on your end, factory operations will start submitting pilot jobs from the production factory.\n\n\nMaintain the Site\n\n\nTo avoid potential issues with OSG job submissions, please \nnotify us\n of major changes\nto your site, including:\n\n\n\n\nMajor OS version changes on the worker nodes (e.g., upgraded from EL 6 to EL 7)\n\n\nAdding or removing \ncontainer support\n\n\nPolicy changes regarding maximum walltime or memory usage\n\n\nScheduled or unscheduled \ndowntimes\n\n\nSite topology changes\n such as additions, modifications, or retirements\n\n\nChanges to site contacts, such as administrative or security staff\n\n\n\n\nIt is also important to keep your software and data (e.g., CA and VO client) up-to-date with the\n\nlatest OSG release\n.\nTo stay abreast of software releases, we recommend subscribing to the \n mailing\nlist.", 
            "title": "Home"
        }, 
        {
            "location": "/#osg-site-administrator-documentation", 
            "text": "Welcome to the home of the Open Science Grid (OSG) Site Administrator documentation!  This documentation aims to\nprovide OSG site admins with the necessary information to install, configure, and operate site services.  If you are not a site adminstrator:   If you are a  researcher  interested in using OSG resources, you may want to view our\n   user documentation .  If you'd like to learn more about the OSG and our mission, visit the OSG consortium's\n   homepage .   This document outlines the overall installation process for an OSG site and provides many links into detailed\ninstallation, configuration, troubleshooting, and similar pages. If you do not see software-related technical\ndocumentation listed here, try the search bar at the top or contacting us at help@opensciencegrid.org .", 
            "title": "OSG Site Administrator Documentation"
        }, 
        {
            "location": "/#plan-the-site", 
            "text": "If you have not done so already,  plan the overall architecture of your OSG site .\nIt is recommended that your plan be sufficiently detailed to include the OSG hosts that are needed and the main software\ncomponents for each host.\nBe sure to consider  the operating systems that OSG supports . For example, a basic site might include:     Purpose  Host  Major Software      Compute Element (CE)  osg-ce.example.edu  OSG CE, HTCondor Central Manager, etc. ( osg-ce-condor )    Worker Nodes  wNNN.cluster.example.edu  OSG worker node client ( osg-wn-client )", 
            "title": "Plan the Site"
        }, 
        {
            "location": "/#prepare-the-batch-system", 
            "text": "The assumption is that you have an existing batch system at your site.\nCurrently, we support  HTCondor , LSF ,  PBS  and TORQUE , SGE , and  Slurm  batch systems.  For smaller sites (less than 50 worker nodes), the most common way to add a site to OSG is to install the OSG Compute\nElement (CE) on the central host of your batch system.\nAt such a site - especially if you have minimal time to maintain a CE - you may want to contact  to ask about using an OSG-hosted CE instead of running your own.\nBefore proceeding with an install, be sure that you can submit and successfully run a job from your OSG CE host into\nyour batch system.", 
            "title": "Prepare the Batch System"
        }, 
        {
            "location": "/#add-osg-software", 
            "text": "If necessary, provision all OSG hosts that are in your site plan that do not exist yet.\nThe general steps to installing an OSG site are:   Install  OSG Yum Repos  and the  Compute Element software \n   on your CE host  Install the  Worker Node client  on your worker nodes.  Install  optional software  to increase the capabilities of your site.    Note  For sites with more than a handful of worker nodes, it is recommended to use some sort of configuration management\ntool to install, configure, and maintain your site.\nWhile beyond the scope of OSG\u2019s documentation to explain how to select and use such a system, some popular\nconfiguration management tools are  Puppet ,  Chef , Ansible , and  CFEngine .", 
            "title": "Add OSG Software"
        }, 
        {
            "location": "/#general-installation-instructions", 
            "text": "Security information for OSG signed RPMs  Using Yum and RPM  Install the OSG Yum repositories  OSG Software release series  - look here to upgrade between releases 3.1, 3.2, 3.3, and 3.4.", 
            "title": "General Installation Instructions"
        }, 
        {
            "location": "/#installing-and-managing-certificates-for-site-security", 
            "text": "Installing the grid certificate authorities (CAs)  How do I get X.509 host certificates?  Automatically updating the grid certificate authorities (CAs)  OSG PKI command line client reference", 
            "title": "Installing and Managing Certificates for Site Security"
        }, 
        {
            "location": "/#installing-and-configuring-the-compute-element", 
            "text": "Install the compute element (HTCondor-CE and other software):  Overview and architecture  Request a Hosted CE  Install HTCondor-CE  Configure the HTCondor-CE job router , including common recipes  Troubleshooting HTCondor-CE installations  Submitting jobs to HTCondor-CE    osg-configure  Reference", 
            "title": "Installing and Configuring the Compute Element"
        }, 
        {
            "location": "/#adding-osg-software-to-worker-nodes", 
            "text": "Worker Node (WN) Client Overview  Install the WN client software on every worker node \u2013 pick a method:  Using RPMs  \u2013 useful when managing your worker nodes with a tool (e.g., Puppet, Chef)  Using a tarball  \u2013 useful for installation onto a shared filesystem (does not\n    require root access)  Using OASIS  \u2013 useful when  CVMFS  is already mounted\n    on your worker nodes    (optional)  Install the CernVM-FS client  to make it easy for user jobs to use needed\n    software from OSG's OASIS repositories  (optional)  Install singularity on the OSG worker node , to allow pilot jobs to\n    isolate user jobs.", 
            "title": "Adding OSG Software to Worker Nodes"
        }, 
        {
            "location": "/#installing-and-configuring-other-services", 
            "text": "All of these node types and their services are optional, although OSG requires an HTTP caching service if you have\ninstalled  CVMFS  on your worker nodes.   Install Frontier Squid , an HTTP caching proxy service.  Storage element:  Existing POSIX-based systems (such as NFS, Lustre, or GPFS):  Install standalone OSG GridFTP : GridFTP server  (optional)  Install load-balanced OSG GridFTP : when a single GridFTP server\n    isn't enough    Hadoop Distributed File System (HDFS):  Hadoop Overview : HDFS information, planning, and guides    XRootD:  XRootd Overview : XRootD information, planning, and guides  Install XRootD Server : XRootD redirector installation      RSV monitoring to monitor and report to OSG on the health of your site  Install RSV    Install the GlideinWMS VO Frontend  if your want your users' jobs to run on the OSG  Install the RSV GlideinWMS Tester  if you want to test your front-end's\n    ability to submit jobs to sites in the OSG", 
            "title": "Installing and Configuring Other Services"
        }, 
        {
            "location": "/#verify-osg-software", 
            "text": "Before receiving real OSG work, your site needs to successfully run test jobs from our GlideinWMS  factory and report usage to the  GRACC .  If you haven't already,  register  any publicly facing resources with OSG software installed,\nincluding HTCondor-CE, Frontier Squid, GridFTP, and/or XRootD.", 
            "title": "Verify OSG Software"
        }, 
        {
            "location": "/#test-locally", 
            "text": "It is useful to test  manual  submission of jobs from inside and outside of your site through your CE to your batch\nsystem.\nIf this process does not work manually, it will probably not work for the GlideinWMS pilot factory either.   Test job submission into an HTCondor-CE", 
            "title": "Test locally"
        }, 
        {
            "location": "/#get-test-jobs", 
            "text": "To begin running pilots at your site, e-mail   and ask for test pilots.\nPlease provide them with the following information:   The fully qualified domain name of the CE  Resource name  Supported OS version of your worker nodes (e.g., EL6, EL7, or both)  Support for multicore jobs  Maximum job walltime  Maximum job memory usage   Once the factory team has enough information, they will start submitting pilots from the test factory to your CE.\nInitially, this will be one pilot at a time but once the factory verifies that pilot jobs are running successfully, that\nnumber will be ramped up to 10, then 100.", 
            "title": "Get test jobs"
        }, 
        {
            "location": "/#verify-reporting-and-monitoring", 
            "text": "To verify that your site is correctly reporting to the OSG, check OSG's Accounting Portal  for records of your site reports\n(select your site from the drop-down box). If you have enabled the OSG VO, you can also check http://flock.opensciencegrid.org/monitoring/condor/sites/all_1day.html .", 
            "title": "Verify reporting and monitoring"
        }, 
        {
            "location": "/#scale-up-to-full-production", 
            "text": "After successfully running all the pilot jobs that are submitted by the test factory and verifying your site reports,\nyour site will be deemed production ready.\nNo action is required on your end, factory operations will start submitting pilot jobs from the production factory.", 
            "title": "Scale Up to Full Production"
        }, 
        {
            "location": "/#maintain-the-site", 
            "text": "To avoid potential issues with OSG job submissions, please  notify us  of major changes\nto your site, including:   Major OS version changes on the worker nodes (e.g., upgraded from EL 6 to EL 7)  Adding or removing  container support  Policy changes regarding maximum walltime or memory usage  Scheduled or unscheduled  downtimes  Site topology changes  such as additions, modifications, or retirements  Changes to site contacts, such as administrative or security staff   It is also important to keep your software and data (e.g., CA and VO client) up-to-date with the latest OSG release .\nTo stay abreast of software releases, we recommend subscribing to the   mailing\nlist.", 
            "title": "Maintain the Site"
        }, 
        {
            "location": "/site-planning/", 
            "text": "Site Planning\n\n\nThe OSG vision is to integrate computing across different resource types and business models to allow campus IT to offer\na maximally flexible \nhigh throughput computing\n (HTC) environment for their researchers.\n\n\nThis document is for \nSystem Administrators\n and aims to provide an overview of the different options to consider when\nplanning to share resources via the OSG.\n\n\nAfter reading, you should be able to understand what software or services you want to provide to support your\nresearchers\n\n\n\n\nNote\n\n\nThis document covers the most common options.\nOSG is a diverse infrastructure: depending on what groups you want to support, you may need to install additional\nservices.\nCoordinate with your local researchers.\n\n\n\n\nOSG Site Services\n\n\nThe OSG Software stack tries to provide a uniform computing and storage fabric across many independently-managed\ncomputing and storage resources.\nThese individual services will be accessed by virtual organizations (VOs), which will delegate the resources to\nscientists, researchers, and students.\n\n\nSharing\n is a fundamental principle for the OSG: your site is encouraged to support as many OSG-registered VOs as\nlocal conditions allow.\n\nAutonomy\n is another principle: you are not required to support any VOs you do not want.\nAs the administrator, your task is to make your existing computing and storage resources available to and reliable for\nyour supported VOs.\n\n\nWe break this down into three tasks:\n\n\n\n\nGetting \n\"pilot jobs\"\n submitted to your site batch system.\n\n\nEstablishing an OSG \nruntime environment\n for running jobs.\n\n\nDelivering data\n to payload applications to be processed.\n\n\n\n\nThere are multiple approaches for each item, depending on the VOs you support, and time you have to invest in the OSG.\n\n\n\n\nNote\n\n\nAn essential concept in the OSG is the \"pilot job\".\nThe pilot, which arrives at your batch system, is sent by the VO to get a resource allocation.\nHowever, it \ndoes not\n contain any research payload.\nOnce started, it will connect back to a resource pool and pull down individuals' research \"payload jobs\".\nHence, we do not think about submitting \"jobs\" to sites but rather \"resource requests\".\n\n\n\n\nPilot Jobs\n\n\nTraditionally, an OSG \nCompute Element\n (CE) provides remote access for VOs to submit pilot jobs to your\n\nlocal batch system\n.\nHowever, today, there are two options for accepting pilot jobs at your site:\n\n\n\n\nHosted CE\n: OSG will run and operate the CE services; the site only needs to provide a SSH pubkey-based\n   authentication access to the central OSG host.\n   OSG will interface with the VO and submit pilots directly to your batch system via SSH.\n   By far, this is the \nsimplest option\n: however, it is less-scalable and the site delegates many of the scheduling\n   decisions to the OSG.\n   Contact \n for more information on the hosted CE.\n\n\nOSG CE\n: The traditional option where the site installs and operates a HTCondor-based CE on a dedicated host.\n   This provides the best scalability and flexibility, but may require an ongoing time investment from the site.\n   The OSG CE install and operation is covered in this documentation page.\n\n\n\n\nRuntime environment\n\n\nThe OSG requires a very minimal runtime environment that can be deployed via \ntarball\n,\n\nRPM\n, or through a \nglobal filesystem\n on your cluster's worker\nnodes.\n\n\nWe believe that all research applications should be portable and self-contained, with no OS dependencies.\nThis provides access to the most resources and minimizes the presence at sites.\nHowever, this ideal is often difficult to achieve in practice.\nFor sites that want to support a uniform runtime environment, we provide a global filesystem called\n\nCVMFS\n that VOs can use to distribute their own software dependencies.\n\n\nFinally, many researchers use applications that require a specific OS environment - not just individual dependencies -\nthat is distributed as a container.\nOSG supports the use of the \nSingularity\n container runtime with\n\nDocker-based\n image distribution.\n\n\nData Services\n\n\nWhether accessed through CVMFS or command-line software like \ncurl\n, the majority of software is moved via HTTP in\ncache-friendly patterns.\nAll sites are highly encouraged to use an \nHTTP proxy\n to reduce the load on the WAN from the\ncluster.\n\n\nDepending on the VOs you want to support, additional data services may be necessary:\n\n\n\n\nSome VOs elect to stream their larger input data from offsite using OSG's \"StashCache\" service.\n  This requires no services to be run by the site\n\n\nThe largest sites will additionally run large-scale data services such as a \"storage element\".\n  This is often required for sites that want to support more complex organizations such as ATLAS or CMS.\n\n\n\n\nSite Policies\n\n\nSites are encouraged to clearly specify and communicate their local policies regarding resource access.\nOne common mechanism to do this is post them on a web page and make this page part of your\n\nsite registration\n.\nWritten policies help external entities understand what your site wants to accomplish with the OSG -- and are often\ninternally clarifying.\n\n\nIn line of our principle of \nsharing\n, we encourage you to allow virtual organizations registered with the OSG\n\"opportunistic use\" of your resources.\nYou may need to preempt those jobs when higher priority jobs come around.\nThe end-users using the OSG generally prefer having access to your site subject to preemption over having no access\nat all.", 
            "title": "Site Planning"
        }, 
        {
            "location": "/site-planning/#site-planning", 
            "text": "The OSG vision is to integrate computing across different resource types and business models to allow campus IT to offer\na maximally flexible  high throughput computing  (HTC) environment for their researchers.  This document is for  System Administrators  and aims to provide an overview of the different options to consider when\nplanning to share resources via the OSG.  After reading, you should be able to understand what software or services you want to provide to support your\nresearchers   Note  This document covers the most common options.\nOSG is a diverse infrastructure: depending on what groups you want to support, you may need to install additional\nservices.\nCoordinate with your local researchers.", 
            "title": "Site Planning"
        }, 
        {
            "location": "/site-planning/#osg-site-services", 
            "text": "The OSG Software stack tries to provide a uniform computing and storage fabric across many independently-managed\ncomputing and storage resources.\nThese individual services will be accessed by virtual organizations (VOs), which will delegate the resources to\nscientists, researchers, and students.  Sharing  is a fundamental principle for the OSG: your site is encouraged to support as many OSG-registered VOs as\nlocal conditions allow. Autonomy  is another principle: you are not required to support any VOs you do not want.\nAs the administrator, your task is to make your existing computing and storage resources available to and reliable for\nyour supported VOs.  We break this down into three tasks:   Getting  \"pilot jobs\"  submitted to your site batch system.  Establishing an OSG  runtime environment  for running jobs.  Delivering data  to payload applications to be processed.   There are multiple approaches for each item, depending on the VOs you support, and time you have to invest in the OSG.   Note  An essential concept in the OSG is the \"pilot job\".\nThe pilot, which arrives at your batch system, is sent by the VO to get a resource allocation.\nHowever, it  does not  contain any research payload.\nOnce started, it will connect back to a resource pool and pull down individuals' research \"payload jobs\".\nHence, we do not think about submitting \"jobs\" to sites but rather \"resource requests\".", 
            "title": "OSG Site Services"
        }, 
        {
            "location": "/site-planning/#pilot-jobs", 
            "text": "Traditionally, an OSG  Compute Element  (CE) provides remote access for VOs to submit pilot jobs to your local batch system .\nHowever, today, there are two options for accepting pilot jobs at your site:   Hosted CE : OSG will run and operate the CE services; the site only needs to provide a SSH pubkey-based\n   authentication access to the central OSG host.\n   OSG will interface with the VO and submit pilots directly to your batch system via SSH.\n   By far, this is the  simplest option : however, it is less-scalable and the site delegates many of the scheduling\n   decisions to the OSG.\n   Contact   for more information on the hosted CE.  OSG CE : The traditional option where the site installs and operates a HTCondor-based CE on a dedicated host.\n   This provides the best scalability and flexibility, but may require an ongoing time investment from the site.\n   The OSG CE install and operation is covered in this documentation page.", 
            "title": "Pilot Jobs"
        }, 
        {
            "location": "/site-planning/#runtime-environment", 
            "text": "The OSG requires a very minimal runtime environment that can be deployed via  tarball , RPM , or through a  global filesystem  on your cluster's worker\nnodes.  We believe that all research applications should be portable and self-contained, with no OS dependencies.\nThis provides access to the most resources and minimizes the presence at sites.\nHowever, this ideal is often difficult to achieve in practice.\nFor sites that want to support a uniform runtime environment, we provide a global filesystem called CVMFS  that VOs can use to distribute their own software dependencies.  Finally, many researchers use applications that require a specific OS environment - not just individual dependencies -\nthat is distributed as a container.\nOSG supports the use of the  Singularity  container runtime with Docker-based  image distribution.", 
            "title": "Runtime environment"
        }, 
        {
            "location": "/site-planning/#data-services", 
            "text": "Whether accessed through CVMFS or command-line software like  curl , the majority of software is moved via HTTP in\ncache-friendly patterns.\nAll sites are highly encouraged to use an  HTTP proxy  to reduce the load on the WAN from the\ncluster.  Depending on the VOs you want to support, additional data services may be necessary:   Some VOs elect to stream their larger input data from offsite using OSG's \"StashCache\" service.\n  This requires no services to be run by the site  The largest sites will additionally run large-scale data services such as a \"storage element\".\n  This is often required for sites that want to support more complex organizations such as ATLAS or CMS.", 
            "title": "Data Services"
        }, 
        {
            "location": "/site-planning/#site-policies", 
            "text": "Sites are encouraged to clearly specify and communicate their local policies regarding resource access.\nOne common mechanism to do this is post them on a web page and make this page part of your site registration .\nWritten policies help external entities understand what your site wants to accomplish with the OSG -- and are often\ninternally clarifying.  In line of our principle of  sharing , we encourage you to allow virtual organizations registered with the OSG\n\"opportunistic use\" of your resources.\nYou may need to preempt those jobs when higher priority jobs come around.\nThe end-users using the OSG generally prefer having access to your site subject to preemption over having no access\nat all.", 
            "title": "Site Policies"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/", 
            "text": "HTCondor-CE Overview\n\n\nThis document serves as an introduction to HTCondor-CE and how it works.\nBefore continuing with the overview, make sure that you are familiar with the following concepts:\n\n\n\n\nAn OSG site plan\n\n\nWhat is a batch system and which one will you use (\nHTCondor\n, PBS, LSF, SGE, or\n    \nSLURM\n)?\n\n\nSecurity in the OSG via \nGSI\n (i.e.,\n    \nCertificate authorities\n, user and host\n    \ncertificates\n, proxies)\n\n\n\n\n\n\nPilot jobs, frontends, and factories (i.e., \nGlideinWMS\n,\n    AutoPyFactory)\n\n\n\n\nWhat is a Compute Element?\n\n\nAn OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install\non a machine that can submit jobs into your local batch system.\nAt the heart of the CE is the \njob gateway\n software, which is responsible for handling incoming jobs, authenticating\nand authorizing them, and delegating them to your batch system for execution.\n\n\nToday in the OSG, most jobs that arrive at a CE (called \ngrid jobs\n) are \nnot\n end-user jobs, but rather pilot jobs\nsubmitted from factories.\nSuccessful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run\nwithin the pilot job container.\nEventually pilot jobs remove themselves, typically after a period of inactivity.\n\n\nWhat is HTCondor-CE?\n\n\nHTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG.\nIt is configured to use the \nJobRouter daemon\n to\ndelegate jobs by transforming and submitting them to the site\u2019s batch system.\n\n\nBenefits of running the HTCondor-CE:\n\n\n\n\nScalability:\n HTCondor-CE is capable of supporting job workloads of large sites\n\n\nDebugging tools:\n HTCondor-CE offers \nmany tools to help troubleshoot\n\n    issues with jobs\n\n\nRouting as configuration:\n HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration\n    variables, which means that customizations will persist across upgrades and will not involve modification of\n    software internals to route jobs\n\n\n\n\nHow Jobs Run\n\n\nOnce an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a\ntransformed copy (called the \nrouted job\n) and submits the copy to the batch system (called the \nbatch system job\n).\nAfter submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which\nin turn notifies the original submitter (e.g., job factory) of any updates.\nWhen the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to\nthe original submitter.\n\n\nHosted CE over SSH\n\n\nThe Hosted CE is intended for small sites or as an introduction to the OSG.\nThe OSG configures and maintains an HTCondor-CE on behalf of the site.\nThe Hosted CE is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH.\nIt provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with\nminimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.\n\n\nIf your site intends to run over 10,000 concurrent OSG jobs, you will need to host your own\n\nHTCondor-CE\n because the Hosted CE has not yet been optimized for such\nloads.\n\n\nIf you are interested in a Hosted CE solution, please follow the instructions on \nthis page\n.\n\n\n\n\nOn HTCondor batch systems\n\n\nFor a site with an HTCondor \nbatch system\n, the JobRouter can use HTCondor protocols to place a transformed copy of\nthe grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the\nsame.\nThus, there are three representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the incoming grid job\u2019s ID\n\n\nHTCondor batch system: the routed job\u2019s ID\n\n\n\n\n\n\nIn an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool\ndirectory using internal HTCondor protocols.\n\n\n\n\nNote\n\n\nThe JobRouter copies the job directly into the batch system and does not make use of \ncondor_submit\n.\nThis means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are\nsubmitted (i.e., \nSUBMIT_EXPRS\n), these attributes will not be added to the routed jobs.\n\n\n\n\nOn other batch systems\n\n\nFor non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job\nsubmits a job into the batch system via a process called the BLAHP.\nThus, there are four representations of your job, each with its own ID (see diagram below):\n\n\n\n\nSubmit host: the HTCondor job ID in the original queue\n\n\nHTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID\n\n\nHTCondor batch system: the batch system\u2019s job ID\n\n\n\n\nAlthough the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:\n\n\n\n\nWith non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool\ndirectory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.\n\n\nHow the CE is Customized\n\n\nAside from the \nbasic configuration\n required in the CE\ninstallation, there are two main ways to customize your CE (if you decide any customization is required at all):\n\n\n\n\nDeciding which VOs are allowed to run at your site:\n The recommended method of authorizing VOs at your site is\n    based on the \nLCMAPS framework\n\n\nHow to filter and transform the grid jobs to be run on your batch system:\n Filtering and transforming grid jobs\n    (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes.\n    For examples of common job routes, consult the \nJobRouter recipes\n page.\n\n\n\n\n\n\nNote\n\n\nIf you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one\nresiding in \n/etc/condor/\n and the other in \n/etc/condor-ce\n) and will need to make sure to differentiate the two\nwhen editing any configuration.\n\n\n\n\nHow Security Works\n\n\nIn the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue\ncertificates.\nWhen these clients and hosts wish to communicate with each other, the identities of each party is confirmed by\ncross-checking their certificates with the signing CA and establishing trust.\n\n\nIn its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate\nchain, which will work with \nLCMAPS VOMS authentication\n.\nAdditionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared\nsecret, or even IP-based authentication.\nMore information about authorization methods can be found\n\nhere\n.\n\n\nNext steps\n\n\nOnce the basic installation is done, additional activities include:\n\n\n\n\nSetting up job routes to customize incoming jobs\n\n\nSubmitting jobs to a HTCondor-CE\n \n\n\nTroubleshooting the HTCondor-CE\n\n\nRegister the CE\n\n\nRegister with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#htcondor-ce-overview", 
            "text": "This document serves as an introduction to HTCondor-CE and how it works.\nBefore continuing with the overview, make sure that you are familiar with the following concepts:   An OSG site plan  What is a batch system and which one will you use ( HTCondor , PBS, LSF, SGE, or\n     SLURM )?  Security in the OSG via  GSI  (i.e.,\n     Certificate authorities , user and host\n     certificates , proxies)    Pilot jobs, frontends, and factories (i.e.,  GlideinWMS ,\n    AutoPyFactory)", 
            "title": "HTCondor-CE Overview"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-a-compute-element", 
            "text": "An OSG Compute Element (CE) is the entry point for the OSG to your local resources: a layer of software that you install\non a machine that can submit jobs into your local batch system.\nAt the heart of the CE is the  job gateway  software, which is responsible for handling incoming jobs, authenticating\nand authorizing them, and delegating them to your batch system for execution.  Today in the OSG, most jobs that arrive at a CE (called  grid jobs ) are  not  end-user jobs, but rather pilot jobs\nsubmitted from factories.\nSuccessful pilot jobs create and make available an environment for actual end-user jobs to match and ultimately run\nwithin the pilot job container.\nEventually pilot jobs remove themselves, typically after a period of inactivity.", 
            "title": "What is a Compute Element?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#what-is-htcondor-ce", 
            "text": "HTCondor-CE is a special configuration of the HTCondor software designed to be a job gateway solution for the OSG.\nIt is configured to use the  JobRouter daemon  to\ndelegate jobs by transforming and submitting them to the site\u2019s batch system.  Benefits of running the HTCondor-CE:   Scalability:  HTCondor-CE is capable of supporting job workloads of large sites  Debugging tools:  HTCondor-CE offers  many tools to help troubleshoot \n    issues with jobs  Routing as configuration:  HTCondor-CE\u2019s mechanism to transform and submit jobs is customized via configuration\n    variables, which means that customizations will persist across upgrades and will not involve modification of\n    software internals to route jobs", 
            "title": "What is HTCondor-CE?"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-jobs-run", 
            "text": "Once an incoming grid job is authorized, it is placed into HTCondor-CE\u2019s scheduler where the JobRouter creates a\ntransformed copy (called the  routed job ) and submits the copy to the batch system (called the  batch system job ).\nAfter submission, HTCondor-CE monitors the batch system job and communicates its status to the original grid job, which\nin turn notifies the original submitter (e.g., job factory) of any updates.\nWhen the job completes, files are transferred along the same chain: from the batch system to the CE, then from the CE to\nthe original submitter.", 
            "title": "How Jobs Run"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#hosted-ce-over-ssh", 
            "text": "The Hosted CE is intended for small sites or as an introduction to the OSG.\nThe OSG configures and maintains an HTCondor-CE on behalf of the site.\nThe Hosted CE is a special configuration of HTCondor-CE that can submit jobs to a remote cluster over SSH.\nIt provides a simple starting point for opportunistic resource owners that want to start contributing to the OSG with\nminimal effort: an organization will be able to accept OSG jobs by allowing SSH access to a submit node in their cluster.  If your site intends to run over 10,000 concurrent OSG jobs, you will need to host your own HTCondor-CE  because the Hosted CE has not yet been optimized for such\nloads.  If you are interested in a Hosted CE solution, please follow the instructions on  this page .", 
            "title": "Hosted CE over SSH"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-htcondor-batch-systems", 
            "text": "For a site with an HTCondor  batch system , the JobRouter can use HTCondor protocols to place a transformed copy of\nthe grid job directly into the batch system\u2019s scheduler, meaning that the routed and batch system jobs are one and the\nsame.\nThus, there are three representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the incoming grid job\u2019s ID  HTCondor batch system: the routed job\u2019s ID    In an HTCondor-CE/HTCondor setup, files are transferred from HTCondor-CE\u2019s spool directory to the batch system\u2019s spool\ndirectory using internal HTCondor protocols.   Note  The JobRouter copies the job directly into the batch system and does not make use of  condor_submit .\nThis means that if the HTCondor batch system is configured to add attributes to incoming jobs when they are\nsubmitted (i.e.,  SUBMIT_EXPRS ), these attributes will not be added to the routed jobs.", 
            "title": "On HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#on-other-batch-systems", 
            "text": "For non-HTCondor batch systems, the JobRouter transforms the grid job into a routed job on the CE and the routed job\nsubmits a job into the batch system via a process called the BLAHP.\nThus, there are four representations of your job, each with its own ID (see diagram below):   Submit host: the HTCondor job ID in the original queue  HTCondor-CE: the incoming grid job\u2019s ID and the routed job\u2019s ID  HTCondor batch system: the batch system\u2019s job ID   Although the following figure specifies the PBS case, it applies to all non-HTCondor batch systems:   With non-HTCondor batch systems, HTCondor-CE cannot use internal HTCondor protocols to transfer files so its spool\ndirectory must be exported to a shared file system that is mounted on the batch system\u2019s worker nodes.", 
            "title": "On other batch systems"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-the-ce-is-customized", 
            "text": "Aside from the  basic configuration  required in the CE\ninstallation, there are two main ways to customize your CE (if you decide any customization is required at all):   Deciding which VOs are allowed to run at your site:  The recommended method of authorizing VOs at your site is\n    based on the  LCMAPS framework  How to filter and transform the grid jobs to be run on your batch system:  Filtering and transforming grid jobs\n    (i.e., setting site-specific attributes or resource limits), requires configuration of your site\u2019s job routes.\n    For examples of common job routes, consult the  JobRouter recipes  page.    Note  If you are running HTCondor as your batch system, you will have two HTCondor configurations side-by-side (one\nresiding in  /etc/condor/  and the other in  /etc/condor-ce ) and will need to make sure to differentiate the two\nwhen editing any configuration.", 
            "title": "How the CE is Customized"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#how-security-works", 
            "text": "In the OSG, security depends on a PKI infrastructure involving Certificate Authorities (CAs) where CAs sign and issue\ncertificates.\nWhen these clients and hosts wish to communicate with each other, the identities of each party is confirmed by\ncross-checking their certificates with the signing CA and establishing trust.  In its default configuration, HTCondor-CE uses GSI-based authentication and authorization to verify the certificate\nchain, which will work with  LCMAPS VOMS authentication .\nAdditionally, it can be reconfigured to provide alternate authentication mechanisms such as Kerberos, SSL, shared\nsecret, or even IP-based authentication.\nMore information about authorization methods can be found here .", 
            "title": "How Security Works"
        }, 
        {
            "location": "/compute-element/htcondor-ce-overview/#next-steps", 
            "text": "Once the basic installation is done, additional activities include:   Setting up job routes to customize incoming jobs  Submitting jobs to a HTCondor-CE    Troubleshooting the HTCondor-CE  Register the CE  Register with the OSG GlideinWMS factories and/or the ATLAS AutoPyFactory", 
            "title": "Next steps"
        }, 
        {
            "location": "/compute-element/hosted-ce/", 
            "text": "Requesting an OSG Hosted CE\n\n\nAn OSG Hosted Compute Element (CE) is the entry point for jobs coming from the OSG;\nit handles authorization and delegation of jobs to your existing campus HPC/HTC cluster.\n\n\nMany sites set up their compute element locally.\nAs an alternative, OSG offers a Hosted CE option\nwherein the OSG team will host and operate the HTCondor compute element,\nand configure it for the science communities that you choose to support.\nThe Hosted CE can support thousands of concurrent job submissions.\n\n\nThis document explains the requirements and the procedure for obtaining a Hosted CE.\nIf you wish to run your own local compute element or expect to support more than 10,000 concurrently running OSG jobs,\nsee \nthis page\n for installing the HTCondor-CE.\n\n\n\n\nBefore Starting\n\n\nBefore preparing your cluster for OSG jobs, consider the following requirements:\n\n\n\n\n\n\nAn existing compute cluster with a \nsupported batch system\n\n    running on a \nsupported operating system\n\n\n\n\n\n\nOutbound network connectivity from the compute nodes (they can be behind NAT)\n\n\n\n\n\n\nA Unix account on your cluster's submit server, accessible via an SSH key.\n    The Hosted CE will use this account to automatically submit jobs,\n    so it must also have permissions to submit jobs to the batch system\n\n\n\n\n\n\nIf your batch system is not HTCondor,\n    there must be a shared file system between the submit server and the compute nodes.\n    See \nthis section\n for details.\n\n\n\n\n\n\nWhich Science Communities and Institutions am I Supporting?\n\n\nThe OSG provides monitoring to view which communities are accessing your site, their fields of science, and home institution.\nBelow is an example of the monitoring views that will be available for your cluster.\n\n\n\n\nSecurity\n\n\nOSG takes multiple precautions to maintain security and prevent unauthorized\nusage of resources:\n\n\n\n\nAccess to the OSG system with SSH keys are restricted to the OSG staff maintaining them\n\n\nUsers are carefully vetted before they are allowed to submit jobs to OSG\n\n\nJobs running through OSG can be traced back to the user that submitted them\n\n\nJob submission can quickly be disabled if needed\n\n\nOSG staff are readily contactable in case of an emergency,\n    through email at \n\n\n\n\nApplying for an OSG Hosted CE\n\n\nBefore making any system changes, you should do the following steps:\n\n\n\n\n\n\nFill out the \ncluster integration questionnaire\n\n    so that the OSG team has basic information about your cluster\n\n\n\n\n\n\nEmail \nhelp@opensciencegrid.org\n\n    to set up a consultation call with the OSG team,\n    in order to discuss how you would like to contribute to the OSG;\n    for example, the number of OSG jobs that should run, what resource limits you have,\n    or which science communities you support\n\n\n\n\n\n\nAfter the consultation, do the following:\n\n\n\n\n\n\nCreate a Unix account for OSG jobs on the submit server and across the cluster as necessary\n\n\n\n\n\n\nInstall OSG-provided public SSH keys for the account\n\n\n\n\n\n\nOnce this is done, the OSG team will:\n\n\n\n\n\n\nConfigure the Hosted CE with your system details\n\n\n\n\n\n\nValidate operation with a set of test jobs\n\n\n\n\n\n\nConfigure central OSG services to schedule jobs\n\n\n\n\n\n\nOptionally, we can assist you in\n\ninstalling and setting up the Squid and OASIS\n\nsoftware on your cluster in order to support application software repositories.\nThis will allow a broader set of jobs to run on your cluster.\n\n\nProviding the OSG Worker Node client (HTCondor batch systems only)\n\n\nAll OSG sites need to provide the OSG Worker Node Client on each worker node in the cluster.\nThis is normally handled by OSG staff for a Hosted CE, but requires shared home directories across the cluster.\n\n\nHowever, for sites with an HTCondor batch system, often there is no shared filesystem set up.\nIf you run an HTCondor site and it is easier to install and maintain the Worker Node Client on each worker node than to\nset up and maintain shared file system, you have the following options:\n\n\n\n\nInstall the \nWorker Node Client from RPM\n\n\nInstall the \nWorker Node Client from tarball\n\n\nInstall the Worker Node Client from \nOASIS\n\n\n\n\nOptional\n: Providing Access to Application Software Using OASIS\n\n\nMany OSG communities use software modules provided by their collaborations or by the OSG User Support team.\nIn order to support these communities, without requiring specific application software on your cluster,\nOSG sites use a distributed software repository system called OASIS,\nbuilt on top of a file system called CVMFS.\n\n\nIn order to use OASIS, you will need the following:\n\n\n\n\n\n\nA cluster-wide Squid proxy service with at least 50GB of cache space;\n    we recommend using the Frontier Squid software provided in the OSG repositories\n\n\n\n\n\n\nA local scratch area on each compute node; typical recommendations are 10 GB per job,\n    plus an additional 20GB for caching OASIS data\n\n\n\n\n\n\nInstallation instructions for Frontier Squid are \nprovided here\n.\n\n\nAfter setting up the Squid proxy, you will need to install the CVMFS software and the OASIS configuration\non each compute node.\nInstallation instructions for CVMFS and OASIS are \nprovided here\n.\n\n\nHow to Get Help\n\n\nIf you need help with setup or troubleshooting, see our \nhelp procedure\n.", 
            "title": "Request a Hosted CE"
        }, 
        {
            "location": "/compute-element/hosted-ce/#requesting-an-osg-hosted-ce", 
            "text": "An OSG Hosted Compute Element (CE) is the entry point for jobs coming from the OSG;\nit handles authorization and delegation of jobs to your existing campus HPC/HTC cluster.  Many sites set up their compute element locally.\nAs an alternative, OSG offers a Hosted CE option\nwherein the OSG team will host and operate the HTCondor compute element,\nand configure it for the science communities that you choose to support.\nThe Hosted CE can support thousands of concurrent job submissions.  This document explains the requirements and the procedure for obtaining a Hosted CE.\nIf you wish to run your own local compute element or expect to support more than 10,000 concurrently running OSG jobs,\nsee  this page  for installing the HTCondor-CE.", 
            "title": "Requesting an OSG Hosted CE"
        }, 
        {
            "location": "/compute-element/hosted-ce/#before-starting", 
            "text": "Before preparing your cluster for OSG jobs, consider the following requirements:    An existing compute cluster with a  supported batch system \n    running on a  supported operating system    Outbound network connectivity from the compute nodes (they can be behind NAT)    A Unix account on your cluster's submit server, accessible via an SSH key.\n    The Hosted CE will use this account to automatically submit jobs,\n    so it must also have permissions to submit jobs to the batch system    If your batch system is not HTCondor,\n    there must be a shared file system between the submit server and the compute nodes.\n    See  this section  for details.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/compute-element/hosted-ce/#which-science-communities-and-institutions-am-i-supporting", 
            "text": "The OSG provides monitoring to view which communities are accessing your site, their fields of science, and home institution.\nBelow is an example of the monitoring views that will be available for your cluster.", 
            "title": "Which Science Communities and Institutions am I Supporting?"
        }, 
        {
            "location": "/compute-element/hosted-ce/#security", 
            "text": "OSG takes multiple precautions to maintain security and prevent unauthorized\nusage of resources:   Access to the OSG system with SSH keys are restricted to the OSG staff maintaining them  Users are carefully vetted before they are allowed to submit jobs to OSG  Jobs running through OSG can be traced back to the user that submitted them  Job submission can quickly be disabled if needed  OSG staff are readily contactable in case of an emergency,\n    through email at", 
            "title": "Security"
        }, 
        {
            "location": "/compute-element/hosted-ce/#applying-for-an-osg-hosted-ce", 
            "text": "Before making any system changes, you should do the following steps:    Fill out the  cluster integration questionnaire \n    so that the OSG team has basic information about your cluster    Email  help@opensciencegrid.org \n    to set up a consultation call with the OSG team,\n    in order to discuss how you would like to contribute to the OSG;\n    for example, the number of OSG jobs that should run, what resource limits you have,\n    or which science communities you support    After the consultation, do the following:    Create a Unix account for OSG jobs on the submit server and across the cluster as necessary    Install OSG-provided public SSH keys for the account    Once this is done, the OSG team will:    Configure the Hosted CE with your system details    Validate operation with a set of test jobs    Configure central OSG services to schedule jobs    Optionally, we can assist you in installing and setting up the Squid and OASIS \nsoftware on your cluster in order to support application software repositories.\nThis will allow a broader set of jobs to run on your cluster.", 
            "title": "Applying for an OSG Hosted CE"
        }, 
        {
            "location": "/compute-element/hosted-ce/#providing-the-osg-worker-node-client-htcondor-batch-systems-only", 
            "text": "All OSG sites need to provide the OSG Worker Node Client on each worker node in the cluster.\nThis is normally handled by OSG staff for a Hosted CE, but requires shared home directories across the cluster.  However, for sites with an HTCondor batch system, often there is no shared filesystem set up.\nIf you run an HTCondor site and it is easier to install and maintain the Worker Node Client on each worker node than to\nset up and maintain shared file system, you have the following options:   Install the  Worker Node Client from RPM  Install the  Worker Node Client from tarball  Install the Worker Node Client from  OASIS", 
            "title": "Providing the OSG Worker Node client (HTCondor batch systems only)"
        }, 
        {
            "location": "/compute-element/hosted-ce/#optional-providing-access-to-application-software-using-oasis", 
            "text": "Many OSG communities use software modules provided by their collaborations or by the OSG User Support team.\nIn order to support these communities, without requiring specific application software on your cluster,\nOSG sites use a distributed software repository system called OASIS,\nbuilt on top of a file system called CVMFS.  In order to use OASIS, you will need the following:    A cluster-wide Squid proxy service with at least 50GB of cache space;\n    we recommend using the Frontier Squid software provided in the OSG repositories    A local scratch area on each compute node; typical recommendations are 10 GB per job,\n    plus an additional 20GB for caching OASIS data    Installation instructions for Frontier Squid are  provided here .  After setting up the Squid proxy, you will need to install the CVMFS software and the OASIS configuration\non each compute node.\nInstallation instructions for CVMFS and OASIS are  provided here .", 
            "title": "Optional: Providing Access to Application Software Using OASIS"
        }, 
        {
            "location": "/compute-element/hosted-ce/#how-to-get-help", 
            "text": "If you need help with setup or troubleshooting, see our  help procedure .", 
            "title": "How to Get Help"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/", 
            "text": "Installing and Maintaining HTCondor-CE\n\n\nThe \nHTCondor-CE\n software is a \njob gateway\n for an OSG Compute Element (CE).\nAs such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs\nto your local batch system.\nIn OSG today, most CEs accept \npilot jobs\n from a factory, which in turn are able to accept and run end-user jobs.\nSee the \nHTCondor-CE Overview\n for a much more detailed introduction.\n\n\nUse this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software\nrepositories.\n\n\n\n\nNote\n\n\nIf you are installing an HTCondor-CE for use outside of the OSG, consult \nthis documentation\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points\n(consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \ncondor\n (UID 4716) and\n    \ngratia\n (UID 42401)\n\n\nSSL certificate:\n The HTCondor-CE service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an\n    accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nDNS entries:\n Forward and reverse DNS must resolve for the HTCondor-CE host\n\n\nNetwork ports:\n The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP)\n\n\nSubmit host:\n HTCondor-CE should be installed on a host that already has the ability to submit jobs into your\n    local cluster\n\n\nFile Systems\n: Non-HTCondor batch systems require a \nshared file system\n\n    between the HTCondor-CE host and the batch system worker nodes.\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling HTCondor-CE\n\n\nAn HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software\n(e.g., GridFTP, a Gratia probe, authentication software).\nTo simplify installation, OSG provides convenience RPMs that install all required software.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\n(Optional)\n If your batch system is already installed via non-RPM means and is in the following list, install the\n   appropriate 'empty' RPM.\n   Otherwise, skip to the next step.\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen run the following command\u2026\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nyum install empty-condor --enablerepo=osg-empty\n\n\n\n\n\n\nSLURM\n\n\nyum install empty-slurm --enablerepo=osg-empty\n\n\n\n\n\n\n\n\n\n\n\n\n(Optional)\n If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to\n\n/etc/yum.repos.d/osg.repo\n.\n   Otherwise, skip to the next step.\n\n\nexclude\n=\ncondor\n\n\n\n\n\n\n\n\n\n\nSelect the appropriate convenience RPM:\n\n\n\n\n\n\n\n\nIf your batch system is...\n\n\nThen use the following package...\n\n\n\n\n\n\n\n\n\n\nHTCondor\n\n\nosg-ce-condor\n\n\n\n\n\n\nLSF\n\n\nosg-ce-lsf\n\n\n\n\n\n\nPBS\n\n\nosg-ce-pbs\n\n\n\n\n\n\nSGE\n\n\nosg-ce-sge\n\n\n\n\n\n\nSLURM\n\n\nosg-ce-slurm\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the CE software:\n\n\nroot@host #\n yum install \nPACKAGE\n\n\n\n\n\n\n\n\n\n\nConfiguring HTCondor-CE\n\n\nThere are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method.\nFor more advanced configuration, see the section on \noptional configurations\n.\n\n\nConfiguring the batch system\n\n\n\n\nImportant\n\n\nHTCondor-CE must be installed on a host that is configured to submit jobs to your batch system.\nThe details of this configuration is likely site-specific and therefore beyond the scope of this document.\n\n\n\n\nEnable your batch system in the HTCondor-CE configuration by editing the \nenabled\n field in the\n\n/etc/osg/config.d/20-\nYOUR BATCH SYSTEM\n.ini\n:\n\n\nenabled\n \n=\n \nTrue\n\n\n\n\n\n\nIf you are using HTCondor as your \nlocal batch system\n (i.e., in addition to your HTCondor-CE), skip to the\n\nconfiguring authorization\n section.\nFor other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.\n\n\nBatch systems other than HTCondor\n\n\nNon-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to\nyour site's worker nodes.\nThe current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on\nthe \nCE host\n.\nIn this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker\nnode mounts the directory in the same location as on the CE.\nFor example, if your spool directory is \n/var/lib/condor-ce\n (the default), you must mount the shared directory to\n\n/var/lib/condor-ce\n on the worker nodes.\n\n\n\n\nNote\n\n\nIf you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE\ndaemons can write to the spool directory.\n\n\n\n\nYou can control the value of the spool directory by setting \nSPOOL\n in \n/etc/condor-ce/config.d/99-local.conf\n (create\nthis file if it doesn't exist).\nFor example, the following sets the \nSPOOL\n directory to \n/home/condor\n:\n\n\nSPOOL\n \n=\n \n/\nhome\n/\ncondor\n\n\n\n\n\n\n\n\nNote\n\n\nThe shared spool directory must be readable and writeable by the \ncondor\n user for HTCondor-CE to function correctly.\n\n\n\n\nConfiguring authorization\n\n\nTo configure which virtual organizations and users are authorized to submit jobs to your, follow the instructions in\n\nthe LCMAPS VOMS plugin document\n.\n\n\n\n\nNote\n\n\nIf your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the\n\ncondor_mapfile\n.\nIf this is not the desired behavior, set \nGSI_AUTHZ_CONF=/dev/null\n in the local HTCondor configuration.\n\n\n\n\nConfiguring CE collector advertising\n\n\nTo split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central\ncollector.\nThe job factories then query the central collector for idle resources and submit pilot jobs to the available sites.\nTo advertise your site, you will need to enter some information about the worker nodes of your clusters.\n\n\nPlease see the \nSubcluster / Resource Entry configuration document\n\nabout configuring the data that will be uploaded to the central collector.\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to\nsoftware automatically.\nSettings that are made outside of the OSG directory take effect immediately or at least when the relevant service is\nrestarted.\nFor the OSG settings, use the \nosg-configure\n tool to validate (to a limited\nextent) and apply the settings to the relevant software components.\nThe \nosg-configure\n software is included automatically in an HTCondor-CE installation.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory\n\n\n\n\nNote\n\n\nThis document describes the critical settings for HTCondor-CE and related software.\nYou may need to configure other software that is installed on your HTCondor-CE host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings\n\n\nroot@host #\n osg-configure -v\n\n\n\n\n\n\n\n\n\nFix any errors (at least) that \nosg-configure\n reports.\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\nroot@host #\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site.\nIf you do not need any of the following special configurations, skip to\n\nthe section on using HTCondor-CE\n.\n\n\n\n\nTransforming and filtering jobs\n\n\nConfiguring for multiple network interfaces\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nAccounting with multiple CEs or local user jobs\n\n\nHTCondor accounting groups\n\n\nHTCondor-CE monitoring web interface\n\n\n\n\nTransforming and filtering jobs\n\n\nIf you need to modify or filter jobs, more information can be found in the \nJob Router Recipes\n\ndocument.\n\n\n\n\nNote\n\n\nIf you need to assign jobs to HTCondor accounting groups, refer to \nthis\n section.\n\n\n\n\nConfiguring for multiple network interfaces\n\n\nIf you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname\nand interface to use when communicating to each other.\nSet \nNETWORK_HOSTNAME\n and \nNETWORK_INTERFACE\n to the hostname and IP address of your public interface, respectively, in\n\n/etc/condor-ce/config.d/99-local.conf\n directory with the line:\n\n\nNETWORK_HOSTNAME\n \n=\n \n%\nRED\n%\ncondorce\n.\nexample\n.\ncom\n%\nENDCOLOR\n%\n\n\nNETWORK_INTERFACE\n \n=\n \n%\nRED\n%\n127\n.\n0\n.\n0\n.\n1\n%\nENDCOLOR\n%\n\n\n\n\n\n\nReplacing \ncondorce.example.com\n text with your public interface\u2019s hostname and \n127.0.0.1\n with your public interface\u2019s\nIP address.\n\n\nLimiting or disabling locally running jobs on the CE\n\n\nIf you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and\nscheduler universes.\nLocal and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting.\nPilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another\nworker node.\n\n\nThe two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be\nconfiguring them in unison.\n\n\n\n\n\n\nTo change the default limit\n on the number of locally run jobs (the current default is 20), add the following to\n  \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n TotalLocalJobsRunning + TotalSchedulerJobsRunning \n \nJOB-LIMIT\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo only allow a specific user\n to start locally run jobs, add the following to\n  \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n target.Owner \n=\n?\n=\n \nUSERNAME\n\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\nTo disable\n locally run jobs, add the following to \n/etc/condor-ce/config.d/99-local.conf\n:\n\n\nSTART_LOCAL_UNIVERSE\n \n=\n False\n\nSTART_SCHEDULER_UNIVERSE\n \n=\n \n$(\nSTART_LOCAL_UNIVERSE\n)\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nRSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs\nfrom the \nrsv\n user.\n\n\n\n\nAccounting with multiple CEs or local user jobs\n\n\n\n\nNote\n\n\nFor non-HTCondor batch systems only\n\n\n\n\nIf your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting\nsoftware needs to be configured so that it doesn't over report the number of jobs.\nUse the following table to determine which file requires editing:\n\n\n\n\n\n\n\n\nIf your batch system is\u2026\n\n\nThen edit the following file on each of your CE(s)\u2026\n\n\n\n\n\n\n\n\n\n\nLSF\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nPBS\n\n\n/etc/gratia/pbs-lsf/ProbeConfig\n\n\n\n\n\n\nSGE\n\n\n/etc/gratia/sge/ProbeConfig\n\n\n\n\n\n\nSLURM\n\n\n/etc/gratia/slurm/ProbeConfig\n\n\n\n\n\n\n\n\nThen edit the value of \nSuppressNoDNRecords\n on each of your CE's so that it reads:\n\n\nSuppressNoDNRecords\n=\n1\n\n\n\n\n\n\nHTCondor accounting groups\n\n\n\n\nNote\n\n\nFor HTCondor batch systems only\n\n\n\n\nIf you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups.\nThey are independent of the Unix groups the user may already be in and are \ndocumented in the HTCondor manual\n.\nIf you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their\nUID, their DN, or their VOMS attributes.\n\n\n\n\n\n\nTo map UIDs to an accounting group,\n add entries to \n/etc/osg/uid_table.txt\n with the following form:\n\n\nuid\n \nGroupName\n\n\n\n\n\n\nThe following is an example \nuid_table.txt\n:\n\n\nuscms02\n \nTestGroup\n\n\nosg\n     \nother\n.\nosgedu\n\n\n\n\n\n\n\n\n\n\nTo map DNs or VOMS attributes to an accounting group,\n add lines to \n/etc/osg/extattr_table.txt\n with the\n    following form:\n\n\nSubjectOrAttribute\n GroupName\n\n\n\n\n\n\nThe \nSubjectOrAttribute\n can be a Perl regular expression. The following is an example \nextattr_table.txt\n:\n\n\ncmsprio\n \ncms\n.\nother\n.\nprio\n\n\ncms\n\\\n/\nRole\n=\nproduction\n \ncms\n.\nprod\n\n\n\\\n/\nDC\n=\ncom\n\\\n/\nDC\n=\nDigiCert\n-\nGrid\n\\\n/\nO\n=\nOpen\n\\\n \nScience\n\\\n \nGrid\n\\\n/\nOU\n=\nPeople\n\\\n/\nCN\n=\nBrian\n\\\n \nLin\n\\\n \n1047\n \nosg\n.\ntest\n\n\n.\n*\n \nother\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nEntries in \n/etc/osg/uid_table.txt\n are honored over \n/etc/osg/extattr_table.txt\n if a job would match to lines in\nboth files.\n\n\n\n\nInstall and run the HTCondor-CE View\n\n\nThe HTCondor-CE View is an optional web interface to the status of your CE. To run the View,\n\n\n\n\n\n\nBegin by installing the package htcondor-ce-view:\n\n\nroot@host #\n yum install htcondor-ce-view\n\n\n\n\n\n\n\n\n\nNext, uncomment the \nDAEMON_LIST\n configuration located at \n/etc/condor-ce/config.d/05-ce-view.conf\n:\n\n\nDAEMON_LIST\n \n=\n \n$(\nDAEMON_LIST\n)\n, CEVIEW, GANGLIAD, SCHEDD\n\n\n\n\n\n\n\n\n\nRestart the CE service:\n\n\nroot@host #\n service condor-ce restart\n\n\n\n\n\n\n\n\n\nVerify the service by entering your CE's hostname into your web browser\n\n\n\n\n\n\nThe website is served on port 80 by default. To change this default, edit the value of \nHTCONDORCE_VIEW_PORT\n in\n\n/etc/condor-ce/config.d/05-ce-view.conf\n.\n\n\nUsing HTCondor-CE\n\n\nAs a site administrator, there are a few ways to use the HTCondor-CE:\n\n\n\n\nManaging the HTCondor-CE and associated services\n\n\nUsing HTCondor-CE administrative tools to monitor and maintain the job gateway\n\n\nUsing HTCondor-CE user tools to test gateway operations\n\n\n\n\nManaging HTCondor-CE and associated services\n\n\nIn addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation.\nThe specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nYour batch system\n\n\ncondor\n or \npbs_server\n or \u2026\n\n\n\n\n\n\n\n\nHTCondor-CE\n\n\ncondor-ce\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nSome of the HTCondor-CE administrative and user tools are documented in \nthe HTCondor-CE troubleshooting guide\n.\n\n\nValidating HTCondor-CE\n\n\nTo validate an HTCondor-CE, perform the following verification steps:\n\n\n\n\n\n\nVerify that local job submissions complete successfully from the CE host.\n   For example, if you have a Slurm cluster, run \nsbatch\n from the CE and verify that it runs and completes with\n   \nscontrol\n and \nsacct\n.\n\n\n\n\n\n\nVerify that all the necessary daemons are running with\n   \ncondor_ce_status -any\n.\n\n\n\n\n\n\nVerify the CE's network configuration using\n   \ncondor_ce_host_network_check\n.\n\n\n\n\n\n\nVerify that jobs can complete successfully using\n   \ncondor_ce_trace\n.\n\n\n\n\n\n\nTroubleshooting HTCondor-CE\n\n\nFor information on how to troubleshoot your HTCondor-CE, please refer to\n\nthe HTCondor-CE troubleshooting guide\n.\n\n\nRegistering the CE\n\n\nTo be part of the OSG Production Grid, your CE must be\n\nregistered with the OSG\n.\nTo register your resource:\n\n\n\n\n\n\nIdentify the facility, site, and resource group where your HTCondor-CE is hosted.\n    For example, the Center for High Throughput Computing at the University of Wisconsin-Madison uses the following\n    information:\n\n\nFacility\n:\n \nUniversity\n \nof\n \nWisconsin\n\n\nSite\n:\n \nCHTC\n\n\nResource\n \nGroup\n:\n \nCHTC\n\n\n\n\n\n\n\n\n\n\nUsing the above information, \ncreate or update\n the\n   appropriate YAML file, using \nthis template\n\n   as a guide.\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nConfiguration\n\n\nThe following directories contain the configuration for HTCondor-CE.\nThe directories are parsed in the order presented and thus configuration within the final directory will override\nconfiguration specified in the previous directories.\n\n\n\n\n\n\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\n/usr/share/condor-ce/config.d/\n\n\nConfiguration defaults (overwritten on package updates)\n\n\n\n\n\n\n/etc/condor-ce/config.d/\n\n\nFiles in this directory are parsed in alphanumeric order (i.e., \n99-local.conf\n will override values in \n01-ce-auth.conf\n)\n\n\n\n\n\n\n\n\nFor a detailed order of the way configuration files are parsed, run the following command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\nUsers\n\n\nThe following users are needed by HTCondor-CE at all sites:\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncondor\n\n\nThe HTCondor-CE will be run as root, but perform most of its operations as the \ncondor\n user.\n\n\n\n\n\n\ngratia\n\n\nRuns the Gratia probes to collect accounting data\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nFile\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHtcondor-CE\n\n\ntcp\n\n\n9619\n\n\nX\n\n\n\n\nHTCondor-CE shared port\n\n\n\n\n\n\n\n\nAllow inbound and outbound network connection to all internal site servers, such as the batch system head-node only\nephemeral outgoing ports are necessary.", 
            "title": "Install HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-and-maintaining-htcondor-ce", 
            "text": "The  HTCondor-CE  software is a  job gateway  for an OSG Compute Element (CE).\nAs such, HTCondor-CE is the entry point for jobs coming from the OSG \u2014 it handles authorization and delegation of jobs\nto your local batch system.\nIn OSG today, most CEs accept  pilot jobs  from a factory, which in turn are able to accept and run end-user jobs.\nSee the  HTCondor-CE Overview  for a much more detailed introduction.  Use this page to learn how to install, configure, run, test, and troubleshoot HTCondor-CE from the OSG software\nrepositories.   Note  If you are installing an HTCondor-CE for use outside of the OSG, consult  this documentation", 
            "title": "Installing and Maintaining HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#before-starting", 
            "text": "Before starting the installation process, consider the following points\n(consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  condor  (UID 4716) and\n     gratia  (UID 42401)  SSL certificate:  The HTCondor-CE service uses a host certificate at  /etc/grid-security/hostcert.pem  and an\n    accompanying key at  /etc/grid-security/hostkey.pem  DNS entries:  Forward and reverse DNS must resolve for the HTCondor-CE host  Network ports:  The pilot factories must be able to contact your HTCondor-CE service on port 9619 (TCP)  Submit host:  HTCondor-CE should be installed on a host that already has the ability to submit jobs into your\n    local cluster  File Systems : Non-HTCondor batch systems require a  shared file system \n    between the HTCondor-CE host and the batch system worker nodes.   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#installing-htcondor-ce", 
            "text": "An HTCondor-CE installation consists of the job gateway (i.e., the HTCondor-CE job router) and other support software\n(e.g., GridFTP, a Gratia probe, authentication software).\nTo simplify installation, OSG provides convenience RPMs that install all required software.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    (Optional)  If your batch system is already installed via non-RPM means and is in the following list, install the\n   appropriate 'empty' RPM.\n   Otherwise, skip to the next step.     If your batch system is\u2026  Then run the following command\u2026      HTCondor  yum install empty-condor --enablerepo=osg-empty    SLURM  yum install empty-slurm --enablerepo=osg-empty       (Optional)  If your HTCondor batch system is already installed via non-OSG RPM means, add the line below to /etc/yum.repos.d/osg.repo .\n   Otherwise, skip to the next step.  exclude = condor     Select the appropriate convenience RPM:     If your batch system is...  Then use the following package...      HTCondor  osg-ce-condor    LSF  osg-ce-lsf    PBS  osg-ce-pbs    SGE  osg-ce-sge    SLURM  osg-ce-slurm       Install the CE software:  root@host #  yum install  PACKAGE", 
            "title": "Installing HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-htcondor-ce", 
            "text": "There are a few required configuration steps to connect HTCondor-CE with your batch system and authentication method.\nFor more advanced configuration, see the section on  optional configurations .", 
            "title": "Configuring HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-the-batch-system", 
            "text": "Important  HTCondor-CE must be installed on a host that is configured to submit jobs to your batch system.\nThe details of this configuration is likely site-specific and therefore beyond the scope of this document.   Enable your batch system in the HTCondor-CE configuration by editing the  enabled  field in the /etc/osg/config.d/20- YOUR BATCH SYSTEM .ini :  enabled   =   True   If you are using HTCondor as your  local batch system  (i.e., in addition to your HTCondor-CE), skip to the configuring authorization  section.\nFor other batch systems (e.g., PBS, LSF, SGE, SLURM), keep reading.", 
            "title": "Configuring the batch system"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#batch-systems-other-than-htcondor", 
            "text": "Non-HTCondor batch systems require a shared file system configuration to support file transfer from the HTCondor-CE to\nyour site's worker nodes.\nThe current recommendation is to run a dedicated NFS server (whose installation is beyond the scope of this document) on\nthe  CE host .\nIn this setup, HTCondor-CE writes to the local spool directory, the NFS server shares the directory, and each worker\nnode mounts the directory in the same location as on the CE.\nFor example, if your spool directory is  /var/lib/condor-ce  (the default), you must mount the shared directory to /var/lib/condor-ce  on the worker nodes.   Note  If you choose not to host the NFS server on your CE, you will need to turn off root squash so that the HTCondor-CE\ndaemons can write to the spool directory.   You can control the value of the spool directory by setting  SPOOL  in  /etc/condor-ce/config.d/99-local.conf  (create\nthis file if it doesn't exist).\nFor example, the following sets the  SPOOL  directory to  /home/condor :  SPOOL   =   / home / condor    Note  The shared spool directory must be readable and writeable by the  condor  user for HTCondor-CE to function correctly.", 
            "title": "Batch systems other than HTCondor"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-authorization", 
            "text": "To configure which virtual organizations and users are authorized to submit jobs to your, follow the instructions in the LCMAPS VOMS plugin document .   Note  If your local batch system is HTCondor, it will attempt to utilize the LCMAPS callouts if enabled in the condor_mapfile .\nIf this is not the desired behavior, set  GSI_AUTHZ_CONF=/dev/null  in the local HTCondor configuration.", 
            "title": "Configuring authorization"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-ce-collector-advertising", 
            "text": "To split jobs between the various sites of the OSG, information about each site's capabilities are uploaded to a central\ncollector.\nThe job factories then query the central collector for idle resources and submit pilot jobs to the available sites.\nTo advertise your site, you will need to enter some information about the worker nodes of your clusters.  Please see the  Subcluster / Resource Entry configuration document \nabout configuring the data that will be uploaded to the central collector.", 
            "title": "Configuring CE collector advertising"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to\nsoftware automatically.\nSettings that are made outside of the OSG directory take effect immediately or at least when the relevant service is\nrestarted.\nFor the OSG settings, use the  osg-configure  tool to validate (to a limited\nextent) and apply the settings to the relevant software components.\nThe  osg-configure  software is included automatically in an HTCondor-CE installation.    Make all changes to  .ini  files in the  /etc/osg/config.d  directory   Note  This document describes the critical settings for HTCondor-CE and related software.\nYou may need to configure other software that is installed on your HTCondor-CE host, too.     Validate the configuration settings  root@host #  osg-configure -v    Fix any errors (at least) that  osg-configure  reports.   Once the validation command succeeds without errors, apply the configuration settings: root@host #  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site.\nIf you do not need any of the following special configurations, skip to the section on using HTCondor-CE .   Transforming and filtering jobs  Configuring for multiple network interfaces  Limiting or disabling locally running jobs on the CE  Accounting with multiple CEs or local user jobs  HTCondor accounting groups  HTCondor-CE monitoring web interface", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#transforming-and-filtering-jobs", 
            "text": "If you need to modify or filter jobs, more information can be found in the  Job Router Recipes \ndocument.   Note  If you need to assign jobs to HTCondor accounting groups, refer to  this  section.", 
            "title": "Transforming and filtering jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuring-for-multiple-network-interfaces", 
            "text": "If you have multiple network interfaces with different hostnames, the HTCondor-CE daemons need to know which hostname\nand interface to use when communicating to each other.\nSet  NETWORK_HOSTNAME  and  NETWORK_INTERFACE  to the hostname and IP address of your public interface, respectively, in /etc/condor-ce/config.d/99-local.conf  directory with the line:  NETWORK_HOSTNAME   =   % RED % condorce . example . com % ENDCOLOR %  NETWORK_INTERFACE   =   % RED % 127 . 0 . 0 . 1 % ENDCOLOR %   Replacing  condorce.example.com  text with your public interface\u2019s hostname and  127.0.0.1  with your public interface\u2019s\nIP address.", 
            "title": "Configuring for multiple network interfaces"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#limiting-or-disabling-locally-running-jobs-on-the-ce", 
            "text": "If you want to limit or disable jobs running locally on your CE, you will need to configure HTCondor-CE's local and\nscheduler universes.\nLocal and scheduler universes allow jobs to be run on the CE itself, mainly for remote troubleshooting.\nPilot jobs will not run as local/scheduler universe jobs so leaving them enabled does NOT turn your CE into another\nworker node.  The two universes are effectively the same (scheduler universe launches a starter process for each job), so we will be\nconfiguring them in unison.    To change the default limit  on the number of locally run jobs (the current default is 20), add the following to\n   /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  TotalLocalJobsRunning + TotalSchedulerJobsRunning    JOB-LIMIT  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To only allow a specific user  to start locally run jobs, add the following to\n   /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  target.Owner  = ? =   USERNAME  START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )     To disable  locally run jobs, add the following to  /etc/condor-ce/config.d/99-local.conf :  START_LOCAL_UNIVERSE   =  False START_SCHEDULER_UNIVERSE   =   $( START_LOCAL_UNIVERSE )      Note  RSV requires the ability to start local universe jobs so if you are using RSV, you need to allow local universe jobs\nfrom the  rsv  user.", 
            "title": "Limiting or disabling locally running jobs on the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#accounting-with-multiple-ces-or-local-user-jobs", 
            "text": "Note  For non-HTCondor batch systems only   If your site has multiple CEs or you have non-grid users submitting to the same local batch system, the OSG accounting\nsoftware needs to be configured so that it doesn't over report the number of jobs.\nUse the following table to determine which file requires editing:     If your batch system is\u2026  Then edit the following file on each of your CE(s)\u2026      LSF  /etc/gratia/pbs-lsf/ProbeConfig    PBS  /etc/gratia/pbs-lsf/ProbeConfig    SGE  /etc/gratia/sge/ProbeConfig    SLURM  /etc/gratia/slurm/ProbeConfig     Then edit the value of  SuppressNoDNRecords  on each of your CE's so that it reads:  SuppressNoDNRecords = 1", 
            "title": "Accounting with multiple CEs or local user jobs"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#htcondor-accounting-groups", 
            "text": "Note  For HTCondor batch systems only   If you want to provide fairshare on a group basis, as opposed to a Unix user basis, you can use HTCondor accounting groups.\nThey are independent of the Unix groups the user may already be in and are  documented in the HTCondor manual .\nIf you are using HTCondor accounting groups, you can map jobs from the CE into HTCondor accounting groups based on their\nUID, their DN, or their VOMS attributes.    To map UIDs to an accounting group,  add entries to  /etc/osg/uid_table.txt  with the following form:  uid   GroupName   The following is an example  uid_table.txt :  uscms02   TestGroup  osg       other . osgedu     To map DNs or VOMS attributes to an accounting group,  add lines to  /etc/osg/extattr_table.txt  with the\n    following form:  SubjectOrAttribute  GroupName   The  SubjectOrAttribute  can be a Perl regular expression. The following is an example  extattr_table.txt :  cmsprio   cms . other . prio  cms \\ / Role = production   cms . prod  \\ / DC = com \\ / DC = DigiCert - Grid \\ / O = Open \\   Science \\   Grid \\ / OU = People \\ / CN = Brian \\   Lin \\   1047   osg . test  . *   other      Note  Entries in  /etc/osg/uid_table.txt  are honored over  /etc/osg/extattr_table.txt  if a job would match to lines in\nboth files.", 
            "title": "HTCondor accounting groups"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#install-and-run-the-htcondor-ce-view", 
            "text": "The HTCondor-CE View is an optional web interface to the status of your CE. To run the View,    Begin by installing the package htcondor-ce-view:  root@host #  yum install htcondor-ce-view    Next, uncomment the  DAEMON_LIST  configuration located at  /etc/condor-ce/config.d/05-ce-view.conf :  DAEMON_LIST   =   $( DAEMON_LIST ) , CEVIEW, GANGLIAD, SCHEDD    Restart the CE service:  root@host #  service condor-ce restart    Verify the service by entering your CE's hostname into your web browser    The website is served on port 80 by default. To change this default, edit the value of  HTCONDORCE_VIEW_PORT  in /etc/condor-ce/config.d/05-ce-view.conf .", 
            "title": "Install and run the HTCondor-CE View"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce", 
            "text": "As a site administrator, there are a few ways to use the HTCondor-CE:   Managing the HTCondor-CE and associated services  Using HTCondor-CE administrative tools to monitor and maintain the job gateway  Using HTCondor-CE user tools to test gateway operations", 
            "title": "Using HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#managing-htcondor-ce-and-associated-services", 
            "text": "In addition to the HTCondor-CE job gateway service itself, there are a number of supporting services in your installation.\nThe specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    Your batch system  condor  or  pbs_server  or \u2026     HTCondor-CE  condor-ce      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Managing HTCondor-CE and associated services"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "Some of the HTCondor-CE administrative and user tools are documented in  the HTCondor-CE troubleshooting guide .", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#validating-htcondor-ce", 
            "text": "To validate an HTCondor-CE, perform the following verification steps:    Verify that local job submissions complete successfully from the CE host.\n   For example, if you have a Slurm cluster, run  sbatch  from the CE and verify that it runs and completes with\n    scontrol  and  sacct .    Verify that all the necessary daemons are running with\n    condor_ce_status -any .    Verify the CE's network configuration using\n    condor_ce_host_network_check .    Verify that jobs can complete successfully using\n    condor_ce_trace .", 
            "title": "Validating HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#troubleshooting-htcondor-ce", 
            "text": "For information on how to troubleshoot your HTCondor-CE, please refer to the HTCondor-CE troubleshooting guide .", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#registering-the-ce", 
            "text": "To be part of the OSG Production Grid, your CE must be registered with the OSG .\nTo register your resource:    Identify the facility, site, and resource group where your HTCondor-CE is hosted.\n    For example, the Center for High Throughput Computing at the University of Wisconsin-Madison uses the following\n    information:  Facility :   University   of   Wisconsin  Site :   CHTC  Resource   Group :   CHTC     Using the above information,  create or update  the\n   appropriate YAML file, using  this template \n   as a guide.", 
            "title": "Registering the CE"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#configuration", 
            "text": "The following directories contain the configuration for HTCondor-CE.\nThe directories are parsed in the order presented and thus configuration within the final directory will override\nconfiguration specified in the previous directories.     Location  Comment      /usr/share/condor-ce/config.d/  Configuration defaults (overwritten on package updates)    /etc/condor-ce/config.d/  Files in this directory are parsed in alphanumeric order (i.e.,  99-local.conf  will override values in  01-ce-auth.conf )     For a detailed order of the way configuration files are parsed, run the following command:  user@host $  condor_ce_config_val -config", 
            "title": "Configuration"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#users", 
            "text": "The following users are needed by HTCondor-CE at all sites:     User  Comment      condor  The HTCondor-CE will be run as root, but perform most of its operations as the  condor  user.    gratia  Runs the Gratia probes to collect accounting data", 
            "title": "Users"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#certificates", 
            "text": "File  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/compute-element/install-htcondor-ce/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Htcondor-CE  tcp  9619  X   HTCondor-CE shared port     Allow inbound and outbound network connection to all internal site servers, such as the batch system head-node only\nephemeral outgoing ports are necessary.", 
            "title": "Networking"
        }, 
        {
            "location": "/compute-element/job-router-recipes/", 
            "text": "Writing Routes For HTCondor-CE\n\n\nThe \nJobRouter\n is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in \n/etc/condor-ce/config.d/02-ce-*.conf\n that provide enough basic functionality for a small site.\n\n\nIf you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.\n\n\n\n\nDefinitions\n\n\n\n\n\n\nIncoming Job\n: A job which was submitted to the CE from an outside source, such as a GlideinWMS Factory.\n\n\n\n\n\n\nRouted Job\n: A job which was transformed by the JobRouter.\n\n\n\n\n\n\nBatch System\n: The underlying batch system that the HTCondor-CE will submit.  This can be Slurm, PBS, HTCondor, SGE, LSF,...\n\n\n\n\n\n\n\n\nQuirks and Pitfalls\n\n\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\nMake sure to run \ncondor_ce_reconfig\n after changing your routes, otherwise they will not take effect.\n\n\nDo \nnot\n set the job environment through the JobRouter. Instead, add any changes to the \n[Local Settings]\n section in \n/etc/osg/config.d/40-localsettings.ini\n and run osg-configure, as documented \nhere\n.\n\n\nHTCondor batch system only: Local universe jobs are excluded from any routing.\n\n\n\n\nHow Job Routes are Constructed\n\n\nEach job route\u2019s \nClassAd\n is constructed by combining each entry from the \nJOB_ROUTER_ENTRIES\n with the \nJOB_ROUTER_DEFAULTS\n. Attributes that are \nset_*\n in \nJOB_ROUTER_ENTRIES\n will override those \nset_*\n in \nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_ENTRIES\n\n\nJOB_ROUTER_ENTRIES\n is a configuration variable whose default is set in \n/etc/condor-ce/config.d/02-ce-*.conf\n but may be overriden by the administrator in \n/etc/condor-ce/config.d/99-local.conf\n. This document outlines the many changes you can make to \nJOB_ROUTER_ENTRIES\n to fit your site\u2019s needs.\n\n\nJOB_ROUTER_DEFAULTS\n\n\nJOB_ROUTER_DEFAULTS\n is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command:\n\n\nuser@host $\n condor_ce_config_val JOB_ROUTER_DEFAULTS \n|\n sed \ns/;/;\\n/g\n\n\n\n\n\n\n\n\nWarning\n\n\nIf a value is set in \nJOB_ROUTER_DEFAULTS\n with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n. Do this at your own risk as it may cause the CE to break.\n\n\n\n\n\n\nWarning\n\n\nDo \nnot\n set the \nJOB_ROUTER_DEFAULTS\n configuration variable yourself. This will cause the CE to stop functioning.\n\n\n\n\nHow Jobs Match to Job Routes\n\n\nThe job router considers jobs in the queue (\ncondor_ce_q\n) that\nmeet the following constraints:\n\n\n\n\nThe job has not already been considered by the job router\n\n\nThe job is associated with an unexpired x509 proxy\n\n\nThe job's universe is standard or vanilla\n\n\n\n\nIf the job meets the above constraints, then the job's ClassAd is compared against each\n\nroute's requirements\n.\nIf the job only meets one route's requirements, the job is matched to that route.\nIf the job meets the requirements of multiple routes,  the route that is chosen depends on your version of HTCondor\n(\ncondor_version\n):\n\n\n\n\n\n\n\n\nIf your version of HTCondor is...\n\n\nThen the route is chosen by...\n\n\n\n\n\n\n\n\n\n\n 8.7.1\n\n\nRound-robin\n between all matching routes. In this case, we recommend making each route's requirements mutually exclusive.\n\n\n\n\n\n\n= 8.7.1\n\n\nFirst matching route\n where routes are considered in the same order that they are configured\n\n\n\n\n\n\n\n\nIf you're using HTCondor \n= 8.7.1 and would like to use round-robin matching, add the following text to a file in\n\n/etc/condor-ce/config.d/\n:\n\n\nJOB_ROUTER_ROUND_ROBIN_SELECTION\n \n=\n \nTrue\n\n\n\n\n\n\nGeneric Routes\n\n\nThis section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in \n/etc/condor-ce/config.d/99-local.conf\n, not the original \n02-ce-*.conf\n.\n\n\nRequired fields\n\n\nThe minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in \n/usr/share/condor-ce/config.d/02-ce-\nbatch system\n-defaults.conf\n, provided by the \nosg-ce-\nbatch system\n packages.\n\n\nBatch system\n\n\nEach route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the \nTargetUniverse\n attribute needs to be set to \n5\n or \n\"vanilla\"\n. For all other batch systems, the \nTargetUniverse\n attribute needs to be set to \n9\n or \n\"grid\"\n and the \nGridResource\n attribute needs to be set to \n\"batch \nbatch system\n\"\n (where \nbatch system\n can be one of \npbs\n, \nslurm\n, \nlsf\n, or \nsge\n).\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  \nTargetUniverse = 5;\n\n\n  name = \nRoute jobs to HTCondor\n;\n\n\n]\n\n\n[\n\n\n  \nGridResource = \nbatch pbs\n;\n\n\n  \nTargetUniverse = 9;\n\n\n  name = \nRoute jobs to PBS\n;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nRoute name\n\n\nTo identify routes, you will need to assign a name to the route with the \nname\n attribute:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  \nname = \nRoute jobs to HTCondor\n;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nThe name of the route will be useful in debugging since it shows up in the output of \ncondor_ce_job_router_info\n, the \nJobRouterLog\n, and in the ClassAd of the routed job, which can be viewed with \ncondor_ce_q\n or \ncondor_ce_history\n.\n\n\nWriting multiple routes\n\n\n\n\nNote\n\n\nBefore writing multiple routes, consider the details of \nhow jobs match to job routes\n\n\n\n\nIf your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues),\nyou will need to write multiple job routes where each route is enclosed by square brackets.\nThe following routes takes incoming jobs that have a \nqueue\n attribute set to \n\"analy\"\n and routes them to the site's\nHTCondor batch system.\nAny other jobs will be sent to that site's PBS batch system.\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n%\nRED\n%[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nRoute jobs to HTCondor\n;\n\n\n  Requirements = (TARGET.queue =?= \nanaly\n);\n\n\n]\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nRoute jobs to PBS\n;\n\n\n  Requirements = (TARGET.queue =!= \nanaly\n);\n\n\n]%\nENDCOLOR\n%\n\n\n@jre\n\n\n\n\n\n\nWriting comments\n\n\nTo write comments you can use \n#\n to comment a line:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \n# comments\n;\n\n\n  # This is a comment\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting attributes for all routes\n\n\nTo set an attribute that will be applied to all routes, you will need to ensure that \nMERGE_JOB_ROUTER_DEFAULT_ADS\n is set to \nTrue\n (check the value with \ncondor_ce_config_val\n) and use the \nset_\n function in the \nJOB_ROUTER_DEFAULTS\n. The following configuration sets the \nPeriodic_Hold\n attribute for all routes:\n\n\n# Use the defaults generated by the condor_ce_router_defaults script.  To add\n\n\n# additional defaults, add additional lines of the form:\n\n\n#\n\n\n#   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]\n\n\n#\n\n\nMERGE_JOB_ROUTER_DEFAULT_ADS\n=\nTrue\n\nJOB_ROUTER_DEFAULTS\n \n=\n \n$(\nJOB_ROUTER_DEFAULTS\n)\n \n[\nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n]\n\n\n\n\n\n\nFiltering jobs based on\u2026\n\n\nTo filter jobs, use the \nRequirements\n attribute. Jobs will evaluate against the ClassAd expression set in the \nRequirements\n and if the expression evaluates to \nTRUE\n, the route will match. More information on the syntax of ClassAd's can be found in the \nHTCondor manual\n. For an example on how incoming jobs interact with filtering in job routes, consult \nthis document\n.\n\n\nWhen setting requirements, you need to prefix job attributes that you are filtering with \nTARGET.\n so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a \nqueue = \"analy\"\n attribute, then the following job route will not match:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nFiltering by queue\n;\n\n\n  queue = \nnot-analy\n;\n\n\n  \nRequirements = (queue =?= \nanaly\n);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nThis is because when evaluating the route requirement, the job route will compare its own \nqueue\n attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the \nHTCondor manual\n.\n\n\n\n\nNote\n\n\nIf you have an HTCondor batch system, note the difference with \nset_requirements\n.\n\n\n\n\n\n\nNote\n\n\nBefore writing multiple routes, consider the details of \nhow jobs match to job routes\n.\n\n\n\n\nGlidein queue\n\n\nTo filter jobs based on their glidein queue attribute, your routes will need a \nRequirements\n expression using the incoming job's \nqueue\n attribute. The following entry routes jobs to PBS if the incoming job (specified by \nTARGET\n) is an \nanaly\n (Analysis) glidein:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nFiltering by queue\n;\n\n\n  \nRequirements = (TARGET.queue =?= \nanaly\n);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nJob submitter\n\n\nTo filter jobs based on who submitted it, your routes will need a \nRequirements\n expression using the incoming job's \nOwner\n attribute. The following entry routes jobs to the HTCondor batch system if the submitter is \nusatlas2\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nFiltering by job submitter\n;\n\n\n  \nRequirements = (TARGET.Owner =?= \nusatlas2\n);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nAlternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with \nusatlas\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nFiltering by job submitter (regular expression)\n;\n\n\n  \nRequirements = regexp(\n^usatlas\n, TARGET.Owner);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nVOMS attribute\n\n\nTo filter jobs based on the subject of the job's proxy, your routes will need a \nRequirements\n expression using the incoming job's \nx509UserProxyFirstFQAN\n attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains \n/cms/Role=Pilot\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nFiltering by VOMS attribute (regex)\n;\n\n\n  \nRequirements = regexp(\n\\/cms\\/Role\\=pilot\n, TARGET.x509UserProxyFirstFQAN);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting a default\u2026\n\n\nThis section outlines how to set default job limits, memory, cores, and maximum walltime. For an example on how users can override these defaults, consult \nthis document\n.\n\n\nMaximum number of jobs\n\n\nTo set a default limit to the maximum number of jobs per route, you can edit the configuration variable \nCONDORCE_MAX_JOBS\n in \n/etc/condor-ce/config.d/01-ce-router.conf\n:\n\n\nCONDORCE_MAX_JOBS\n \n=\n \n10000\n\n\n\n\n\n\n\n\nNote\n\n\nThe above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route.\n\n\n\n\nMaximum memory\n\n\nTo set a default maximum memory (in MB) for routed jobs, set the attribute \ndefault_maxMemory\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nRequest memory\n;\n\n\n  # Set the requested memory to 1 GB\n\n\n  \nset_default_maxMemory = 1000;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nNumber of cores to request\n\n\nTo set a default number of cores for routed jobs, set the attribute \ndefault_xcount\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nRequest CPU\n;\n\n\n  # Set the requested cores to 8\n\n\n  \nset_default_xcount = 8;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nMaximum walltime\n\n\nTo set a default maximum walltime (in minutes) for routed jobs, set the attribute \ndefault_maxWallTime\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nSetting WallTime\n;\n\n\n  # Set the max walltime to 1 hr\n\n\n  \nset_default_maxWallTime = 60;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nEditing attributes\u2026\n\n\nThe following functions are operations that affect job attributes and are evaluated in the following order:\n\n\n\n\ncopy_*\n\n\ndelete_*\n\n\nset_*\n\n\neval_set_*\n\n\n\n\nAfter each job route\u2019s ClassAd is \nconstructed\n, the above operations are evaluated in order. For example, if the attribute \nfoo\n is set using \neval_set_foo\n in the \nJOB_ROUTER_DEFAULTS\n, you'll be unable to use \ndelete_foo\n to remove it from your jobs since the attribute is set using \neval_set_foo\n after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in \nJOB_ROUTER_DEFAULTS\n get overridden by the same operation in \nJOB_ROUTER_ENTRIES\n. So to 'delete' \nfoo\n, we would add \neval_set_foo = \"\"\n to the route in the \nJOB_ROUTER_ENTRIES\n, resulting in \nfoo\n being absent from the routed job.\n\n\nMore documentation can be found in the \nHTCondor manual\n.\n\n\nCopying attributes\n\n\nTo copy the value of an attribute of the incoming job to an attribute of the routed job, use \ncopy_\n. The following route copies the \nenvironment\n attribute of the incoming job and sets the attribute \nOriginal_Environment\n on the routed job to the same value:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nCopying attributes\n;\n\n\n  \ncopy_environment = \nOriginal_Environment\n;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nRemoving attributes\n\n\nTo remove an attribute of the incoming job from the routed job, use \ndelete_\n. The following route removes the \nenvironment\n attribute from the routed job:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nCopying attributes\n;\n\n\n  \ndelete_environment = True;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting attributes\n\n\nTo set an attribute on the routed job, use \nset_\n. The following route sets the Job's \nRank\n attribute to 5:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nSetting an attribute\n;\n\n\n  \nset_Rank = 5;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting attributes with ClassAd expressions\n\n\nTo set an attribute to a ClassAd expression to be evaluated, use \neval_set\n. The following route sets the \nExperiment\n attribute to \natlas.osguser\n if the Owner of the incoming job is \nosguser\n:\n\n\n\n\nNote\n\n\nIf a value is set in JOB_ROUTER_DEFAULTS with \neval_set_\nvariable\n, override it by using \neval_set_\nvariable\n in the \nJOB_ROUTER_ENTRIES\n.\n\n\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nSetting an attribute with a !ClassAd expression\n;\n\n\n  \neval_set_Experiment = strcat(\natlas.\n, Owner);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nLimiting the number of jobs\n\n\nThis section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).\n\n\n\n\nNote\n\n\nIf you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via \nuser priorities and group accounting\n.\n\n\n\n\nTotal jobs\n\n\nTo set a limit on the number of jobs for a specific route, set the \nMaxJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nLimit the total number of jobs to 100\n;\n\n\n  \nMaxJobs = 100;\n\n\n]\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nLimit the total number of jobs to 75\n;\n\n\n  \nMaxJobs = 75;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nIdle jobs\n\n\nTo set a limit on the number of idle jobs for a specific route, set the \nMaxIdleJobs\n attribute:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nLimit the total number of idle jobs to 100\n;\n\n\n  \nMaxIdleJobs = 100;\n\n\n]\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  name = \nLimit the total number of idle jobs to 75\n;\n\n\n  \nMaxIdleJobs = 75;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nDebugging routes\n\n\nTo help debug expressions in your routes, you can use the \ndebug()\n function. First, set the debug mode for the JobRouter by editing a file in \n/etc/condor-ce/config.d/\n to read\n\n\nJOB_ROUTER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\nThen wrap the problematic attribute in \ndebug()\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nDebugging a difficult !ClassAd expression\n;\n\n\n  \neval_set_Experiment = debug(strcat(\natlas\n, Name));\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nYou will find the debugging output in \n/var/log/condor-ce/JobRouterLog\n.\n\n\nRoutes for HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running an HTCondor batch system at your site.\n\n\nSetting periodic hold, release or remove\n\n\nTo release, remove or put a job on hold if it meets certain criteria, use the \nPERIODIC_*\n family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting \nPERIODIC_EXPR_INTERVAL\n in your CE's configuration.\n\n\nIn this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once.  This will catch jobs which are starting and stopping multiple times.\n\n\nJOB_ROUTER_ENTRIES\n @\n=\njre\n\n[\n  \nTargetUniverse\n \n=\n \n5\n;\n\n  \nname\n \n=\n \nSetting periodic statements\n;\n\n  \n%\nRED\n%\n# \nPuts\n \nthe\n \nrouted\n \njob\n \non\n \nhold\n \nif\n \nthe\n \njob\ns been idle and has been started at least once or if the job has tried to start more than once\n\n  \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts\n \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n \nNumJobStarts\n \n \n1\n;\n\n  # \nRemove\n \nrouted\n \njobs\n \nif\n \ntheir\n \nwalltime\n \nis\n \nlonger\n \nthan\n \n3\n \ndays\n \nand\n \n5\n \nminutes\n\n  \nset_Periodic_Remove\n \n=\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n;\n\n  # \nRelease\n \nrouted\n \njobs\n \nif\n \nthe\n \ncondor_starter\n \ncouldn\nt start the executable and \nVMGAHP_ERR_INTERNAL\n is in the HoldReason\n\n  \nset_Periodic_Release\n \n=\n \nHoldReasonCode\n \n==\n \n6\n \n \nregexp\n(\nVMGAHP_ERR_INTERNAL\n, \nHoldReason\n)\n;\n\n]\n@\njre\n\n\n\n\n\n\nSetting routed job requirements\n\n\nIf you need to set requirements on your routed job, you will need to use \nset_Requirements\n instead of \nRequirements\n. The \nRequirements\n attribute filters jobs coming into your CE into different job routes whereas \nset_Requirements\n will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the \nHTCondor manual\n.\n\n\nTo ensure that your job lands on a Linux machine in your pool:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  TargetUniverse = 5;\n\n\n  \nset_Requirements =  OpSys == \nLINUX\n;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting accounting groups\n\n\nTo assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using \nUID and ExtAttr tables\n.\n\n\nRoutes for non-HTCondor Batch Systems\n\n\nThis section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.\n\n\nSetting a default batch queue\n\n\nTo set a default queue for routed jobs, set the attribute \ndefault_queue\n:\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n\n\n[\n\n\n  GridResource = \nbatch pbs\n;\n\n\n  TargetUniverse = 9;\n\n\n  name = \nSetting batch system queues\n;\n\n\n  \nset_default_queue = \nosg_queue\n;\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nSetting batch system directives\n\n\nTo write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in \n/etc/blahp/\n (e.g., if your local batch system is PBS, edit \n/etc/blahp/pbs_local_submit_attributes.sh\n). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the \ndefault_remote_cerequirements\n attribute, which can take the following form:\n\n\ndefault_remote_cerequirements\n \n=\n \nfoo == X \n bar == \\\nY\n\\\n \n ...\n\n\n\n\n\n\nThis sets \nfoo\n to value \nX\n and \nbar\n to the string \nY\n (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the \nx509UserProxyFirstFQAN\n attribute of the job submitted to a PBS batch system\n\n\nJOB_ROUTER_ENTRIES\n \n@\n=\njre\n \n[\n\n\n     GridResource = \nbatch pbs\n;\n\n\n     TargetUniverse = 9;\n\n\n     name = \nSetting job submit variables\n;\n\n\n     \nset_default_remote_cerequirements = strcat(\nWalltime == 3600 \n AccountingGroup ==\n, x509UserProxyFirstFQAN, \n\\\n);\n\n\n]\n\n\n@jre\n\n\n\n\n\n\nWith \n/etc/blahp/pbs_local_submit_attributes.sh\n containing.\n\n\n1\n2\n3\n#!/bin/bash\n\n\necho\n \n#PBS -l walltime=\n$Walltime\n\n\necho\n \n#PBS -A \n$AccountingGroup\n\n\n\n\n\n\n\nThis results in the following being appended to the script that gets submitted to your batch system:\n\n\n#\nPBS\n \n-\nl\n \nwalltime\n=\n3600\n\n\n#\nPBS\n \n-\nA\n \n%\nRED\n%\nCE\n \njob\ns\n \nx509UserProxyFirstFQAN\n \nattribute\n%\nENDCOLOR\n%\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nThe HTCondor-CE troubleshooting guide\n\n\nSubmitting jobs to HTCondor-CE\n\n\n\n\nExample Configurations\n\n\nAGLT2's job routes\n\n\nAtlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.\n\n\n\n\nSetting various HTCondor-specific attributes like \nRank\n, \nAccountingGroup\n, \nJobPrio\n and \nPeriodic_Remove\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nLastandFrac\n, \nIdleMP8Pressure\n, \nlocalQue\n, \nIsAnalyJob\n and \nJobMemoryLimit\n.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttps://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content\n\n\nJOB_ROUTER_ENTRIES\n @\n=\njre\n\n# \nStill\n \nto\n \ndo\n \non\n \nall\n \nroutes\n, \nget\n \njob\n \nrequirements\n \nand\n \nadd\n \nthem\n \nhere\n\n# \nRoute\n \nno\n \n1\n\n# \nAnalysis\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n==\nanaly\n;\n\n    \nName\n \n=\n \nAnalysis Queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \neval_set_IdleMP8Pressure\n \n=\n $\n(\nIdleMP8Pressure\n)\n;\n\n    \neval_set_LastAndFrac\n \n=\n $\n(\nLastAndFrac\n)\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n \n \n(\nIfThenElse\n((\nOwner\n \n==\n \natlasconnect\n \n||\n \nOwner\n \n==\n \nmuoncal\n)\n,\nIfThenElse\n(\nIdleMP8Pressure\n,\n(\nTARGET\n.\nPARTITIONED\n \n=!=\n \nTRUE\n)\n,\nTrue\n)\n,\nIfThenElse\n(\nLastAndFrac\n,\n(\nTARGET\n.\nPARTITIONED\n \n=!=\n \nTRUE\n)\n,\nTrue\n)))\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.prod.analy.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n\n    \nset_IsAnalyJob\n \n=\n \nTrue\n;\n\n    \nset_JobPrio\n \n=\n \n5\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n2\n\n# \nsplitterNT\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \n==\n \nsplitterNT\n;\n\n    \nName\n \n=\n \nSplitter ntuple queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_JobPrio\n \n=\n \n10\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n3\n\n# \nsplitter\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \n==\n \nsplitter\n;\n\n    \nName\n \n=\n \nSplitter queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \ngroup_calibrate.muoncal\n;\n\n    \nset_localQue\n \n=\n \nSplitter\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_JobPrio\n \n=\n \n15\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n4\n\n# \nxrootd\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \n==\n \nxrootd\n;\n\n    \nName\n \n=\n \nXrootd queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.prod.analy.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nAnalysis\n;\n\n    \nset_IsAnalyJob\n \n=\n \nTrue\n;\n\n    \nset_JobPrio\n \n=\n \n35\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n5\n\n# \nTier3Test\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \n==\n \nTier3Test\n;\n\n    \nName\n \n=\n \nTier3 Test Queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n \n \n(\n \nIS_TIER3_TEST_QUEUE\n \n=\n?\n=\n \nTrue\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.prod.analy.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nTier3Test\n;\n\n    \nset_IsTier3TestJob\n \n=\n \nTrue\n;\n\n    \nset_IsAnalyJob\n \n=\n \nTrue\n;\n\n    \nset_JobPrio\n \n=\n \n20\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n6\n\n# \nmp8\n \nqueue\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n==\nmp8\n;\n\n    \nName\n \n=\n \nMCORE Queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n \n \n((\n \nTARGET\n.\nCpus\n \n==\n \n8\n \n \nTARGET\n.\nCPU_TYPE\n \n=\n?\n=\n \nmp8\n \n)\n \n||\n \nTARGET\n.\nPARTITIONED\n \n=\n?\n=\n \nTrue\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.prod.mcore.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nMP8\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_JobPrio\n \n=\n \n25\n;\n\n    \nset_Rank\n \n=\n \n0\n.\n0\n;\n\n    \neval_set_RequestCpus\n \n=\n \n8\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n33552000\n;\n\n    \nset_Slot_Type\n \n=\n \nmp8\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n7\n\n# \nInstallation\n \nqueue\n, \ntriggered\n \nby\n \nusatlas2\n \nuser\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \nis\n \nundefined\n \n \ntarget\n.\nOwner\n \n==\n \nusatlas2\n;\n\n    \nName\n \n=\n \nInstall Queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n \n \n(\n \nTARGET\n.\nIS_INSTALL_QUE\n \n=\n?\n=\n \nTrue\n \n)\n \n \n(\nTARGET\n.\nAGLT2_SITE\n \n==\n \nUM\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.other.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nDefault\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_IsInstallJob\n \n=\n \nTrue\n;\n\n    \nset_JobPrio\n \n=\n \n15\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n8\n\n# \nDefault\n \nqueue\n \nfor\n \nusatlas1\n \nuser\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \nis\n \nundefined\n \n \nregexp\n(\nusatlas1\n,\ntarget\n.\nOwner\n)\n;\n\n    \nName\n \n=\n \nATLAS Production Queue\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.prod.prod.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nDefault\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n9\n\n# \nDefault\n \nqueue\n \nfor\n \nany\n \nother\n \nusatlas\n \naccount\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \nis\n \nundefined\n \n \n(\nregexp\n(\nusatlas2\n,\ntarget\n.\nOwner\n)\n \n||\n \nregexp\n(\nusatlas3\n,\ntarget\n.\nOwner\n))\n;\n\n    \nName\n \n=\n \nOther ATLAS Production\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_gatekpr.other.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nDefault\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n# \nRoute\n \nno\n \n10\n\n# \nAnything\n \nelse\n. \nSet\n \nqueue\n \nas\n \nDefault\n \nand\n \nassign\n \nto\n \nother\n \nVOs\n\n  [\n    \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n    \neval_set_GridResource\n \n=\n \nstrcat\n(\ncondor \n, \n$(FULL_HOSTNAME)\n, \n $(JOB_ROUTER_SCHEDD2_POOL)\n)\n;\n\n    \nRequirements\n \n=\n \ntarget\n.\nqueue\n \nis\n \nundefined\n \n \nifThenElse\n(\nregexp\n(\nusatlas\n,\ntarget\n.\nOwner\n)\n,\nfalse\n,\ntrue\n)\n;\n\n    \nName\n \n=\n \nOther Jobs\n;\n\n    \nTargetUniverse\n \n=\n \n5\n;\n\n    \nset_requirements\n \n=\n \n(\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n?\n=\n \nundefined\n \n)\n \n||\n \n(\n \nTARGET\n.\nTotalDisk\n \n=\n \n21000000\n \n)\n \n)\n \n \n(\n \nTARGET\n.\nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nTARGET\n.\nOpSys\n \n==\n \nLINUX\n \n)\n \n \n(\n \nTARGET\n.\nDisk\n \n=\n \nRequestDisk\n \n)\n \n \n(\n \nTARGET\n.\nMemory\n \n=\n \nRequestMemory\n \n)\n \n \n(\n \nTARGET\n.\nHasFileTransfer\n \n)\n;\n\n    \neval_set_AccountingGroup\n \n=\n \nstrcat\n(\ngroup_VOgener.\n,\nOwner\n)\n;\n\n    \nset_localQue\n \n=\n \nDefault\n;\n\n    \nset_IsAnalyJob\n \n=\n \nFalse\n;\n\n    \nset_Rank\n \n=\n \n(\nSlotID\n \n+\n \n(\n64\n-\nTARGET\n.\nDETECTED_CORES\n))\n*\n1\n.\n0\n;\n\n    \nset_JobMemoryLimit\n \n=\n \n4194000\n;\n\n    \nset_Periodic_Remove\n \n=\n \n(\n \n(\n \nRemoteWallClockTime\n \n \n(\n3\n*\n24\n*\n60\n*\n60\n \n+\n \n5\n*\n60\n)\n \n)\n \n||\n \n(\nImageSize\n \n \nJobMemoryLimit\n)\n \n)\n;\n\n  ]\n  @\njre\n\n\n\n\n\n\nBNL's job routes\n\n\nAtlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:\n\n\n\n\nSetting various HTCondor-specific attributes like \nJobLeaseDuration\n, \nRequirements\n and \nPeriodic_Hold\n (see the \nHTCondor manual\n for more). Some of these are site-specific like \nRACF_Group\n, \nExperiment\n, \nJob_Type\n and \nVO\n.\n\n\nJobs are split into different routes based on the \nGlideIn\n queue that they're in.\n\n\nThere is a difference between \nRequirements\n and \nset_requirements\n. The \nRequirements\n attribute matches \nincoming\n jobs to specific routes while the \nset_requirements\n sets the \nRequirements\n attribute on the \nrouted\n job, which confines which machines that the routed job can land on.\n\n\n\n\nSource: \nhttp://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html\n\n\nJOB_ROUTER_ENTRIES\n \n@=jre\n\n   \n[\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n\n     \nTargetUniverse\n \n=\n \n5\n;\n\n     \nname\n \n=\n \nBNL_Condor_Pool_long\n;\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.long\n;\n\n     \neval_set_RACF_Group\n \n=\n \nlong\n;\n\n     \nset_Experiment\n \n=\n \natlas\n;\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n\n   \n]\n\n   \n[\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n\n     \nTargetUniverse\n \n=\n \n5\n;\n\n     \nname\n \n=\n \nBNL_Condor_Pool_short\n;\n\n     \nRequirements\n \n=\n target.queue\n==\nanalysis.short\n;\n\n     \neval_set_RACF_Group\n \n=\n \nshort\n;\n\n     \nset_Experiment\n \n=\n \natlas\n;\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n\n   \n]\n\n   \n[\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n\n     \nTargetUniverse\n \n=\n \n5\n;\n\n     \nname\n \n=\n \nBNL_Condor_Pool_grid\n;\n\n     \nRequirements\n \n=\n target.queue\n==\ngrid\n;\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n\n     \nset_Experiment\n \n=\n \natlas\n;\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \natlas\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n\n   \n]\n\n   \n[\n\n     \nGridResource\n \n=\n \ncondor localhost localhost\n;\n\n     \neval_set_GridResource\n \n=\n strcat\n(\ncondor \n, \n$(\nFULL_HOSTNAME\n)\n, \n$(\nFULL_HOSTNAME\n)\n)\n;\n\n     \nTargetUniverse\n \n=\n \n5\n;\n\n     \nname\n \n=\n \nBNL_Condor_Pool\n;\n\n     \nRequirements\n \n=\n target.queue is undefined\n;\n\n     \neval_set_RACF_Group\n \n=\n \ngrid\n;\n\n     \nset_requirements\n \n=\n \n(\n \n(\n \nArch\n \n==\n \nINTEL\n \n||\n \nArch\n \n==\n \nX86_64\n \n)\n \n \n(\n \nCPU_Experiment\n \n==\n \nrcf\n \n)\n \n)\n \n \n(\n TARGET.OpSys \n==\n \nLINUX\n \n)\n \n \n(\n TARGET.Disk \n=\n RequestDisk \n)\n \n \n(\n TARGET.Memory \n=\n RequestMemory \n)\n \n \n(\n TARGET.HasFileTransfer \n)\n;\n\n     \nset_Experiment\n \n=\n \natlas\n;\n\n     \nset_Job_Type\n \n=\n \ncas\n;\n\n     \nset_JobLeaseDuration\n \n=\n \n3600\n;\n\n     \nset_Periodic_Hold\n \n=\n \n(\nNumJobStarts \n=\n \n1\n \n \nJobStatus\n \n==\n \n1\n)\n \n||\n NumJobStarts \n \n1\n;\n\n     \neval_set_VO\n \n=\n x509UserProxyVOName\n;\n\n   \n]\n\n   @jre", 
            "title": "Job Router Recipes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-routes-for-htcondor-ce", 
            "text": "The  JobRouter  is at the heart of HTCondor-CE and allows admins to transform and direct jobs to specific batch systems. Customizations are made in the form of job routes where each route corresponds to a separate job transformation: If an incoming job matches a job route's requirements, the route creates a transformed job (referred to as the 'routed job') that is then submitted to the batch system. The CE package comes with default routes located in  /etc/condor-ce/config.d/02-ce-*.conf  that provide enough basic functionality for a small site.  If you have needs beyond delegating all incoming jobs to your batch system as they are, this document provides examples of common job routes and job route problems.   Definitions    Incoming Job : A job which was submitted to the CE from an outside source, such as a GlideinWMS Factory.    Routed Job : A job which was transformed by the JobRouter.    Batch System : The underlying batch system that the HTCondor-CE will submit.  This can be Slurm, PBS, HTCondor, SGE, LSF,...", 
            "title": "Writing Routes For HTCondor-CE"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#quirks-and-pitfalls", 
            "text": "If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.  Make sure to run  condor_ce_reconfig  after changing your routes, otherwise they will not take effect.  Do  not  set the job environment through the JobRouter. Instead, add any changes to the  [Local Settings]  section in  /etc/osg/config.d/40-localsettings.ini  and run osg-configure, as documented  here .  HTCondor batch system only: Local universe jobs are excluded from any routing.", 
            "title": "Quirks and Pitfalls"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#how-job-routes-are-constructed", 
            "text": "Each job route\u2019s  ClassAd  is constructed by combining each entry from the  JOB_ROUTER_ENTRIES  with the  JOB_ROUTER_DEFAULTS . Attributes that are  set_*  in  JOB_ROUTER_ENTRIES  will override those  set_*  in  JOB_ROUTER_DEFAULTS", 
            "title": "How Job Routes are Constructed"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_entries", 
            "text": "JOB_ROUTER_ENTRIES  is a configuration variable whose default is set in  /etc/condor-ce/config.d/02-ce-*.conf  but may be overriden by the administrator in  /etc/condor-ce/config.d/99-local.conf . This document outlines the many changes you can make to  JOB_ROUTER_ENTRIES  to fit your site\u2019s needs.", 
            "title": "JOB_ROUTER_ENTRIES"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job_router_defaults", 
            "text": "JOB_ROUTER_DEFAULTS  is a python-generated configuration variable that sets default job route values that are required for the HTCondor-CE's functionality. To view its contents in a readable format, run the following command:  user@host $  condor_ce_config_val JOB_ROUTER_DEFAULTS  |  sed  s/;/;\\n/g    Warning  If a value is set in  JOB_ROUTER_DEFAULTS  with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES . Do this at your own risk as it may cause the CE to break.    Warning  Do  not  set the  JOB_ROUTER_DEFAULTS  configuration variable yourself. This will cause the CE to stop functioning.", 
            "title": "JOB_ROUTER_DEFAULTS"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#how-jobs-match-to-job-routes", 
            "text": "The job router considers jobs in the queue ( condor_ce_q ) that\nmeet the following constraints:   The job has not already been considered by the job router  The job is associated with an unexpired x509 proxy  The job's universe is standard or vanilla   If the job meets the above constraints, then the job's ClassAd is compared against each route's requirements .\nIf the job only meets one route's requirements, the job is matched to that route.\nIf the job meets the requirements of multiple routes,  the route that is chosen depends on your version of HTCondor\n( condor_version ):     If your version of HTCondor is...  Then the route is chosen by...       8.7.1  Round-robin  between all matching routes. In this case, we recommend making each route's requirements mutually exclusive.    = 8.7.1  First matching route  where routes are considered in the same order that they are configured     If you're using HTCondor  = 8.7.1 and would like to use round-robin matching, add the following text to a file in /etc/condor-ce/config.d/ :  JOB_ROUTER_ROUND_ROBIN_SELECTION   =   True", 
            "title": "How Jobs Match to Job Routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#generic-routes", 
            "text": "This section contains general information about job routes that can be used regardless of the type of batch system at your site. New routes should be placed in  /etc/condor-ce/config.d/99-local.conf , not the original  02-ce-*.conf .", 
            "title": "Generic Routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#required-fields", 
            "text": "The minimum requirements for a route are that you specify the type of batch system that jobs should be routed to and a name for each route. Default routes can be found in  /usr/share/condor-ce/config.d/02-ce- batch system -defaults.conf , provided by the  osg-ce- batch system  packages.", 
            "title": "Required fields"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#batch-system", 
            "text": "Each route needs to indicate the type of batch system that jobs should be routed to. For HTCondor batch systems, the  TargetUniverse  attribute needs to be set to  5  or  \"vanilla\" . For all other batch systems, the  TargetUniverse  attribute needs to be set to  9  or  \"grid\"  and the  GridResource  attribute needs to be set to  \"batch  batch system \"  (where  batch system  can be one of  pbs ,  slurm ,  lsf , or  sge ).  JOB_ROUTER_ENTRIES   @ = jre  [     TargetUniverse = 5;    name =  Route jobs to HTCondor ;  ]  [     GridResource =  batch pbs ;     TargetUniverse = 9;    name =  Route jobs to PBS ;  ]  @jre", 
            "title": "Batch system"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#route-name", 
            "text": "To identify routes, you will need to assign a name to the route with the  name  attribute:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;     name =  Route jobs to HTCondor ;  ]  @jre   The name of the route will be useful in debugging since it shows up in the output of  condor_ce_job_router_info , the  JobRouterLog , and in the ClassAd of the routed job, which can be viewed with  condor_ce_q  or  condor_ce_history .", 
            "title": "Route name"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-multiple-routes", 
            "text": "Note  Before writing multiple routes, consider the details of  how jobs match to job routes   If your batch system needs incoming jobs to be sorted (e.g. if different VO's need to go to separate queues),\nyou will need to write multiple job routes where each route is enclosed by square brackets.\nThe following routes takes incoming jobs that have a  queue  attribute set to  \"analy\"  and routes them to the site's\nHTCondor batch system.\nAny other jobs will be sent to that site's PBS batch system.  JOB_ROUTER_ENTRIES   @ = jre  % RED %[    TargetUniverse = 5;    name =  Route jobs to HTCondor ;    Requirements = (TARGET.queue =?=  analy );  ]  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Route jobs to PBS ;    Requirements = (TARGET.queue =!=  analy );  ]% ENDCOLOR %  @jre", 
            "title": "Writing multiple routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#writing-comments", 
            "text": "To write comments you can use  #  to comment a line:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;    name =  # comments ;    # This is a comment  ]  @jre", 
            "title": "Writing comments"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-for-all-routes", 
            "text": "To set an attribute that will be applied to all routes, you will need to ensure that  MERGE_JOB_ROUTER_DEFAULT_ADS  is set to  True  (check the value with  condor_ce_config_val ) and use the  set_  function in the  JOB_ROUTER_DEFAULTS . The following configuration sets the  Periodic_Hold  attribute for all routes:  # Use the defaults generated by the condor_ce_router_defaults script.  To add  # additional defaults, add additional lines of the form:  #  #   JOB_ROUTER_DEFAULTS = $(JOB_ROUTER_DEFAULTS) [set_foo = 1;]  #  MERGE_JOB_ROUTER_DEFAULT_ADS = True JOB_ROUTER_DEFAULTS   =   $( JOB_ROUTER_DEFAULTS )   [ set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; ]", 
            "title": "Setting attributes for all routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#filtering-jobs-based-on", 
            "text": "To filter jobs, use the  Requirements  attribute. Jobs will evaluate against the ClassAd expression set in the  Requirements  and if the expression evaluates to  TRUE , the route will match. More information on the syntax of ClassAd's can be found in the  HTCondor manual . For an example on how incoming jobs interact with filtering in job routes, consult  this document .  When setting requirements, you need to prefix job attributes that you are filtering with  TARGET.  so that the job route knows to compare the attribute of the incoming job rather than the route\u2019s own attribute. For example, if an incoming job has a  queue = \"analy\"  attribute, then the following job route will not match:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;    name =  Filtering by queue ;    queue =  not-analy ;     Requirements = (queue =?=  analy );  ]  @jre   This is because when evaluating the route requirement, the job route will compare its own  queue  attribute to \"analy\" and see that it does not match. You can read more about comparing two ClassAds in the  HTCondor manual .   Note  If you have an HTCondor batch system, note the difference with  set_requirements .    Note  Before writing multiple routes, consider the details of  how jobs match to job routes .", 
            "title": "Filtering jobs based on\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#glidein-queue", 
            "text": "To filter jobs based on their glidein queue attribute, your routes will need a  Requirements  expression using the incoming job's  queue  attribute. The following entry routes jobs to PBS if the incoming job (specified by  TARGET ) is an  analy  (Analysis) glidein:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;    name =  Filtering by queue ;     Requirements = (TARGET.queue =?=  analy );  ]  @jre", 
            "title": "Glidein queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#job-submitter", 
            "text": "To filter jobs based on who submitted it, your routes will need a  Requirements  expression using the incoming job's  Owner  attribute. The following entry routes jobs to the HTCondor batch system if the submitter is  usatlas2 :  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;    name =  Filtering by job submitter ;     Requirements = (TARGET.Owner =?=  usatlas2 );  ]  @jre   Alternatively, you can match based on regular expression. The following entry routes jobs to the PBS batch system if the submitter's name begins with  usatlas :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Filtering by job submitter (regular expression) ;     Requirements = regexp( ^usatlas , TARGET.Owner);  ]  @jre", 
            "title": "Job submitter"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#voms-attribute", 
            "text": "To filter jobs based on the subject of the job's proxy, your routes will need a  Requirements  expression using the incoming job's  x509UserProxyFirstFQAN  attribute. The following entry routes jobs to the PBS batch system if the proxy subject contains  /cms/Role=Pilot :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Filtering by VOMS attribute (regex) ;     Requirements = regexp( \\/cms\\/Role\\=pilot , TARGET.x509UserProxyFirstFQAN);  ]  @jre", 
            "title": "VOMS attribute"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default", 
            "text": "This section outlines how to set default job limits, memory, cores, and maximum walltime. For an example on how users can override these defaults, consult  this document .", 
            "title": "Setting a default\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-number-of-jobs", 
            "text": "To set a default limit to the maximum number of jobs per route, you can edit the configuration variable  CONDORCE_MAX_JOBS  in  /etc/condor-ce/config.d/01-ce-router.conf :  CONDORCE_MAX_JOBS   =   10000    Note  The above configuration is to be placed directly into the HTCondor-CE configuration, not into a job route.", 
            "title": "Maximum number of jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-memory", 
            "text": "To set a default maximum memory (in MB) for routed jobs, set the attribute  default_maxMemory :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Request memory ;    # Set the requested memory to 1 GB     set_default_maxMemory = 1000;  ]  @jre", 
            "title": "Maximum memory"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#number-of-cores-to-request", 
            "text": "To set a default number of cores for routed jobs, set the attribute  default_xcount :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Request CPU ;    # Set the requested cores to 8     set_default_xcount = 8;  ]  @jre", 
            "title": "Number of cores to request"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#maximum-walltime", 
            "text": "To set a default maximum walltime (in minutes) for routed jobs, set the attribute  default_maxWallTime :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Setting WallTime ;    # Set the max walltime to 1 hr     set_default_maxWallTime = 60;  ]  @jre", 
            "title": "Maximum walltime"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#editing-attributes", 
            "text": "The following functions are operations that affect job attributes and are evaluated in the following order:   copy_*  delete_*  set_*  eval_set_*   After each job route\u2019s ClassAd is  constructed , the above operations are evaluated in order. For example, if the attribute  foo  is set using  eval_set_foo  in the  JOB_ROUTER_DEFAULTS , you'll be unable to use  delete_foo  to remove it from your jobs since the attribute is set using  eval_set_foo  after the deletion occurs according to the order of operations. To get around this, we can take advantage of the fact that operations defined in  JOB_ROUTER_DEFAULTS  get overridden by the same operation in  JOB_ROUTER_ENTRIES . So to 'delete'  foo , we would add  eval_set_foo = \"\"  to the route in the  JOB_ROUTER_ENTRIES , resulting in  foo  being absent from the routed job.  More documentation can be found in the  HTCondor manual .", 
            "title": "Editing attributes\u2026"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#copying-attributes", 
            "text": "To copy the value of an attribute of the incoming job to an attribute of the routed job, use  copy_ . The following route copies the  environment  attribute of the incoming job and sets the attribute  Original_Environment  on the routed job to the same value:  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Copying attributes ;     copy_environment =  Original_Environment ;  ]  @jre", 
            "title": "Copying attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#removing-attributes", 
            "text": "To remove an attribute of the incoming job from the routed job, use  delete_ . The following route removes the  environment  attribute from the routed job:  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Copying attributes ;     delete_environment = True;  ]  @jre", 
            "title": "Removing attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes", 
            "text": "To set an attribute on the routed job, use  set_ . The following route sets the Job's  Rank  attribute to 5:  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Setting an attribute ;     set_Rank = 5;  ]  @jre", 
            "title": "Setting attributes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-attributes-with-classad-expressions", 
            "text": "To set an attribute to a ClassAd expression to be evaluated, use  eval_set . The following route sets the  Experiment  attribute to  atlas.osguser  if the Owner of the incoming job is  osguser :   Note  If a value is set in JOB_ROUTER_DEFAULTS with  eval_set_ variable , override it by using  eval_set_ variable  in the  JOB_ROUTER_ENTRIES .   JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Setting an attribute with a !ClassAd expression ;     eval_set_Experiment = strcat( atlas. , Owner);  ]  @jre", 
            "title": "Setting attributes with ClassAd expressions"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#limiting-the-number-of-jobs", 
            "text": "This section outlines how to limit the number of total or idle jobs in a specific route (i.e., if this limit is reached, jobs will no longer be placed in this route).   Note  If you are using an HTCondor batch system, limiting the number of jobs is not the preferred solution: HTCondor manages fair share on its own via  user priorities and group accounting .", 
            "title": "Limiting the number of jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#total-jobs", 
            "text": "To set a limit on the number of jobs for a specific route, set the  MaxJobs  attribute:  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Limit the total number of jobs to 100 ;     MaxJobs = 100;  ]  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Limit the total number of jobs to 75 ;     MaxJobs = 75;  ]  @jre", 
            "title": "Total jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#idle-jobs", 
            "text": "To set a limit on the number of idle jobs for a specific route, set the  MaxIdleJobs  attribute:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;    name =  Limit the total number of idle jobs to 100 ;     MaxIdleJobs = 100;  ]  [    TargetUniverse = 5;    name =  Limit the total number of idle jobs to 75 ;     MaxIdleJobs = 75;  ]  @jre", 
            "title": "Idle jobs"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#debugging-routes", 
            "text": "To help debug expressions in your routes, you can use the  debug()  function. First, set the debug mode for the JobRouter by editing a file in  /etc/condor-ce/config.d/  to read  JOB_ROUTER_DEBUG   =   D_ALWAYS : 2   D_CAT   Then wrap the problematic attribute in  debug() :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Debugging a difficult !ClassAd expression ;     eval_set_Experiment = debug(strcat( atlas , Name));  ]  @jre   You will find the debugging output in  /var/log/condor-ce/JobRouterLog .", 
            "title": "Debugging routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running an HTCondor batch system at your site.", 
            "title": "Routes for HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-periodic-hold-release-or-remove", 
            "text": "To release, remove or put a job on hold if it meets certain criteria, use the  PERIODIC_*  family of attributes. By default, periodic expressions are evaluated once every 300 seconds but this can be changed by setting  PERIODIC_EXPR_INTERVAL  in your CE's configuration.  In this example, we set the routed job on hold if the job is idle and has been started at least once or if the job has tried to start more than once.  This will catch jobs which are starting and stopping multiple times.  JOB_ROUTER_ENTRIES  @ = jre \n[\n   TargetUniverse   =   5 ; \n   name   =   Setting periodic statements ; \n   % RED % #  Puts   the   routed   job   on   hold   if   the   job s been idle and has been started at least once or if the job has tried to start more than once \n   set_Periodic_Hold   =   ( NumJobStarts   =   1     JobStatus   ==   1 )   ||   NumJobStarts     1 ; \n  #  Remove   routed   jobs   if   their   walltime   is   longer   than   3   days   and   5   minutes \n   set_Periodic_Remove   =   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   ) ; \n  #  Release   routed   jobs   if   the   condor_starter   couldn t start the executable and  VMGAHP_ERR_INTERNAL  is in the HoldReason \n   set_Periodic_Release   =   HoldReasonCode   ==   6     regexp ( VMGAHP_ERR_INTERNAL ,  HoldReason ) ; \n]\n@ jre", 
            "title": "Setting periodic hold, release or remove"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-routed-job-requirements", 
            "text": "If you need to set requirements on your routed job, you will need to use  set_Requirements  instead of  Requirements . The  Requirements  attribute filters jobs coming into your CE into different job routes whereas  set_Requirements  will set conditions on the routed job that must be met by the worker node it lands on. For more information on requirements, consult the  HTCondor manual .  To ensure that your job lands on a Linux machine in your pool:  JOB_ROUTER_ENTRIES   @ = jre  [    TargetUniverse = 5;     set_Requirements =  OpSys ==  LINUX ;  ]  @jre", 
            "title": "Setting routed job requirements"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-accounting-groups", 
            "text": "To assign jobs to an HTCondor accounting group to manage fair share on your local batch system, we recommend using  UID and ExtAttr tables .", 
            "title": "Setting accounting groups"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#routes-for-non-htcondor-batch-systems", 
            "text": "This section contains information about job routes that can be used if you are running a non-HTCondor batch system at your site.", 
            "title": "Routes for non-HTCondor Batch Systems"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-a-default-batch-queue", 
            "text": "To set a default queue for routed jobs, set the attribute  default_queue :  JOB_ROUTER_ENTRIES   @ = jre  [    GridResource =  batch pbs ;    TargetUniverse = 9;    name =  Setting batch system queues ;     set_default_queue =  osg_queue ;  ]  @jre", 
            "title": "Setting a default batch queue"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#setting-batch-system-directives", 
            "text": "To write batch system directives that are not supported in the route examples above, you will need to edit the job submit script for your local batch system in  /etc/blahp/  (e.g., if your local batch system is PBS, edit  /etc/blahp/pbs_local_submit_attributes.sh ). This file is sourced during submit time and anything printed to stdout is appended to the batch system job submit script. ClassAd attributes can be passed from the routed job to the local submit attributes script via the  default_remote_cerequirements  attribute, which can take the following form:  default_remote_cerequirements   =   foo == X   bar == \\ Y \\    ...   This sets  foo  to value  X  and  bar  to the string  Y  (escaped double-quotes are required for string values) in the environment of the local submit attributes script. The following example sets the maximum walltime to 1 hour and the accounting group to the  x509UserProxyFirstFQAN  attribute of the job submitted to a PBS batch system  JOB_ROUTER_ENTRIES   @ = jre   [       GridResource =  batch pbs ;       TargetUniverse = 9;       name =  Setting job submit variables ;        set_default_remote_cerequirements = strcat( Walltime == 3600   AccountingGroup == , x509UserProxyFirstFQAN,  \\ );  ]  @jre   With  /etc/blahp/pbs_local_submit_attributes.sh  containing.  1\n2\n3 #!/bin/bash  echo   #PBS -l walltime= $Walltime  echo   #PBS -A  $AccountingGroup    This results in the following being appended to the script that gets submitted to your batch system:  # PBS   - l   walltime = 3600  # PBS   - A   % RED % CE   job s   x509UserProxyFirstFQAN   attribute % ENDCOLOR %", 
            "title": "Setting batch system directives"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  The HTCondor-CE troubleshooting guide  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#example-configurations", 
            "text": "", 
            "title": "Example Configurations"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#aglt2s-job-routes", 
            "text": "Atlas AGLT2 is using an HTCondor batch system. Here are some things to note about their routes.   Setting various HTCondor-specific attributes like  Rank ,  AccountingGroup ,  JobPrio  and  Periodic_Remove  (see the  HTCondor manual  for more). Some of these are site-specific like  LastandFrac ,  IdleMP8Pressure ,  localQue ,  IsAnalyJob  and  JobMemoryLimit .  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  https://www.aglt2.org/wiki/bin/view/AGLT2/CondorCE#The_JobRouter_configuration_file_content  JOB_ROUTER_ENTRIES  @ = jre \n#  Still   to   do   on   all   routes ,  get   job   requirements   and   add   them   here \n#  Route   no   1 \n#  Analysis   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue == analy ; \n     Name   =   Analysis Queue ; \n     TargetUniverse   =   5 ; \n     eval_set_IdleMP8Pressure   =  $ ( IdleMP8Pressure ) ; \n     eval_set_LastAndFrac   =  $ ( LastAndFrac ) ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   )     ( IfThenElse (( Owner   ==   atlasconnect   ||   Owner   ==   muoncal ) , IfThenElse ( IdleMP8Pressure , ( TARGET . PARTITIONED   =!=   TRUE ) , True ) , IfThenElse ( LastAndFrac , ( TARGET . PARTITIONED   =!=   TRUE ) , True ))) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.prod.analy. , Owner ) ; \n     set_localQue   =   Analysis ; \n     set_IsAnalyJob   =   True ; \n     set_JobPrio   =   5 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   2 \n#  splitterNT   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   ==   splitterNT ; \n     Name   =   Splitter ntuple queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ; \n     set_localQue   =   Splitter ; \n     set_IsAnalyJob   =   False ; \n     set_JobPrio   =   10 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   3 \n#  splitter   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   ==   splitter ; \n     Name   =   Splitter queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   group_calibrate.muoncal ; \n     set_localQue   =   Splitter ; \n     set_IsAnalyJob   =   False ; \n     set_JobPrio   =   15 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   4 \n#  xrootd   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   ==   xrootd ; \n     Name   =   Xrootd queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.prod.analy. , Owner ) ; \n     set_localQue   =   Analysis ; \n     set_IsAnalyJob   =   True ; \n     set_JobPrio   =   35 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   5 \n#  Tier3Test   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   ==   Tier3Test ; \n     Name   =   Tier3 Test Queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   )     (   IS_TIER3_TEST_QUEUE   = ? =   True   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.prod.analy. , Owner ) ; \n     set_localQue   =   Tier3Test ; \n     set_IsTier3TestJob   =   True ; \n     set_IsAnalyJob   =   True ; \n     set_JobPrio   =   20 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   6 \n#  mp8   queue \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue == mp8 ; \n     Name   =   MCORE Queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   )     ((   TARGET . Cpus   ==   8     TARGET . CPU_TYPE   = ? =   mp8   )   ||   TARGET . PARTITIONED   = ? =   True   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.prod.mcore. , Owner ) ; \n     set_localQue   =   MP8 ; \n     set_IsAnalyJob   =   False ; \n     set_JobPrio   =   25 ; \n     set_Rank   =   0 . 0 ; \n     eval_set_RequestCpus   =   8 ; \n     set_JobMemoryLimit   =   33552000 ; \n     set_Slot_Type   =   mp8 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   7 \n#  Installation   queue ,  triggered   by   usatlas2   user \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   is   undefined     target . Owner   ==   usatlas2 ; \n     Name   =   Install Queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   )     (   TARGET . IS_INSTALL_QUE   = ? =   True   )     ( TARGET . AGLT2_SITE   ==   UM   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.other. , Owner ) ; \n     set_localQue   =   Default ; \n     set_IsAnalyJob   =   False ; \n     set_IsInstallJob   =   True ; \n     set_JobPrio   =   15 ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   8 \n#  Default   queue   for   usatlas1   user \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   is   undefined     regexp ( usatlas1 , target . Owner ) ; \n     Name   =   ATLAS Production Queue ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.prod.prod. , Owner ) ; \n     set_localQue   =   Default ; \n     set_IsAnalyJob   =   False ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   9 \n#  Default   queue   for   any   other   usatlas   account \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   is   undefined     ( regexp ( usatlas2 , target . Owner )   ||   regexp ( usatlas3 , target . Owner )) ; \n     Name   =   Other ATLAS Production ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_gatekpr.other. , Owner ) ; \n     set_localQue   =   Default ; \n     set_IsAnalyJob   =   False ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n#  Route   no   10 \n#  Anything   else .  Set   queue   as   Default   and   assign   to   other   VOs \n  [\n     GridResource   =   condor localhost localhost ; \n     eval_set_GridResource   =   strcat ( condor  ,  $(FULL_HOSTNAME) ,   $(JOB_ROUTER_SCHEDD2_POOL) ) ; \n     Requirements   =   target . queue   is   undefined     ifThenElse ( regexp ( usatlas , target . Owner ) , false , true ) ; \n     Name   =   Other Jobs ; \n     TargetUniverse   =   5 ; \n     set_requirements   =   (   (   TARGET . TotalDisk   = ? =   undefined   )   ||   (   TARGET . TotalDisk   =   21000000   )   )     (   TARGET . Arch   ==   X86_64   )     (   TARGET . OpSys   ==   LINUX   )     (   TARGET . Disk   =   RequestDisk   )     (   TARGET . Memory   =   RequestMemory   )     (   TARGET . HasFileTransfer   ) ; \n     eval_set_AccountingGroup   =   strcat ( group_VOgener. , Owner ) ; \n     set_localQue   =   Default ; \n     set_IsAnalyJob   =   False ; \n     set_Rank   =   ( SlotID   +   ( 64 - TARGET . DETECTED_CORES )) * 1 . 0 ; \n     set_JobMemoryLimit   =   4194000 ; \n     set_Periodic_Remove   =   (   (   RemoteWallClockTime     ( 3 * 24 * 60 * 60   +   5 * 60 )   )   ||   ( ImageSize     JobMemoryLimit )   ) ; \n  ]\n  @ jre", 
            "title": "AGLT2's job routes"
        }, 
        {
            "location": "/compute-element/job-router-recipes/#bnls-job-routes", 
            "text": "Atlas BNL T1, they are using an HTCondor batch system. Here are some things to note about their routes:   Setting various HTCondor-specific attributes like  JobLeaseDuration ,  Requirements  and  Periodic_Hold  (see the  HTCondor manual  for more). Some of these are site-specific like  RACF_Group ,  Experiment ,  Job_Type  and  VO .  Jobs are split into different routes based on the  GlideIn  queue that they're in.  There is a difference between  Requirements  and  set_requirements . The  Requirements  attribute matches  incoming  jobs to specific routes while the  set_requirements  sets the  Requirements  attribute on the  routed  job, which confines which machines that the routed job can land on.   Source:  http://www.usatlas.bnl.gov/twiki/bin/view/Admins/HTCondorCE.html  JOB_ROUTER_ENTRIES   @=jre \n    [ \n      GridResource   =   condor localhost localhost ; \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ; \n      TargetUniverse   =   5 ; \n      name   =   BNL_Condor_Pool_long ; \n      Requirements   =  target.queue == analysis.long ; \n      eval_set_RACF_Group   =   long ; \n      set_Experiment   =   atlas ; \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ; \n      set_Job_Type   =   cas ; \n      set_JobLeaseDuration   =   3600 ; \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; \n      eval_set_VO   =  x509UserProxyVOName ; \n    ] \n    [ \n      GridResource   =   condor localhost localhost ; \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ; \n      TargetUniverse   =   5 ; \n      name   =   BNL_Condor_Pool_short ; \n      Requirements   =  target.queue == analysis.short ; \n      eval_set_RACF_Group   =   short ; \n      set_Experiment   =   atlas ; \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ; \n      set_Job_Type   =   cas ; \n      set_JobLeaseDuration   =   3600 ; \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; \n      eval_set_VO   =  x509UserProxyVOName ; \n    ] \n    [ \n      GridResource   =   condor localhost localhost ; \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ; \n      TargetUniverse   =   5 ; \n      name   =   BNL_Condor_Pool_grid ; \n      Requirements   =  target.queue == grid ; \n      eval_set_RACF_Group   =   grid ; \n      set_Experiment   =   atlas ; \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   atlas   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ; \n      set_Job_Type   =   cas ; \n      set_JobLeaseDuration   =   3600 ; \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; \n      eval_set_VO   =  x509UserProxyVOName ; \n    ] \n    [ \n      GridResource   =   condor localhost localhost ; \n      eval_set_GridResource   =  strcat ( condor  ,  $( FULL_HOSTNAME ) ,  $( FULL_HOSTNAME ) ) ; \n      TargetUniverse   =   5 ; \n      name   =   BNL_Condor_Pool ; \n      Requirements   =  target.queue is undefined ; \n      eval_set_RACF_Group   =   grid ; \n      set_requirements   =   (   (   Arch   ==   INTEL   ||   Arch   ==   X86_64   )     (   CPU_Experiment   ==   rcf   )   )     (  TARGET.OpSys  ==   LINUX   )     (  TARGET.Disk  =  RequestDisk  )     (  TARGET.Memory  =  RequestMemory  )     (  TARGET.HasFileTransfer  ) ; \n      set_Experiment   =   atlas ; \n      set_Job_Type   =   cas ; \n      set_JobLeaseDuration   =   3600 ; \n      set_Periodic_Hold   =   ( NumJobStarts  =   1     JobStatus   ==   1 )   ||  NumJobStarts    1 ; \n      eval_set_VO   =  x509UserProxyVOName ; \n    ] \n   @jre", 
            "title": "BNL's job routes"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/", 
            "text": "HTCondor-CE Troubleshooting Guide\n\n\nIn this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of\ncommon issues with suggested troubleshooting steps.\n\n\nKnown Issues\n\n\nSUBMIT_EXPRS are not applied to jobs on the local HTCondor\n\n\nIf you are adding attributes to jobs submitted to your HTCondor pool with \nSUBMIT_EXPRS\n, these will \nnot\n be applied to\njobs that are entering your pool from the HTCondor-CE. \nTo get around this, you will want to add the attributes to your \njob routes\n. \nIf the CE is the only entry point for jobs into your pool, you can get rid of \nSUBMIT_EXPRS\n on your backend. Otherwise,\nyou will have to maintain your list of attributes both in your list of routes and in your \nSUBMIT_EXPRS\n.\n\n\nGeneral Troubleshooting Items\n\n\nMaking sure packages are up-to-date\n\n\nIt is important to make sure that the HTCondor-CE and related RPMs are up-to-date.\n\n\nroot@host #\n yum update \nhtcondor-ce*\n blahp condor\n\n\n\n\n\nIf you just want to see the packages to update, but do not want to perform the update now, answer \nN\n at the prompt.\n\n\nVerify package contents\n\n\nIf the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the\ncontents of your packages (ignoring changes to configuration files):\n\n\nuser@host $\n rpm -q --verify htcondor-ce htcondor-ce-client blahp \n|\n grep -v \n/var/\n \n|\n awk \n$2 != \nc\n {print $0}\n\n\n\n\n\n\nIf the verification command returns output, this means that your packages have been changed. To fix this, you can\nreinstall the packages:\n\n\nuser@host $\n yum reinstall htcondor-ce htcondor-ce-client blahp\n\n\n\n\n\n\n\nNote\n\n\nThe reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an \n.rpmnew\n suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.\n\n\n\n\nVerify clocks are synchronized\n\n\nLike all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is\nsynchronized using a utility such as \nntpd\n. \nAdditionally, HTCondor itself is sensitive to time skews on the NFS server.\nIf you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.\n\n\nVerify host cerificates and CRLs are valid\n\n\nAn expired host certificate or CRLs will cause various issues with GSI authentication. \nVerify that your host certificate is valid by running:\n\n\nroot@host #\n openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates\n\n\n\n\n\nLikewise, run the \nfetch-crl\n script to update your CRLs:\n\n\nroot@host #\n fetch-crl\n\n\n\n\n\nIf updating CRLs fix your issues, make sure that the \nfetch-crl-cron\n and\n\nfetch-crl-boot\n services are enabled and running.\n\n\nHTCondor-CE Troubleshooting Items\n\n\nThis section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. \nBefore troubleshooting, we recommend increasing the log level:\n\n\n\n\n\n\nWrite the following into \n/etc/condor-ce/config.d/99-local.conf\n to increase the log level for all daemons:\n\n\nALL_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nEnsure that the configuration is in place:\n\n\nroot\n@host\n \n#\n \ncondor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\nReproduce the issue\n\n\n\n\n\n\n\n\nNote\n\n\nBefore spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running \ncondor_ce_reconfig\n.\n\n\n\n\nDaemons fail to start\n\n\nIf there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to\nstartup. \nCheck the following subsections in order:\n\n\nSymptoms\n\n\nDaemon startup failure may manifest in many ways, the following are few symptoms of the problem.\n\n\n\n\n\n\nThe service fails to start:\n\n\nroot@host #\n service condor-ce start\n\nStarting Condor-CE daemons: [ FAIL ]\n\n\n\n\n\n\n\n\n\n\ncondor_ce_q\n fails with a lengthy error message:\n\n\nuser@host $\n condor_ce_q\n\nError:\n\n\n\nExtra Info: You probably saw this error because the condor_schedd is not running\n\n\non the machine you are trying to query. If the condor_schedd is not running, the\n\n\nCondor system will not be able to find an address and port to connect to and\n\n\nsatisfy this request. Please make sure the Condor daemons are running and try\n\n\nagain.\n\n\n\nExtra Info: If the condor_schedd is running on the machine you are trying to\n\n\nquery and you still see the error, the most likely cause is that you have setup\n\n\na personal Condor, you have not defined SCHEDD_NAME in your condor_config file,\n\n\nand something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define\n\n\neither or both of those settings in your config file, or you must use the -name\n\n\noption to condor_q. Please see the Condor manual for details on SCHEDD_NAME and\n\n\nSCHEDD_ADDRESS_FILE.\n\n\n\n\n\n\n\n\n\n\nNext actions\n\n\n\n\nIf the MasterLog is filled with \nERROR:SECMAN...TCP connection to collector...failed\n:\n This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in \nthis\n section of the install guide.\n\n\nIf the MasterLog is filled with \nDC_AUTHENTICATE\n errors:\n The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in \n/etc/condor-ce/condor_mapfile\n.\n\n\nIf the SchedLog is filled with \nCan\u2019t find address for negotiator\n:\n You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.\n\n\n\n\nJobs fail to submit to the CE\n\n\nIf a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the\nculprit, then you may have encountered an authentication or authorization issue. \nYou may see error messages like the following in your \nSchedLog\n:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!\n\n\n\n\n\nNext actions\n\n\n\n\nCheck voms-mapfile or grid-mapfile\n and ensure that the user's DN or VOMS attributes are known to your\n    \nauthentication method\n, and that the mapped users exist\n    on your CE and cluster.\n\n\nCheck for lcmaps errors\n in \n/var/log/messages\n\n\nIf you do not see helpful error messages in \n/var/log/messages\n,\n adjust the debug level by adding \nexport LCMAPS_DEBUG_LEVEL=5\n to \n/etc/sysconfig/condor-ce\n, restarting the condor-ce service, and checking \n/var/log/messages\n for errors again.\n\n\n\n\nJobs stay idle on the CE\n\n\nCheck the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is\nbusy.\n\n\nIdle jobs on CE: Make sure the underlying batch system can run jobs\n\n\nHTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes.\nIf you cannot manually submit jobs (e.g., \ncondor_submit\n, \nqsub\n) on the CE host to your batch system, then HTCondor-CE\nwon't be able to either.\n\n\nProcedure\n\n\n\n\nManually create and submit a simple job (e.g., one that runs \nsleep\n)\n\n\nCheck for errors in the submission itself\n\n\nWatch the job in the batch system queue (e.g., \ncondor_q\n, \nqstat\n)\n\n\nIf the job does not run, check for errors on the batch system\n\n\n\n\nNext actions\n\n\nConsult troubleshooting documentation or support avenues for your batch system.\nOnce you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.\n\n\nIdle jobs on CE: Is the job router handling the incoming job?\n\n\nJobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things\nif you suspect that the jobs are not being matched. \nCheck if the JobRouter sees a job before that by looking at the \njob router log\n and looking for the text\n\nsrc=\nJOB-ID\n\u2026claimed job\n.\n\n\nNext actions\n\n\nUse \ncondor_ce_job_router_info\n to see why your idle job does not match any routes\n\n\nIdle jobs on CE: Verify correct operation between the CE and your local batch system\n\n\nFor HTCondor batch systems\n\n\nHTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch\nsystem interaction will appear in the \nJobRouterLog\n.\n\n\nNext actions\n\n\n\n\nCheck the \nJobRouterLog\n for failures.\n\n\nVerify that the local HTCondor is functional.\n\n\nUse \ncondor_ce_config_val\n to verify that the \nJOB_ROUTER_SCHEDD2_NAME\n, \nJOB_ROUTER_SCHEDD2_POOL\n, and \nJOB_ROUTER_SCHEDD2_SPOOL\n configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.\n\n\nUse \ncondor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE\n and verify that it is set to \n.*\n.\n\n\n\n\nFor non-HTCondor batch systems\n\n\nHTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch\nsystem interaction will appear in the \nGridmanagerLog\n. \nLook for \ngm state change\u2026\n lines to figure out where the issures are occuring.\n\n\nNext actions\n\n\n\n\nIf you see failures in the GridmanagerLog during job submission:\n Save the submit files by adding the appropriate entry to \nblah.config\n and submit it \nmanually\n to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the \nbatch system\n_binpath\n in \n/etc/blah.config\n.\n\n\n\n\nIf you see failures in the GridmanagerLog during queries for job status:\n Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in \n/usr/libexec/blahp/\nbatch system\n_status.sh\n (e.g., \n/usr/libexec/blahp/lsf_status.sh\n) that take the argument \nbatch system/YYYMMDD/job ID\n (e.g., \nlsf/20141008/65053\n). Run the appropriate status script for your batch system and upon success, you should see the following output:\n\n\nroot@host #\n /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053\n\n[ BatchjobId = \n894862\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nIf the script fails, \nrequest help\n from the OSG.\n\n\n\n\n\n\nIdle jobs on CE: Verify ability to change permissions on key files\n\n\nHTCondor-CE needs the ability to write and chown files in its \nspool\n directory and if it cannot, jobs will not run at\nall. \nSpool permission errors can appear in the \nSchedLog\n and the \nJobRouterLog\n.\n\n\nSymptoms\n\n\n09\n/\n17\n/\n14\n \n14\n:\n45\n:\n42\n \nError\n:\n \nUnable\n \nto\n \nchown\n \n/var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env\n \nfrom\n \n12345\n \nto\n \n54321\n\n\n\n\n\n\nNext actions\n\n\n\n\nAs root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.\n\n\nVerify that there aren't any underlying file system issues in the specified location\n\n\n\n\nJobs stay idle on a remote host submitting to the CE\n\n\nIf you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not\nsee a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not\nauthorized to run there. \nNote that jobs may take several minutes or longer if the CE is busy.\n\n\nRemote idle jobs: Can you contact the CE?\n\n\nTo check basic connectivity to a CE, use \ncondor_ce_ping\n:\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nERROR: couldn\nt locate condorce.example.com!\n\n\n\n\n\n\nNext actions\n\n\n\n\nMake sure that the HTCondor-CE daemons are running with \ncondor_ce_status\n.\n\n\nVerify that your CE is reachable from your submit host, replacing \ncondorce.example.com\n with the hostname of your CE:\nuser@host $\n ping condorce.example.com\n\n\n\n\n\n\n\n\n\nRemote idle jobs: Are you authorized to run jobs on the CE?\n\n\nThe CE will only accept jobs from users that authenticate via \nLCMAPS VOMS\n.\nYou can use \ncondor_ce_ping\n to check if you are authorized and what user your proxy is being mapped\nto.\n\n\nSymptoms\n\n\nuser@host $\n condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:3343:1412790611:0\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              gsi@unmapped\n\n\nAuthorized:                  FALSE\n\n\n\n\n\n\nNotice the failures in the above message: \nRemote Mapping: gsi@unmapped\n and \nAuthorized: FALSE\n\n\nNext actions\n\n\n\n\nVerify that an \nauthentication method\n is set up on the CE\n\n\nVerify that your user DN is mapped to an existing system user\n\n\n\n\nJobs go on hold\n\n\nJobs will be put on held with a \nHoldReason\n attribute that can be inspected with \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q -l \nJOB-ID\n -attr HoldReason\n\nHoldReason = \nCE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.\n\n\n\n\n\n\nHeld jobs: no matching routes, route job limit, or route failure threshold\n\n\nJobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes.\nThe most common cases for this behavior are as follows:\n\n\n\n\nThe job does not match any job routes:\n\n  use \ncondor_ce_job_router_info\n to see why your idle job does not match any\n  \nroutes\n.\n\n\nThe route(s) that the job matches to are full:\n\n  See \nlimiting the number of jobs\n.\n\n\nThe job router is throttling submission to your batch system due to submission failures:\n\n  See the HTCondor manual for \nFailureRateThreshold\n.\n  Check for errors in the \nJobRouterLog\n or \nGridmanagerLog\n for HTCondor and\n  non-HTCondor batch systems, respectively.\n\n\n\n\nHeld jobs: Missing/expired user proxy\n\n\nHTCondor-CE requires a valid user proxy for each job that is submitted. \nYou can check the status of your proxy with the following\n\n\nuser@host $\n voms-proxy-info -all\n\n\n\n\n\nNext actions\n\n\nEnsure that the owner of the job generates their proxy with \nvoms-proxy-init\n.\n\n\nHeld jobs: Invalid job universe\n\n\nThe HTCondor-CE only accepts jobs that have \nuniverse\n in their submit files set to \nvanilla\n, \nstandard\n, \nlocal\n, or\n\nscheduler\n. \nThese universes also have corresponding integer values that can be found in the \nHTCondor\nmanual\n.\n\n\nNext actions\n\n\n\n\nEnsure jobs submitted locally, from the CE host, are submitted with \nuniverse = vanilla\n\n\n\n\nEnsure jobs submitted from a remote submit point are submitted with:\n\n\nuniverse\n \n=\n \ngrid\n\n\ngrid_resource\n \n=\n \ncondor\n \ncondorce\n.\nexample\n.\ncom\n \ncondorce\n.\nexample\n.\ncom\n:\n9619\n\n\n\n\n\n\nreplacing \ncondorce.example.com\n with the hostname of the CE.\n\n\n\n\n\n\nIdentifying the corresponding job ID on the local batch system\n\n\nWhen troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID\nand the resultant job ID on the batch system. \nThe methods for finding the resultant job ID differs between batch systems.\n\n\nHTCondor batch systems\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n or \ncondor_ce_history\n:\n\n\n\n\n\n\nUse \ncondor_ce_q\n if the job is still in the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_history\n if the job has left the CE\u2019s queue:\n\n\nuser@host $\n condor_ce_history \nJOB-ID\n -af RoutedToJobId\n\n\n\n\n\n\n\n\n\n\n\n\n\nParse the \nJobRouterLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nNon-HTCondor batch systems\n\n\nWhen HTCondor-CE records the corresponding batch system job ID, it is written in the form \nBATCH-SYSTEM\n/\nDATE\n/\nJOB\nID\n:\n\n\nlsf\n/\n20141206\n/\n482046\n\n\n\n\n\n\n\n\n\n\nTo inspect the CE\u2019s job ad, use \ncondor_ce_q\n:\n\n\nuser@host $\n condor_ce_q \nJOB-ID\n -af GridJobId\n\n\n\n\n\n\n\n\n\nParse the \nGridmanagerLog\n for the CE\u2019s job ID.\n\n\n\n\n\n\nJobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)\n\n\nBy design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. \nTherefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:\n\n\n\n\nIdentify the misbehaving job ID in your batch system queue\n\n\n\n\nFind the job's corresponding CE job ID:\n\n\nuser@host $\n condor_q \nJOB-ID\n -af RoutedFromJobId\n\n\n\n\n\n\n\n\n\nUse \ncondor_ce_rm\n to remove the CE job from the queue\n\n\n\n\n\n\nMissing HTCondor tools\n\n\nMost of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. \nIf you are trying to use HTCondor-CE tools and you see the following error:\n\n\nuser@host $\n condor_ce_job_router_info\n\n/usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found\n\n\n\n\n\n\nThis means that the \ncondor_job_router_info\n (note this is not the CE version), is not in your \nPATH\n.\n\n\nNext Actions\n\n\n\n\nEither the condor RPM is missing or there are some other issues with it (try \nrpm --verify condor\n).\n\n\nYou have installed HTCondor in a non-standard location that is not in your \nPATH\n.\n\n\nThe \ncondor_job_router_info\n tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).\n\n\n\n\nHTCondor-CE Troubleshooting Tools\n\n\nHTCondor-CE has its own separate set of of the HTCondor tools with \nce\n in the name (i.e., \ncondor_ce_submit\n vs\n\ncondor_submit\n). \nSome of the the commands are only for the CE (e.g., \ncondor_ce_run\n and \ncondor_ce_trace\n) but many of them are just\nHTCondor commands configured to interact with the CE (e.g., \ncondor_ce_q\n, \ncondor_ce_status\n). \nIt is important to differentiate the two: \ncondor_ce_config_val\n will provide configuration values for your HTCondor-CE\nwhile\n\ncondor_config_val\n will provide configuration values for your HTCondor batch system. \nIf you are not running an HTCondor batch system, the non-CE commands will return errors.\n\n\ncondor_ce_trace\n\n\nUsage\n\n\ncondor_ce_trace\n is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_trace condorce.example.com\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. \nIf you are familiar with the output of condor commands, the command also takes a \n--debug\n option that displays verbose\ncondor output.\n\n\nTroubleshooting\n\n\n\n\nIf the command fails with \u201cFailed ping\u2026\u201d:\n Make sure that the HTCondor-CE daemons are running on the CE\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:\n Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\nIf the job submits but does not complete:\n Look at the status of the job and perform the relevant \ntroubleshooting steps\n.\n\n\n\n\ncondor_ce_host_network_check\n\n\nUsage\n\n\ncondor_ce_host_network_check\n is a tool for testing an HTCondor-CE's networking configuration:\n\n\nroot@host #\n condor_ce_host_network_check\n\nStarting analysis of host networking for HTCondor-CE\n\n\nSystem hostname: fermicloud360.fnal.gov\n\n\nFQDN matches hostname\n\n\nForward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96.\n\n\nBackward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov.\n\n\nForward and backward resolution match!\n\n\nHTCondor is considering all network interfaces and addresses.\n\n\nHTCondor would pick address of 131.225.155.96 as primary address.\n\n\nHTCondor primary address 131.225.155.96 matches system preferred address.\n\n\nHost network configuration should work with HTCondor-CE\n\n\n\n\n\n\nTroubleshooting\n\n\nIf the tool reports that \nHost network configuration not expected to work with HTCondor-CE\n, ensure that forward and\nreverse DNS resolution return the public IP and hostname.\n\n\ncondor_ce_run\n\n\nUsage\n\n\nSimilar to \nglobus-job-run\n, \ncondor_ce_run\n is a tool that submits a simple job to your CE, so it is useful for quickly\nsubmitting jobs through your CE. \nTo submit a job to the CE and run the \nenv\n command on the remote batch system:\n\n\n\n\nNote\n\n\nYou must have generated a proxy (e.g., \nvoms-proxy-init\n) and your DN must be added to your \nchosen authentication method\n.\n\n\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing the \ncondorce.example.com\n with the hostname of the CE. \nIf you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you\ncan run commands locally on the CE with \ncondor_ce_run\n with the \n-l\n option. \nThe following example outputs the JobRouterLog of the CE in question:\n\n\nuser@host $\n condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog\n\n\n\n\n\nReplacing the \ncondorce.example.com\n text with the hostname of the CE. \nTo disable this feature on your CE, consult\n\nthis\n section of the install documentation.\n\n\nTroubleshooting\n\n\n\n\nIf you do not see any results:\n \ncondor_ce_run\n does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use \ncondor_ce_q\n in a separate terminal to track the job on the CE. If you never see any results, use \ncondor_ce_trace\n to pinpoint errors.\n\n\nIf you see an error message that begins with \u201cFailed to\u2026\u201d:\n Check connectivity to the CE with \ncondor_ce_trace\n or \ncondor_ce_ping\n\n\n\n\ncondor_ce_submit\n\n\nSee the \nsubmitting to HTCondor-CE\n document for details.\n\n\ncondor_ce_ping\n\n\nUsage\n\n\nUse the following \ncondor_ce_ping\n command to test your ability to submit jobs to an HTCondor-CE, replacing\n\ncondorce.example.com\n with the hostname of your CE:\n\n\nuser@host $\n condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE\n\n\n\n\n\nThe following shows successful output where I am able to submit jobs (\nAuthorized: TRUE\n) as the glow user (\nRemote\nMapping: glow@users.opensciencegrid.org\n):\n\n\nRemote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nLocal  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $\n\n\nSession ID:                  condorce:27407:1412286981:3\n\n\nInstruction:                 WRITE\n\n\nCommand:                     60021\n\n\nEncryption:                  none\n\n\nIntegrity:                   MD5\n\n\nAuthenticated using:         GSI\n\n\nAll authentication methods:  GSI\n\n\nRemote Mapping:              glow@users.opensciencegrid.org\n\n\nAuthorized:                  TRUE\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_ping\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_ping\n takes the same arguments as \ncondor_ping\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\n\n\n\n\nIf you see \u201cERROR: couldn\u2019t locate (null)\u201d\n, that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:\n\n\nMASTER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\nSCHEDD_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n for any errors.\n\n\n\n\n\n\nIf you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line\n, this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our \ninstallation document\n.\n\n\n\n\n\n\ncondor_ce_q\n\n\nUsage\n\n\ncondor_ce_q\n can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJOB-ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_q\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_q\n takes the same arguments as \ncondor_q\n and is documented in the \nHTCondor manual\n.\n\n\n\n\nTroubleshooting\n\n\nIf the jobs that you are submiting to a CE are not completing, \ncondor_ce_q\n can tell you the status of your jobs.\n\n\n\n\n\n\nIf the schedd is not running:\n You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:\n\n\nMASTER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\nSCHEDD_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\nThen look in the \nMasterLog\n and \nSchedLog\n on the CE for any errors.\n\n\n\n\n\n\nIf there are issues with contacting the collector:\n You will see the following message:\n\n\nuser@host $\n condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu\n\n\n-- Failed to fetch ads from: \n129.59.197.223:9620?sock`33630_8b33_4\n : ce1.accre.vanderbilt.edu\n\n\n\n\n\n\nThis may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the \nALLOW_READ\n configuration value is not set:\n\n\nuser@host $\n condor_ce_config_val -v ALLOW_READ\n\nNot defined: ALLOW_READ\n\n\n\n\n\n\nIf it is defined, remove it from the file that is returned in the output.\n\n\n\n\n\n\nIf a job is held:\n There should be an accompanying \nHoldReason\n that will tell you why it is being held. The \nHoldReason\n is in the job\u2019s ClassAd, so you can use the long form of \ncondor_ce_q\n to extract its value:\n\n\nuser@host $\n condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n \n|\n grep HoldReason\n\n\n\n\n\n\n\n\n\nIf a job is idle:\n The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the \ncondor_ce_job_router_info\n.\n\n\n\n\n\n\ncondor_ce_history\n\n\nUsage\n\n\ncondor_ce_history\n can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619\n\n\n\n\n\nTo inspect the full ClassAd for a specific job, specify the \n-l\n flag and the job ID:\n\n\nuser@host $\n condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l \nJob ID\n\n\n\n\n\n\n\n\nNote\n\n\nIf you run the \ncondor_ce_history\n command on the CE that you are testing, omit the \n-name\n and \n-pool\n options. \ncondor_ce_history\n takes the same arguments as \ncondor_history\n and is documented in the \nHTCondor manual\n.\n\n\n\n\ncondor_ce_job_router_info\n\n\nUsage\n\n\nUse the \ncondor_ce_job_router_info\n command to help troubleshoot your routes and how jobs will match to them. \nTo see all of your routes (the output is long because it combines your routes with the\n\nJOB_ROUTER_DEFAULTS\n configuration variable):\n\n\nroot@host #\n condor_ce_job_router_info -config\n\n\n\n\n\nTo see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of \ncondor_ce_q\n\n(replace the \nJOB-ID\n with the job ID that you are interested in):\n\n\nroot@host #\n condor_ce_q -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nTo inspect a job that has already left the queue, use \ncondor_ce_history\n instead of \ncondor_ce_q\n:\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\n\n\nNote\n\n\nIf the proxy for the job has expired, the job will not match any routes. To work around this constraint:\n\n\n\n\nroot@host #\n condor_ce_history -l \nJOB-ID\n \n|\n sed \ns/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date \n+1 sec\n`/\n \n|\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -\n\n\n\n\n\nAlternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:\n\n\nroot@host #\n condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads \nJOBAD-FILE\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\nIf the job does not match any route:\n You can identify this case when you see \n0 candidate jobs found\n in the \ncondor_job_router_info\n output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to \ntrue\n. When troubleshooting, look at all of the expressions prior to the \ntarget.ProcId \n= 0\n expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.\n\n\n\n\nIf your job matches more than one route:\n the tool will tell you by showing all matching routes after the job ID:\n\n\nChecking\n \nJob\n \nsrc\n=\n162\n,\n0\n \nagainst\n \nall\n \nroutes\n\n\nRoute\n \nMatches\n:\n \nLocal_PBS\n\n\nRoute\n \nMatches\n:\n \nCondor_Test\n\n\n\n\n\n\nTo troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is \nhighlighted below\n:\n\n\nUmbrella\n \nconstraint\n:\n \n((\ntarget\n.\nx509userproxysubject\n \n=!=\n \nUNDEFINED\n)\n \n\n\n(\ntarget\n.\nx509UserProxyExpiration\n \n=!=\n \nUNDEFINED\n)\n \n\n\n(\ntime\n()\n \n \ntarget\n.\nx509UserProxyExpiration\n)\n \n\n\n(\ntarget\n.\nJobUniverse\n \n=?=\n \n5\n \n||\n \ntarget\n.\nJobUniverse\n \n=?=\n \n1\n))\n \n\n\n%\nRED\n%\n(\n \n(\ntarget\n.\nosgTestPBS\n \nis\n \ntrue\n)\n \n||\n \n(\ntrue\n)\n \n)\n%\nENDCOLOR\n%\n \n\n\n(\ntarget\n.\nProcId\n \n=\n \n0\n \n \ntarget\n.\nJobStatus\n \n==\n \n1\n \n\n\n(\ntarget\n.\nStageInStart\n \nis\n \nundefined\n \n||\n \ntarget\n.\nStageInFinish\n \nisnt\n \nundefined\n)\n \n\n\ntarget\n.\nManaged\n \nisnt\n \nScheddDone\n \n\n\ntarget\n.\nManaged\n \nisnt\n \nExtenal\n \n\n\ntarget\n.\nOwner\n \nisnt\n \nUndefined\n \n\n\ntarget\n.\nRoutedBy\n \nisnt\n \nhtcondor-ce\n)\n\n\n\n\n\n\nBoth routes evaluate to \ntrue\n for the job\u2019s ClassAd because it contained \nosgTestPBS = true\n. Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the \njob route configuration page\n for more details.\n\n\n\n\n\n\nIf it is unclear why jobs are matching a route:\n wrap the route's requirements expression in \ndebug()\n and check the \nJobRouterLog\n for more information.\n\n\n\n\n\n\ncondor_ce_router_q\n\n\nUsage\n\n\nIf you have multiple job routes and many jobs, \ncondor_ce_router_q\n is a useful tool to see how jobs are being routed\nand their statuses:\n\n\nuser@host $\n condor_ce_router_q\n\n\n\n\n\ncondor_ce_router_q\n takes the same options as \ncondor_router_q\n and \ncondor_q\n and is documented in the \nHTCondor\nmanual\n\n\ncondor_ce_status\n\n\nUsage\n\n\nTo see the daemons running on a CE, run the following command:\n\n\nuser@host $\n condor_ce_status -any\n\n\n\n\n\ncondor_ce_status\n takes the same arguments as \ncondor_status\n, which are documented in the\n\nHTCondor manual\n.\n\n\n\n\n\"Missing\" Worker Nodes\n\n\nAn HTCondor-CE will not show any worker nodes (e.g. \nMachine\n entries in the \ncondor_ce_status -any\n output) if\nit does not have any running GlideinWMS pilot jobs.\nThis is expected since HTCondor-CE only forwards incoming grid jobs to your batch system and does not match jobs to\nworker nodes.\n\n\n\n\nTroubleshooting\n\n\nIf the output of \ncondor_ce_status -any\n does not show at least the following daemons:\n\n\n\n\nCollector\n\n\nScheduler\n\n\nDaemonMaster\n\n\nJob_Router\n\n\n\n\nIncrease the \ndebug level\n and consult the\n\nHTCondor-CE logs\n for errors.\n\n\ncondor_ce_config_val\n\n\nUsage\n\n\nTo see the value of configuration variables and where they are set, use \ncondor_ce_config_val\n. \nPrimarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. \nTo see the value of a single variable and where it is set:\n\n\nuser@host $\n condor_ce_config_val -v \nCONFIGURATION-VARIABLE\n\n\n\n\n\n\nTo see a list of all configuration variables and their values:\n\n\nuser@host $\n condor_ce_config_val -dump\n\n\n\n\n\nTo see a list of all the files that are used to create your configuration and the order that they are parsed, use the\nfollowing command:\n\n\nuser@host $\n condor_ce_config_val -config\n\n\n\n\n\ncondor_ce_config_val\n takes the same arguments as \ncondor_config_val\n and is documented in the \nHTCondor manual\n.\n\n\ncondor_ce_reconfig\n\n\nUsage\n\n\nTo ensure that your configuration changes have taken effect, run \ncondor_ce_reconfig\n.\n\n\nuser@host $\n condor_ce_reconfig\n\n\n\n\n\ncondor_ce_{on,off,restart}\n\n\nUsage\n\n\nTo turn on/off/restart HTCondor-CE daemons, use the following commands:\n\n\nroot@host #\n condor_ce_on\n\nroot@host #\n condor_ce_off\n\nroot@host #\n condor_ce_restart\n\n\n\n\n\nThe HTCondor-CE service uses the previous commands with default values. \nUsing these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:\n\n\n\n\n\n\nIf you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:\n\n\nroot@host #\n condor_ce_restart -fast\n\n\n\n\n\nThis will cause HTCondor-CE to restart and quickly reconnect to all running jobs.\n\n\n\n\n\n\nIf you need to stop running new jobs, run the following:\n\n\nroot@host #\n condor_ce_off -peaceful\n\n\n\n\n\nThis will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.\n\n\n\n\n\n\nHTCondor-CE Troubleshooting Data\n\n\nThe following files are located on the CE host.\n\n\nMasterLog\n\n\nThe HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if\nthey fail to start.\n\n\n\n\nLocation: \n/var/log/condor-ce/MasterLog\n\n\nKey contents: Start-up, shut-down, and communication with other HTCondor daemons\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMASTER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for:\n\n\nSuccessful daemon start-up. \nThe following line shows that the Collector daemon started successfully:\n\n\n10\n/\n07\n/\n14\n \n14\n:\n20\n:\n27\n \nStarted\n \nDaemonCore\n \nprocess\n \n/usr/sbin/condor_collector -f -port 9619\n,\n \npid\n \nand\n \npgroup\n \n=\n \n7318\n\n\n\n\n\n\nSchedLog\n\n\nThe HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. \nIt contains valuable information when trying to troubleshoot authentication issues.\n\n\n\n\nLocation: \n/var/log/condor-ce/SchedLog\n\n\nKey contents:\n\n\nEvery job submitted to the CE\n\n\nUser authorization events\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSCHEDD_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the CE queue:\n\n\n10\n/\n07\n/\n14\n \n16\n:\n52\n:\n17\n \nSubmitting\n \nnew\n \njob\n \n234\n.\n0\n\n\n\n\n\n\nIn this example, the ID of the submitted job is \n234.0\n.\n\n\n\n\n\n\nJob owner is authorized and mapped:\n\n\n10\n/\n07\n/\n14\n \n16\n:\n52\n:\n17\n \nCommand\n=\nQMGMT_WRITE_CMD\n,\n \npeer\n=\n131.225.154.68\n:\n42262\n\n\n10\n/\n07\n/\n14\n \n16\n:\n52\n:\n17\n \nAuthMethod\n=\nGSI\n,\n \nAuthId\n=/\nDC\n=\ncom\n/\nDC\n=\nDigiCert\n-\nGrid\n/\nO\n=\nOpen\n \nScience\n \nGrid\n/\nOU\n=\nPeople\n/\nCN\n=\nBrian\n \nLin\n \n1047\n,\n\n\n                  \n/\nGLOW\n/\nRole\n=\nNULL\n/\nCapability\n=\nNULL\n,\n \nCondorId\n=\nglow\n@users\n.\nopensciencegrid\n.\norg\n\n\n\n\n\n\nIn this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the \nglow\n user.\n\n\n\n\n\n\nUser job submission fails\n due to improper authentication or authorization:\n\n\n08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to \ngsi@unmapped\n\n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done\n\n\n\n\n\n\n\n\n\nMissing negotiator:\n\n\n10\n/\n18\n/\n14\n \n17\n:\n32\n:\n21\n \nCan\nt find address for negotiator\n\n\n10\n/\n18\n/\n14\n \n17\n:\n32\n:\n21\n \nFailed\n \nto\n \nsend\n \nRESCHEDULE\n \nto\n \nunknown\n \ndaemon\n:\n\n\n\n\n\nSince HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:\n\n\n06\n/\n23\n/\n15\n \n11\n:\n15\n:\n03\n \nNumber\n \nof\n \nActive\n \nWorkers\n \n0\n\n\n\n\n\n\n\n\n\n\nCorrupted \njob_queue.log\n:\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \nWARNING\n:\n \nEncountered\n \ncorrupt\n \nlog\n \nrecord\n \n_654\n \n(\nbyte\n \noffset\n \n5046225\n)\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \n103\n \n1354325\n.\n0\n \nPeriodicRemove\n \n(\n \nStageInFinish\n \n \n0\n \n)\n \n105\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \nLines\n \nfollowing\n \ncorrupt\n \nlog\n \nrecord\n \n_654\n \n(\nup\n \nto\n \n3\n):\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \n103\n \n1346101\n.\n0\n \nRemoteWallClockTime\n \n116668\n.\n000000\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \n104\n \n1346101\n.\n0\n \nWallClockCheckpoint\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \n104\n \n1346101\n.\n0\n \nShadowBday\n\n\n02\n/\n07\n/\n17\n \n10\n:\n55\n:\n49\n \nERROR\n \nError: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,\n\n\n                  recovery failed\n \nat\n \nline\n \n1080\n \nin\n \nfile\n \n/\nbuilddir\n/\nbuild\n/\nBUILD\n/\ncondor\n-\n8\n.\n4\n.\n8\n/\nsrc\n/\ncondor_utils\n/\nclassad_log\n.\ncpp\n\n\n\n\n\n\nThis means \n/var/lib/condor-ce/spool/job_queue.log\n has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the \nLines following corrupt log record...\n line. The most common culprit of the corruption is that the disk containing the \njob_queue.log\n has filled up. To avoid this problem, you can change the location of \njob_queue.log\n by setting \nJOB_QUEUE_LOG\n in \n/etc/condor-ce/config.d/\n to a path, preferably one on a large SSD.\n\n\n\n\n\n\nJobRouterLog\n\n\nThe HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to\ntroubleshoot issues with job routing.\n\n\n\n\nLocation: \n/var/log/condor-ce/JobRouterLog\n\n\nKey contents:\n\n\nEvery attempt to route a job\n\n\nRouting success messages\n\n\nJob attribute changes, based on chosen route\n\n\nJob submission errors to an HTCondor batch system\n\n\nCorresponding job IDs on an HTCondor batch system\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nJOB_ROUTER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nApply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnown Errors\n\n\n\n\n\n\n(HTCondor batch systems only)\n If you see the following error message:\n\n\nCan\nt\n \nfind\n \naddress\n \nof\n \nschedd\n\n\n\n\n\n\nThis means that HTCondor-CE cannot communicate with your HTCondor batch system.\nVerify that the \ncondor\n service is running on the HTCondor-CE host and is configured for your central manager.\n\n\n\n\n\n\n(HTCondor batch systems only)\n If you see the following error message:\n\n\nJobRouter\n \nfailure\n \n(\nsrc\n=\n2810466\n.\n0\n,\ndest\n=\n47968\n.\n0\n,\nroute\n=\nMWT2_UCORE\n):\n \ngiving\n \nup\n,\n \nbecause\n \nsubmitted\n \njob\n \nis\n \nstill\n \nnot\n \nin\n \njob\n \nqueue\n \nmirror\n \n(\nsubmitted\n \n605\n \nseconds\n \nago\n).\n  \nPerhaps\n \nit\n \nhas\n \nbeen\n \nremoved\n?\n\n\n\n\n\n\nEnsure that \ncondor_config_val SPOOL\n and \ncondor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL\n return the same value.\nIf they don't, change the value of \nJOB_ROUTER_SCHEDD2_SPOOL\n in your HTCondor-CE configuration to match \nSPOOL\n\nfrom your HTCondor configuration.\n\n\n\n\n\n\nIf you have \nD_ALWAYS:2\n turned on for the job router, you will see errors like the following:\n\n\n06\n/\n12\n/\n15\n \n14\n:\n00\n:\n28\n \nHOOK_UPDATE_JOB_INFO\n \nnot\n \nconfigured\n.\n\n\n\n\n\n\nYou can safely ignore these.\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is considered for routing:\n\n\n09\n/\n17\n/\n14\n \n15\n:\n00\n:\n56\n \nJobRouter\n \n(\nsrc\n=\n86\n.\n0\n,\nroute\n=\nLocal_LSF\n):\n \nfound\n \ncandidate\n \njob\n\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the route (e.g., \nLocal_LSF\n).\n\n\n\n\n\n\nJob is successfully routed:\n\n\n09\n/\n17\n/\n14\n \n15\n:\n00\n:\n57\n \nJobRouter\n \n(\nsrc\n=\n86\n.\n0\n,\nroute\n=\nLocal_LSF\n):\n \nclaimed\n \njob\n\n\n\n\n\n\n\n\n\n\nFinding the corresponding job ID on your HTCondor batch system:\n\n\n09\n/\n17\n/\n14\n \n15\n:\n00\n:\n57\n \nJobRouter\n \n(\nsrc\n=\n86\n.\n0\n,\ndest\n=\n205\n.\n0\n,\nroute\n=\nLocal_Condor\n):\n \nclaimed\n \njob\n\n\n\n\n\n\nIn parentheses are the original HTCondor-CE job ID (e.g., \n86.0\n) and the resultant job ID on the HTCondor batch system (e.g., \n205.0\n)\n\n\n\n\n\n\nIf your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the \ncondor_ce_job_router_info\n\n\n\n\nHTCondor batch systems only\n: The following error occurs when the job router daemon cannot submit the routed job:\n10/19/14 13:09:15 Can\nt resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can\nt find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job\n\n\n\n\n\n\n\n\n\nGridmanagerLog\n\n\nThe HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. \nIt contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. \nDetails on how to read the Gridmanager log can be found on the \nHTCondor\nWiki\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/GridmanagerLog.\nJOB-OWNER\n\n\nKey contents:\n\n\nEvery attempt to submit a job to a batch system or other grid resource\n\n\nStatus updates of submitted jobs\n\n\nCorresponding job IDs on non-HTCondor batch systems\n\n\n\n\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nMAX_GRIDMANAGER_LOG\n \n=\n \n6\nh\n\n\nMAX_NUM_GRIDMANAGER_LOG\n \n=\n \n8\n\n\nGRIDMANAGER_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to look for\n\n\n\n\n\n\nJob is submitted to the batch system:\n\n\n09\n/\n17\n/\n14\n \n09\n:\n51\n:\n34\n \n[\n12997\n]\n \n(\n85\n.\n0\n)\n \ngm\n \nstate\n \nchange\n:\n \nGM_SUBMIT_SAVE\n \n-\n \nGM_SUBMITTED\n\n\n\n\n\n\nEvery state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).\n\n\n\n\n\n\nJob status being updated:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n09/17/14 15:07:24 [25543] GAHP[25563] -\n \nS\n\n09/17/14 15:07:25 [25543] GAHP[25563] \n- \nRESULTS\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nR\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \nS\n \n1\n\n09/17/14 15:07:25 [25543] GAHP[25563] -\n \n3\n \n0\n \nNo Error\n \n4\n \n[ BatchjobId = \n482046\n; JobStatus = 4; ExitCode = 0; WorkerNode = \natl-prod08\n ]\n\n\n\n\n\n\nThe first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.\n\n\n\n\n\n\nFinding the corresponding job ID on your non-HTCondor batch system:\n\n\n09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -\n GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563] \n- \nBLAH_JOB_STATUS 3 lsf/20140917/482046\n\n\n\n\n\n\nOn the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses, \n(87.0)\n. At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes, \nlsf/20140917/482046\n.\n\n\n\n\n\n\nJob completion on the batch system:\n\n\n09\n/\n17\n/\n14\n \n15\n:\n07\n:\n25\n \n[\n25543\n]\n \n(\n87\n.\n0\n)\n \ngm\n \nstate\n \nchange\n:\n \nGM_TRANSFER_OUTPUT\n \n-\n \nGM_DONE_SAVE\n\n\n\n\n\n\n\n\n\n\nSharedPortLog\n\n\nThe HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the\ncollector. \nThis log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found\n\nhere\n.\n\n\n\n\nLocation: \n/var/log/condor-ce/SharedPortLog\n\n\nKey contents: Every attempt to connect to HTCondor-CE (except collector queries)\n\n\n\n\nIncreasing the debug level:\n\n\n\n\n\n\nSet the following value in \n/etc/condor-ce/config.d/99-local.conf\n on the CE host:\n\n\nSHARED_PORT_DEBUG\n \n=\n \nD_ALWAYS\n:\n2\n \nD_CAT\n\n\n\n\n\n\n\n\n\n\nTo apply these changes, reconfigure HTCondor-CE:\n\n\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nMessages log\n\n\nThe messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. \nIf there are issues with the \nauthentication setup\n, the errors may\nappear here.\n\n\n\n\nLocation: \n/var/log/messages\n\n\nKey contents: User authentication\n\n\n\n\nWhat to look for\n\n\nA user is mapped:\n\n\nOct\n \n6\n \n10\n:\n35\n:\n32\n \nosgserv06\n \nhtondor\n-\nce\n-\nllgt\n[\n12147\n]:\n \nCallout\n \nto\n \nLCMAPS\n \nreturned\n \nlocal\n \nuser\n \n(\nservice\n \ncondor\n):\n \nosgglow01\n\n\n\n\n\n\nBLAHP Configuration File\n\n\nHTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client\ntools.\n\n\n\n\nLocation: \n/etc/blah.config\n\n\nKey contents:\n\n\nLocations of the batch system's client binaries and logs\n\n\nLocation to save files that are submitted to the local batch system\n\n\n\n\n\n\n\n\nYou can also tell the BLAHP to save the files that are being submitted to the local batch system to \nDIR-NAME\n by\nadding the following line:\n\n\nblah_debug_save_submit_info\n=\nDIR_NAME\n\n\n\n\n\n\nThe BLAHP will then create a directory with the format \nbl_*\n for each submission to the local jobmanager with the\nsubmit file and proxy used.\n\n\n\n\nNote\n\n\nWhitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within \nDIR_NAME\n.\n\n\n\n\nGetting Help\n\n\nIf you are still experiencing issues after using this document, please let us know!\n\n\n\n\nGather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)\n\n\n\n\nGather system information:\n\n\nroot\n@host\n \n#\n \nosg\n-\nsystem\n-\nprofiler\n\n\n\n\n\n\n\n\n\n\nStart a support request using \na web interface\n or by email to \n\n\n\n\nDescribe issue and expected or desired behavior\n\n\nInclude basic HTCondor-CE and related information\n\n\nAttach the osg-system-profiler output\n\n\n\n\n\n\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nSubmitting jobs to HTCondor-CE", 
            "title": "Troubleshooting HTCondor-CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-guide", 
            "text": "In this document, you will find a collection of files and commands to help troubleshoot HTCondor-CE along with a list of\ncommon issues with suggested troubleshooting steps.", 
            "title": "HTCondor-CE Troubleshooting Guide"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#submit_exprs-are-not-applied-to-jobs-on-the-local-htcondor", 
            "text": "If you are adding attributes to jobs submitted to your HTCondor pool with  SUBMIT_EXPRS , these will  not  be applied to\njobs that are entering your pool from the HTCondor-CE. \nTo get around this, you will want to add the attributes to your  job routes . \nIf the CE is the only entry point for jobs into your pool, you can get rid of  SUBMIT_EXPRS  on your backend. Otherwise,\nyou will have to maintain your list of attributes both in your list of routes and in your  SUBMIT_EXPRS .", 
            "title": "SUBMIT_EXPRS are not applied to jobs on the local HTCondor"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#general-troubleshooting-items", 
            "text": "", 
            "title": "General Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#making-sure-packages-are-up-to-date", 
            "text": "It is important to make sure that the HTCondor-CE and related RPMs are up-to-date.  root@host #  yum update  htcondor-ce*  blahp condor  If you just want to see the packages to update, but do not want to perform the update now, answer  N  at the prompt.", 
            "title": "Making sure packages are up-to-date"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-package-contents", 
            "text": "If the contents of your HTCondor-CE packages have been changed, the CE may cease to function properly. To verify the\ncontents of your packages (ignoring changes to configuration files):  user@host $  rpm -q --verify htcondor-ce htcondor-ce-client blahp  |  grep -v  /var/   |  awk  $2 !=  c  {print $0}   If the verification command returns output, this means that your packages have been changed. To fix this, you can\nreinstall the packages:  user@host $  yum reinstall htcondor-ce htcondor-ce-client blahp   Note  The reinstall command may place original versions of configuration files alongside the versions that you have modified. If this is the case, the reinstall command will notify you that the original versions will have an  .rpmnew  suffix. Further inspection of these files may be required as to whether or not you need to merge them into your current configuration.", 
            "title": "Verify package contents"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-clocks-are-synchronized", 
            "text": "Like all GSI-based authentication, HTCondor-CE is sensitive to time skews. Make sure the clock on your CE is\nsynchronized using a utility such as  ntpd . \nAdditionally, HTCondor itself is sensitive to time skews on the NFS server.\nIf you see empty stdout / err being returned to the submitter, verify there is no NFS server time skew.", 
            "title": "Verify clocks are synchronized"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#verify-host-cerificates-and-crls-are-valid", 
            "text": "An expired host certificate or CRLs will cause various issues with GSI authentication. \nVerify that your host certificate is valid by running:  root@host #  openssl x509 -in /etc/grid-security/hostcert.pem -noout -dates  Likewise, run the  fetch-crl  script to update your CRLs:  root@host #  fetch-crl  If updating CRLs fix your issues, make sure that the  fetch-crl-cron  and fetch-crl-boot  services are enabled and running.", 
            "title": "Verify host cerificates and CRLs are valid"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-items", 
            "text": "This section contains common issues you may encounter using HTCondor-CE and next actions to take when you do. \nBefore troubleshooting, we recommend increasing the log level:    Write the following into  /etc/condor-ce/config.d/99-local.conf  to increase the log level for all daemons:  ALL_DEBUG   =   D_ALWAYS : 2   D_CAT     Ensure that the configuration is in place:  root @host   #   condor_ce_reconfig     Reproduce the issue     Note  Before spending any time on troubleshooting, you should ensure that the state of configuration is as expected by running  condor_ce_reconfig .", 
            "title": "HTCondor-CE Troubleshooting Items"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#daemons-fail-to-start", 
            "text": "If there are errors in your configuration of HTCondor-CE, this may cause some of its required daemons to fail to\nstartup. \nCheck the following subsections in order:  Symptoms  Daemon startup failure may manifest in many ways, the following are few symptoms of the problem.    The service fails to start:  root@host #  service condor-ce start Starting Condor-CE daemons: [ FAIL ]     condor_ce_q  fails with a lengthy error message:  user@host $  condor_ce_q Error:  Extra Info: You probably saw this error because the condor_schedd is not running  on the machine you are trying to query. If the condor_schedd is not running, the  Condor system will not be able to find an address and port to connect to and  satisfy this request. Please make sure the Condor daemons are running and try  again.  Extra Info: If the condor_schedd is running on the machine you are trying to  query and you still see the error, the most likely cause is that you have setup  a personal Condor, you have not defined SCHEDD_NAME in your condor_config file,  and something is wrong with your SCHEDD_ADDRESS_FILE setting. You must define  either or both of those settings in your config file, or you must use the -name  option to condor_q. Please see the Condor manual for details on SCHEDD_NAME and  SCHEDD_ADDRESS_FILE.     Next actions   If the MasterLog is filled with  ERROR:SECMAN...TCP connection to collector...failed :  This is likely due to a misconfiguration for a host with multiple network interfaces. Verify that you have followed the instructions in  this  section of the install guide.  If the MasterLog is filled with  DC_AUTHENTICATE  errors:  The HTCondor-CE daemons use the host certificate to authenticate with each other. Verify that your host certificate\u2019s DN matches one of the regular expressions found in  /etc/condor-ce/condor_mapfile .  If the SchedLog is filled with  Can\u2019t find address for negotiator :  You can ignore this error! The negotiator daemon is used in HTCondor batch systems to match jobs with resources but since HTCondor-CE does not manage any resources directly, it does not run one.", 
            "title": "Daemons fail to start"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-fail-to-submit-to-the-ce", 
            "text": "If a user is having issues submitting jobs to the CE and you've ruled out general connectivity or firewalls as the\nculprit, then you may have encountered an authentication or authorization issue. \nYou may see error messages like the following in your  SchedLog :  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189 failed: AUTHENTICATE:1003:Failed to authenticate with any method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to authenticate because the remote (client) side was not able to acquire its credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to gsi@unmapped from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE: reason: WRITE authorization policy contains no matching ALLOW entry for this request; identifiers used for this host: 72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done!  Next actions   Check voms-mapfile or grid-mapfile  and ensure that the user's DN or VOMS attributes are known to your\n     authentication method , and that the mapped users exist\n    on your CE and cluster.  Check for lcmaps errors  in  /var/log/messages  If you do not see helpful error messages in  /var/log/messages ,  adjust the debug level by adding  export LCMAPS_DEBUG_LEVEL=5  to  /etc/sysconfig/condor-ce , restarting the condor-ce service, and checking  /var/log/messages  for errors again.", 
            "title": "Jobs fail to submit to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-the-ce", 
            "text": "Check the following subsections in order, but note that jobs may take several minutes or longer to run if the CE is\nbusy.", 
            "title": "Jobs stay idle on the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-make-sure-the-underlying-batch-system-can-run-jobs", 
            "text": "HTCondor-CE delegates jobs to your batch system, which is then responsible for matching jobs to worker nodes.\nIf you cannot manually submit jobs (e.g.,  condor_submit ,  qsub ) on the CE host to your batch system, then HTCondor-CE\nwon't be able to either.  Procedure   Manually create and submit a simple job (e.g., one that runs  sleep )  Check for errors in the submission itself  Watch the job in the batch system queue (e.g.,  condor_q ,  qstat )  If the job does not run, check for errors on the batch system   Next actions  Consult troubleshooting documentation or support avenues for your batch system.\nOnce you can run simple manual jobs on your batch system, try submitting to the HTCondor-CE again.", 
            "title": "Idle jobs on CE: Make sure the underlying batch system can run jobs"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-is-the-job-router-handling-the-incoming-job", 
            "text": "Jobs on the CE will be put on hold if they do not match any job routes after 30 minutes, but you can check a few things\nif you suspect that the jobs are not being matched. \nCheck if the JobRouter sees a job before that by looking at the  job router log  and looking for the text src= JOB-ID \u2026claimed job .  Next actions  Use  condor_ce_job_router_info  to see why your idle job does not match any routes", 
            "title": "Idle jobs on CE: Is the job router handling the incoming job?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-correct-operation-between-the-ce-and-your-local-batch-system", 
            "text": "", 
            "title": "Idle jobs on CE: Verify correct operation between the CE and your local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs directly to an HTCondor batch system via the JobRouter, so any issues with the CE/local batch\nsystem interaction will appear in the  JobRouterLog .  Next actions   Check the  JobRouterLog  for failures.  Verify that the local HTCondor is functional.  Use  condor_ce_config_val  to verify that the  JOB_ROUTER_SCHEDD2_NAME ,  JOB_ROUTER_SCHEDD2_POOL , and  JOB_ROUTER_SCHEDD2_SPOOL  configuration variables are set to the hostname of your CE, the hostname and port of your local HTCondor\u2019s collector, and the location of your local HTCondor\u2019s spool directory, respectively.  Use  condor_config_val QUEUE_SUPER_USER_MAY_IMPERSONATE  and verify that it is set to  .* .", 
            "title": "For HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#for-non-htcondor-batch-systems", 
            "text": "HTCondor-CE submits jobs to a non-HTCondor batch system via the Gridmanager, so any issues with the CE/local batch\nsystem interaction will appear in the  GridmanagerLog . \nLook for  gm state change\u2026  lines to figure out where the issures are occuring.  Next actions   If you see failures in the GridmanagerLog during job submission:  Save the submit files by adding the appropriate entry to  blah.config  and submit it  manually  to the batch system. If that succeeds, make sure that the BLAHP knows where your binaries are located by setting the  batch system _binpath  in  /etc/blah.config .   If you see failures in the GridmanagerLog during queries for job status:  Query the resultant job with your batch system tools from the CE. If you can, the BLAHP uses scripts to query for status in  /usr/libexec/blahp/ batch system _status.sh  (e.g.,  /usr/libexec/blahp/lsf_status.sh ) that take the argument  batch system/YYYMMDD/job ID  (e.g.,  lsf/20141008/65053 ). Run the appropriate status script for your batch system and upon success, you should see the following output:  root@host #  /usr/libexec/blahp/lsf_status.sh lsf/20141008/65053 [ BatchjobId =  894862 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   If the script fails,  request help  from the OSG.", 
            "title": "For non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#idle-jobs-on-ce-verify-ability-to-change-permissions-on-key-files", 
            "text": "HTCondor-CE needs the ability to write and chown files in its  spool  directory and if it cannot, jobs will not run at\nall. \nSpool permission errors can appear in the  SchedLog  and the  JobRouterLog .  Symptoms  09 / 17 / 14   14 : 45 : 42   Error :   Unable   to   chown   /var/lib/condor-ce/spool/1/0/cluster1.proc0.subproc0/env   from   12345   to   54321   Next actions   As root, try to change ownership of the file or directory in question. If the file does not exist, a parent directory may have improper permissions.  Verify that there aren't any underlying file system issues in the specified location", 
            "title": "Idle jobs on CE: Verify ability to change permissions on key files"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-stay-idle-on-a-remote-host-submitting-to-the-ce", 
            "text": "If you are submitting your job from a separate submit host to the CE, it stays idle in the queue forever, and you do not\nsee a resultant job in the CE's queue, this means that your job cannot contact the CE for submission or it is not\nauthorized to run there. \nNote that jobs may take several minutes or longer if the CE is busy.", 
            "title": "Jobs stay idle on a remote host submitting to the CE"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-can-you-contact-the-ce", 
            "text": "To check basic connectivity to a CE, use  condor_ce_ping :  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE ERROR: couldn t locate condorce.example.com!   Next actions   Make sure that the HTCondor-CE daemons are running with  condor_ce_status .  Verify that your CE is reachable from your submit host, replacing  condorce.example.com  with the hostname of your CE: user@host $  ping condorce.example.com", 
            "title": "Remote idle jobs: Can you contact the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#remote-idle-jobs-are-you-authorized-to-run-jobs-on-the-ce", 
            "text": "The CE will only accept jobs from users that authenticate via  LCMAPS VOMS .\nYou can use  condor_ce_ping  to check if you are authorized and what user your proxy is being mapped\nto.  Symptoms  user@host $  condor_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:3343:1412790611:0  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              gsi@unmapped  Authorized:                  FALSE   Notice the failures in the above message:  Remote Mapping: gsi@unmapped  and  Authorized: FALSE  Next actions   Verify that an  authentication method  is set up on the CE  Verify that your user DN is mapped to an existing system user", 
            "title": "Remote idle jobs: Are you authorized to run jobs on the CE?"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-go-on-hold", 
            "text": "Jobs will be put on held with a  HoldReason  attribute that can be inspected with  condor_ce_q :  user@host $  condor_ce_q -l  JOB-ID  -attr HoldReason HoldReason =  CE job in status 5 put on hold by SYSTEM_PERIODIC_HOLD due to no matching routes, route job limit, or route failure threshold.", 
            "title": "Jobs go on hold"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-no-matching-routes-route-job-limit-or-route-failure-threshold", 
            "text": "Jobs on the CE will be put on hold if they are not claimed by the job router within 30 minutes.\nThe most common cases for this behavior are as follows:   The job does not match any job routes: \n  use  condor_ce_job_router_info  to see why your idle job does not match any\n   routes .  The route(s) that the job matches to are full: \n  See  limiting the number of jobs .  The job router is throttling submission to your batch system due to submission failures: \n  See the HTCondor manual for  FailureRateThreshold .\n  Check for errors in the  JobRouterLog  or  GridmanagerLog  for HTCondor and\n  non-HTCondor batch systems, respectively.", 
            "title": "Held jobs: no matching routes, route job limit, or route failure threshold"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-missingexpired-user-proxy", 
            "text": "HTCondor-CE requires a valid user proxy for each job that is submitted. \nYou can check the status of your proxy with the following  user@host $  voms-proxy-info -all  Next actions  Ensure that the owner of the job generates their proxy with  voms-proxy-init .", 
            "title": "Held jobs: Missing/expired user proxy"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#held-jobs-invalid-job-universe", 
            "text": "The HTCondor-CE only accepts jobs that have  universe  in their submit files set to  vanilla ,  standard ,  local , or scheduler . \nThese universes also have corresponding integer values that can be found in the  HTCondor\nmanual .  Next actions   Ensure jobs submitted locally, from the CE host, are submitted with  universe = vanilla   Ensure jobs submitted from a remote submit point are submitted with:  universe   =   grid  grid_resource   =   condor   condorce . example . com   condorce . example . com : 9619   replacing  condorce.example.com  with the hostname of the CE.", 
            "title": "Held jobs: Invalid job universe"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#identifying-the-corresponding-job-id-on-the-local-batch-system", 
            "text": "When troubleshooting interactions between your CE and your local batch system, you will need to associate the CE job ID\nand the resultant job ID on the batch system. \nThe methods for finding the resultant job ID differs between batch systems.", 
            "title": "Identifying the corresponding job ID on the local batch system"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-batch-systems", 
            "text": "To inspect the CE\u2019s job ad, use  condor_ce_q  or  condor_ce_history :    Use  condor_ce_q  if the job is still in the CE\u2019s queue:  user@host $  condor_ce_q  JOB-ID  -af RoutedToJobId    Use  condor_ce_history  if the job has left the CE\u2019s queue:  user@host $  condor_ce_history  JOB-ID  -af RoutedToJobId      Parse the  JobRouterLog  for the CE\u2019s job ID.", 
            "title": "HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#non-htcondor-batch-systems", 
            "text": "When HTCondor-CE records the corresponding batch system job ID, it is written in the form  BATCH-SYSTEM / DATE / JOB\nID :  lsf / 20141206 / 482046     To inspect the CE\u2019s job ad, use  condor_ce_q :  user@host $  condor_ce_q  JOB-ID  -af GridJobId    Parse the  GridmanagerLog  for the CE\u2019s job ID.", 
            "title": "Non-HTCondor batch systems"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobs-removed-from-the-local-htcondor-pool-become-resubmitted-htcondor-batch-systems-only", 
            "text": "By design, HTCondor-CE will resubmit jobs that have been removed from the underlying HTCondor pool. \nTherefore, to remove misbehaving jobs, they will need to be removed on the CE level following these steps:   Identify the misbehaving job ID in your batch system queue   Find the job's corresponding CE job ID:  user@host $  condor_q  JOB-ID  -af RoutedFromJobId    Use  condor_ce_rm  to remove the CE job from the queue", 
            "title": "Jobs removed from the local HTCondor pool become resubmitted (HTCondor batch systems only)"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#missing-htcondor-tools", 
            "text": "Most of the HTCondor-CE tools are just wrappers around existing HTCondor tools that load the CE-specific config. \nIf you are trying to use HTCondor-CE tools and you see the following error:  user@host $  condor_ce_job_router_info /usr/bin/condor_ce_job_router_info: line 6: exec: condor_job_router_info: not found   This means that the  condor_job_router_info  (note this is not the CE version), is not in your  PATH .  Next Actions   Either the condor RPM is missing or there are some other issues with it (try  rpm --verify condor ).  You have installed HTCondor in a non-standard location that is not in your  PATH .  The  condor_job_router_info  tool itself wasn't available until Condor-8.2.3-1.1 (available in osg-upcoming).", 
            "title": "Missing HTCondor tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-tools", 
            "text": "HTCondor-CE has its own separate set of of the HTCondor tools with  ce  in the name (i.e.,  condor_ce_submit  vs condor_submit ). \nSome of the the commands are only for the CE (e.g.,  condor_ce_run  and  condor_ce_trace ) but many of them are just\nHTCondor commands configured to interact with the CE (e.g.,  condor_ce_q ,  condor_ce_status ). \nIt is important to differentiate the two:  condor_ce_config_val  will provide configuration values for your HTCondor-CE\nwhile condor_config_val  will provide configuration values for your HTCondor batch system. \nIf you are not running an HTCondor batch system, the non-CE commands will return errors.", 
            "title": "HTCondor-CE Troubleshooting Tools"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_trace", 
            "text": "", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage", 
            "text": "condor_ce_trace  is a useful tool for testing end-to-end job submission. It contacts both the CE\u2019s Schedd and Collector daemons to verify your permission to submit to the CE, displays the submit script that it submits to the CE, and tracks the resultant job.   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_trace condorce.example.com  Replacing the  condorce.example.com  with the hostname of the CE. \nIf you are familiar with the output of condor commands, the command also takes a  --debug  option that displays verbose\ncondor output.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting", 
            "text": "If the command fails with \u201cFailed ping\u2026\u201d:  Make sure that the HTCondor-CE daemons are running on the CE  If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line:  Either your credentials are not mapped on the CE or authentication is not set up at all. To set up authentication, refer to our  installation document .  If the job submits but does not complete:  Look at the status of the job and perform the relevant  troubleshooting steps .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_host_network_check", 
            "text": "", 
            "title": "condor_ce_host_network_check"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_1", 
            "text": "condor_ce_host_network_check  is a tool for testing an HTCondor-CE's networking configuration:  root@host #  condor_ce_host_network_check Starting analysis of host networking for HTCondor-CE  System hostname: fermicloud360.fnal.gov  FQDN matches hostname  Forward resolution of hostname fermicloud360.fnal.gov is 131.225.155.96.  Backward resolution of IPv4 131.225.155.96 is fermicloud360.fnal.gov.  Forward and backward resolution match!  HTCondor is considering all network interfaces and addresses.  HTCondor would pick address of 131.225.155.96 as primary address.  HTCondor primary address 131.225.155.96 matches system preferred address.  Host network configuration should work with HTCondor-CE", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_1", 
            "text": "If the tool reports that  Host network configuration not expected to work with HTCondor-CE , ensure that forward and\nreverse DNS resolution return the public IP and hostname.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_run", 
            "text": "", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_2", 
            "text": "Similar to  globus-job-run ,  condor_ce_run  is a tool that submits a simple job to your CE, so it is useful for quickly\nsubmitting jobs through your CE. \nTo submit a job to the CE and run the  env  command on the remote batch system:   Note  You must have generated a proxy (e.g.,  voms-proxy-init ) and your DN must be added to your  chosen authentication method .   user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing the  condorce.example.com  with the hostname of the CE. \nIf you are troubleshooting an HTCondor-CE that you do not have a login for and the CE accepts local universe jobs, you\ncan run commands locally on the CE with  condor_ce_run  with the  -l  option. \nThe following example outputs the JobRouterLog of the CE in question:  user@host $  condor_ce_run -lr condorce.example.com:9619 cat /var/log/condor-ce/JobRouterLog  Replacing the  condorce.example.com  text with the hostname of the CE. \nTo disable this feature on your CE, consult this  section of the install documentation.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_2", 
            "text": "If you do not see any results:   condor_ce_run  does not display results until the job completes on the CE, which may take several minutes or longer if the CE is busy. In the meantime, can use  condor_ce_q  in a separate terminal to track the job on the CE. If you never see any results, use  condor_ce_trace  to pinpoint errors.  If you see an error message that begins with \u201cFailed to\u2026\u201d:  Check connectivity to the CE with  condor_ce_trace  or  condor_ce_ping", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_submit", 
            "text": "See the  submitting to HTCondor-CE  document for details.", 
            "title": "condor_ce_submit"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_ping", 
            "text": "", 
            "title": "condor_ce_ping"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_3", 
            "text": "Use the following  condor_ce_ping  command to test your ability to submit jobs to an HTCondor-CE, replacing condorce.example.com  with the hostname of your CE:  user@host $  condor_ce_ping -verbose -name condorce.example.com -pool condorce.example.com:9619 WRITE  The following shows successful output where I am able to submit jobs ( Authorized: TRUE ) as the glow user ( Remote\nMapping: glow@users.opensciencegrid.org ):  Remote Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Local  Version:              $CondorVersion: 8.0.7 Sep 24 2014 $  Session ID:                  condorce:27407:1412286981:3  Instruction:                 WRITE  Command:                     60021  Encryption:                  none  Integrity:                   MD5  Authenticated using:         GSI  All authentication methods:  GSI  Remote Mapping:              glow@users.opensciencegrid.org  Authorized:                  TRUE    Note  If you run the  condor_ce_ping  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_ping  takes the same arguments as  condor_ping  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_3", 
            "text": "If you see \u201cERROR: couldn\u2019t locate (null)\u201d , that means the HTCondor-CE schedd (the daemon that schedules jobs) cannot be reached. To track down the issue, increase debugging levels on the CE:  MASTER_DEBUG   =   D_ALWAYS : 2   D_CAT  SCHEDD_DEBUG   =   D_ALWAYS : 2   D_CAT   Then look in the  MasterLog  and  SchedLog  for any errors.    If you see \u201cgsi@unmapped\u201d in the \u201cRemote Mapping\u201d line , this means that either your credentials are not mapped on the CE or that authentication is not set up at all. To set up authentication, refer to our  installation document .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_q", 
            "text": "", 
            "title": "condor_ce_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_4", 
            "text": "condor_ce_q  can display job status or specific job attributes for jobs that are still in the CE\u2019s queue. To list jobs that are queued on a CE:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  JOB-ID    Note  If you run the  condor_ce_q  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_q  takes the same arguments as  condor_q  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_4", 
            "text": "If the jobs that you are submiting to a CE are not completing,  condor_ce_q  can tell you the status of your jobs.    If the schedd is not running:  You will see a lengthy message about being unable to contact the schedd. To track down the issue, increase the debugging levels on the CE with:  MASTER_DEBUG   =   D_ALWAYS : 2   D_CAT  SCHEDD_DEBUG   =   D_ALWAYS : 2   D_CAT   To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig  Then look in the  MasterLog  and  SchedLog  on the CE for any errors.    If there are issues with contacting the collector:  You will see the following message:  user@host $  condor_ce_q -pool ce1.accre.vanderbilt.edu -name ce1.accre.vanderbilt.edu -- Failed to fetch ads from:  129.59.197.223:9620?sock`33630_8b33_4  : ce1.accre.vanderbilt.edu   This may be due to network issues or bad HTCondor daemon permissions. To fix the latter issue, ensure that the  ALLOW_READ  configuration value is not set:  user@host $  condor_ce_config_val -v ALLOW_READ Not defined: ALLOW_READ   If it is defined, remove it from the file that is returned in the output.    If a job is held:  There should be an accompanying  HoldReason  that will tell you why it is being held. The  HoldReason  is in the job\u2019s ClassAd, so you can use the long form of  condor_ce_q  to extract its value:  user@host $  condor_ce_q -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID   |  grep HoldReason    If a job is idle:  The most common cause is that it is not matching any routes in the CE\u2019s job router. To find out whether this is the case, use the  condor_ce_job_router_info .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_history", 
            "text": "", 
            "title": "condor_ce_history"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_5", 
            "text": "condor_ce_history  can display job status or specific job attributes for jobs that have that have left the CE\u2019s queue. To list jobs that have run on the CE:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619  To inspect the full ClassAd for a specific job, specify the  -l  flag and the job ID:  user@host $  condor_ce_history -name condorce.example.com -pool condorce.example.com:9619 -l  Job ID    Note  If you run the  condor_ce_history  command on the CE that you are testing, omit the  -name  and  -pool  options.  condor_ce_history  takes the same arguments as  condor_history  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_job_router_info", 
            "text": "", 
            "title": "condor_ce_job_router_info"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_6", 
            "text": "Use the  condor_ce_job_router_info  command to help troubleshoot your routes and how jobs will match to them. \nTo see all of your routes (the output is long because it combines your routes with the JOB_ROUTER_DEFAULTS  configuration variable):  root@host #  condor_ce_job_router_info -config  To see how the job router is handling a job that is currently in the CE\u2019s queue, analyze the output of  condor_ce_q \n(replace the  JOB-ID  with the job ID that you are interested in):  root@host #  condor_ce_q -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  To inspect a job that has already left the queue, use  condor_ce_history  instead of  condor_ce_q :  root@host #  condor_ce_history -l  JOB-ID   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -   Note  If the proxy for the job has expired, the job will not match any routes. To work around this constraint:   root@host #  condor_ce_history -l  JOB-ID   |  sed  s/^\\(x509UserProxyExpiration\\) = .*/\\1 = `date +%s --date  +1 sec `/   |  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads -  Alternatively, you can provide a file containing a job\u2019s ClassAd as the input and edit attributes within that file:  root@host #  condor_ce_job_router_info -match-jobs -ignore-prior-routing -jobads  JOBAD-FILE", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_5", 
            "text": "If the job does not match any route:  You can identify this case when you see  0 candidate jobs found  in the  condor_job_router_info  output. This message means that, when compared to your job\u2019s ClassAd, the Umbrella constraint does not evaluate to  true . When troubleshooting, look at all of the expressions prior to the  target.ProcId  = 0  expression, because it and everything following it is logic that the job router added so that routed jobs do not get routed again.   If your job matches more than one route:  the tool will tell you by showing all matching routes after the job ID:  Checking   Job   src = 162 , 0   against   all   routes  Route   Matches :   Local_PBS  Route   Matches :   Condor_Test   To troubleshoot why this is occuring, look at the combined Requirements expressions for all routes and compare it to the job\u2019s ClassAd provided. The combined Requirements expression is  highlighted below :  Umbrella   constraint :   (( target . x509userproxysubject   =!=   UNDEFINED )    ( target . x509UserProxyExpiration   =!=   UNDEFINED )    ( time ()     target . x509UserProxyExpiration )    ( target . JobUniverse   =?=   5   ||   target . JobUniverse   =?=   1 ))    % RED % (   ( target . osgTestPBS   is   true )   ||   ( true )   ) % ENDCOLOR %    ( target . ProcId   =   0     target . JobStatus   ==   1    ( target . StageInStart   is   undefined   ||   target . StageInFinish   isnt   undefined )    target . Managed   isnt   ScheddDone    target . Managed   isnt   Extenal    target . Owner   isnt   Undefined    target . RoutedBy   isnt   htcondor-ce )   Both routes evaluate to  true  for the job\u2019s ClassAd because it contained  osgTestPBS = true . Make sure your routes are mutually exclusive, otherwise you may have jobs routed incorrectly! See the  job route configuration page  for more details.    If it is unclear why jobs are matching a route:  wrap the route's requirements expression in  debug()  and check the  JobRouterLog  for more information.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_router_q", 
            "text": "", 
            "title": "condor_ce_router_q"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_7", 
            "text": "If you have multiple job routes and many jobs,  condor_ce_router_q  is a useful tool to see how jobs are being routed\nand their statuses:  user@host $  condor_ce_router_q  condor_ce_router_q  takes the same options as  condor_router_q  and  condor_q  and is documented in the  HTCondor\nmanual", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_status", 
            "text": "", 
            "title": "condor_ce_status"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_8", 
            "text": "To see the daemons running on a CE, run the following command:  user@host $  condor_ce_status -any  condor_ce_status  takes the same arguments as  condor_status , which are documented in the HTCondor manual .   \"Missing\" Worker Nodes  An HTCondor-CE will not show any worker nodes (e.g.  Machine  entries in the  condor_ce_status -any  output) if\nit does not have any running GlideinWMS pilot jobs.\nThis is expected since HTCondor-CE only forwards incoming grid jobs to your batch system and does not match jobs to\nworker nodes.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#troubleshooting_6", 
            "text": "If the output of  condor_ce_status -any  does not show at least the following daemons:   Collector  Scheduler  DaemonMaster  Job_Router   Increase the  debug level  and consult the HTCondor-CE logs  for errors.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_config_val", 
            "text": "", 
            "title": "condor_ce_config_val"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_9", 
            "text": "To see the value of configuration variables and where they are set, use  condor_ce_config_val . \nPrimarily, This tool is used with the other troubleshooting tools to make sure your configuration is set properly. \nTo see the value of a single variable and where it is set:  user@host $  condor_ce_config_val -v  CONFIGURATION-VARIABLE   To see a list of all configuration variables and their values:  user@host $  condor_ce_config_val -dump  To see a list of all the files that are used to create your configuration and the order that they are parsed, use the\nfollowing command:  user@host $  condor_ce_config_val -config  condor_ce_config_val  takes the same arguments as  condor_config_val  and is documented in the  HTCondor manual .", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_reconfig", 
            "text": "", 
            "title": "condor_ce_reconfig"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_10", 
            "text": "To ensure that your configuration changes have taken effect, run  condor_ce_reconfig .  user@host $  condor_ce_reconfig", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#condor_ce_onoffrestart", 
            "text": "", 
            "title": "condor_ce_{on,off,restart}"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#usage_11", 
            "text": "To turn on/off/restart HTCondor-CE daemons, use the following commands:  root@host #  condor_ce_on root@host #  condor_ce_off root@host #  condor_ce_restart  The HTCondor-CE service uses the previous commands with default values. \nUsing these commands directly gives you more fine-grained control over the behavior of HTCondor-CE's on/off/restart:    If you have installed a new version of HTCondor-CE and want to restart the CE under the new version, run the following command:  root@host #  condor_ce_restart -fast  This will cause HTCondor-CE to restart and quickly reconnect to all running jobs.    If you need to stop running new jobs, run the following:  root@host #  condor_ce_off -peaceful  This will cause HTCondor-CE to accept new jobs without starting them and will wait for currently running jobs to complete before shutting down.", 
            "title": "Usage"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#htcondor-ce-troubleshooting-data", 
            "text": "The following files are located on the CE host.", 
            "title": "HTCondor-CE Troubleshooting Data"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#masterlog", 
            "text": "The HTCondor-CE master log tracks status of all of the other HTCondor daemons and thus contains valuable information if\nthey fail to start.   Location:  /var/log/condor-ce/MasterLog  Key contents: Start-up, shut-down, and communication with other HTCondor daemons   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MASTER_DEBUG   =   D_ALWAYS : 2   D_CAT     To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "MasterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for", 
            "text": "Successful daemon start-up. \nThe following line shows that the Collector daemon started successfully:  10 / 07 / 14   14 : 20 : 27   Started   DaemonCore   process   /usr/sbin/condor_collector -f -port 9619 ,   pid   and   pgroup   =   7318", 
            "title": "What to look for:"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#schedlog", 
            "text": "The HTCondor-CE schedd log contains information on all jobs that are submitted to the CE. \nIt contains valuable information when trying to troubleshoot authentication issues.   Location:  /var/log/condor-ce/SchedLog  Key contents:  Every job submitted to the CE  User authorization events     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SCHEDD_DEBUG   =   D_ALWAYS : 2   D_CAT     To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SchedLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_1", 
            "text": "Job is submitted to the CE queue:  10 / 07 / 14   16 : 52 : 17   Submitting   new   job   234 . 0   In this example, the ID of the submitted job is  234.0 .    Job owner is authorized and mapped:  10 / 07 / 14   16 : 52 : 17   Command = QMGMT_WRITE_CMD ,   peer = 131.225.154.68 : 42262  10 / 07 / 14   16 : 52 : 17   AuthMethod = GSI ,   AuthId =/ DC = com / DC = DigiCert - Grid / O = Open   Science   Grid / OU = People / CN = Brian   Lin   1047 ,                     / GLOW / Role = NULL / Capability = NULL ,   CondorId = glow @users . opensciencegrid . org   In this example, the job is authorized with the job\u2019s proxy subject using GSI and is mapped to the  glow  user.    User job submission fails  due to improper authentication or authorization:  08/30/16 16:52:56 DC_AUTHENTICATE: required authentication of 72.33.0.189\n                  failed: AUTHENTICATE:1003:Failed to authenticate with any\n                  method|AUTHENTICATE:1004:Failed to authenticate using GSI|GSI:5002:Failed to\n                  authenticate because the remote (client) side was not able to acquire its\n                  credentials.|AUTHENTICATE:1004:Failed to authenticate using FS|FS:1004:Unable to\n                  lstat(/tmp/FS_XXXZpUlYa)\n08/30/16 16:53:12 PERMISSION DENIED to  gsi@unmapped \n                  from host 72.33.0.189 for command 60021 (DC_NOP_WRITE), access level WRITE:\n                  reason: WRITE authorization policy contains no matching ALLOW entry for this\n                  request; identifiers used for this host:\n                  72.33.0.189,dyn-72-33-0-189.uwnet.wisc.edu, hostname size = 1, original ip\n                  address = 72.33.0.189\n08/30/16 16:53:12 DC_AUTHENTICATE: Command not authorized, done    Missing negotiator:  10 / 18 / 14   17 : 32 : 21   Can t find address for negotiator  10 / 18 / 14   17 : 32 : 21   Failed   to   send   RESCHEDULE   to   unknown   daemon :  Since HTCondor-CE does not manage any resources, it does not run a negotiator daemon by default and this error message is expected. In the same vein, you may see messages that there are 0 worker nodes:  06 / 23 / 15   11 : 15 : 03   Number   of   Active   Workers   0     Corrupted  job_queue.log :  02 / 07 / 17   10 : 55 : 49   WARNING :   Encountered   corrupt   log   record   _654   ( byte   offset   5046225 )  02 / 07 / 17   10 : 55 : 49   103   1354325 . 0   PeriodicRemove   (   StageInFinish     0   )   105  02 / 07 / 17   10 : 55 : 49   Lines   following   corrupt   log   record   _654   ( up   to   3 ):  02 / 07 / 17   10 : 55 : 49   103   1346101 . 0   RemoteWallClockTime   116668 . 000000  02 / 07 / 17   10 : 55 : 49   104   1346101 . 0   WallClockCheckpoint  02 / 07 / 17   10 : 55 : 49   104   1346101 . 0   ShadowBday  02 / 07 / 17   10 : 55 : 49   ERROR   Error: corrupt log record _654 (byte offset 5046225) occurred inside closed transaction,                    recovery failed   at   line   1080   in   file   / builddir / build / BUILD / condor - 8 . 4 . 8 / src / condor_utils / classad_log . cpp   This means  /var/lib/condor-ce/spool/job_queue.log  has been corrupted and you will need to hand-remove the offending record by searching for the text specified after the  Lines following corrupt log record...  line. The most common culprit of the corruption is that the disk containing the  job_queue.log  has filled up. To avoid this problem, you can change the location of  job_queue.log  by setting  JOB_QUEUE_LOG  in  /etc/condor-ce/config.d/  to a path, preferably one on a large SSD.", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#jobrouterlog", 
            "text": "The HTCondor-CE job router log produced by the job router itself and thus contains valuable information when trying to\ntroubleshoot issues with job routing.   Location:  /var/log/condor-ce/JobRouterLog  Key contents:  Every attempt to route a job  Routing success messages  Job attribute changes, based on chosen route  Job submission errors to an HTCondor batch system  Corresponding job IDs on an HTCondor batch system     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  JOB_ROUTER_DEBUG   =   D_ALWAYS : 2   D_CAT     Apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "JobRouterLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#known-errors", 
            "text": "(HTCondor batch systems only)  If you see the following error message:  Can t   find   address   of   schedd   This means that HTCondor-CE cannot communicate with your HTCondor batch system.\nVerify that the  condor  service is running on the HTCondor-CE host and is configured for your central manager.    (HTCondor batch systems only)  If you see the following error message:  JobRouter   failure   ( src = 2810466 . 0 , dest = 47968 . 0 , route = MWT2_UCORE ):   giving   up ,   because   submitted   job   is   still   not   in   job   queue   mirror   ( submitted   605   seconds   ago ).    Perhaps   it   has   been   removed ?   Ensure that  condor_config_val SPOOL  and  condor_ce_config_val JOB_ROUTER_SCHEDD2_SPOOL  return the same value.\nIf they don't, change the value of  JOB_ROUTER_SCHEDD2_SPOOL  in your HTCondor-CE configuration to match  SPOOL \nfrom your HTCondor configuration.    If you have  D_ALWAYS:2  turned on for the job router, you will see errors like the following:  06 / 12 / 15   14 : 00 : 28   HOOK_UPDATE_JOB_INFO   not   configured .   You can safely ignore these.", 
            "title": "Known Errors"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_2", 
            "text": "Job is considered for routing:  09 / 17 / 14   15 : 00 : 56   JobRouter   ( src = 86 . 0 , route = Local_LSF ):   found   candidate   job   In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the route (e.g.,  Local_LSF ).    Job is successfully routed:  09 / 17 / 14   15 : 00 : 57   JobRouter   ( src = 86 . 0 , route = Local_LSF ):   claimed   job     Finding the corresponding job ID on your HTCondor batch system:  09 / 17 / 14   15 : 00 : 57   JobRouter   ( src = 86 . 0 , dest = 205 . 0 , route = Local_Condor ):   claimed   job   In parentheses are the original HTCondor-CE job ID (e.g.,  86.0 ) and the resultant job ID on the HTCondor batch system (e.g.,  205.0 )    If your job is not routed, there will not be any evidence of it within the log itself. To investigate why your jobs are not being considered for routing, use the  condor_ce_job_router_info   HTCondor batch systems only : The following error occurs when the job router daemon cannot submit the routed job: 10/19/14 13:09:15 Can t resolve collector condorce.example.com; skipping\n10/19/14 13:09:15 ERROR (pool condorce.example.com) Can t find address of schedd\n10/19/14 13:09:15 JobRouter failure (src=5.0,route=Local_Condor): failed to submit job", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#gridmanagerlog", 
            "text": "The HTCondor-CE grid manager log tracks the submission and status of jobs on non-HTCondor batch systems. \nIt contains valuable information when trying to troubleshoot jobs that have been routed but failed to complete. \nDetails on how to read the Gridmanager log can be found on the  HTCondor\nWiki .   Location:  /var/log/condor-ce/GridmanagerLog. JOB-OWNER  Key contents:  Every attempt to submit a job to a batch system or other grid resource  Status updates of submitted jobs  Corresponding job IDs on non-HTCondor batch systems     Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  MAX_GRIDMANAGER_LOG   =   6 h  MAX_NUM_GRIDMANAGER_LOG   =   8  GRIDMANAGER_DEBUG   =   D_ALWAYS : 2   D_CAT     To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "GridmanagerLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_3", 
            "text": "Job is submitted to the batch system:  09 / 17 / 14   09 : 51 : 34   [ 12997 ]   ( 85 . 0 )   gm   state   change :   GM_SUBMIT_SAVE   -   GM_SUBMITTED   Every state change the Gridmanager tracks should have the job ID in parentheses (i.e.=(85.0)).    Job status being updated:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046 \n09/17/14 15:07:24 [25543] GAHP[25563] -   S \n09/17/14 15:07:25 [25543] GAHP[25563]  -  RESULTS \n09/17/14 15:07:25 [25543] GAHP[25563] -   R \n09/17/14 15:07:25 [25543] GAHP[25563] -   S   1 \n09/17/14 15:07:25 [25543] GAHP[25563] -   3   0   No Error   4   [ BatchjobId =  482046 ; JobStatus = 4; ExitCode = 0; WorkerNode =  atl-prod08  ]   The first line tells us that the Gridmanager is initiating a status update and the following lines are the results. The most interesting line is the second highlighted section that notes the job ID on the batch system and its status. If there are errors querying the job on the batch system, they will appear here.    Finding the corresponding job ID on your non-HTCondor batch system:  09/17/14 15:07:24 [25543] (87.0) gm state change: GM_SUBMITTED -  GM_POLL_ACTIVE\n09/17/14 15:07:24 [25543] GAHP[25563]  -  BLAH_JOB_STATUS 3 lsf/20140917/482046   On the first line, after the timestamp and PID of the Gridmanager process, you will find the CE\u2019s job ID in parentheses,  (87.0) . At the end of the second line, you will find the batch system, date, and batch system job id separated by slashes,  lsf/20140917/482046 .    Job completion on the batch system:  09 / 17 / 14   15 : 07 : 25   [ 25543 ]   ( 87 . 0 )   gm   state   change :   GM_TRANSFER_OUTPUT   -   GM_DONE_SAVE", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#sharedportlog", 
            "text": "The HTCondor-CE shared port log keeps track of all connections to all of the HTCondor-CE daemons other than the\ncollector. \nThis log is a good place to check if experiencing connectivity issues with HTCondor-CE. More information can be found here .   Location:  /var/log/condor-ce/SharedPortLog  Key contents: Every attempt to connect to HTCondor-CE (except collector queries)   Increasing the debug level:    Set the following value in  /etc/condor-ce/config.d/99-local.conf  on the CE host:  SHARED_PORT_DEBUG   =   D_ALWAYS : 2   D_CAT     To apply these changes, reconfigure HTCondor-CE:  root@host #  condor_ce_reconfig", 
            "title": "SharedPortLog"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#messages-log", 
            "text": "The messages file can include output from lcmaps, which handles mapping of X.509 proxies to Unix usernames. \nIf there are issues with the  authentication setup , the errors may\nappear here.   Location:  /var/log/messages  Key contents: User authentication", 
            "title": "Messages log"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#what-to-look-for_4", 
            "text": "A user is mapped:  Oct   6   10 : 35 : 32   osgserv06   htondor - ce - llgt [ 12147 ]:   Callout   to   LCMAPS   returned   local   user   ( service   condor ):   osgglow01", 
            "title": "What to look for"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#blahp-configuration-file", 
            "text": "HTCondor-CE uses the BLAHP to submit jobs to your local non-HTCondor batch system using your batch system's client\ntools.   Location:  /etc/blah.config  Key contents:  Locations of the batch system's client binaries and logs  Location to save files that are submitted to the local batch system     You can also tell the BLAHP to save the files that are being submitted to the local batch system to  DIR-NAME  by\nadding the following line:  blah_debug_save_submit_info = DIR_NAME   The BLAHP will then create a directory with the format  bl_*  for each submission to the local jobmanager with the\nsubmit file and proxy used.   Note  Whitespace is important so do not put any spaces around the = sign. In addition, the directory must be created and HTCondor-CE should have sufficient permissions to create directories within  DIR_NAME .", 
            "title": "BLAHP Configuration File"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#getting-help", 
            "text": "If you are still experiencing issues after using this document, please let us know!   Gather basic HTCondor-CE and related information (versions, relevant configuration, problem description, etc.)   Gather system information:  root @host   #   osg - system - profiler     Start a support request using  a web interface  or by email to    Describe issue and expected or desired behavior  Include basic HTCondor-CE and related information  Attach the osg-system-profiler output", 
            "title": "Getting Help"
        }, 
        {
            "location": "/compute-element/troubleshoot-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  Submitting jobs to HTCondor-CE", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/", 
            "text": "Submitting Jobs to an HTCondor-CE\n\n\nThis document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).\n\n\n\n\nNote\n\n\nMost incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.\n\n\n\n\nSubmitting Jobs...\n\n\nThere are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the \nhtcondor-ce-client\n package and using the \ncondor_submit\n command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.\n\n\nBefore attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:\n\n\nuser@host $\n voms-proxy-init\n\n\n\n\n\nUsing HTCondor-CE tools\n\n\nThere are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE: \ncondor_ce_trace\n and \ncondor_ce_run\n. The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has \nhtcondor-ce-client\n installed, which you may wish to do if you are testing availability of your CE from an external source.\n\n\ncondor_ce_trace\n\n\ncondor_ce_trace\n is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with \ncondor_ce_trace\n, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug condorce.example.com\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. On success, you will see \nJob status: Completed\n and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nRequesting resources\n\n\ncondor_ce_trace\n doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the \n--attribute\n option on the command line:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+resource1=value1\n...--attribute\n=\n+resourceN=valueN\n condorce.example.com\n\n\n\n\n\nTo submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:\n\n\nuser@host $\n condor_ce_trace --debug --attribute\n=\n+xcount=4\n --attribute\n=\n+maxMemory=4000\n --attribute\n=\n+maxWallTime=120\n --attribute\n=\n+remote_queue=osg\n condorce.example.com\n\n\n\n\n\nFor a list of other attributes that can be set with the \n--attribute\n option, consult the \njob attributes\n section.\n\n\n\n\nNote\n\n\nNon HTCondor batch systems may need additional configuration to support these job attributes.  See the \njob router recipes\n for details on how to support them.\n\n\n\n\ncondor_ce_run\n\n\ncondor_ce_run\n is a Python script that calls \ncondor_submit\n on a generated submit file and tracks its progress with \ncondor_q\n. To submit a job with \ncondor_ce_run\n, run the following command:\n\n\nuser@host $\n condor_ce_run -r condorce.example.com:9619 /bin/env\n\n\n\n\n\nReplacing \ncondorce.example.com\n with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the \ntroubleshooting guide\n.\n\n\nUsing a submit file...\n\n\nIf you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use \ncondor_submit\n (or in one of our cases, \ncondor_ce_submit\n) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.\n\n\nFrom the CE host\n\n\nThis method uses \ncondor_ce_submit\n to submit directly to an HTCondor-CE. The only reason we use \ncondor_ce_submit\n in this case is to take advantage of the already running daemons on the CE host.\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# \nRequired\n \nfor\n \nlocal\n \nHTCondor\n-\nCE\n \nsubmission\n\n\nuniverse\n \n=\n \nvanilla\n\n\nuse_x509userproxy\n \n=\n \ntrue\n\n\n+\nOwner\n \n=\n \nundefined\n\n\n# \nFiles\n\n\nexecutable\n \n=\n \n%\nRED\n%\nce_test\n.\nsh\n%\nENDCOLOR\n%\n\n\noutput\n \n=\n \nce_test\n.\nout\n\n\nerror\n \n=\n \nce_test\n.\nerr\n\n\nlog\n \n=\n \nce_test\n.\nlog\n\n\n# \nFile\n \ntransfer\n \nbehavior\n\n\nShouldTransferFiles\n \n=\n \nYES\n\n\nWhenToTransferOutput\n \n=\n \nON_EXIT\n\n\n# \nOptional\n \nresource\n \nrequests\n\n#\n+\nxcount\n \n=\n \n4\n            # \nRequest\n \n4\n \ncores\n\n#\n+\nmaxMemory\n \n=\n \n4000\n      # \nRequest\n \n4\nGB\n \nof\n \nRAM\n\n#\n+\nmaxWallTime\n \n=\n \n120\n     # \nRequest\n \n2\n \nhrs\n \nof\n \nwall\n \nclock\n \ntime\n\n#\n+\nremote_queue\n \n=\n \nosg\n  # \nRequest\n \nthe\n \nOSG\n \nqueue\n\n\n# \nRun\n \njob\n \nonce\n\n\nqueue\n\n\n\n\n\n\nReplacing \nce_test.sh\n with the path to the executable you wish to run.\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_ce_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nFrom another host\n\n\nFor this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the \ncondor\n package from the OSG repository to get a simple submit node:\n\n\n\n\n\n\nInstall HTCondor:\n\n\nroot@host #\n yum install condor\n\n\n\n\n\n\n\n\n\nStart the \ncondor\n service:\n\n\nroot@host $\n service condor start\n\n\n\n\n\n\n\n\n\nOnce the \ncondor\n service is running, write a submit file and submit your job:\n\n\n\n\n\n\nWrite a submit file, \nce_test.sub\n:\n\n\n# \nRequired\n \nfor\n \nremote\n \nHTCondor\n-\nCE\n \nsubmission\n\n\nuniverse\n \n=\n \ngrid\n\n\nuse_x509userproxy\n \n=\n \ntrue\n\n\ngrid_resource\n \n=\n \ncondor\n \n%\nRED\n%\ncondorce\n.\nexample\n.\ncom\n \ncondorce\n.\nexample\n.\ncom\n%\nENDCOLOR\n%\n:\n9619\n\n\n# \nFiles\n\n\nexecutable\n \n=\n \n%\nRED\n%\nce_test\n.\nsh\n%\nENDCOLOR\n%\n\n\noutput\n \n=\n \nce_test\n.\nout\n\n\nerror\n \n=\n \nce_test\n.\nerr\n\n\nlog\n \n=\n \nce_test\n.\nlog\n\n\n# \nFile\n \ntransfer\n \nbehavior\n\n\nShouldTransferFiles\n \n=\n \nYES\n\n\nWhenToTransferOutput\n \n=\n \nON_EXIT\n\n\n# \nOptional\n \nresource\n \nrequests\n\n#\n+\nxcount\n \n=\n \n4\n            # \nRequest\n \n4\n \ncores\n\n#\n+\nmaxMemory\n \n=\n \n4000\n      # \nRequest\n \n4\nGB\n \nof\n \nRAM\n\n#\n+\nmaxWallTime\n \n=\n \n120\n     # \nRequest\n \n2\n \nhrs\n \nof\n \nwall\n \nclock\n \ntime\n\n#\n+\nremote_queue\n \n=\n \nosg\n  # \nRequest\n \nthe\n \nOSG\n \nqueue\n\n\n# \nRun\n \njob\n \nonce\n\n\nqueue\n\n\n\n\n\n\n\n\nNote\n\n\nThe \ngrid_resource\n line should start with \ncondor\n and is not related to which batch system you are using.\n\n\n\n\n\n\n\n\nYou can use any executable you choose for the \nexecutable\n field. If you don't have one in mind, you may use the following example test script:\n\n\n1\n2\n3\n4\n5\n#!/bin/bash\n\n\ndate\nhostname\nenv\n\n\n\n\n\n\n\n\n\n\nMark the test script as executable:\n\n\nuser@host $\n chmod +x ce_test.sh\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit the job:\n\n\nuser@host $\n condor_submit ce_test.sub\n\n\n\n\n\n\n\n\n\nTracking job progress\n\n\nWhen the job completes, stdout will be placed into \nce_test.out\n, stderr will be placed into \nce_test.err\n, and HTCondor logging information will be placed in \nce_test.log\n. You can track job progress by looking at the condor queue by running the following command on the CE host:\n\n\nuser@host $\n condor_ce_q\n\n\n\n\n\nUsing the following table to determine job status:\n\n\n\n\n\n\n\n\nThis value in the \nST\n column...\n\n\nMeans that the job is...\n\n\n\n\n\n\n\n\n\n\nI\n\n\nidle\n\n\n\n\n\n\nC\n\n\ncomplete\n\n\n\n\n\n\nX\n\n\nbeing removed\n\n\n\n\n\n\nH\n\n\nheld\n\n\n\n\n\n\n\n\ntransferring input\n\n\n\n\n\n\n\n\ntransferring output\n\n\n\n\n\n\n\n\nHow Job Routes Affect Your Job\n\n\nUpon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.\n\n\nMatching\n\n\nSee \nthis section\n for details on how jobs are matched\nto job routes.\n\n\nExamples\n\n\nThe following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:\n\n\n\n\nRoute 1:\n Matches jobs whose attribute \nfoo\n is equal to \nbar\n.\n\n\nRoute 2:\n Matches jobs whose attribute \nfoo\n is equal to \nbaz\n.\n\n\nRoute 3:\n Matches jobs whose attribute \nfoo\n is neither equal to \nbar\n nor \nbaz\n.\n\n\n\n\n\n\nNote\n\n\nSetting a custom attribute for job submission requires the \n+\n prefix in your submit file but it is unnecessary in the job routes.\n\n\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n[\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nRoute 1\n;\n \n\\\n\n     \nRequirements\n \n=\n \n(\nTARGET\n.\nfoo\n \n=?=\n \nbar\n);\n \n\\\n\n\n]\n \n\\\n\n\n[\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nRoute 2\n;\n \n\\\n\n     \nRequirements\n \n=\n \n(\nTARGET\n.\nfoo\n \n=?=\n \nbaz\n);\n \n\\\n\n\n]\n \n\\\n\n\n[\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nRoute 3\n;\n \n\\\n\n     \nRequirements\n \n=\n \n(\nTARGET\n.\nfoo\n \n=!=\n \nbar\n)\n \n \n(\nTARGET\n.\nfoo\n \n=!=\n \nbaz\n);\n \n\\\n\n\n]\n\n\n\n\n\n\nIf a user submitted their job with \n+foo = bar\n in their submit file, the job would match \nRoute 1\n.\n\n\nRoute defaults\n\n\nRoute defaults\n can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding \nattribute\n in their job.\n\n\nExamples\n\n\nThe following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.\n\n\nJOB_ROUTER_ENTRIES\n \n=\n \n[\n \n\\\n\n     \nTargetUniverse\n \n=\n \n5\n;\n \n\\\n\n     \nname\n \n=\n \nRoute 1\n;\n \n\\\n\n     \nset_default_maxMemory\n \n=\n \n1000\n;\n \n\\\n\n\n]\n\n\n\n\n\n\nA user could submit their job with the attribute \n+maxMemory=2000\n and that job would be submitted requesting 2GB memory instead of the default of 1GB.\n\n\nTroubleshooting Your Jobs\n\n\nTroubleshooting\n\n\nAll interactions between \ncondor_submit\n and the CE will be recorded in the file specified by the \nlog\n attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:\n\n\n000\n \n(\n786\n.\n000\n.\n000\n)\n \n12\n/\n09\n \n16\n:\n49\n:\n55\n \nJob\n \nsubmitted\n \nfrom\n \nhost\n: \n131\n.\n225\n.\n154\n.\n68\n:\n53134\n\n...\n\n027\n \n(\n786\n.\n000\n.\n000\n)\n \n12\n/\n09\n \n16\n:\n50\n:\n09\n \nJob\n \nsubmitted\n \nto\n \ngrid\n \nresource\n\n    \nGridResource\n: \ncondor\n \ncondorce\n.\nexample\n.\ncom\n \ncondorce\n.\nexample\n.\ncom\n:\n9619\n\n    \nGridJobId\n: \ncondor\n \ncondorce\n.\nexample\n.\ncom\n \ncondorce\n.\nexample\n.\ncom\n:\n9619\n \n796\n.\n0\n\n...\n\n005\n \n(\n786\n.\n000\n.\n000\n)\n \n12\n/\n09\n \n16\n:\n52\n:\n19\n \nJob\n \nterminated\n.\n        \n(\n1\n)\n \nNormal\n \ntermination\n \n(\nreturn\n \nvalue\n \n0\n)\n\n                \nUsr\n \n0\n \n00\n:\n00\n:\n00\n, \nSys\n \n0\n \n00\n:\n00\n:\n00\n  \n-\n  \nRun\n \nRemote\n \nUsage\n\n                \nUsr\n \n0\n \n00\n:\n00\n:\n00\n, \nSys\n \n0\n \n00\n:\n00\n:\n00\n  \n-\n  \nRun\n \nLocal\n \nUsage\n\n                \nUsr\n \n0\n \n00\n:\n00\n:\n00\n, \nSys\n \n0\n \n00\n:\n00\n:\n00\n  \n-\n  \nTotal\n \nRemote\n \nUsage\n\n                \nUsr\n \n0\n \n00\n:\n00\n:\n00\n, \nSys\n \n0\n \n00\n:\n00\n:\n00\n  \n-\n  \nTotal\n \nLocal\n \nUsage\n\n        \n0\n  \n-\n  \nRun\n \nBytes\n \nSent\n \nBy\n \nJob\n\n        \n0\n  \n-\n  \nRun\n \nBytes\n \nReceived\n \nBy\n \nJob\n\n        \n0\n  \n-\n  \nTotal\n \nBytes\n \nSent\n \nBy\n \nJob\n\n        \n0\n  \n-\n  \nTotal\n \nBytes\n \nReceived\n \nBy\n \nJob\n\n\n\n\n\n\nIf there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':\n\n\n020\n \n(\n788\n.\n000\n.\n000\n)\n \n12\n/\n09\n \n16\n:\n56\n:\n17\n \nDetected\n \nDown\n \nGlobus\n \nResource\n\n    \nRM\n-\nContact\n:\n \nfermicloud133\n.\nfnal\n.\ngov\n\n\n...\n\n\n026\n \n(\n788\n.\n000\n.\n000\n)\n \n12\n/\n09\n \n16\n:\n56\n:\n17\n \nDetected\n \nDown\n \nGrid\n \nResource\n\n    \nGridResource\n:\n \ncondor\n \ncondorce\n.\nexample\n.\ncom\n \ncondorce\n.\nexample\n.\ncom\n:\n9619\n\n\n\n\n\n\nThis indicates a communication issue with your CE that can be diagnosed with \ncondor_ce_ping\n.\n\n\nReference\n\n\nHere are some other HTCondor-CE documents that might be helpful:\n\n\n\n\nHTCondor-CE overview and architecture\n\n\nInstalling HTCondor-CE\n\n\nConfiguring HTCondor-CE job routes\n\n\nThe HTCondor-CE troubleshooting guide\n\n\n\n\nJob attributes\n\n\nThe following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the \nHTCondor manual\n.\n\n\n\n\n\n\n\n\nHTCondor Attribute\n\n\nGlobus RSL\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\narguments\n\n\narguments\n\n\nArguments that will be provided to the executable for the job.\n\n\n\n\n\n\nerror\n\n\nstderr\n\n\nPath to the file on the client machine that stores stderr from the job.\n\n\n\n\n\n\nexecutable\n\n\nexecutable\n\n\nPath to the file on the client machine that the job will execute.\n\n\n\n\n\n\ninput\n\n\nstdin\n\n\nPath to the file on the client machine that stores input to be piped into the stdin of the job.\n\n\n\n\n\n\n+maxMemory\n\n\nmaxMemory\n\n\nThe amount of memory in MB that you wish to allocate to the job.\n\n\n\n\n\n\n+maxWallTime\n\n\nmaxWallTime\n\n\nThe maximum walltime (in minutes) the job is allowed to run before it is removed.\n\n\n\n\n\n\noutput\n\n\nstdout\n\n\nPath to the file on the client machine that stores stdout from the job.\n\n\n\n\n\n\n+remote_queue\n\n\nqueue\n\n\nAssign job to the target queue in the scheduler. Note that the queue name should be in quotes.\n\n\n\n\n\n\ntransfer_input_files\n\n\nfile_stage_in\n\n\nA comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.\n\n\n\n\n\n\ntransfer_output_files\n\n\ntransfer_output_files\n\n\nA comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.\n\n\n\n\n\n\n+xcount\n\n\nxcount\n\n\nThe number of cores to allocate for the job.\n\n\n\n\n\n\n\n\nIf you are setting an attribute to a string value, make sure enclose the string in double-quotes (\n\"\n), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Submitting to HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs-to-an-htcondor-ce", 
            "text": "This document outlines methods of manual submission to an HTCondor-CE. It is intended for site administrators wishing to verify the functionality of their HTCondor-CE installation and developers writing software to submit jobs to an HTCondor-CE (e.g., pilot jobs).   Note  Most incoming jobs are pilots from factories and that manual submission does not reflect the standard method that jobs are submitted to OSG CE\u2019s.", 
            "title": "Submitting Jobs to an HTCondor-CE"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#submitting-jobs", 
            "text": "There are two main methods for submitting files to an HTCondor-CE: using the tools bundled with the  htcondor-ce-client  package and using the  condor_submit  command with a submit file. Both methods will test end-to-end job submission but the former method is simpler while the latter will walk you through writing your own submit file.  Before attempting to submit jobs, you will need to generate a proxy from a user certificate before running any jobs. To generate a proxy, run the following command on the host you plan on submitting from:  user@host $  voms-proxy-init", 
            "title": "Submitting Jobs..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-htcondor-ce-tools", 
            "text": "There are two HTCondor-CE tools that allow users to test the functionality of their HTCondor-CE:  condor_ce_trace  and  condor_ce_run . The former is the preferred tool as it provides useful feedback if failure occurs while the latter is simply an automated submission tool. These commands may be run from any host that has  htcondor-ce-client  installed, which you may wish to do if you are testing availability of your CE from an external source.", 
            "title": "Using HTCondor-CE tools"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_trace", 
            "text": "condor_ce_trace  is a Python script that uses HTCondor's Python bindings to run diagnostics, including job submission, against your HTCondor-CE. To submit a job with  condor_ce_trace , run the following command:  user@host $  condor_ce_trace --debug condorce.example.com  Replacing  condorce.example.com  with the hostname of the CE you wish to test. On success, you will see  Job status: Completed  and the environment of the job on the worker node it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_trace"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#requesting-resources", 
            "text": "condor_ce_trace  doesn't make any specific resource requests so its jobs are only given the default resources by the CE. To request specific resources (or other job attributes), you can specify the  --attribute  option on the command line:  user@host $  condor_ce_trace --debug --attribute = +resource1=value1 ...--attribute = +resourceN=valueN  condorce.example.com  To submit a job that requests 4 cores, 4 GB of RAM, a wall clock time of 2 hours, and the 'osg' queue, run the following command:  user@host $  condor_ce_trace --debug --attribute = +xcount=4  --attribute = +maxMemory=4000  --attribute = +maxWallTime=120  --attribute = +remote_queue=osg  condorce.example.com  For a list of other attributes that can be set with the  --attribute  option, consult the  job attributes  section.   Note  Non HTCondor batch systems may need additional configuration to support these job attributes.  See the  job router recipes  for details on how to support them.", 
            "title": "Requesting resources"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#condor_ce_run", 
            "text": "condor_ce_run  is a Python script that calls  condor_submit  on a generated submit file and tracks its progress with  condor_q . To submit a job with  condor_ce_run , run the following command:  user@host $  condor_ce_run -r condorce.example.com:9619 /bin/env  Replacing  condorce.example.com  with the hostname of the CE you wish to test. The command will not return any output until it completes: When it does you will see the environment of the job on the worker noded it landed on. If you do not get the expected output, refer to the  troubleshooting guide .", 
            "title": "condor_ce_run"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#using-a-submit-file", 
            "text": "If you are familiar with HTCondor, submitting a job to an HTCondor-CE using a submit file follows the same procedure as submitting a job to an HTCondor batch system: Write a submit file and use  condor_submit  (or in one of our cases,  condor_ce_submit ) to submit the job. This is by virtue of the fact that HTCondor-CE is just a special configuration of HTCondor. The major differences occur in the specific attributes for the submit files outlined below.", 
            "title": "Using a submit file..."
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-the-ce-host", 
            "text": "This method uses  condor_ce_submit  to submit directly to an HTCondor-CE. The only reason we use  condor_ce_submit  in this case is to take advantage of the already running daemons on the CE host.    Write a submit file,  ce_test.sub :  #  Required   for   local   HTCondor - CE   submission  universe   =   vanilla  use_x509userproxy   =   true  + Owner   =   undefined \n\n#  Files  executable   =   % RED % ce_test . sh % ENDCOLOR %  output   =   ce_test . out  error   =   ce_test . err  log   =   ce_test . log \n\n#  File   transfer   behavior  ShouldTransferFiles   =   YES  WhenToTransferOutput   =   ON_EXIT \n\n#  Optional   resource   requests \n# + xcount   =   4             #  Request   4   cores \n# + maxMemory   =   4000       #  Request   4 GB   of   RAM \n# + maxWallTime   =   120      #  Request   2   hrs   of   wall   clock   time \n# + remote_queue   =   osg   #  Request   the   OSG   queue \n\n#  Run   job   once  queue   Replacing  ce_test.sh  with the path to the executable you wish to run.    You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_ce_submit ce_test.sub", 
            "title": "From the CE host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#from-another-host", 
            "text": "For this method, you will need a functional HTCondor submit node. If you do not have one readily available, you can install the  condor  package from the OSG repository to get a simple submit node:    Install HTCondor:  root@host #  yum install condor    Start the  condor  service:  root@host $  service condor start    Once the  condor  service is running, write a submit file and submit your job:    Write a submit file,  ce_test.sub :  #  Required   for   remote   HTCondor - CE   submission  universe   =   grid  use_x509userproxy   =   true  grid_resource   =   condor   % RED % condorce . example . com   condorce . example . com % ENDCOLOR % : 9619 \n\n#  Files  executable   =   % RED % ce_test . sh % ENDCOLOR %  output   =   ce_test . out  error   =   ce_test . err  log   =   ce_test . log \n\n#  File   transfer   behavior  ShouldTransferFiles   =   YES  WhenToTransferOutput   =   ON_EXIT \n\n#  Optional   resource   requests \n# + xcount   =   4             #  Request   4   cores \n# + maxMemory   =   4000       #  Request   4 GB   of   RAM \n# + maxWallTime   =   120      #  Request   2   hrs   of   wall   clock   time \n# + remote_queue   =   osg   #  Request   the   OSG   queue \n\n#  Run   job   once  queue    Note  The  grid_resource  line should start with  condor  and is not related to which batch system you are using.     You can use any executable you choose for the  executable  field. If you don't have one in mind, you may use the following example test script:  1\n2\n3\n4\n5 #!/bin/bash \n\ndate\nhostname\nenv     Mark the test script as executable:  user@host $  chmod +x ce_test.sh      Submit the job:  user@host $  condor_submit ce_test.sub", 
            "title": "From another host"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#tracking-job-progress", 
            "text": "When the job completes, stdout will be placed into  ce_test.out , stderr will be placed into  ce_test.err , and HTCondor logging information will be placed in  ce_test.log . You can track job progress by looking at the condor queue by running the following command on the CE host:  user@host $  condor_ce_q  Using the following table to determine job status:     This value in the  ST  column...  Means that the job is...      I  idle    C  complete    X  being removed    H  held     transferring input     transferring output", 
            "title": "Tracking job progress"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#how-job-routes-affect-your-job", 
            "text": "Upon successful submission of your job, the Job Router takes control of your job by matching it to routes and submitting a transformed job to your batch system.", 
            "title": "How Job Routes Affect Your Job"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#matching", 
            "text": "See  this section  for details on how jobs are matched\nto job routes.  Examples  The following three routes only perform filtering and submission of routed jobs to an HTCondor batch system. The only differences are in the types of jobs that they match:   Route 1:  Matches jobs whose attribute  foo  is equal to  bar .  Route 2:  Matches jobs whose attribute  foo  is equal to  baz .  Route 3:  Matches jobs whose attribute  foo  is neither equal to  bar  nor  baz .    Note  Setting a custom attribute for job submission requires the  +  prefix in your submit file but it is unnecessary in the job routes.   JOB_ROUTER_ENTRIES   =   [   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   Route 1 ;   \\ \n      Requirements   =   ( TARGET . foo   =?=   bar );   \\  ]   \\  [   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   Route 2 ;   \\ \n      Requirements   =   ( TARGET . foo   =?=   baz );   \\  ]   \\  [   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   Route 3 ;   \\ \n      Requirements   =   ( TARGET . foo   =!=   bar )     ( TARGET . foo   =!=   baz );   \\  ]   If a user submitted their job with  +foo = bar  in their submit file, the job would match  Route 1 .", 
            "title": "Matching"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#route-defaults", 
            "text": "Route defaults  can be set for batch system queue, maximum memory, number of cores to request, and maximum walltime. The submitting user can override any of these by setting the corresponding  attribute  in their job.  Examples  The following route takes all incoming jobs and submits them to an HTCondor batch system requesting 1GB of memory.  JOB_ROUTER_ENTRIES   =   [   \\ \n      TargetUniverse   =   5 ;   \\ \n      name   =   Route 1 ;   \\ \n      set_default_maxMemory   =   1000 ;   \\  ]   A user could submit their job with the attribute  +maxMemory=2000  and that job would be submitted requesting 2GB memory instead of the default of 1GB.", 
            "title": "Route defaults"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting-your-jobs", 
            "text": "", 
            "title": "Troubleshooting Your Jobs"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#troubleshooting", 
            "text": "All interactions between  condor_submit  and the CE will be recorded in the file specified by the  log  attribute in your submit file. This includes acknowledgement of the job in your local queue, connection to the CE, and a record of job completion:  000   ( 786 . 000 . 000 )   12 / 09   16 : 49 : 55   Job   submitted   from   host :  131 . 225 . 154 . 68 : 53134 \n... 027   ( 786 . 000 . 000 )   12 / 09   16 : 50 : 09   Job   submitted   to   grid   resource \n     GridResource :  condor   condorce . example . com   condorce . example . com : 9619 \n     GridJobId :  condor   condorce . example . com   condorce . example . com : 9619   796 . 0 \n... 005   ( 786 . 000 . 000 )   12 / 09   16 : 52 : 19   Job   terminated .\n         ( 1 )   Normal   termination   ( return   value   0 ) \n                 Usr   0   00 : 00 : 00 ,  Sys   0   00 : 00 : 00    -    Run   Remote   Usage \n                 Usr   0   00 : 00 : 00 ,  Sys   0   00 : 00 : 00    -    Run   Local   Usage \n                 Usr   0   00 : 00 : 00 ,  Sys   0   00 : 00 : 00    -    Total   Remote   Usage \n                 Usr   0   00 : 00 : 00 ,  Sys   0   00 : 00 : 00    -    Total   Local   Usage \n         0    -    Run   Bytes   Sent   By   Job \n         0    -    Run   Bytes   Received   By   Job \n         0    -    Total   Bytes   Sent   By   Job \n         0    -    Total   Bytes   Received   By   Job   If there are issues contacting the CE, you will see error messages about a 'Down Globus Resource':  020   ( 788 . 000 . 000 )   12 / 09   16 : 56 : 17   Detected   Down   Globus   Resource \n     RM - Contact :   fermicloud133 . fnal . gov  ...  026   ( 788 . 000 . 000 )   12 / 09   16 : 56 : 17   Detected   Down   Grid   Resource \n     GridResource :   condor   condorce . example . com   condorce . example . com : 9619   This indicates a communication issue with your CE that can be diagnosed with  condor_ce_ping .", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#reference", 
            "text": "Here are some other HTCondor-CE documents that might be helpful:   HTCondor-CE overview and architecture  Installing HTCondor-CE  Configuring HTCondor-CE job routes  The HTCondor-CE troubleshooting guide", 
            "title": "Reference"
        }, 
        {
            "location": "/compute-element/submit-htcondor-ce/#job-attributes", 
            "text": "The following table is a reference of job attributes that can be included in HTCondor submit files and their GlobusRSL equivalents. A more comprehensive list of submit file attributes specific to HTCondor-CE can be found in the  HTCondor manual .     HTCondor Attribute  Globus RSL  Summary      arguments  arguments  Arguments that will be provided to the executable for the job.    error  stderr  Path to the file on the client machine that stores stderr from the job.    executable  executable  Path to the file on the client machine that the job will execute.    input  stdin  Path to the file on the client machine that stores input to be piped into the stdin of the job.    +maxMemory  maxMemory  The amount of memory in MB that you wish to allocate to the job.    +maxWallTime  maxWallTime  The maximum walltime (in minutes) the job is allowed to run before it is removed.    output  stdout  Path to the file on the client machine that stores stdout from the job.    +remote_queue  queue  Assign job to the target queue in the scheduler. Note that the queue name should be in quotes.    transfer_input_files  file_stage_in  A comma-delimited list of all the files and directories to be transferred into the working directory for the job, before the job is started.    transfer_output_files  transfer_output_files  A comma-delimited list of all the files and directories to be transferred back to the client, after the job completes.    +xcount  xcount  The number of cores to allocate for the job.     If you are setting an attribute to a string value, make sure enclose the string in double-quotes ( \" ), otherwise HTCondor-CE will try to find an attribute by that name.", 
            "title": "Job attributes"
        }, 
        {
            "location": "/worker-node/using-wn/", 
            "text": "Worker Node Overview\n\n\nThe Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources.\n\n\nThis page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.\n\n\nThe OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools (\ngcc\n, \nlapack\n, \nblas\n, etc.) beyond the ones in the OSG worker node client and the base OS.\n\n\nIf you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in \nthe OSG Docker image\n.\n\n\nHardware Recommendations\n\n\n\n\n\n\n\n\nHardware\n\n\nMinimum\n\n\nRecommended\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nCore per pilot\n\n\n1\n\n\n8\n\n\nDepends on the supported VOs. The total core count on every node in the cluster must be divisible by core per pilot.\n\n\n\n\n\n\nMemory per core\n\n\n1024MB\n\n\n2048MB\n\n\nMemory per core times core per pilot needs to be less than the total memory on every node. Do not overcommit.\n\n\n\n\n\n\nScratch disk per core (\nOSG_WN_TMP\n)\n\n\n2GB\n\n\n10 GB\n\n\nThis can be overcommitted if a mix of different VO jobs is expected.\n\n\n\n\n\n\nCVMFS \nCache\n (optional)\n\n\n10 GB\n\n\n20 GB\n\n\nThis is a value per node and not per core.\n\n\n\n\n\n\n\n\nCommon Software Available on Worker Nodes\n\n\nThe OSG worker node environment contains the following software:\n\n\n\n\nThe supported set of CA certificates (located in \n$X509_CERT_DIR\n after the environment is set up)\n\n\nProxy management tools:\n\n\nCreate proxies: \nvoms-proxy-init\n and \ngrid-proxy-init\n\n\nShow proxy info: \nvoms-proxy-info\n and \ngrid-proxy-info\n\n\nDestroy the current proxy: \nvoms-proxy-destroy\n and \ngrid-proxy-destroy\n\n\n\n\n\n\nData transfer tools:\n\n\nHTTP/plain FTP protocol tools (via system dependencies):\n\n\nwget\n and \ncurl\n: standard tools for downloading files with HTTP and FTP\n\n\n\n\n\n\nTransfer clients\n\n\nGFAL\n-based client (\ngfal-copy\n and others).  GFAL supports SRM, GridFTP, and HTTP protocols.\n\n\nGlobus GridFTP client (\nglobus-url-copy\n)\n\n\n\n\n\n\n\n\n\n\nMyProxy client tools\n\n\n\n\nAt some sites, these tools may not be available at the pilot launch.  To setup the environment, do the following:\n\n\nuser@host $\n \nsource\n \n$OSG_GRID\n/setup.sh\n\n\n\n\n\nThis should be done by a pilot job, not by the end-user payload.\n\n\nThe Worker Node Environment\n\n\nThe following table outlines the various important directories and information in the worker node environment.\nA job running on an OSG worker node can refer to each directory using the corresponding environment variable.\nSeveral of them are defined as options in your OSG-Configure \n.ini\n files in \n/etc/osg/config.d\n.\nCustom variables and those that aren't listed may be defined in the \nLocal Settings section\n.\n\n\n\n\n\n\n\n\nEnvironment Variable\n\n\nOSG-Configure section/option\n\n\nPurpose\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n$OSG_GRID\n\n\nStorage\n/\ngrid_dir\n\n\nLocation of additional environment variables.\n\n\nPilots should source \n$OSG_GRID/setup.sh\n in order to guarantee the environment contains the worker node binaries in \n$PATH\n.\n\n\n\n\n\n\n$OSG_SQUID_LOCATION\n,\n\n\nSquid\n/\nlocation\n\n\nLocation of a HTTP caching proxy server\n\n\nUtilize this service for downloading files via HTTP for cache-friendly workflows.\n\n\n\n\n\n\n$OSG_WN_TMP\n\n\nStorage\n/\nworker_node_temp\n\n\nTemporary storage area in which your job(s) run\n\n\nLocal to each worker node. See \nthis section\n below for details.\n\n\n\n\n\n\n$X509_CERT_DIR\n\n\n\n\nLocation of the CA certificates\n\n\nIf not defined, defaults to \n/etc/grid-security/certificates\n.\n\n\n\n\n\n\n$_CONDOR_SCRATCH_DIR\n\n\n\n\nSuggested temporary storage for glideinWMS-based payloads.\n\n\nUsers should prefer this environment variable over \n$OSG_WN_TMP\n if running inside glideinWMS.\n\n\n\n\n\n\n\n\nOSG_WN_TMP\n\n\nSite administrators are responsible for ensuring that \n$OSG_WN_TMP\n is cleaned up.\nWe recommend one of the following solutions:\n\n\n\n\n\n\n(Recommended)\n Use batch-system capabilities to create directories in the job scratch directory and bind mount\n  them for the job so that the batch system performs the clean up.\n  For example, HTCondor has this ability through\n  \nMOUNT_UNDER_SCRATCH\n:\n\n\nMOUNT_UNDER_SCRATCH\n \n=\n \n$(\nMOUNT_UNDER_SCRATCH\n)\n, \nPATH TO OSG_WN_TMP\n\n\n\n\n\n\nIf using this method, space set aside for \nOSG_WN_TMP\n should be reallocated to the partition containing the job\nscratch directories.\nIf using HTCondor, this will be the partition containing the path defined by the HTCondor \nEXECUTE\n configuration\nvariable.\n\n\n\n\n\n\nUse batch-system capabilities to create a temporary, per-job directory that is cleaned up after each job is run.\n\n\n\n\nPeriodically purge the directory (e.g. \ntmpwatch\n).\n\n\n\n\nFor VO managers\n\n\n\n\nNote\n\n\nThe following advice applies to VO managers or maintainers of \npilot\n software; end-users should contact their VO\nfor the proper locations to stage temporary work (often, this will be either \n$TMPDIR\n or \n$_CONDOR_SCRATCH_DIR\n).\n\n\n\n\nBe careful with using \n$OSG_WN_TMP\n; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution:\n\n\nmkdir -p \n$OSG_WN_TMP\n/MYVO\n\nexport\n \nmydir\n=\n`\nmktemp -d -t MYVO\n`\n\n\ncd\n \n$mydir\n\n\n# Run the rest of your application\n\nrm -rf \n$mydir\n\n\n\n\n\n\nThe pilot should utilize \n$TMPDIR\n to communicate the location of temporary storage to payloads.\n\n\nA significant number of sites use the batch system to make an independent directory for each user job, and change \n$OSG_WN_TMP\n on the fly to point to this directory.\n\n\nThere is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "Overview"
        }, 
        {
            "location": "/worker-node/using-wn/#worker-node-overview", 
            "text": "The Worker Node Client is a collection of useful software components that is expected to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that can be used to locate resources.  This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.  The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself (preferred; most portable) or utilize CVMFS. Sites are not required to provide any specific tools ( gcc ,  lapack ,  blas , etc.) beyond the ones in the OSG worker node client and the base OS.  If you would like to test the minimal OS environment that jobs can expect, you can test out your scientific software in  the OSG Docker image .", 
            "title": "Worker Node Overview"
        }, 
        {
            "location": "/worker-node/using-wn/#hardware-recommendations", 
            "text": "Hardware  Minimum  Recommended  Notes      Core per pilot  1  8  Depends on the supported VOs. The total core count on every node in the cluster must be divisible by core per pilot.    Memory per core  1024MB  2048MB  Memory per core times core per pilot needs to be less than the total memory on every node. Do not overcommit.    Scratch disk per core ( OSG_WN_TMP )  2GB  10 GB  This can be overcommitted if a mix of different VO jobs is expected.    CVMFS  Cache  (optional)  10 GB  20 GB  This is a value per node and not per core.", 
            "title": "Hardware Recommendations"
        }, 
        {
            "location": "/worker-node/using-wn/#common-software-available-on-worker-nodes", 
            "text": "The OSG worker node environment contains the following software:   The supported set of CA certificates (located in  $X509_CERT_DIR  after the environment is set up)  Proxy management tools:  Create proxies:  voms-proxy-init  and  grid-proxy-init  Show proxy info:  voms-proxy-info  and  grid-proxy-info  Destroy the current proxy:  voms-proxy-destroy  and  grid-proxy-destroy    Data transfer tools:  HTTP/plain FTP protocol tools (via system dependencies):  wget  and  curl : standard tools for downloading files with HTTP and FTP    Transfer clients  GFAL -based client ( gfal-copy  and others).  GFAL supports SRM, GridFTP, and HTTP protocols.  Globus GridFTP client ( globus-url-copy )      MyProxy client tools   At some sites, these tools may not be available at the pilot launch.  To setup the environment, do the following:  user@host $   source   $OSG_GRID /setup.sh  This should be done by a pilot job, not by the end-user payload.", 
            "title": "Common Software Available on Worker Nodes"
        }, 
        {
            "location": "/worker-node/using-wn/#the-worker-node-environment", 
            "text": "The following table outlines the various important directories and information in the worker node environment.\nA job running on an OSG worker node can refer to each directory using the corresponding environment variable.\nSeveral of them are defined as options in your OSG-Configure  .ini  files in  /etc/osg/config.d .\nCustom variables and those that aren't listed may be defined in the  Local Settings section .     Environment Variable  OSG-Configure section/option  Purpose  Notes      $OSG_GRID  Storage / grid_dir  Location of additional environment variables.  Pilots should source  $OSG_GRID/setup.sh  in order to guarantee the environment contains the worker node binaries in  $PATH .    $OSG_SQUID_LOCATION ,  Squid / location  Location of a HTTP caching proxy server  Utilize this service for downloading files via HTTP for cache-friendly workflows.    $OSG_WN_TMP  Storage / worker_node_temp  Temporary storage area in which your job(s) run  Local to each worker node. See  this section  below for details.    $X509_CERT_DIR   Location of the CA certificates  If not defined, defaults to  /etc/grid-security/certificates .    $_CONDOR_SCRATCH_DIR   Suggested temporary storage for glideinWMS-based payloads.  Users should prefer this environment variable over  $OSG_WN_TMP  if running inside glideinWMS.", 
            "title": "The Worker Node Environment"
        }, 
        {
            "location": "/worker-node/using-wn/#osg_wn_tmp", 
            "text": "Site administrators are responsible for ensuring that  $OSG_WN_TMP  is cleaned up.\nWe recommend one of the following solutions:    (Recommended)  Use batch-system capabilities to create directories in the job scratch directory and bind mount\n  them for the job so that the batch system performs the clean up.\n  For example, HTCondor has this ability through\n   MOUNT_UNDER_SCRATCH :  MOUNT_UNDER_SCRATCH   =   $( MOUNT_UNDER_SCRATCH ) ,  PATH TO OSG_WN_TMP   If using this method, space set aside for  OSG_WN_TMP  should be reallocated to the partition containing the job\nscratch directories.\nIf using HTCondor, this will be the partition containing the path defined by the HTCondor  EXECUTE  configuration\nvariable.    Use batch-system capabilities to create a temporary, per-job directory that is cleaned up after each job is run.   Periodically purge the directory (e.g.  tmpwatch ).", 
            "title": "OSG_WN_TMP"
        }, 
        {
            "location": "/worker-node/using-wn/#for-vo-managers", 
            "text": "Note  The following advice applies to VO managers or maintainers of  pilot  software; end-users should contact their VO\nfor the proper locations to stage temporary work (often, this will be either  $TMPDIR  or  $_CONDOR_SCRATCH_DIR ).   Be careful with using  $OSG_WN_TMP ; at some sites, this directory might be shared with other VOs. We recommend creating a new sub-directory as a precaution:  mkdir -p  $OSG_WN_TMP /MYVO export   mydir = ` mktemp -d -t MYVO `  cd   $mydir  # Run the rest of your application \nrm -rf  $mydir   The pilot should utilize  $TMPDIR  to communicate the location of temporary storage to payloads.  A significant number of sites use the batch system to make an independent directory for each user job, and change  $OSG_WN_TMP  on the fly to point to this directory.  There is no way to know in advance how much scratch disk space any given worker node has available; recall, what disk space is available may be shared among a number of job slots.", 
            "title": "For VO managers"
        }, 
        {
            "location": "/worker-node/install-wn/", 
            "text": "Installing the Worker Node Client From RPMs\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found \nhere\n.\n\n\n\n\nNote\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on your local site:\n\n\n\n\nInstall using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\nUse from OASIS\n - useful when worker nodes already mount \nCVMFS\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker\nNode Client software available from an RPM.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall the Worker Node Client\n\n\nInstall the Worker Node Client RPM:\n\n\nroot@host #\n yum install osg-wn-client\n\n\n\n\n\nServices\n\n\nFetch-CRL is the only service required to support the WN Client.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nfetch-crl-boot\n will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in \n/etc/fetch-crl.conf\n:\n\n\nnoerrors\n\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to \n/etc/osg/wn-client\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nPlease see the documentation on using \nYum and RPM\n, and using the \nOSG Yum repositories\n.\n\n\nWorker node contents\n\n\nThe worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:\n\n\n\n\nOSG Certificates\n\n\ncurl\n\n\nFetch CRL\n\n\nFTS client\n\n\ngfal2\n\n\nglobus-url-copy (GridFTP client)\n\n\nglobus-xio-udt-driver\n\n\nldapsearch\n\n\nMyProxy\n\n\nosg-system-profiler\n\n\nUberFTP\n\n\nvo-client (includes /etc/vomses file)\n\n\nVOMS client\n\n\nwget\n\n\nxrdcp\n\n\n\n\nTo see the currently installed version of the worker node package, run the following command:\n\n\nroot@host #\n rpm -q --requires osg-wn-client\n\n\n\n\n\nClick \nhere\n for more details on using RPM to see what was installed.", 
            "title": "Install from RPM"
        }, 
        {
            "location": "/worker-node/install-wn/#installing-the-worker-node-client-from-rpms", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found  here .   Note  It is possible to install the Worker Node Client software in a variety of ways, depending on your local site:   Install using RPMs and Yum (this guide) - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs  Use from OASIS  - useful when worker nodes already mount  CVMFS  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes    This document is intended to guide system administrators through the process of configuring a site to make the Worker\nNode Client software available from an RPM.", 
            "title": "Installing the Worker Node Client From RPMs"
        }, 
        {
            "location": "/worker-node/install-wn/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn/#install-the-worker-node-client", 
            "text": "Install the Worker Node Client RPM:  root@host #  yum install osg-wn-client", 
            "title": "Install the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#services", 
            "text": "Fetch-CRL is the only service required to support the WN Client.     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info      Note  fetch-crl-boot  will begin fetching CRLS, which can take a few minutes and fail on transient errors. You can add configuration to ignore these transient errors in  /etc/fetch-crl.conf :  noerrors    As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service SERVICE-NAME start    Stop a service  service SERVICE-NAME stop    Enable a service to start during boot  chkconfig SERVICE-NAME on    Disable a service from starting during boot  chkconfig SERVICE-NAME off", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to  /etc/osg/wn-client", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn/#reference", 
            "text": "Please see the documentation on using  Yum and RPM , and using the  OSG Yum repositories .", 
            "title": "Reference"
        }, 
        {
            "location": "/worker-node/install-wn/#worker-node-contents", 
            "text": "The worker node may be updated from time to time. As of OSG 3.3.21 in February 2017, the OSG worker node client contains:   OSG Certificates  curl  Fetch CRL  FTS client  gfal2  globus-url-copy (GridFTP client)  globus-xio-udt-driver  ldapsearch  MyProxy  osg-system-profiler  UberFTP  vo-client (includes /etc/vomses file)  VOMS client  wget  xrdcp   To see the currently installed version of the worker node package, run the following command:  root@host #  rpm -q --requires osg-wn-client  Click  here  for more details on using RPM to see what was installed.", 
            "title": "Worker node contents"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/", 
            "text": "Installing the Worker Node Client via Tarball\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found \nhere\n.\n\n\n\n\nNote\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on your local site:\n\n\n\n\nInstall using a tarball (this guide) - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nUse from OASIS\n - useful when worker nodes already mount \nCVMFS\n\n\nInstall using RPMs and Yum\n - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\n\n\n\n\nThis document is intended to guide users through the process of installing the worker node software and configuring the\ninstalled worker node software.  Although this document is oriented to system administrators, any unprivileged user\nmay install and use the client.\n\n\nBefore starting, ensure the host has \na supported operating system\n.\n\n\nDownload the WN Client\n\n\nPlease pick the \nosg-wn-client\n tarball that is appropriate for your distribution and architecture. You will find them in \nhttps://repo.opensciencegrid.org/tarball-install/\n .\n\n\nFor OSG 3.4:\n\n\n\n\nBinaries for 64-bit RHEL6\n\n\nBinaries for RHEL7\n\n\n\n\nInstall the WN Client\n\n\n\n\nUnpack the tarball.\n\n\nMove the directory that was created to where you want the tarball client to be.\n\n\nRun \nosg-post-install\n (\nPATH_TO_CLIENT\n/osg/osg-post-install\n) to fix the directories in the installation.\n\n\nSource the setup \nsource \nPATH_TO_CLIENT\n/setup.sh\n (or \nsetup.csh\n depending on the shell).\n\n\nDownload and set up CA certificates using \nosg-ca-manage\n (See the \nCA management documentation\n for the available options).\n\n\nDownload CRLs using \nfetch-crl\n.\n\n\n\n\n\n\nWarning\n\n\nOnce \nosg-post-install\n is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.\n\n\n\n\nExample installation (in \n/home/user/test-install\n, the \nPATH_TO_CLIENT\n/\n is \n/home/user/test-install/osg-wn-client\n ):\n\n\nroot@host #\n mkdir /home/user/test-install\n\nroot@host #\n \ncd\n /home/user/test-install\n\nroot@host #\n wget http://repo.opensciencegrid.org/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz\n\nroot@host #\n tar xzf osg-wn-client-latest.el6.x86_64.tar.gz\n\nroot@host #\n \ncd\n osg-wn-client\n\nroot@host #\n ./osg/osg-post-install\n\nroot@host #\n \nsource\n setup.sh\n\nroot@host #\n osg-ca-manage setupCA --url osg\n\nroot@host #\n fetch-crl\n\n\n\n\n\nConfigure the CE\n\n\nUsing the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.\n\n\nSet \ngrid_dir\n in the \nStorage\n section of your OSG-Configure configs: \nCE configuration instructions\n. \ngrid_dir\n is used as the \n$OSG_GRID\n environment variable in running jobs - see the \nworker node environment document\n. Pilot jobs source \n$OSG_GRID/setup.sh\n before performing any work. The value set for \ngrid_dir\n must be the path of the wn-client installation directory. This is the path returned by \necho $OSG_LOCATION\n once you source the setup file created by this installation.\n\n\nServices\n\n\nThe WN client is a collection of client programs that do not require service startup or shutdown. The only services are \nosg-update-certs\n that keeps the CA certificates up-to-date and \nfetch-crl\n that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.\n\n\nAuto-updating certificates and CRLs\n\n\nYou must create cron jobs to run \nfetch-crl\n and \nosg-update-certs\n to update your CRLs and certificates automatically.\n\n\nHere is what they should look like. (Note: fill in \nOSG_LOCATION\n with the full path of your tarball install, including \nosg-wn-client\n that is created by the tarball).\n\n\n# Cron job to update certs.\n# Runs every hour by default, though does not update certs until they\nre at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( . \nOSG_LOCATION\n/setup.sh \n osg-update-certs --random-sleep 2700 --called-from-cron )\n\n\n\n\n\n# Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( . \nOSG_LOCATION\n/setup.sh \n fetch-crl -q -r 360 )\n\n\n\n\n\nYou might want to configure proxy settings in \n$OSG_LOCATION/etc/fetch-crl.conf\n.\n\n\nEnabling and Disabling Services\n\n\nTo enable the CRL updates, you must edit your cron with \ncrontab -e\n and add the lines above.  To disable, remove\nthe lines from the \ncrontab\n.\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \n$OSG_GRID\n is set to the directory of your worker node client installation\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Install from Tarball"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#installing-the-worker-node-client-via-tarball", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found  here .   Note  It is possible to install the Worker Node Client software in a variety of ways, depending on your local site:   Install using a tarball (this guide) - useful when installing onto a shared filesystem for distribution to worker nodes  Use from OASIS  - useful when worker nodes already mount  CVMFS  Install using RPMs and Yum  - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs    This document is intended to guide users through the process of installing the worker node software and configuring the\ninstalled worker node software.  Although this document is oriented to system administrators, any unprivileged user\nmay install and use the client.  Before starting, ensure the host has  a supported operating system .", 
            "title": "Installing the Worker Node Client via Tarball"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#download-the-wn-client", 
            "text": "Please pick the  osg-wn-client  tarball that is appropriate for your distribution and architecture. You will find them in  https://repo.opensciencegrid.org/tarball-install/  .  For OSG 3.4:   Binaries for 64-bit RHEL6  Binaries for RHEL7", 
            "title": "Download the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#install-the-wn-client", 
            "text": "Unpack the tarball.  Move the directory that was created to where you want the tarball client to be.  Run  osg-post-install  ( PATH_TO_CLIENT /osg/osg-post-install ) to fix the directories in the installation.  Source the setup  source  PATH_TO_CLIENT /setup.sh  (or  setup.csh  depending on the shell).  Download and set up CA certificates using  osg-ca-manage  (See the  CA management documentation  for the available options).  Download CRLs using  fetch-crl .    Warning  Once  osg-post-install  is run to relocate the install, it cannot be run again.  You will need to unpack a fresh copy.   Example installation (in  /home/user/test-install , the  PATH_TO_CLIENT /  is  /home/user/test-install/osg-wn-client  ):  root@host #  mkdir /home/user/test-install root@host #   cd  /home/user/test-install root@host #  wget http://repo.opensciencegrid.org/tarball-install/3.4/osg-wn-client-latest.el6.x86_64.tar.gz root@host #  tar xzf osg-wn-client-latest.el6.x86_64.tar.gz root@host #   cd  osg-wn-client root@host #  ./osg/osg-post-install root@host #   source  setup.sh root@host #  osg-ca-manage setupCA --url osg root@host #  fetch-crl", 
            "title": "Install the WN Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#configure-the-ce", 
            "text": "Using the wn-client software installed from the tarball will require a few changes on the compute element so that the resource's configuration can be correctly reported.  Set  grid_dir  in the  Storage  section of your OSG-Configure configs:  CE configuration instructions .  grid_dir  is used as the  $OSG_GRID  environment variable in running jobs - see the  worker node environment document . Pilot jobs source  $OSG_GRID/setup.sh  before performing any work. The value set for  grid_dir  must be the path of the wn-client installation directory. This is the path returned by  echo $OSG_LOCATION  once you source the setup file created by this installation.", 
            "title": "Configure the CE"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#services", 
            "text": "The WN client is a collection of client programs that do not require service startup or shutdown. The only services are  osg-update-certs  that keeps the CA certificates up-to-date and  fetch-crl  that keeps the CRLs up-to-date. Following the instructions below you'll add the services to your crontab that will take care to run them periodically until you remove them.", 
            "title": "Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#auto-updating-certificates-and-crls", 
            "text": "You must create cron jobs to run  fetch-crl  and  osg-update-certs  to update your CRLs and certificates automatically.  Here is what they should look like. (Note: fill in  OSG_LOCATION  with the full path of your tarball install, including  osg-wn-client  that is created by the tarball).  # Cron job to update certs.\n# Runs every hour by default, though does not update certs until they re at\n# least 24 hours old.  There is a random sleep time for up to 45 minutes (2700\n# seconds) to avoid overloading cert servers.\n10 * * * *   ( .  OSG_LOCATION /setup.sh   osg-update-certs --random-sleep 2700 --called-from-cron )  # Cron job to update CRLs\n# Runs every 6 hours at, 45 minutes +/- 3 minutes.\n42 */6 * * *   ( .  OSG_LOCATION /setup.sh   fetch-crl -q -r 360 )  You might want to configure proxy settings in  $OSG_LOCATION/etc/fetch-crl.conf .", 
            "title": "Auto-updating certificates and CRLs"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#enabling-and-disabling-services", 
            "text": "To enable the CRL updates, you must edit your cron with  crontab -e  and add the lines above.  To disable, remove\nthe lines from the  crontab .", 
            "title": "Enabling and Disabling Services"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  $OSG_GRID  is set to the directory of your worker node client installation", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-tarball/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/", 
            "text": "Installing the Worker Node Client via OASIS\n\n\nThe \nOSG Worker Node Client\n is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found \nhere\n.\n\n\n\n\nNote\n\n\nIt is possible to install the Worker Node Client software in a variety of ways, depending on your local site:\n\n\n\n\nUse from OASIS (this guide) - useful when \nCVMFS\n is already mounted on your worker nodes\n\n\nInstall using a tarball\n - useful when installing onto a shared filesystem for distribution to worker nodes\n\n\nInstall using RPMs and Yum\n - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs\n\n\n\n\n\n\nThis document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nOn every worker node, \ninstall and configure CVMFS\n\n\n\n\nOnce configured to use OASIS, grid jobs will download the worker-node software on demand (into the local disk cache).\nThis may result in extra network activity, especially on first use of the client tools.\n\n\nConfigure the CE\n\n\nDetermine the OASIS path to the Worker Node Client software for your worker nodes:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nUse\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\n/cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64\n\n\n\n\n\n\n\n\nOn the CE, in the \n/etc/osg/config.d/10-storage.ini\n file, set the \ngrid_dir\n configuration setting to the path from the previous step.\n\n\nOnce you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:\n\n\nroot@host #\n osg-configure -v\n\nroot@host #\n osg-configure -c\n\n\n\n\n\nFor more information, see the \nOSG worker node environment documentation\n and the\n\nCE configuration instructions\n.\n\n\nValidating the Worker Node Client\n\n\nTo verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.\n\n\n\n\nSubmit a job that executes the \nenv\n command (e.g. Run \ncondor_ce_trace\n with the \n-d\n flag from your HTCondor CE)\n\n\nVerify that the value of \nOSG_GRID\n is set to the directory of your WN Client installation\n\n\n\n\nManually Using the Worker Node Client From OASIS\n\n\nIf you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:\n\n\n\n\n\n\n\n\nWorker Node OS\n\n\nRun the following command\u2026\n\n\n\n\n\n\n\n\n\n\nEL\u00a06 (32-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh\n\n\n\n\n\n\nEL\u00a06 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh\n\n\n\n\n\n\nEL\u00a07 (64-bit)\n\n\nsource /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nSome possible issues that may come up:\n\n\n\n\n\n\nA missing softlink to the CA certs directory. To check this, run:\n\n\nuser@host $\n ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/\n\n\n\n\n\nand check that \ncertificates\n is linked to somewhere. The fix is to yum update the \noasis-config\n package to version 4 or higher. A known workaround is to run:\n\n\nuser@host $\n \nexport\n \nX509_CERT_DIR\n=\n/cvmfs/oasis.opensciencegrid.org/mis/certificates\n\n\n\n\n\nbefore any commands.\n\n\n\n\n\n\nHow to get Help?\n\n\nTo get assistance please use this \nHelp Procedure\n.", 
            "title": "Install from OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#installing-the-worker-node-client-via-oasis", 
            "text": "The  OSG Worker Node Client  is a collection of software components that is expected to be added to every worker node\nthat can run OSG jobs. It provides a common environment and a minimal set of common tools that all OSG jobs can expect\nto use. Contents of the worker node client can be found  here .   Note  It is possible to install the Worker Node Client software in a variety of ways, depending on your local site:   Use from OASIS (this guide) - useful when  CVMFS  is already mounted on your worker nodes  Install using a tarball  - useful when installing onto a shared filesystem for distribution to worker nodes  Install using RPMs and Yum  - useful when managing your worker nodes with a tool (e.g., Puppet, Chef) that can automate RPM installs    This document is intended to guide system administrators through the process of configuring a site to make the Worker Node Client software available from OASIS.", 
            "title": "Installing the Worker Node Client via OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  On every worker node,  install and configure CVMFS   Once configured to use OASIS, grid jobs will download the worker-node software on demand (into the local disk cache).\nThis may result in extra network activity, especially on first use of the client tools.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#configure-the-ce", 
            "text": "Determine the OASIS path to the Worker Node Client software for your worker nodes:     Worker Node OS  Use\u2026      EL\u00a06 (32-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386    EL\u00a06 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64    EL\u00a07 (64-bit)  /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64     On the CE, in the  /etc/osg/config.d/10-storage.ini  file, set the  grid_dir  configuration setting to the path from the previous step.  Once you finish making changes to configuration files on your CE, validate, fix, and apply the configuration:  root@host #  osg-configure -v root@host #  osg-configure -c  For more information, see the  OSG worker node environment documentation  and the CE configuration instructions .", 
            "title": "Configure the CE"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#validating-the-worker-node-client", 
            "text": "To verify functionality of the worker node client, you will need to submit a test job against your CE and verify the job's output.   Submit a job that executes the  env  command (e.g. Run  condor_ce_trace  with the  -d  flag from your HTCondor CE)  Verify that the value of  OSG_GRID  is set to the directory of your WN Client installation", 
            "title": "Validating the Worker Node Client"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#manually-using-the-worker-node-client-from-oasis", 
            "text": "If you must log onto a worker node and use the Worker Node Client software directly during your login session, consult the following table for the command to set up your environment:     Worker Node OS  Run the following command\u2026      EL\u00a06 (32-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-i386/setup.sh    EL\u00a06 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el6-x86_64/setup.sh    EL\u00a07 (64-bit)  source /cvmfs/oasis.opensciencegrid.org/osg-software/osg-wn-client/3.3/current/el7-x86_64/setup.sh", 
            "title": "Manually Using the Worker Node Client From OASIS"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#troubleshooting", 
            "text": "Some possible issues that may come up:    A missing softlink to the CA certs directory. To check this, run:  user@host $  ls -l /cvmfs/oasis.opensciencegrid.org/mis/osg-wn-client/3.3/current/el6-x86_64/etc/grid-security/  and check that  certificates  is linked to somewhere. The fix is to yum update the  oasis-config  package to version 4 or higher. A known workaround is to run:  user@host $   export   X509_CERT_DIR = /cvmfs/oasis.opensciencegrid.org/mis/certificates  before any commands.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/worker-node/install-wn-oasis/#how-to-get-help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/", 
            "text": "Installing and Maintaining the CernVM File System Client\n\n\nThe CernVM File System (\nCVMFS\n) is an HTTP-based file distribution service\nused to provide data and software for jobs.\nBy installing CVMFS, you have access to an alternative installation method for required worker node software and your\nsite you will be able to support a wider range of user jobs.\nFor example, CVMFS provides easy access to the following:\n\n\n\n\nThe \nworker node client\n\n\nCA and VO security data\n\n\nSoftware used by VOs\n\n\nData stored in \nStashCache\n.\n\n\n\n\nUse this page to learn how to install, configure, run, test, and troubleshoot the CVMFS client from the OSG software\nrepositories.\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are OSG Version \n= 3.4.3.\nThe version of CVMFS installed should be \n= 2.4.1\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \ncvmfs\n Linux user\n\n\nGroup IDs:\n If they do not exist already, the installation will create the Linux groups \ncvmfs\n and \nfuse\n\n\nNetwork ports:\n You will need network access to a local squid server such as the \nsquid distributed by OSG\n. The squid will need out-bound access to cvmfs stratum 1 servers.\n\n\nHost choice:\n -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details \nbelow\n)\n\n\nFUSE\n: CVMFS requires FUSE\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling CVMFS\n\n\nThe following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall CVMFS software:\n\n\nroot@host #\n yum install osg-oasis\n\n\n\n\n\n\n\n\n\nAutomount setup\n\n\nCVMFS uses automount, and the steps to configure it are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:\n\n\n\n\nFor EL6 hosts\n\n\nFor EL7 hosts\n\n\n\n\nFor EL6 hosts\n\n\n\n\n\n\nIf automount is not yet in use on the system, do the following:\n\n\nroot@host #\n chkconfig autofs on\n\nroot@host #\n service autofs start\n\n\n\n\n\n\n\n\n\nCreate or edit \n/etc/auto.master\n to have the following contents:\n\n\n/\ncvmfs\n \n/\netc\n/\nauto\n.\ncvmfs\n\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nroot@host #\n service autofs restart\n\n\n\n\n\n\n\n\n\nFor EL7 hosts\n\n\n\n\n\n\nIf automount is not yet in use on the system, do the following:\n\n\nroot@host #\n systemctl \nenable\n autofs\n\nroot@host #\n systemctl start autofs\n\n\n\n\n\n\n\n\n\nPut the following in \n/etc/auto.master.d/cvmfs.autofs\n:\n\n\n/\ncvmfs\n \n/\netc\n/\nauto\n.\ncvmfs\n\n\n\n\n\n\n\n\n\n\nRestart autofs to make the change take effect:\n\n\nroot@host #\n systemctl restart autofs\n\n\n\n\n\n\n\n\n\nConfiguring CVMFS\n\n\nCreate or edit \n/etc/cvmfs/default.local\n, a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to \nedit the parts in angle brackets\n. In\nparticular, the \nCVMFS_HTTP_PROXY\n line below must be edited for your\nsite.\n\n\nCVMFS_REPOSITORIES\n=\n`echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr \n \n ,`\n\n\nCVMFS_QUOTA_LIMIT\n=\nQUOTA\n \nLIMIT\n\n\nCVMFS_HTTP_PROXY\n=\nSQUID URL\n:\nSQUID PORT\n\n\n\n\n\n\nCVMFS by default allows any repository to be mounted, no matter what\nthe setting of \nCVMFS_REPOSITORIES\n is; that variable is used by support\ntools such as \ncvmfs_config\n and \ncvmfs_talk\n when they need to know a\nlist of repositories.  The recommended \nCVMFS_REPOSITORIES\n setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.\n\n\nSet up a list of CVMFS HTTP proxies to retrieve from in\n\nCVMFS_HTTP_PROXY\n. If you do not have any squid at your site follow\nthe instructions to \ninstall squid from OSG\n.\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. For example:\n\n\nCVMFS_HTTP_PROXY\n=\nhttp://squid1.example.com:3128|http://squid2.example.com:3128;http://old-squid.example.com:3128\n\n\n\n\n\n\nA special proxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.\n\n\nSet up the cache limit in \nCVMFS_QUOTA_LIMIT\n (in Megabytes). The\nrecommended value for most applications is \n20000\n MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in \n/var/lib/cvmfs\n by default; to override the\nlocation, set \nCVMFS_CACHE_BASE\n in \n/etc/cvmfs/default.local\n. Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in \n$CVMFS_CACHE_BASE/osgstorage\n. To be safe, make\nsure that at least 20% more than \n$CVMFS_QUOTA_LIMIT\n + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.\n\n\n\n\nWarning\n\n\nIf you use SELinux and change \nCVMFS_CACHE_BASE\n, then the\nnew cache directory must be labeled with SELinux type\n\ncvmfs_cache_t\n. This can be done by executing the following command:\n\n\nuser@host $\n chcon -R -t cvmfs_cache_t \nCVMFS_CACHE_BASE\n\n\n\n\n\n\n\n\nValidating CVMFS\n\n\nAfter CVMFS is installed, you should be able to see the \n/cvmfs\n\ndirectory. But note that it will initially appear to be empty:\n\n\nuser@host $\n ls /cvmfs\n\nuser@host $\n\n\n\n\n\n\nDirectories within \n/cvmfs\n will not be mounted until you examine them. For instance:\n\n\nuser@host $\n ls /cvmfs\n\nuser@host $\n ls -l /cvmfs/atlas.cern.ch\n\ntotal 1\n\n\ndrwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo\n\n\nuser@host $\n ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft\n\ntotal 1\n\n\nlrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -\n /cvmfs/cms.cern.ch\n\n\nuser@host $\n ls -l /cvmfs/glast.egi.eu\n\ntotal 5\n\n\ndrwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast\n\n\nuser@host $\n ls -l /cvmfs/nova.osgstorage.org\n\ntotal 6\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -\n pnfs/fnal.gov/usr/nova/persistent/analysis/\n\n\nlrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -\n pnfs/fnal.gov/usr/nova/data/flux\n\n\ndrwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs\n\n\nuser@host $\n ls /cvmfs\n\natlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org\n\n\nconfig-osg.opensciencegrid.org  nova.osgstorage.org\n\n\n\n\n\n\nTroubleshooting problems\n\n\nIf no directories exist under \n/cvmfs/\n, you can try the following\nsteps to debug:\n\n\n\n\nMount it manually with \nmkdir -p /mnt/cvmfs\n and then\n  \nmount -t cvmfs REPOSITORYNAME /mnt/cvmfs\n where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.\n\n\nIf that doesn't work and doesn't give any explanatory errors, try\n   \ncvmfs_config chksetup\n or \ncvmfs_config showconfig REPOSITORYNAME\n\n   to verify your setup.\n\n\nIf chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.\n\n\nIf you have changed settings in \n/etc/cvmfs/default.local\n, and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of \n/etc/cvmfs/default.conf\n regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.\n\n\nMore things to try are in the\n  \nupstream documentation\n.\n\n\n\n\nStarting and Stopping services\n\n\nOnce it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.\n\n\nCVMFS can be stopped via:\n\n\nroot@host #\n cvmfs_config umount\n\nUnmounting /cvmfs/config-osg.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/atlas.cern.ch: OK\n\n\nUnmounting /cvmfs/oasis.opensciencegrid.org: OK\n\n\nUnmounting /cvmfs/glast.egi.eu: OK\n\n\nUnmounting /cvmfs/nova.osgstorage.org: OK\n\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, there are several ways to receive help:\n\n\n\n\nFor bug reporting and OSG-specific issues, see our \nhelp procedure\n\n\nFor community support and best-effort software team support contact\n   \n.\n\n\nFor general CernVM File System support contact \n.\n\n\n\n\nReferences\n\n\n\n\nhttp://cernvm.cern.ch/portal/filesystem/techinformation\n\n\nhttps://cvmfs.readthedocs.io/en/latest/\n\n\n\n\nUsers and Groups\n\n\nThis installation will create one user unless it already exists\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\n\n\n\n\n\n\nThe installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse RPM is not\nalready installed, installing cvmfs will also install fuse and\nthat will create another group:\n\n\n\n\n\n\n\n\nGroup\n\n\nComment\n\n\nGroup members\n\n\n\n\n\n\n\n\n\n\ncvmfs\n\n\nCernVM-FS service account\n\n\nnone\n\n\n\n\n\n\nfuse\n\n\nFUSE service account\n\n\ncvmfs", 
            "title": "Install CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#installing-and-maintaining-the-cernvm-file-system-client", 
            "text": "The CernVM File System ( CVMFS ) is an HTTP-based file distribution service\nused to provide data and software for jobs.\nBy installing CVMFS, you have access to an alternative installation method for required worker node software and your\nsite you will be able to support a wider range of user jobs.\nFor example, CVMFS provides easy access to the following:   The  worker node client  CA and VO security data  Software used by VOs  Data stored in  StashCache .   Use this page to learn how to install, configure, run, test, and troubleshoot the CVMFS client from the OSG software\nrepositories.   Applicable versions  The applicable software versions for this document are OSG Version  = 3.4.3.\nThe version of CVMFS installed should be  = 2.4.1", 
            "title": "Installing and Maintaining the CernVM File System Client"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If it does not exist already, the installation will create the  cvmfs  Linux user  Group IDs:  If they do not exist already, the installation will create the Linux groups  cvmfs  and  fuse  Network ports:  You will need network access to a local squid server such as the  squid distributed by OSG . The squid will need out-bound access to cvmfs stratum 1 servers.  Host choice:  -  Sufficient (~20GB+20%) cache space reserved, preferably in a separate filesystem (details  below )  FUSE : CVMFS requires FUSE   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#installing-cvmfs", 
            "text": "The following will install CVMFS from the OSG yum repository. It will also install fuse and autofs if you do not have them, and it will install the configuration for the OSG CVMFS distribution which is called OASIS. To simplify installation, OSG provides convenience RPMs that install all required software with a single command.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install CVMFS software:  root@host #  yum install osg-oasis", 
            "title": "Installing CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#automount-setup", 
            "text": "CVMFS uses automount, and the steps to configure it are different on EL6 vs EL7. Follow the section that is appropriate for your host's OS:   For EL6 hosts  For EL7 hosts", 
            "title": "Automount setup"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el6-hosts", 
            "text": "If automount is not yet in use on the system, do the following:  root@host #  chkconfig autofs on root@host #  service autofs start    Create or edit  /etc/auto.master  to have the following contents:  / cvmfs   / etc / auto . cvmfs     Restart autofs to make the change take effect:  root@host #  service autofs restart", 
            "title": "For EL6 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#for-el7-hosts", 
            "text": "If automount is not yet in use on the system, do the following:  root@host #  systemctl  enable  autofs root@host #  systemctl start autofs    Put the following in  /etc/auto.master.d/cvmfs.autofs :  / cvmfs   / etc / auto . cvmfs     Restart autofs to make the change take effect:  root@host #  systemctl restart autofs", 
            "title": "For EL7 hosts"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#configuring-cvmfs", 
            "text": "Create or edit  /etc/cvmfs/default.local , a file that controls the\nCVMFS configuration. Below is a sample configuration, but please note\nthat you will need to  edit the parts in angle brackets . In\nparticular, the  CVMFS_HTTP_PROXY  line below must be edited for your\nsite.  CVMFS_REPOSITORIES = `echo $((echo oasis.opensciencegrid.org;echo cms.cern.ch;ls /cvmfs)|sort -u)|tr     ,`  CVMFS_QUOTA_LIMIT = QUOTA   LIMIT  CVMFS_HTTP_PROXY = SQUID URL : SQUID PORT   CVMFS by default allows any repository to be mounted, no matter what\nthe setting of  CVMFS_REPOSITORIES  is; that variable is used by support\ntools such as  cvmfs_config  and  cvmfs_talk  when they need to know a\nlist of repositories.  The recommended  CVMFS_REPOSITORIES  setting\nabove is so that those tools will use two common repositories plus any\nadditional that have been mounted.  You may want to choose a different\nset of always-known repositories.  Set up a list of CVMFS HTTP proxies to retrieve from in CVMFS_HTTP_PROXY . If you do not have any squid at your site follow\nthe instructions to  install squid from OSG .\nVertical bars separating proxies means to load balance between them\nand try them all before continuing. A semicolon between proxies means\nto try that one only after the previous ones have failed. For example:  CVMFS_HTTP_PROXY = http://squid1.example.com:3128|http://squid2.example.com:3128;http://old-squid.example.com:3128   A special proxy called DIRECT can be placed last in the list to indicate\ndirectly connecting to servers if all other proxies fail. A DIRECT\nproxy is acceptable for small sites but discouraged for large sites\nbecause of the potential load that could be put upon globally shared\nservers.  Set up the cache limit in  CVMFS_QUOTA_LIMIT  (in Megabytes). The\nrecommended value for most applications is  20000  MB. This is the\ncombined limit for all but the osgstorage.org repositories. This cache\nwill be stored in  /var/lib/cvmfs  by default; to override the\nlocation, set  CVMFS_CACHE_BASE  in  /etc/cvmfs/default.local . Note\nthat an additional 1000 MB is allocated for a separate osgstorage.org\nrepositories cache in  $CVMFS_CACHE_BASE/osgstorage . To be safe, make\nsure that at least 20% more than  $CVMFS_QUOTA_LIMIT  + 1000 MB of space\nstays available for CVMFS in that filesystem. This is very important,\nsince if that space is not available it can cause many I/O errors and\napplication crashes. Many system administrators choose to put the\ncache space in a separate filesystem, which is a good way to manage\nit.   Warning  If you use SELinux and change  CVMFS_CACHE_BASE , then the\nnew cache directory must be labeled with SELinux type cvmfs_cache_t . This can be done by executing the following command:  user@host $  chcon -R -t cvmfs_cache_t  CVMFS_CACHE_BASE", 
            "title": "Configuring CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#validating-cvmfs", 
            "text": "After CVMFS is installed, you should be able to see the  /cvmfs \ndirectory. But note that it will initially appear to be empty:  user@host $  ls /cvmfs user@host $   Directories within  /cvmfs  will not be mounted until you examine them. For instance:  user@host $  ls /cvmfs user@host $  ls -l /cvmfs/atlas.cern.ch total 1  drwxr-xr-x 8 cvmfs cvmfs 3 Apr 13 14:50 repo  user@host $  ls -l /cvmfs/oasis.opensciencegrid.org/cmssoft total 1  lrwxrwxrwx 1 cvmfs cvmfs 18 May 13  2015 cms -  /cvmfs/cms.cern.ch  user@host $  ls -l /cvmfs/glast.egi.eu total 5  drwxr-xr-x 9 cvmfs cvmfs 4096 Feb  7  2014 glast  user@host $  ls -l /cvmfs/nova.osgstorage.org total 6  lrwxrwxrwx 1 cvmfs cvmfs   43 Jun 14  2016 analysis -  pnfs/fnal.gov/usr/nova/persistent/analysis/  lrwxrwxrwx 1 cvmfs cvmfs   32 Jan 19 11:40 flux -  pnfs/fnal.gov/usr/nova/data/flux  drwxr-xr-x 3 cvmfs cvmfs 4096 Jan 19 11:39 pnfs  user@host $  ls /cvmfs atlas.cern.ch                   glast.egi.eu         oasis.opensciencegrid.org  config-osg.opensciencegrid.org  nova.osgstorage.org", 
            "title": "Validating CVMFS"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#troubleshooting-problems", 
            "text": "If no directories exist under  /cvmfs/ , you can try the following\nsteps to debug:   Mount it manually with  mkdir -p /mnt/cvmfs  and then\n   mount -t cvmfs REPOSITORYNAME /mnt/cvmfs  where REPOSITORYNAME is\n  the repository, for example config-osg.opensciencegrid.org (which is\n  the best one to try first because other repositories require it to\n  be mounted).  If this works, then CVMFS is working, but there is a\n  problem with automount.  If that doesn't work and doesn't give any explanatory errors, try\n    cvmfs_config chksetup  or  cvmfs_config showconfig REPOSITORYNAME \n   to verify your setup.  If chksetup reports access problems to proxies, it may be caused by\n  access control settings in the squids.  If you have changed settings in  /etc/cvmfs/default.local , and they\n  do not seem to be taking effect, note that there are other\n  configuration files that can override the settings. See the comments\n  at the beginning of  /etc/cvmfs/default.conf  regarding the order in\n  which configuration files are evaluated and look for old files that\n  may have been left from a previous installation.  More things to try are in the\n   upstream documentation .", 
            "title": "Troubleshooting problems"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#starting-and-stopping-services", 
            "text": "Once it is set up, CVMFS is always automatically started when one of\nthe repositories are accessed; there are no system services to start.  CVMFS can be stopped via:  root@host #  cvmfs_config umount Unmounting /cvmfs/config-osg.opensciencegrid.org: OK  Unmounting /cvmfs/atlas.cern.ch: OK  Unmounting /cvmfs/oasis.opensciencegrid.org: OK  Unmounting /cvmfs/glast.egi.eu: OK  Unmounting /cvmfs/nova.osgstorage.org: OK", 
            "title": "Starting and Stopping services"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#how-to-get-help", 
            "text": "If you cannot resolve the problem, there are several ways to receive help:   For bug reporting and OSG-specific issues, see our  help procedure  For community support and best-effort software team support contact\n    .  For general CernVM File System support contact  .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#references", 
            "text": "http://cernvm.cern.ch/portal/filesystem/techinformation  https://cvmfs.readthedocs.io/en/latest/", 
            "title": "References"
        }, 
        {
            "location": "/worker-node/install-cvmfs/#users-and-groups", 
            "text": "This installation will create one user unless it already exists     User  Comment      cvmfs  CernVM-FS service account     The installation will also create a cvmfs group and default the cvmfs\nuser to that group. In addition, if the fuse RPM is not\nalready installed, installing cvmfs will also install fuse and\nthat will create another group:     Group  Comment  Group members      cvmfs  CernVM-FS service account  none    fuse  FUSE service account  cvmfs", 
            "title": "Users and Groups"
        }, 
        {
            "location": "/worker-node/install-singularity/", 
            "text": "Install Singularity\n\n\nSingularity\n is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.\n\n\nKernels with a version 3.10.0-957 or newer include a feature that allows\nsingularity to run completely unprivileged. This kernel version is the\ndefault for RHEL/CentOS/Scientific Linux 7.6 and is available as a\nsecurity update for previous 7.x releases.  Although the feature is\navailable, it needs to be enabled to be usable (instructions below).\nThis kernel version is not available for RHEL 6 and derivatives.\n\n\nWithout this kernel version, singularity must be installed and run with\nsetuid-root executables. Singularity keeps the privileged code to a\n\nminimum\n\nin order to reduce the potential for vulnerabilities.\n\n\nThe OSG has installed singularity in \nOASIS\n,\nso many sites will eventually (after it is supported by VOs) not need to\ninstall singularity locally if they enable it to run unprivileged.\nMeanwhile an RPM installation can be configured to be unprivileged or\nprivileged.\n\n\n\n\nKernel vs. Userspace Security\n\n\nEnabling unprivileged user namespaces increases the risk to the\nkernel. However, the kernel is more widely reviewed than singularity and\nthe additional capability given to users is more limited.\nOSG Security considers the non-setuid, kernel-based method to have a\nlower security risk.\n\n\n\n\nThis document is intended for system administrators that wish to install\nand/or configure singularity.\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host)\nsteps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nIn addition, this is highly recommended for image distribution:\n\n\n\n\nInstall \nCVMFS\n\n\n\n\nChoosing Unprivileged vs Privileged Singularity\n\n\nThere are two sets of instructions on this page:\n\n\n\n\nEnabling Unprivileged Singularity\n\n\nSingularity via RPM\n\n\n\n\nOSG VOs are working to support running singularity directly from OASIS,\nthe OSG Software \nCVMFS distribution\n, when unprivileged\nsingularity is enabled.  At that point sites will not have to install\nthe singularity RPM themselves.  As of April 2019, no VO in the OSG is\nyet ready to do this, but OSG recommends that all RHEL 7.x installations\nenable support for unprivileged singularity and for now also install the\nRPM.  Sites may also choose to configure their RHEL 7.x RPM\ninstallations to run unprivileged.  RHEL 6.x installations have no\noption for unprivileged singularity so there the RPM has to be installed\nand left configured as privileged.\n\n\nIn addition to improved security, unprivileged singularity enables\n\ncondor_ssh_to_job\n to enter a container namespace without itself\nneeding privileges.  Also, unprivileged singularity enables nesting\ncontainers within another container (when the outer container is started\nby singularity 3.x, which is currently in the osg-upcoming repository).\n\n\nOn the other hand, there are a few rare use cases that require\nsingularity to run privileged:\n\n\n\n\n\n\nUsing single-file container images.\n  Some systems, especially\n    High Performance Computing (HPC) systems, deal poorly with\n    collections of small files.  In this case, container images stored\n    in a single file (as opposed to an unpacked directory) may be\n    needed.\n\n\nHowever, known images from OSG VOs are directory-based, and we\n\nrecommend disabling this feature\n on\nprivileged installations in order to avoid potential kernel\nexploits.\n\n\n\n\n\n\nThe overlay feature.\n  The \"overlay\" feature of singularity uses\n    overlayfs to add bind mounts where mount points don't exist in the\n    underlying image.\n\n\nHowever, this feature doesn't work if the image is a directory\ndistributed in CVMFS, singularity has an \"underlay\" feature that is\nequivalent which does work with CVMFS and does not require\nprivileges, and the overlay feature has been a source of security\nvulnerabilities in the past.  For these reasons, \nwe recommend\nreplacing overlay with underlay\n even on\nprivileged installations.\n\n\n\n\n\n\nAllocating new pseudo-tty devices.\n  Support for allocating\n    pseudo-tty devices was accidentally left out of the user namespace\n    support in the RHEL 7.6 kernel.\n\n\nHowever, this feature is only required for a small number of\napplications, and singularity 3.x works around the limitation for\nmost of them without needing privileges.\n\n\n\n\n\n\nEnabling Unprivileged Singularity\n\n\nThe instructions in this section are for enabling singularity to run\nunprivileged.\n\n\nIf the operating system is an EL 7 variant and has been updated to the EL\n7.6 kernel or later, enable unprivileged singularity with the following\nsteps:\n\n\n\n\n\n\nEnable user namespaces via \nsysctl\n:\n\n\nroot@host #\n \necho\n \nuser.max_user_namespaces = 15000\n \n\\\n\n    \n /etc/sysctl.d/90-max_user_namespaces.conf\n\nroot@host #\n sysctl -p /etc/sysctl.d/90-max_user_namespaces.conf\n\n\n\n\n\n\n\n\n\n(Optional) Disable network namespaces:\n\n\nroot@host #\n \necho\n \nuser.max_net_namespaces = 0\n \n\\\n\n    \n /etc/sysctl.d/90-max_net_namespaces.conf\n\nroot@host #\n sysctl -p /etc/sysctl.d/90-max_net_namespaces.conf\n\n\n\n\n\nOSG VOs do not need network namespaces with singularity, and\ndisabling them reduces the risk profile of enabling user\nnamespaces.\n\n\nNetwork namespaces are, however, utilized by other software,\nsuch as Docker.  Disabling network namespaces may break\nother software, or limit its capabilities (such as\nrequiring the \n--net=host\n option in Docker).\n\n\n\n\n\n\nIf you haven't yet installed \nCVMFS\n, do so.\n\n\n\n\n\n\nValidating Unprivileged Singularity\n\n\nOnce you have the host configured properly, log in as an ordinary\nunprivileged user and verify that singularity in OASIS works:\n\n\nuser@host $\n /cvmfs/oasis.opensciencegrid.org/mis/singularity/bin/singularity \n\\\n\n                \nexec\n --contain --ipc --pid --bind /cvmfs \n\\\n\n                /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 \n\\\n\n                ps -ef\n\nWARNING: Container does not have an exec helper script, calling \nps\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  2 21:27 ?        00:00:00 shim-init\n\n\nuser         2     1  0 21:27 ?        00:00:00 ps -ef\n\n\n\n\n\n\nSingularity via RPM\n\n\nThe instructions in this section are for the singularity RPM, which \nincludes setuid-root executables.  The setuid-root executables can\nhowever be disabled by configuration, details below.\n\n\nInstalling Singularity via RPM\n\n\nTo install the singularity RPM, make sure that your host is up to date\nbefore installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall Singularity\n\n\nroot@host #\n yum install singularity\n\n\n\n\n\n\n\n\n\nConfiguring Singularity\n\n\nsingularity includes an option called \nunderlay\n that enables using bind\nmount points that do not exist in the container image.\nBy default it is enabled, but only if the similar \noverlay\n option cannot\nbe used, such as on RHEL6 where kernel support for overlayfs is missing\nor when running in unprivileged mode.  On RHEL7 it is recommended to\ncompletely disable \noverlay\n, because it is more vulnerable to security\nproblems than \nunderlay\n and because it does not work correctly when\ncontainer images are distributed by CVMFS.\n\n\nSet this option in \n/etc/singularity/singularity.conf\n:\n\n\n    \nenable\n \noverlay\n \n=\n \nno\n\n\n\n\n\n\n\n\nWarning\n\n\nIf you modify \n/etc/singularity/singularity.conf\n, be careful with\nyour upgrade procedures.\nRPM will not automatically merge your changes with new upstream\nconfiguration keys, which may cause a broken install or inadvertently\nchange the site configuration.  Singularity changes its default\nconfiguration file more frequently than typical OSG software.\n\n\nLook for \nsingularity.conf.rpmnew\n after upgrades and merge in any\nchanges to the defaults.\n\n\n\n\nConfiguring the RPM to be Unprivileged\n\n\nIf you choose to run the RPM unprivileged, after\n\nenabling unprivileged singularity\n,\nchange the line in \n/etc/singularity/singularity.conf\n that says\n\nallow setuid = yes\n to\n\n\n    \nallow\n \nsetuid\n \n=\n \nno\n\n\n\n\n\n\nNote that the setuid-root executables stay installed, but they will exit\nvery early if invoked when the configuration file disallows setuid, so\nthe risk is very low.  There are non-setuid equivalent executables that\nare used instead when setuid is disallowed.\n\n\nLimiting Image Types\n\n\nA side effect of disabling privileged singularity is that loopback\nmounts are disallowed.  If the installation is privileged, also consider\nthe following.\n\n\nImages based on loopback devices carry an inherently higher exposure to\nunknown kernel exploits compared to directory-based images distributed via\nCVMFS.  See \nthis article\n for further\ndiscussion.\n\n\nThe loopback-based images are the default image type produced by singularity\nusers and are common at sites with direct user logins.  However (as of April\n2019) we are only aware of directory-based images being used by OSG VOs.\nHence, it is reasonable to disable the loopback-based images by setting\nthe following option in \n/etc/singularity/singularity.conf\n:\n\n\n    \nmax\n \nloop\n \ndevices\n \n=\n \n0\n\n\n\n\n\n\nWhile reasonable for some sites, this is not required as there are currently\nno public kernel exploits for this issue; any exploits are patched by\nRed Hat when they are discovered.\n\n\nValidating Singularity RPM\n\n\nAfter singularity is installed, as an ordinary user run the following\ncommand to verify it:\n\n\nuser@host $\n singularity \nexec\n --contain --ipc --pid --bind /cvmfs \n\\\n\n                /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6 \n\\\n\n                ps -ef\n\nWARNING: Container does not have an exec helper script, calling \nps\n directly\n\n\nUID        PID  PPID  C STIME TTY          TIME CMD\n\n\nuser         1     0  1 21:41 ?        00:00:00 shim-init\n\n\nuser         2     1  0 21:41 ?        00:00:00 ps -ef\n\n\n\n\n\n\nStarting and Stopping Services\n\n\nsingularity has no services to start or stop.\n\n\nReferences\n\n\n\n\nSingularity Documentation\n\n\nSingularity Support\n\n\nAdditional guidance for CMS sites", 
            "title": "Install Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#install-singularity", 
            "text": "Singularity  is a tool that creates\ndocker-like process containers but without giving extra privileges to\nunprivileged users.  It is used by grid pilot jobs (which are\nsubmitted by per-VO grid workload management systems) to isolate user\njobs from the pilot's files and processes and from other users' files\nand processes.  It also supplies a chroot environment in order to run\nuser jobs in different operating system images under one Linux kernel.  Kernels with a version 3.10.0-957 or newer include a feature that allows\nsingularity to run completely unprivileged. This kernel version is the\ndefault for RHEL/CentOS/Scientific Linux 7.6 and is available as a\nsecurity update for previous 7.x releases.  Although the feature is\navailable, it needs to be enabled to be usable (instructions below).\nThis kernel version is not available for RHEL 6 and derivatives.  Without this kernel version, singularity must be installed and run with\nsetuid-root executables. Singularity keeps the privileged code to a minimum \nin order to reduce the potential for vulnerabilities.  The OSG has installed singularity in  OASIS ,\nso many sites will eventually (after it is supported by VOs) not need to\ninstall singularity locally if they enable it to run unprivileged.\nMeanwhile an RPM installation can be configured to be unprivileged or\nprivileged.   Kernel vs. Userspace Security  Enabling unprivileged user namespaces increases the risk to the\nkernel. However, the kernel is more widely reviewed than singularity and\nthe additional capability given to users is more limited.\nOSG Security considers the non-setuid, kernel-based method to have a\nlower security risk.   This document is intended for system administrators that wish to install\nand/or configure singularity.", 
            "title": "Install Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host)\nsteps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories   In addition, this is highly recommended for image distribution:   Install  CVMFS", 
            "title": "Before Starting"
        }, 
        {
            "location": "/worker-node/install-singularity/#choosing-unprivileged-vs-privileged-singularity", 
            "text": "There are two sets of instructions on this page:   Enabling Unprivileged Singularity  Singularity via RPM   OSG VOs are working to support running singularity directly from OASIS,\nthe OSG Software  CVMFS distribution , when unprivileged\nsingularity is enabled.  At that point sites will not have to install\nthe singularity RPM themselves.  As of April 2019, no VO in the OSG is\nyet ready to do this, but OSG recommends that all RHEL 7.x installations\nenable support for unprivileged singularity and for now also install the\nRPM.  Sites may also choose to configure their RHEL 7.x RPM\ninstallations to run unprivileged.  RHEL 6.x installations have no\noption for unprivileged singularity so there the RPM has to be installed\nand left configured as privileged.  In addition to improved security, unprivileged singularity enables condor_ssh_to_job  to enter a container namespace without itself\nneeding privileges.  Also, unprivileged singularity enables nesting\ncontainers within another container (when the outer container is started\nby singularity 3.x, which is currently in the osg-upcoming repository).  On the other hand, there are a few rare use cases that require\nsingularity to run privileged:    Using single-file container images.   Some systems, especially\n    High Performance Computing (HPC) systems, deal poorly with\n    collections of small files.  In this case, container images stored\n    in a single file (as opposed to an unpacked directory) may be\n    needed.  However, known images from OSG VOs are directory-based, and we recommend disabling this feature  on\nprivileged installations in order to avoid potential kernel\nexploits.    The overlay feature.   The \"overlay\" feature of singularity uses\n    overlayfs to add bind mounts where mount points don't exist in the\n    underlying image.  However, this feature doesn't work if the image is a directory\ndistributed in CVMFS, singularity has an \"underlay\" feature that is\nequivalent which does work with CVMFS and does not require\nprivileges, and the overlay feature has been a source of security\nvulnerabilities in the past.  For these reasons,  we recommend\nreplacing overlay with underlay  even on\nprivileged installations.    Allocating new pseudo-tty devices.   Support for allocating\n    pseudo-tty devices was accidentally left out of the user namespace\n    support in the RHEL 7.6 kernel.  However, this feature is only required for a small number of\napplications, and singularity 3.x works around the limitation for\nmost of them without needing privileges.", 
            "title": "Choosing Unprivileged vs Privileged Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#enabling-unprivileged-singularity", 
            "text": "The instructions in this section are for enabling singularity to run\nunprivileged.  If the operating system is an EL 7 variant and has been updated to the EL\n7.6 kernel or later, enable unprivileged singularity with the following\nsteps:    Enable user namespaces via  sysctl :  root@host #   echo   user.max_user_namespaces = 15000   \\ \n      /etc/sysctl.d/90-max_user_namespaces.conf root@host #  sysctl -p /etc/sysctl.d/90-max_user_namespaces.conf    (Optional) Disable network namespaces:  root@host #   echo   user.max_net_namespaces = 0   \\ \n      /etc/sysctl.d/90-max_net_namespaces.conf root@host #  sysctl -p /etc/sysctl.d/90-max_net_namespaces.conf  OSG VOs do not need network namespaces with singularity, and\ndisabling them reduces the risk profile of enabling user\nnamespaces.  Network namespaces are, however, utilized by other software,\nsuch as Docker.  Disabling network namespaces may break\nother software, or limit its capabilities (such as\nrequiring the  --net=host  option in Docker).    If you haven't yet installed  CVMFS , do so.", 
            "title": "Enabling Unprivileged Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#validating-unprivileged-singularity", 
            "text": "Once you have the host configured properly, log in as an ordinary\nunprivileged user and verify that singularity in OASIS works:  user@host $  /cvmfs/oasis.opensciencegrid.org/mis/singularity/bin/singularity  \\ \n                 exec  --contain --ipc --pid --bind /cvmfs  \\ \n                /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6  \\ \n                ps -ef WARNING: Container does not have an exec helper script, calling  ps  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  2 21:27 ?        00:00:00 shim-init  user         2     1  0 21:27 ?        00:00:00 ps -ef", 
            "title": "Validating Unprivileged Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#singularity-via-rpm", 
            "text": "The instructions in this section are for the singularity RPM, which \nincludes setuid-root executables.  The setuid-root executables can\nhowever be disabled by configuration, details below.", 
            "title": "Singularity via RPM"
        }, 
        {
            "location": "/worker-node/install-singularity/#installing-singularity-via-rpm", 
            "text": "To install the singularity RPM, make sure that your host is up to date\nbefore installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install Singularity  root@host #  yum install singularity", 
            "title": "Installing Singularity via RPM"
        }, 
        {
            "location": "/worker-node/install-singularity/#configuring-singularity", 
            "text": "singularity includes an option called  underlay  that enables using bind\nmount points that do not exist in the container image.\nBy default it is enabled, but only if the similar  overlay  option cannot\nbe used, such as on RHEL6 where kernel support for overlayfs is missing\nor when running in unprivileged mode.  On RHEL7 it is recommended to\ncompletely disable  overlay , because it is more vulnerable to security\nproblems than  underlay  and because it does not work correctly when\ncontainer images are distributed by CVMFS.  Set this option in  /etc/singularity/singularity.conf :       enable   overlay   =   no    Warning  If you modify  /etc/singularity/singularity.conf , be careful with\nyour upgrade procedures.\nRPM will not automatically merge your changes with new upstream\nconfiguration keys, which may cause a broken install or inadvertently\nchange the site configuration.  Singularity changes its default\nconfiguration file more frequently than typical OSG software.  Look for  singularity.conf.rpmnew  after upgrades and merge in any\nchanges to the defaults.", 
            "title": "Configuring Singularity"
        }, 
        {
            "location": "/worker-node/install-singularity/#configuring-the-rpm-to-be-unprivileged", 
            "text": "If you choose to run the RPM unprivileged, after enabling unprivileged singularity ,\nchange the line in  /etc/singularity/singularity.conf  that says allow setuid = yes  to       allow   setuid   =   no   Note that the setuid-root executables stay installed, but they will exit\nvery early if invoked when the configuration file disallows setuid, so\nthe risk is very low.  There are non-setuid equivalent executables that\nare used instead when setuid is disallowed.", 
            "title": "Configuring the RPM to be Unprivileged"
        }, 
        {
            "location": "/worker-node/install-singularity/#limiting-image-types", 
            "text": "A side effect of disabling privileged singularity is that loopback\nmounts are disallowed.  If the installation is privileged, also consider\nthe following.  Images based on loopback devices carry an inherently higher exposure to\nunknown kernel exploits compared to directory-based images distributed via\nCVMFS.  See  this article  for further\ndiscussion.  The loopback-based images are the default image type produced by singularity\nusers and are common at sites with direct user logins.  However (as of April\n2019) we are only aware of directory-based images being used by OSG VOs.\nHence, it is reasonable to disable the loopback-based images by setting\nthe following option in  /etc/singularity/singularity.conf :       max   loop   devices   =   0   While reasonable for some sites, this is not required as there are currently\nno public kernel exploits for this issue; any exploits are patched by\nRed Hat when they are discovered.", 
            "title": "Limiting Image Types"
        }, 
        {
            "location": "/worker-node/install-singularity/#validating-singularity-rpm", 
            "text": "After singularity is installed, as an ordinary user run the following\ncommand to verify it:  user@host $  singularity  exec  --contain --ipc --pid --bind /cvmfs  \\ \n                /cvmfs/singularity.opensciencegrid.org/opensciencegrid/osgvo:el6  \\ \n                ps -ef WARNING: Container does not have an exec helper script, calling  ps  directly  UID        PID  PPID  C STIME TTY          TIME CMD  user         1     0  1 21:41 ?        00:00:00 shim-init  user         2     1  0 21:41 ?        00:00:00 ps -ef", 
            "title": "Validating Singularity RPM"
        }, 
        {
            "location": "/worker-node/install-singularity/#starting-and-stopping-services", 
            "text": "singularity has no services to start or stop.", 
            "title": "Starting and Stopping Services"
        }, 
        {
            "location": "/worker-node/install-singularity/#references", 
            "text": "Singularity Documentation  Singularity Support  Additional guidance for CMS sites", 
            "title": "References"
        }, 
        {
            "location": "/worker-node/using-wn-containers/", 
            "text": "Using the Worker Node Containers\n\n\nThe OSG worker node containers contain the suggested base environment for worker nodes.  They can be used as a base image to build containers or to perform testing.\n\n\nThe containers are available on \nDocker Hub\n.\n\n\nAvailable Containers\n\n\nAvailable tags include:\n\n\n\n\nlatest\n: The latest version of the OSG worker node environment on the most recent supported OS.  As of June 2017, this is OSG 3.4 and RHEL7.\n\n\n3.4\n: The OSG 3.4 release series on top of the most recent supported OS.  As of June 2017, this is RHEL7.\n\n\n3.4-el7\n: The OSG 3.4 release series on top of a RHEL7 environment.\n\n\n3.4-el6\n: The OSG 3.4 release series on top of a RHEL6 environment.\n\n\n\n\nBuilding Upon the Container\n\n\nYou may base the container on the OSG worker node by including it inside your \nDockerfile\n:\n\n\nFROM\n \nopensciencegrid\n/\nosg\n-\nwn\n:\nlatest\n\n\n\n\n\n\nYou can replace \nlatest\n with any tag listed above.\n\n\nPerform Testing\n\n\nYou may perform testing from within the OSG worker node envionment by running the command:\n\n\nroot\n@host\n \n#\n \ndocker\n \nrun\n \n-\nti\n \n--rm opensciencegrid/osg-wn:latest /bin/bash", 
            "title": "Using WN Containers"
        }, 
        {
            "location": "/worker-node/using-wn-containers/#using-the-worker-node-containers", 
            "text": "The OSG worker node containers contain the suggested base environment for worker nodes.  They can be used as a base image to build containers or to perform testing.  The containers are available on  Docker Hub .", 
            "title": "Using the Worker Node Containers"
        }, 
        {
            "location": "/worker-node/using-wn-containers/#available-containers", 
            "text": "Available tags include:   latest : The latest version of the OSG worker node environment on the most recent supported OS.  As of June 2017, this is OSG 3.4 and RHEL7.  3.4 : The OSG 3.4 release series on top of the most recent supported OS.  As of June 2017, this is RHEL7.  3.4-el7 : The OSG 3.4 release series on top of a RHEL7 environment.  3.4-el6 : The OSG 3.4 release series on top of a RHEL6 environment.", 
            "title": "Available Containers"
        }, 
        {
            "location": "/worker-node/using-wn-containers/#building-upon-the-container", 
            "text": "You may base the container on the OSG worker node by including it inside your  Dockerfile :  FROM   opensciencegrid / osg - wn : latest   You can replace  latest  with any tag listed above.", 
            "title": "Building Upon the Container"
        }, 
        {
            "location": "/worker-node/using-wn-containers/#perform-testing", 
            "text": "You may perform testing from within the OSG worker node envionment by running the command:  root @host   #   docker   run   - ti   --rm opensciencegrid/osg-wn:latest /bin/bash", 
            "title": "Perform Testing"
        }, 
        {
            "location": "/data/frontier-squid/", 
            "text": "Install the Frontier Squid HTTP Caching Proxy\n\n\nFrontier Squid is a distribution of the well-known \nsquid HTTP caching\nproxy software\n that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has\n\nmany advantages\n\nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.\n\n\nThis document is intended for System Administrators who are installing\n\nfrontier-squid\n, the OSG distribution of the Frontier Squid software.\n\n\n\n\nApplicable versions\n\n\nThis document applies to software from the OSG 3.4 Release Series.\nThe version of frontier-squid installed should be \n= 3.5.24-3.1.\nWhen using OSG software from a previous Release Series (eg, OSG 3.3)\nand a frontier-squid version in the 2.7STABLE9 series, refer to the\n\nold upstream install documentation\n\ninstead of the current links included below. There are some\nincompatibilities between the two versions of frontier-squid, so if you\nare upgrading from a 2.7STABLE9 version to a 3.5 version, see the\n\nupstream documentation on upgrading\n.\n\n\n\n\nFrontier Squid Is Recommended\n\n\nOSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.\n\n\nFor large sites that expect heavy load on the proxy, it is best to run the proxy on its own host.\nIf you are unsure if your site qualifies, we recommend initially running the proxy on your CE host and monitoring its\nbandwidth.\nIf the network usage regularly peaks at over one third of the bandwidth capacity, move the proxy to a new host.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \nsquid\n Linux user\n\n\nNetwork ports:\n Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)\n\n\nHost choice:\n If you will be supporting the Frontier application at your site, review the\n\nupstream documentation\n to determine how to size your equipment.\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling Frontier Squid\n\n\nTo install Frontier Squid, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall Frontier Squid:\n\n\nroot@host #\n yum install frontier-squid\n\n\n\n\n\n\n\n\n\nConfiguring Frontier Squid\n\n\nConfiguring the Frontier Squid Service\n\n\nTo configure the Frontier Squid service itself:\n\n\n\n\nFollow the\n    \nConfiguration section of the upstream Frontier Squid documentation\n.\n\n\nEnable, start, and test the service (as described below).\n\n\nRegister the squid\n.\n\n\nIf your site is part of the WLCG, enable WLCG monitoring as described in the\n    \nupstream documentation on enabling monitoring\n.\n\n\n\n\n\n\nNote\n\n\nAn important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in\n\n/etc/squid/customize.sh\n instead of \n/etc/squid/squid.conf\n.\n\n\n\n\nConfiguring the OSG CE\n\n\nTo configure the OSG Compute Element (CE) to know about your Frontier Squid service:\n\n\n\n\n\n\nOn your CE host (which may be different than your Frontier Squid host), edit \n/etc/osg/config.d/01-squid.ini\n\n\n\n\nMake sure that \nenabled\n is set to \nTrue\n\n\nSet \nlocation\n to the hostname and port of your Frontier Squid\n    service (e.g., \nmy.squid.host.edu:3128\n)\n\n\nLeave the other settings at \nDEFAULT\n unless you have specific\n    reasons to change them\n\n\n\n\n\n\n\n\nRun \nosg-configure -c\n to propagate the changes on your CE.\n\n\n\n\n\n\n\n\nNote\n\n\nYou may want to finish other CE configuration tasks before running\n\nosg-configure\n. Just be sure to run it once before starting CE\nservices.\n\n\n\n\nUsing Frontier-Squid\n\n\nStart the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice frontier-squid start\n\n\nsystemctl start frontier-squid\n\n\n\n\n\n\nStop a  service\n\n\nservice frontier-squid stop\n\n\nsystemctl stop frontier-squid\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig frontier-squid on\n\n\nsystemctl enable frontier-squid\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig frontier-squid off\n\n\nsystemctl disable frontier-squid\n\n\n\n\n\n\n\n\nValidating Frontier Squid\n\n\nAs any user on another computer, do the following (where\n\nmy.squid.host.edu\n is the fully qualified domain name of your\nsquid server):\n\n\nuser@host $\n \nexport\n \nhttp_proxy\n=\nhttp://\nmy.squid.host.edu\n:3128\n\nuser@host $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: MISS from \nmy.squid.host.edu\n\n\nuser@host $\n wget -qdO/dev/null http://frontier.cern.ch \n2\n1\n|\ngrep X-Cache\n\nX-Cache: HIT from \nmy.squid.host.edu\n\n\n\n\n\n\nIf the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid\n\naccess.log file\n\nto try to see what's wrong.\n\n\nIf your squid will be supporting the Frontier application, it is also\ngood to do the test in the\n\nupstream documentation Testing the installation section\n.\n\n\nRegistering Frontier Squid\n\n\nTo register your Frontier Squid host, follow the general registration instructions\n\nhere\n with the following Frontier Squid-specific details:\n\n\n\n\n\n\nAdd a \nSquid:\n section to the \nServices:\n list, with any relevant fields for that service.\n    This is a partial example:\n\n\n...\n\n\nFQDN: \nFULLY QUALIFIED DOMAIN NAME\n\n\nServices:\n\n\n  Squid:\n\n\n    Description: Generic squid service\n\n\n...\n\n\n\n\n\n\nReplacing \nFULLY QUALIFIED DOMAIN NAME\n with your Frontier Squid server's DNS entry or in the case of multiple\nFrontier Squid servers for a single resource, the round-robin DNS entry.\n\n\nSee the \nBNL_ATLAS_Frontier_Squid\n \nfor a complete example.\n\n\n\n\n\n\nIf you are setting up a new resource, set \nActive: false\n.\n    Only set \nActive: true\n for a resource when it is accepting requests and ready for production.\n\n\n\n\n\n\nIf you are running a WLCG site, a few hours after a squid is registered\nand marked \nActive\n, \n\nverify that it is monitored by WLCG\n.\n\n\nReference\n\n\nUsers\n\n\nThe frontier-squid installation will create one user account unless it\nalready exists.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nsquid\n\n\nReduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".\n\n\n\n\n\n\n\n\nThe package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the\n\nupstream documentation Preparation section\n.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nSquid\n\n\ntcp\n\n\n3128\n\n\n\u2713\n\n\n\u2713\n\n\nAlso limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously\n\n\n\n\n\n\nSquid monitor\n\n\nudp\n\n\n3401\n\n\n\u2713\n\n\n\n\nAlso limited in squid ACLs. Should be limited to monitoring server addresses\n\n\n\n\n\n\n\n\nThe addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the\n\nupstream documentation Enabling monitoring section\n.\n\n\nFrontier Squid Log Files\n\n\nLog file contents are explained in the\n\nupstream documentation Log file contents section\n.", 
            "title": "Install Squid HTTP Cache"
        }, 
        {
            "location": "/data/frontier-squid/#install-the-frontier-squid-http-caching-proxy", 
            "text": "Frontier Squid is a distribution of the well-known  squid HTTP caching\nproxy software  that is optimized for use with\napplications on the Worldwide LHC Computing Grid (WLCG). It has many advantages \nover regular squid for common grid applications, especially Frontier\nand CVMFS. The OSG distribution of frontier-squid is a straight rebuild of the\nupstream frontier-squid package for the convenience of OSG users.  This document is intended for System Administrators who are installing frontier-squid , the OSG distribution of the Frontier Squid software.   Applicable versions  This document applies to software from the OSG 3.4 Release Series.\nThe version of frontier-squid installed should be  = 3.5.24-3.1.\nWhen using OSG software from a previous Release Series (eg, OSG 3.3)\nand a frontier-squid version in the 2.7STABLE9 series, refer to the old upstream install documentation \ninstead of the current links included below. There are some\nincompatibilities between the two versions of frontier-squid, so if you\nare upgrading from a 2.7STABLE9 version to a 3.5 version, see the upstream documentation on upgrading .", 
            "title": "Install the Frontier Squid HTTP Caching Proxy"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-is-recommended", 
            "text": "OSG recommends that all sites run a caching proxy for HTTP and HTTPS\nto help reduce bandwidth and improve throughput. To that end, Compute\nElement (CE) installations include Frontier Squid automatically. We\nencourage all sites to configure and use this service, as described\nbelow.  For large sites that expect heavy load on the proxy, it is best to run the proxy on its own host.\nIf you are unsure if your site qualifies, we recommend initially running the proxy on your CE host and monitoring its\nbandwidth.\nIf the network usage regularly peaks at over one third of the bandwidth capacity, move the proxy to a new host.", 
            "title": "Frontier Squid Is Recommended"
        }, 
        {
            "location": "/data/frontier-squid/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If it does not exist already, the installation will create the  squid  Linux user  Network ports:  Frontier squid communicates on ports 3128 (TCP) and 3401 (UDP)  Host choice:  If you will be supporting the Frontier application at your site, review the upstream documentation  to determine how to size your equipment.   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/frontier-squid/#installing-frontier-squid", 
            "text": "To install Frontier Squid, make sure that your host is up to date before installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install Frontier Squid:  root@host #  yum install frontier-squid", 
            "title": "Installing Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-frontier-squid", 
            "text": "", 
            "title": "Configuring Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-frontier-squid-service", 
            "text": "To configure the Frontier Squid service itself:   Follow the\n     Configuration section of the upstream Frontier Squid documentation .  Enable, start, and test the service (as described below).  Register the squid .  If your site is part of the WLCG, enable WLCG monitoring as described in the\n     upstream documentation on enabling monitoring .    Note  An important difference between the standard Squid software and\nthe Frontier Squid variant is that Frontier Squid changes are in /etc/squid/customize.sh  instead of  /etc/squid/squid.conf .", 
            "title": "Configuring the Frontier Squid Service"
        }, 
        {
            "location": "/data/frontier-squid/#configuring-the-osg-ce", 
            "text": "To configure the OSG Compute Element (CE) to know about your Frontier Squid service:    On your CE host (which may be different than your Frontier Squid host), edit  /etc/osg/config.d/01-squid.ini   Make sure that  enabled  is set to  True  Set  location  to the hostname and port of your Frontier Squid\n    service (e.g.,  my.squid.host.edu:3128 )  Leave the other settings at  DEFAULT  unless you have specific\n    reasons to change them     Run  osg-configure -c  to propagate the changes on your CE.     Note  You may want to finish other CE configuration tasks before running osg-configure . Just be sure to run it once before starting CE\nservices.", 
            "title": "Configuring the OSG CE"
        }, 
        {
            "location": "/data/frontier-squid/#using-frontier-squid", 
            "text": "Start the frontier-squid service and enable it to start at boot time. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service frontier-squid start  systemctl start frontier-squid    Stop a  service  service frontier-squid stop  systemctl stop frontier-squid    Enable a service to start on boot  chkconfig frontier-squid on  systemctl enable frontier-squid    Disable a service from starting on boot  chkconfig frontier-squid off  systemctl disable frontier-squid", 
            "title": "Using Frontier-Squid"
        }, 
        {
            "location": "/data/frontier-squid/#validating-frontier-squid", 
            "text": "As any user on another computer, do the following (where my.squid.host.edu  is the fully qualified domain name of your\nsquid server):  user@host $   export   http_proxy = http:// my.squid.host.edu :3128 user@host $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: MISS from  my.squid.host.edu  user@host $  wget -qdO/dev/null http://frontier.cern.ch  2 1 | grep X-Cache X-Cache: HIT from  my.squid.host.edu   If the grep doesn't print anything, try removing it from the pipeline\nto see if errors are obvious. If the second try says MISS again,\nsomething is probably wrong with the squid cache writes. Look at the squid access.log file \nto try to see what's wrong.  If your squid will be supporting the Frontier application, it is also\ngood to do the test in the upstream documentation Testing the installation section .", 
            "title": "Validating Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#registering-frontier-squid", 
            "text": "To register your Frontier Squid host, follow the general registration instructions here  with the following Frontier Squid-specific details:    Add a  Squid:  section to the  Services:  list, with any relevant fields for that service.\n    This is a partial example:  ...  FQDN:  FULLY QUALIFIED DOMAIN NAME  Services:    Squid:      Description: Generic squid service  ...   Replacing  FULLY QUALIFIED DOMAIN NAME  with your Frontier Squid server's DNS entry or in the case of multiple\nFrontier Squid servers for a single resource, the round-robin DNS entry.  See the  BNL_ATLAS_Frontier_Squid  \nfor a complete example.    If you are setting up a new resource, set  Active: false .\n    Only set  Active: true  for a resource when it is accepting requests and ready for production.    If you are running a WLCG site, a few hours after a squid is registered\nand marked  Active ,  verify that it is monitored by WLCG .", 
            "title": "Registering Frontier Squid"
        }, 
        {
            "location": "/data/frontier-squid/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/data/frontier-squid/#users", 
            "text": "The frontier-squid installation will create one user account unless it\nalready exists.     User  Comment      squid  Reduced privilege user that the squid process runs under. Set the default gid of the \"squid\" user to be a group that is also called \"squid\".     The package can instead use another user name of your choice if you\ncreate a configuration file before installation. Details are in the upstream documentation Preparation section .", 
            "title": "Users"
        }, 
        {
            "location": "/data/frontier-squid/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      Squid  tcp  3128  \u2713  \u2713  Also limited in squid ACLs. Both in and outbound must not be wide open to internet simultaneously    Squid monitor  udp  3401  \u2713   Also limited in squid ACLs. Should be limited to monitoring server addresses     The addresses of the WLCG monitoring servers for use in firewalls are\nlisted in the upstream documentation Enabling monitoring section .", 
            "title": "Networking"
        }, 
        {
            "location": "/data/frontier-squid/#frontier-squid-log-files", 
            "text": "Log file contents are explained in the upstream documentation Log file contents section .", 
            "title": "Frontier Squid Log Files"
        }, 
        {
            "location": "/data/update-oasis/", 
            "text": "Updating Software in OASIS\n\n\nOASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.\n\n\nThis document is a step by step explanation of how a Virtual Organization (VO) Software Adminstrator can enable the OASIS service and use it to publish and update software on OSG Worker Nodes under \n/cvmfs/oasis.opensciencegrid.org\n.\n\n\n\n\nNote\n\n\n\n\nFor information on how to configure a client for OASIS see the \nCVMFS installation documentation\n.\n\n\nFor information on hosting your own repository see the \nOASIS repository installation\n.\n\n\n\n\n\n\nRequirements\n\n\nTo begin the process to distribute software on OASIS using the service hosted by OSG Operations, you must:\n\n\n\n\nObtain a personal grid certificate\n, if you don't have one already.\n\n\nRegister yourself in the OSG Information Management System (OIM) by sending an email to\n    \n\n\nBe associated with a \nVO registered in OIM\n.\n\n\n\n\nHow to use OASIS\n\n\nEnable OASIS\n\n\nWhen you are ready to distribute your software with OASIS, submit a \nsupport ticket\n with a request to enable OASIS for your VO. In your request, please specify your VO and provide a list of people who will install and administer the VO software in OASIS.\n\n\nOSG Operations will enable OASIS for your VO in \nOSG topology\n and add your list of administrators to the \"OASIS Managers\" list (which is near the bottom of the page of information about each VO in OIM). oasis-login will then grant access to the people who are listed as OASIS managers. Any time the list is to be modified, submit another ticket.\n\n\nLog in with GSISSH\n\n\nThe next step is to generate a proxy and log into \noasis-login.opensciencegrid.org\n with \ngsissh\n. These commands should be run on a computer that has the \nOSG worker node client\n software. First make sure that your grid certificate is installed in \n~/.globus/usercred.p12\n on that computer and that it is mode 600, then run these commands:\n\n\nuser@host $\n voms-proxy-init\n\nuser@host $\n gsissh -o \nGSSAPIDelegateCredentials\n=\nyes oasis-login.opensciencegrid.org\n\n\n\n\n\nIn case the user can be mapped to more than one account, specify it explicitly in a command like this\n\n\nuser@host $\n gsissh -o \nGSSAPIDelegateCredentials\n=\nyes ouser.\nVO\n@oasis-login.opensciencegrid.org\n\n\n\n\n\nInstead of putting \n-o GSSAPIDelegateCredentials=yes\n on the command line, you can put it in your \n~/.ssh/config\n like this:\n\n\nHost oasis-login.opensciencegrid.org\n\n\n    GSSAPIDelegateCredentials yes\n\n\n\n\n\n\nInstall and update software\n\n\nOnce you log in, you can add/modify/remove content on a staging area at \n/stage/oasis/$VO\n where $VO is the name of the VO represented by the manager.\n\n\nFiles here are visible to both \noasis-login\n and the Stratum 0 server (oasis.opensciencegrid.org).  There is a symbolic link at \n/cvmfs/oasis.opensciencegrid.org/$VO\n that points to the same staging area.  \n\n\nRequest an oasis publish with this command:\n\n\nuser@host $\n osg-oasis-update\n\n\n\n\n\nThis command queues a process to sync the content of OASIS with the content of \n/stage/oasis/$VO\n\n\nosg-oasis-update\n returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a \nsupport ticket\n.\n\n\nLimitations on repository content\n\n\nAlthough CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the \nCVMFS repository content limitations\n so please review those guidelines carefully.\n\n\nTesting\n\n\nAfter \nosg-oasis-update\n completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under \n/cvmfs/oasis.opensciencegrid.org\n on a computer that has the \nCVMFS client installed\n. A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with\n\n\nroot@host #\n cvmfs_talk -i oasis.opensciencegrid.org remount\n\n\n\n\n\nThis can be done while the filesystem is mounted (despite the name, it does not do an OS-level umount/mount of the filesystem). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted.\n\n\nIn order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest \nosg-oasis-update\n time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine:\n\n\nuser@host $\n date -d \n1970-1-1 GMT + \n$(\nwget -qO- \n$(\nattr -qg host /cvmfs/oasis.opensciencegrid.org\n)\n/.cvmfspublished \n|\n \n\\\n\n                                                            cat -v\n|\nsed -n \n/^T/{s/^T//p;q;}\n)\n sec\n\n\n\n\n\n\nReferences\n\n\nCVMFS Documentation", 
            "title": "Update OASIS Shared Repo"
        }, 
        {
            "location": "/data/update-oasis/#updating-software-in-oasis", 
            "text": "OASIS is the OSG Application Software Installation Service. It is the recommended method to install software on the Open Science Grid. It is implemented using CernVM FileSystem (CVMFS) technology.  This document is a step by step explanation of how a Virtual Organization (VO) Software Adminstrator can enable the OASIS service and use it to publish and update software on OSG Worker Nodes under  /cvmfs/oasis.opensciencegrid.org .   Note   For information on how to configure a client for OASIS see the  CVMFS installation documentation .  For information on hosting your own repository see the  OASIS repository installation .", 
            "title": "Updating Software in OASIS"
        }, 
        {
            "location": "/data/update-oasis/#requirements", 
            "text": "To begin the process to distribute software on OASIS using the service hosted by OSG Operations, you must:   Obtain a personal grid certificate , if you don't have one already.  Register yourself in the OSG Information Management System (OIM) by sending an email to\n      Be associated with a  VO registered in OIM .", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/update-oasis/#how-to-use-oasis", 
            "text": "", 
            "title": "How to use OASIS"
        }, 
        {
            "location": "/data/update-oasis/#enable-oasis", 
            "text": "When you are ready to distribute your software with OASIS, submit a  support ticket  with a request to enable OASIS for your VO. In your request, please specify your VO and provide a list of people who will install and administer the VO software in OASIS.  OSG Operations will enable OASIS for your VO in  OSG topology  and add your list of administrators to the \"OASIS Managers\" list (which is near the bottom of the page of information about each VO in OIM). oasis-login will then grant access to the people who are listed as OASIS managers. Any time the list is to be modified, submit another ticket.", 
            "title": "Enable OASIS"
        }, 
        {
            "location": "/data/update-oasis/#log-in-with-gsissh", 
            "text": "The next step is to generate a proxy and log into  oasis-login.opensciencegrid.org  with  gsissh . These commands should be run on a computer that has the  OSG worker node client  software. First make sure that your grid certificate is installed in  ~/.globus/usercred.p12  on that computer and that it is mode 600, then run these commands:  user@host $  voms-proxy-init user@host $  gsissh -o  GSSAPIDelegateCredentials = yes oasis-login.opensciencegrid.org  In case the user can be mapped to more than one account, specify it explicitly in a command like this  user@host $  gsissh -o  GSSAPIDelegateCredentials = yes ouser. VO @oasis-login.opensciencegrid.org  Instead of putting  -o GSSAPIDelegateCredentials=yes  on the command line, you can put it in your  ~/.ssh/config  like this:  Host oasis-login.opensciencegrid.org      GSSAPIDelegateCredentials yes", 
            "title": "Log in with GSISSH"
        }, 
        {
            "location": "/data/update-oasis/#install-and-update-software", 
            "text": "Once you log in, you can add/modify/remove content on a staging area at  /stage/oasis/$VO  where $VO is the name of the VO represented by the manager.  Files here are visible to both  oasis-login  and the Stratum 0 server (oasis.opensciencegrid.org).  There is a symbolic link at  /cvmfs/oasis.opensciencegrid.org/$VO  that points to the same staging area.    Request an oasis publish with this command:  user@host $  osg-oasis-update  This command queues a process to sync the content of OASIS with the content of  /stage/oasis/$VO  osg-oasis-update  returns immediately, but only one update can run at a time (across all VOs); your request may be queued behind a different VO. If you encounter severe delays before the update is finished being published (more than 4 hours), please file a  support ticket .", 
            "title": "Install and update software"
        }, 
        {
            "location": "/data/update-oasis/#limitations-on-repository-content", 
            "text": "Although CVMFS provides a POSIX filesystem, it does not work well with all types of content. Content in OASIS is expected to adhere to the  CVMFS repository content limitations  so please review those guidelines carefully.", 
            "title": "Limitations on repository content"
        }, 
        {
            "location": "/data/update-oasis/#testing", 
            "text": "After  osg-oasis-update  completes and the changes have been propagated to the CVMFS stratum 1 servers (typically between 0 and 60 minutes, but possibly longer if the servers are busy with updates of other repositories) then the changes can be visible under  /cvmfs/oasis.opensciencegrid.org  on a computer that has the  CVMFS client installed . A client normally only checks for updates if at least an hour has passed since it last checked, but people who have superuser access on the client machine can force it to check again with  root@host #  cvmfs_talk -i oasis.opensciencegrid.org remount  This can be done while the filesystem is mounted (despite the name, it does not do an OS-level umount/mount of the filesystem). If the filesystem is not mounted, it will automatically check for new updates the next time it is mounted.  In order to find out if an update has reached the CVMFS stratum 1 server, you can find out the latest  osg-oasis-update  time seen by the stratum 1 most favored by your CVMFS client with the following long command on your client machine:  user@host $  date -d  1970-1-1 GMT +  $( wget -qO-  $( attr -qg host /cvmfs/oasis.opensciencegrid.org ) /.cvmfspublished  |   \\ \n                                                            cat -v | sed -n  /^T/{s/^T//p;q;} )  sec", 
            "title": "Testing"
        }, 
        {
            "location": "/data/update-oasis/#references", 
            "text": "CVMFS Documentation", 
            "title": "References"
        }, 
        {
            "location": "/data/external-oasis-repos/", 
            "text": "Install an OASIS Repository\n\n\nOASIS (the \nOSG\n \nA\npplication \nS\noftware \nI\nnstallation \nS\nervice) is an infrastructure, based on\n\nCVMFS\n, for distributing software throughout the OSG.  Once software is installed into\nan OASIS repository, the goal is to make it available across about 90% of the OSG within an hour.\n\n\nOASIS consists of keysigning infrastructure, a content distribution network (CDN), and a shared CVMFS repository that\nis hosted by the OSG.  Many use cases will be covered by utilizing the \nshared repository\n; this document\ncovers how to install, configure, and host your own CVMFS \nrepository server\n.  This server will distribute software\nvia OASIS, but will be hosted and operated externally from the OSG project.\n\n\nOASIS-based distribution and key signing is available to OSG VOs or repositories affiliated with an OSG VO.\nSee the \npolicy page\n for more information\non what repositories OSG is willing to distribute.\n\n\nBefore Starting\n\n\nCVMFS repositories work at the kernel filesystem layer, which adds more stringent host requirements than a typical\nOSG install.  The host OS must meet ONE of the following:\n\n\n\n\nRHEL 7.3 (or equivalent) or later.  \nThis option is recommended\n.\n\n\nRHEL 6 with the aufs kernel module.\n\n\n\n\nAdditionally,\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the \ncvmfs\n Linux user\n\n\nGroup IDs:\n If they do not exist already, the installation will create the Linux groups \ncvmfs\n and \nfuse\n\n\nNetwork ports:\n This page will configure the repository to distribute using Apache HTTPD on port 8000.  At the\n    minimum, the repository needs in-bound access from the OASIS CDN.\n\n\nDisk space:\n This host will need enough free disk space to host \ntwo\n copies of the software: one compressed\n    and one uncompressed. \n/srv/cvmfs\n will hold all the published data (compressed and de-deuplicated).  The\n    \n/var/spool/cvmfs\n directory will contain all the data in all current transactions (uncompressed).\n\n\nRoot access\n will be needed to install.  Software install will be done as an unprivileged user.\n\n\nYum\n will need to be \nconfigured to use the OSG repositories\n.\n\n\n\n\n\n\nOverlay-FS limitations\n\n\nCVMFS on RHEL7 only supports Overlay-FS if the underlying filesystem is \next3\n or \next4\n; make sure\n\n/var/spool/cvmfs\n is one of these filesystem types.\n\n\nIf this is not possible, add \nCVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes\n to your CVMFS configuration.  Using\n\nxfs\n will work if it was created with \nftype=1\n\n\n\n\nInstallation\n\n\nRHEL7-based Systems\n\n\nFor a RHEL7-based system, installation is a straightforward install via \nyum\n:\n\n\nroot@host #\n yum install cvmfs-server osg-oasis \n\n\n\n\n\nRHEL6-based Systems\n\n\nA RHEL6 host needs additional steps in order to add the AUFS2 kernel module.\n\n\nroot@host #\n rpm -i https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm\n\nroot@host #\n yum install --enablerepo\n=\ncernvm-kernel --disablerepo\n=\ncernvm kernel aufs2-util cvmfs-server.x86_64 osg-oasis\n\nroot@host #\n reboot\n\n\n\n\n\nApache and Repository Mounts\n\n\nFor all installs, we recommend mounting all the local repositories on startup:\n\n\nroot@host #\n \necho\n \ncvmfs_server mount -a\n \n/etc/rc.local\n\nroot@host #\n chmod +x /etc/rc.local\n\n\n\n\n\nThe Apache HTTPD service should be configured to listen on port 8000, have the \nKeepAlive\n option enabled, and be\nstarted:\n\n\nroot@host #\n \necho\n Listen \n8000\n \n/etc/httpd/conf.d/cvmfs.conf \n\nroot@host #\n \necho\n KeepAlive on \n/etc/httpd/conf.d/cvmfs.conf \n\nroot@host #\n chkconfig httpd on \n\nroot@host #\n service httpd start\n\n\n\n\n\n\n\nCheck Firewalls\n\n\nMake sure that port 8000 is available to the Internet.  Check the setting of the host- and site-level firewalls.\nThe next steps will fail if the web server is not accessible.\n\n\n\n\nCreating a Repository\n\n\nPrior to creation, the repository administrator will need to make two decisions:\n\n\n\n\nSelect a repository name\n; typically, this is derived from the VO or project's name and ends in\n    \nopensciencegrid.org\n.  For example, the NoVA VO runs the repository \nnova.opensciencegrid.org\n.  For this section,\n    we will use \nexample.opensciencegrid.org\n.\n\n\nSelect a repository owner\n: Software publication will need to run by a non-\nroot\n Unix user account; for this\n    document, we will use \nLIBRARIAN\n as the account name of the repository owner.\n\n\n\n\nThe initial repository creation must be run as \nroot\n:\n\n\nroot@host #\n \necho\n -e \n\\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384\n \n/etc/security/limits.conf\n\nroot@host #\n \nulimit\n -n \n16384\n\n\nroot@host #\n cvmfs_server mkfs -o \nLIBRARIAN\n \nexample.opensciencegrid.org\n\n\nroot@host #\n cat \n/srv/cvmfs/\nexample.opensciencegrid.org\n/.htaccess \nxEOFx\n\n\nOrder deny,allow\n\n\nDeny from all\n\n\nAllow from 127.0.0.1\n\n\nAllow from ::1\n\n\nAllow from 129.79.53.0/24 129.93.244.192/26 129.93.227.64/26\n\n\nAllow from 2001:18e8:2:6::/56 2600:900:6::/48 \n\n\nxEOFx\n\n\n\n\n\n\nHere, we increase the number of open files allowed, create the repository using the \nmkfs\n command, and then limit the hosts that are allowed to access the repo to the OSG CDN.\n\n\nNext, adjust the configuration in the repository as follows.  \n\n\nroot@host #\n cat \n/etc/cvmfs/repositories.d/\nexample.opensciencegrid.org\n/server.conf \nxEOFx\n\n\nCVMFS_AUTO_TAG_TIMESPAN=\n2 weeks ago\n\n\nCVMFS_IGNORE_XDIR_HARDLINKS=true\n\n\nCVMFS_GENERATE_LEGACY_BULK_CHUNKS=false\n\n\nCVMFS_AUTOCATALOGS=true\n\n\nCVMFS_ENFORCE_LIMITS=true\n\n\nCVMFS_FORCE_REMOUNT_WARNING=false\n\n\nxEOFx\n\n\n\n\n\n\nAlso, check the \ncvmfs documentation\n for additional recommendations for special purpose repositories.\n\n\nNow verify that the repository is readable over HTTP:\n\n\nroot@host #\n wget -qO- http://localhost:8000/cvmfs/\nexample.opensciencegrid.org\n/.cvmfswhitelist \n|\n cat -v\n\n\n\n\n\nThat should print several lines including some gibberish at the end.\n\n\nHosting a Repository on OASIS\n\n\n\n\n\nIn order to host a repository on OASIS, perform the following steps:\n\n\n\n\n\n\nVerify your VO's registration is up-to-date\n.  All repositories need to be associated with a VO; the VO needs to\n    assign an \nOASIS manager\n in Topology who would be responsible for the contents of any of the VO's repositories and\n    will be contacted in case of issues. To designate an OASIS manager, have the VO manager update the\n    \nTopology registration\n.\n\n\n\n\n\n\nCreate a \nsupport ticket\n using the following template:\n\n\nPlease\n \nadd\n \na\n \nnew\n \nCVMFS\n \nrepository\n \nto\n \nOASIS\n \nfor\n \nVO\n \nVO\n \nNAME\n \nusing\n \nthe\n \nURL\n\n    \nhttp\n:\n//\nFQDN\n:\n8000\n/\ncvmfs\n/\nOASIS\n \nREPOSITORY\n\n\nThe\n \nVO\n \nresponsible\n \nmanager\n \nwill\n \nbe\n \nOASIS\n \nMANAGER\n.\n\n\n\n\n\nReplace the \nANGLE BRACKET TEXT\n items with the appropriate values.\n\n\n\n\n\n\nIf the repository name matches \n*.opensciencegrid.org\n or \n*.osgstorage.org\n, wait for the go-ahead from the OSG\n    representative before continuing with the remaining instructions; for all other repositories (such as \n*.egi.eu\n),\n    you are done.\n\n\n\n\n\n\nOne you are told in the ticket to proceed to the next step, execute the following commands:\n\n\nroot@host #\n wget -O /srv/cvmfs/\nexample.opensciencegrid.org\n/.cvmfswhitelist \n\\\n\n            http://oasis.opensciencegrid.org/cvmfs/\nexample.opensciencegrid.org\n/.cvmfswhitelist \n\nroot@host #\n /bin/cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \n\\\n\n            /etc/cvmfs/keys/\nexample.opensciencegrid.org\n.pub\n\n\n\n\n\nReplace \nexample.opensciencegrid.org\n as appropriate.\n\n\n\n\n\n\nVerify that publishing operation succeeds:\n\n\nroot@host #\n su \nLIBRARIAN\n -c \ncvmfs_server transaction \nexample.opensciencegrid.org\n\n\nroot@host #\n su \nLIBRARIAN\n -c \ncvmfs_server publish \nexample.opensciencegrid.org\n\n\n\n\n\n\nWithin an hour, the repository updates should appear at the OSG Operations and FNAL Stratum-1 servers.\n\n\nOn success, make sure the whitelist update happens daily by creating \n/etc/cron.d/fetch-cvmfs-whitelist\n with the\nfollowing contents:\n\n\n5\n \n4\n \n*\n \n*\n \n*\n \n%\nBLUE\n%\nLIBRARIAN\n%\nENDCOLOR\n%\n \ncd\n \n/\nsrv\n/\ncvmfs\n/%\nRED\n%\nexample\n.\nopensciencegrid\n.\norg\n%\nENDCOLOR\n%\n \n \nwget\n \n-\nqO\n \n.\ncvmfswhitelist\n.\nnew\n \nhttp\n:\n//\noasis\n.\nopensciencegrid\n.\norg\n/\ncvmfs\n/%\nRED\n%\nexample\n.\nopensciencegrid\n.\norg\n%\nENDCOLOR\n%/\n.\ncvmfswhitelist\n \n \nmv\n \n.\ncvmfswhitelist\n.\nnew\n \n.\ncvmfswhitelist\n\n\n\n\n\n\n\n\nNote\n\n\nThis cronjob eliminates the need for the repository service administrator to periodically use\n\ncvmfs_server resign\n to update \n.cvmfswhitelist\n as described in the upstream CVMFS documentation.\n\n\n\n\n\n\n\n\nUpdate the open support ticket to indicate that the previous steps have been completed\n\n\n\n\n\n\nOnce the repository is fully replicated on the OSG, the VO may proceed in publishing into CVMFS using the\n\nLIBRARIAN\n account on the repository server.\n\n\n\n\nTip\n\n\nWe strongly recommend the repository maintainer read through the upstream documentation on\n\nmaintaining repositories\n and\n\ncontent limitations\n.\n\n\n\n\nIf the repository ends in \n.opensciencegrid.org\n or \n.osgstorage.org\n, the VO may ask for it to be replicated outside the US.  The\nVO should open a GGUS ticket following EGI's \nPROC20\n.\n\n\nReplacing an Existing OASIS Repository Server\n\n\nIf a need arises to replace a server for an existing \n*.opensciencegrid.org\n or \n*.osgstorage.org\n repository, there are two ways to do it:\none without changing the DNS name and one with changing it.\nThe latter can take longer because it requires OSG Operations intervention.\n\n\n\n\nRevision numbers must increase\n\n\nCVMFS does not allow repository revision numbers to decrease, so the instructions below make sure the revision numbers only go up.\n\n\n\n\nWithout changing the server DNS name\n\n\nIf you are recreating the repository on the same machine, use the following command to \nremove the repository configuration while preserving the data and keys:\n\n\nroot@host #\n cvmfs_server rmfs -p example.opensciencegrid.org\n\n\n\n\n\nOtherwise if it is a new machine, copy the keys from /etc/cvmfs/keys/\nexample.opensciencegrid.org\n.* and the data from /srv/cvmfs/\nexample.opensciencegrid.org\n from the old server to the new, making sure that no publish operations happen on the old server while you copy the data.\n\n\nThen in either case use \ncvmfs_server import\n instead of \ncvmfs_server mkfs\n in the above instructions for \nCreating the Repository\n, in order to reuse old data and keys.\n\n\nIf you run an old and a new machine in parallel for a while, make sure that when you put the new machine into production (by moving the DNS name) that the new machine has had at least as many publishes as the old machine, so the revision number does not decrease.\n\n\nWith changing the server DNS name\n\n\n\n\nNote\n\n\nIf you create a repository from scratch, as opposed to copying the data and keys from an old server, it is in fact better to change the DNS name of the server because that causes the OSG Operations server to reinitialize the .cvmfswhitelist.\n\n\n\n\nIf you create a replacement repository on a new machine from scratch, follow the normal instructions on this page above, but with the following differences in the \nHosting a Repository on OASIS\n section:\n\n\n\n\nIn step 2, instead of asking in the support ticket to create a new repository, give the new URL and ask them to change the repository registration to that URL.\n\n\n\n\nWhen you do the publish in step 5, add a \n-n NNNN\n option where \nNNNN\n is a revision number greater than the number on the existing repository.\n    That number can be found by this command on a client machine:\n\n\nuser@host $\n attr -qg revision /cvmfs/\nexample.opensciencegrid.org\n\n\n\n\n\n\n\n\n\n\nSkip step 6; there is no need to tell OSG Operations when you are finished.\n\n\n\n\nAfter enough time has elapsed for the publish to propagate to clients, typically around 15 minutes, verify that the new chosen revision has reached a client.\n\n\n\n\nRemoving a Repository from OASIS\n\n\nIn order to remove a repository that is being hosted on OASIS, perform the following steps:\n\n\n\n\nIf the repository has been replicated outside of the U.S., open a GGUS ticket asking that the replication be removed\n    from EGI Stratum-1s. Wait until this ticket is resolved before proceeding.\n\n\nOpen a \nsupport ticket\n asking to shut down the repository, giving the repository\n    name (e.g., \nexample.opensciencegrid.org\n), and the corresponding VO.", 
            "title": "Install an OASIS Repo"
        }, 
        {
            "location": "/data/external-oasis-repos/#install-an-oasis-repository", 
            "text": "OASIS (the  OSG   A pplication  S oftware  I nstallation  S ervice) is an infrastructure, based on CVMFS , for distributing software throughout the OSG.  Once software is installed into\nan OASIS repository, the goal is to make it available across about 90% of the OSG within an hour.  OASIS consists of keysigning infrastructure, a content distribution network (CDN), and a shared CVMFS repository that\nis hosted by the OSG.  Many use cases will be covered by utilizing the  shared repository ; this document\ncovers how to install, configure, and host your own CVMFS  repository server .  This server will distribute software\nvia OASIS, but will be hosted and operated externally from the OSG project.  OASIS-based distribution and key signing is available to OSG VOs or repositories affiliated with an OSG VO.\nSee the  policy page  for more information\non what repositories OSG is willing to distribute.", 
            "title": "Install an OASIS Repository"
        }, 
        {
            "location": "/data/external-oasis-repos/#before-starting", 
            "text": "CVMFS repositories work at the kernel filesystem layer, which adds more stringent host requirements than a typical\nOSG install.  The host OS must meet ONE of the following:   RHEL 7.3 (or equivalent) or later.   This option is recommended .  RHEL 6 with the aufs kernel module.   Additionally,   User IDs:  If it does not exist already, the installation will create the  cvmfs  Linux user  Group IDs:  If they do not exist already, the installation will create the Linux groups  cvmfs  and  fuse  Network ports:  This page will configure the repository to distribute using Apache HTTPD on port 8000.  At the\n    minimum, the repository needs in-bound access from the OASIS CDN.  Disk space:  This host will need enough free disk space to host  two  copies of the software: one compressed\n    and one uncompressed.  /srv/cvmfs  will hold all the published data (compressed and de-deuplicated).  The\n     /var/spool/cvmfs  directory will contain all the data in all current transactions (uncompressed).  Root access  will be needed to install.  Software install will be done as an unprivileged user.  Yum  will need to be  configured to use the OSG repositories .    Overlay-FS limitations  CVMFS on RHEL7 only supports Overlay-FS if the underlying filesystem is  ext3  or  ext4 ; make sure /var/spool/cvmfs  is one of these filesystem types.  If this is not possible, add  CVMFS_DONT_CHECK_OVERLAYFS_VERSION=yes  to your CVMFS configuration.  Using xfs  will work if it was created with  ftype=1", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/external-oasis-repos/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/data/external-oasis-repos/#rhel7-based-systems", 
            "text": "For a RHEL7-based system, installation is a straightforward install via  yum :  root@host #  yum install cvmfs-server osg-oasis", 
            "title": "RHEL7-based Systems"
        }, 
        {
            "location": "/data/external-oasis-repos/#rhel6-based-systems", 
            "text": "A RHEL6 host needs additional steps in order to add the AUFS2 kernel module.  root@host #  rpm -i https://cvmrepo.web.cern.ch/cvmrepo/yum/cvmfs-release-latest.noarch.rpm root@host #  yum install --enablerepo = cernvm-kernel --disablerepo = cernvm kernel aufs2-util cvmfs-server.x86_64 osg-oasis root@host #  reboot", 
            "title": "RHEL6-based Systems"
        }, 
        {
            "location": "/data/external-oasis-repos/#apache-and-repository-mounts", 
            "text": "For all installs, we recommend mounting all the local repositories on startup:  root@host #   echo   cvmfs_server mount -a   /etc/rc.local root@host #  chmod +x /etc/rc.local  The Apache HTTPD service should be configured to listen on port 8000, have the  KeepAlive  option enabled, and be\nstarted:  root@host #   echo  Listen  8000   /etc/httpd/conf.d/cvmfs.conf  root@host #   echo  KeepAlive on  /etc/httpd/conf.d/cvmfs.conf  root@host #  chkconfig httpd on  root@host #  service httpd start   Check Firewalls  Make sure that port 8000 is available to the Internet.  Check the setting of the host- and site-level firewalls.\nThe next steps will fail if the web server is not accessible.", 
            "title": "Apache and Repository Mounts"
        }, 
        {
            "location": "/data/external-oasis-repos/#creating-a-repository", 
            "text": "Prior to creation, the repository administrator will need to make two decisions:   Select a repository name ; typically, this is derived from the VO or project's name and ends in\n     opensciencegrid.org .  For example, the NoVA VO runs the repository  nova.opensciencegrid.org .  For this section,\n    we will use  example.opensciencegrid.org .  Select a repository owner : Software publication will need to run by a non- root  Unix user account; for this\n    document, we will use  LIBRARIAN  as the account name of the repository owner.   The initial repository creation must be run as  root :  root@host #   echo  -e  \\*\\\\t\\\\t-\\\\tnofile\\\\t\\\\t16384   /etc/security/limits.conf root@host #   ulimit  -n  16384  root@host #  cvmfs_server mkfs -o  LIBRARIAN   example.opensciencegrid.org  root@host #  cat  /srv/cvmfs/ example.opensciencegrid.org /.htaccess  xEOFx  Order deny,allow  Deny from all  Allow from 127.0.0.1  Allow from ::1  Allow from 129.79.53.0/24 129.93.244.192/26 129.93.227.64/26  Allow from 2001:18e8:2:6::/56 2600:900:6::/48   xEOFx   Here, we increase the number of open files allowed, create the repository using the  mkfs  command, and then limit the hosts that are allowed to access the repo to the OSG CDN.  Next, adjust the configuration in the repository as follows.    root@host #  cat  /etc/cvmfs/repositories.d/ example.opensciencegrid.org /server.conf  xEOFx  CVMFS_AUTO_TAG_TIMESPAN= 2 weeks ago  CVMFS_IGNORE_XDIR_HARDLINKS=true  CVMFS_GENERATE_LEGACY_BULK_CHUNKS=false  CVMFS_AUTOCATALOGS=true  CVMFS_ENFORCE_LIMITS=true  CVMFS_FORCE_REMOUNT_WARNING=false  xEOFx   Also, check the  cvmfs documentation  for additional recommendations for special purpose repositories.  Now verify that the repository is readable over HTTP:  root@host #  wget -qO- http://localhost:8000/cvmfs/ example.opensciencegrid.org /.cvmfswhitelist  |  cat -v  That should print several lines including some gibberish at the end.", 
            "title": "Creating a Repository"
        }, 
        {
            "location": "/data/external-oasis-repos/#hosting-a-repository-on-oasis", 
            "text": "In order to host a repository on OASIS, perform the following steps:    Verify your VO's registration is up-to-date .  All repositories need to be associated with a VO; the VO needs to\n    assign an  OASIS manager  in Topology who would be responsible for the contents of any of the VO's repositories and\n    will be contacted in case of issues. To designate an OASIS manager, have the VO manager update the\n     Topology registration .    Create a  support ticket  using the following template:  Please   add   a   new   CVMFS   repository   to   OASIS   for   VO   VO   NAME   using   the   URL \n     http : // FQDN : 8000 / cvmfs / OASIS   REPOSITORY  The   VO   responsible   manager   will   be   OASIS   MANAGER .  Replace the  ANGLE BRACKET TEXT  items with the appropriate values.    If the repository name matches  *.opensciencegrid.org  or  *.osgstorage.org , wait for the go-ahead from the OSG\n    representative before continuing with the remaining instructions; for all other repositories (such as  *.egi.eu ),\n    you are done.    One you are told in the ticket to proceed to the next step, execute the following commands:  root@host #  wget -O /srv/cvmfs/ example.opensciencegrid.org /.cvmfswhitelist  \\ \n            http://oasis.opensciencegrid.org/cvmfs/ example.opensciencegrid.org /.cvmfswhitelist  root@host #  /bin/cp /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  \\ \n            /etc/cvmfs/keys/ example.opensciencegrid.org .pub  Replace  example.opensciencegrid.org  as appropriate.    Verify that publishing operation succeeds:  root@host #  su  LIBRARIAN  -c  cvmfs_server transaction  example.opensciencegrid.org  root@host #  su  LIBRARIAN  -c  cvmfs_server publish  example.opensciencegrid.org   Within an hour, the repository updates should appear at the OSG Operations and FNAL Stratum-1 servers.  On success, make sure the whitelist update happens daily by creating  /etc/cron.d/fetch-cvmfs-whitelist  with the\nfollowing contents:  5   4   *   *   *   % BLUE % LIBRARIAN % ENDCOLOR %   cd   / srv / cvmfs /% RED % example . opensciencegrid . org % ENDCOLOR %     wget   - qO   . cvmfswhitelist . new   http : // oasis . opensciencegrid . org / cvmfs /% RED % example . opensciencegrid . org % ENDCOLOR %/ . cvmfswhitelist     mv   . cvmfswhitelist . new   . cvmfswhitelist    Note  This cronjob eliminates the need for the repository service administrator to periodically use cvmfs_server resign  to update  .cvmfswhitelist  as described in the upstream CVMFS documentation.     Update the open support ticket to indicate that the previous steps have been completed    Once the repository is fully replicated on the OSG, the VO may proceed in publishing into CVMFS using the LIBRARIAN  account on the repository server.   Tip  We strongly recommend the repository maintainer read through the upstream documentation on maintaining repositories  and content limitations .   If the repository ends in  .opensciencegrid.org  or  .osgstorage.org , the VO may ask for it to be replicated outside the US.  The\nVO should open a GGUS ticket following EGI's  PROC20 .", 
            "title": "Hosting a Repository on OASIS"
        }, 
        {
            "location": "/data/external-oasis-repos/#replacing-an-existing-oasis-repository-server", 
            "text": "If a need arises to replace a server for an existing  *.opensciencegrid.org  or  *.osgstorage.org  repository, there are two ways to do it:\none without changing the DNS name and one with changing it.\nThe latter can take longer because it requires OSG Operations intervention.   Revision numbers must increase  CVMFS does not allow repository revision numbers to decrease, so the instructions below make sure the revision numbers only go up.", 
            "title": "Replacing an Existing OASIS Repository Server"
        }, 
        {
            "location": "/data/external-oasis-repos/#without-changing-the-server-dns-name", 
            "text": "If you are recreating the repository on the same machine, use the following command to \nremove the repository configuration while preserving the data and keys:  root@host #  cvmfs_server rmfs -p example.opensciencegrid.org  Otherwise if it is a new machine, copy the keys from /etc/cvmfs/keys/ example.opensciencegrid.org .* and the data from /srv/cvmfs/ example.opensciencegrid.org  from the old server to the new, making sure that no publish operations happen on the old server while you copy the data.  Then in either case use  cvmfs_server import  instead of  cvmfs_server mkfs  in the above instructions for  Creating the Repository , in order to reuse old data and keys.  If you run an old and a new machine in parallel for a while, make sure that when you put the new machine into production (by moving the DNS name) that the new machine has had at least as many publishes as the old machine, so the revision number does not decrease.", 
            "title": "Without changing the server DNS name"
        }, 
        {
            "location": "/data/external-oasis-repos/#with-changing-the-server-dns-name", 
            "text": "Note  If you create a repository from scratch, as opposed to copying the data and keys from an old server, it is in fact better to change the DNS name of the server because that causes the OSG Operations server to reinitialize the .cvmfswhitelist.   If you create a replacement repository on a new machine from scratch, follow the normal instructions on this page above, but with the following differences in the  Hosting a Repository on OASIS  section:   In step 2, instead of asking in the support ticket to create a new repository, give the new URL and ask them to change the repository registration to that URL.   When you do the publish in step 5, add a  -n NNNN  option where  NNNN  is a revision number greater than the number on the existing repository.\n    That number can be found by this command on a client machine:  user@host $  attr -qg revision /cvmfs/ example.opensciencegrid.org     Skip step 6; there is no need to tell OSG Operations when you are finished.   After enough time has elapsed for the publish to propagate to clients, typically around 15 minutes, verify that the new chosen revision has reached a client.", 
            "title": "With changing the server DNS name"
        }, 
        {
            "location": "/data/external-oasis-repos/#removing-a-repository-from-oasis", 
            "text": "In order to remove a repository that is being hosted on OASIS, perform the following steps:   If the repository has been replicated outside of the U.S., open a GGUS ticket asking that the replication be removed\n    from EGI Stratum-1s. Wait until this ticket is resolved before proceeding.  Open a  support ticket  asking to shut down the repository, giving the repository\n    name (e.g.,  example.opensciencegrid.org ), and the corresponding VO.", 
            "title": "Removing a Repository from OASIS"
        }, 
        {
            "location": "/data/gridftp/", 
            "text": "Installing and Maintaining a GridFTP Server\n\n\nAbout This Guide\n\n\nThis page explains how to install the stand-alone Globus GridFTP server.\n\n\nThe GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:\n\n\n\n\nYou are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores\n\n\nYour site will be managing more than 50 TB of disk space\n\n\nA simple front-end to a filesystem allowing access over WAN - for example NFS.\n\n\n\n\n\n\nNote\n\n\nThis document is for a standalone GridFTP server on top of POSIX storage.  We have two specialized documents\nfor Hadoop Distributed File System (HDFS) and XRootD based storage:\n\n\n\n\nInstall and configure a GridFTP server on top of HDFS.\n\n\nInstall and configure a GridFTP server on top of XRootD.\n\n\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process you will need to fulfill these prerequisites.\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\nService certificate: The GridFTP service uses a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports: GridFTP listens on TCP port 2811 and the list of ports configured by the \nGLOBUS_TCP_SOURCE_RANGE\n environment variable.\n\n\n\n\nInstalling GridFTP\n\n\nFirst, you will need to install the GridFTP meta-package:\n\n\nroot@host #\n yum install osg-gridftp\n\n\n\n\n\nConfiguring GridFTP\n\n\nConfiguring authentication\n\n\nTo configure which virtual organizations and users are allowed to use your GridFTP server, follow the instructions in\n\nthe LCMAPS VOMS plugin document\n.\n\n\nEnabling GridFTP transfer probe\n\n\nThe Gratia probe requires the file \nuser-vo-map\n to exist and be up to date.\nAssuming you installed GridFTP using the \nosg-se-hadoop-gridftp\n rpm, the Gratia Transfer Probe will already be installed.\n\n\nHere are the most relevant file and directory locations:\n\n\n\n\n\n\n\n\nPurpose\n\n\nNeeds Editing?\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nProbe Configuration\n\n\nYes\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\n\n\n\n\nProbe Executables\n\n\nNo\n\n\n/usr/share/gratia/gridftp-transfer\n\n\n\n\n\n\nLog files\n\n\nNo\n\n\n/var/log/gratia\n\n\n\n\n\n\nTemporary files\n\n\nNo\n\n\n/var/lib/gratia/tmp\n\n\n\n\n\n\n\n\nThe RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file\n\n\n/\netc\n/\ngratia\n/\ngridftp\n-\ntransfer\n/\nProbeConfig\n\n\n\n\n\n\nThis is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nProbeName\n\n\nMaybe\n\n\nThis should be set to \"gridftp-transfer:\n\", where \n is the fully-qualified domain name of your gridftp host.\n\n\n\n\n\n\nCollectorHost\n\n\nMaybe\n\n\nSet to the hostname and port of the central collector. By default it sends to the OSG collector. See below.\n\n\n\n\n\n\nSiteName\n\n\nYes\n\n\nSet to the resource group name of your site as registered in OIM.\n\n\n\n\n\n\nGridftpLogDir\n\n\nYes\n\n\nSet to /var/log, or wherever your current gridftp logs are located\n\n\n\n\n\n\nGrid\n\n\nMaybe\n\n\nSet to \"ITB\" if this is a test resource; otherwise, leave as OSG.\n\n\n\n\n\n\nUserVOMapFile\n\n\nNo\n\n\nThis should be set to /var/lib/osg/user-vo-map; see below for information about this file.\n\n\n\n\n\n\nSuppressUnknownVORecords\n\n\nMaybe\n\n\nSet to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended.\n\n\n\n\n\n\nSuppressNoDNRecords\n\n\nMaybe\n\n\nSet to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.\n\n\n\n\n\n\nEnableProbe\n\n\nYes\n\n\nSet to 1 to enable the probe.\n\n\n\n\n\n\n\n\nSelecting a collector host\n\n\nThe collector is the central server which logs the GridFTP transfers into a database. There are usually three options:\n\n\n\n\nOSG Transfer Collector\n: This is the primary collector for transfers in the OSG. Use \nCollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\"\n.\n\n\nOSG-ITB Transfer Collector\n: This is the test collector for transfers in the OSG. Use \nCollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\"\n.\n\n\n\n\nValidation\n\n\nRun the Gratia probe once by hand to check for functionality:\n\n\nroot@host #\n /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver\n\n\n\n\n\nLook for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in \n/var/log/gratia/\ndate\n.log\n and make sure there are no error messages printed.\n\n\nOptional configuration\n\n\nSetting transfer limits for GridFTP-HDFS\n\n\nTo set a limit on the total or per-user number of transfers, create \n/etc/sysconfig/gridftp-hdfs\n and set the following configuration:\n\n\nexport\n \nGRIDFTP_TRANSFER_LIMIT\n=\n80\n\n\nexport\n \nGRIDFTP_DEFAULT_USER_TRANSFER_LIMIT\n=\n50\n\n\nexport\n \nGRIDFTP_\n%\nRED\n%\nUNIX\n \nUSERNAME\n%\nENDCOLOR\n%\n_USER_TRANSFER_LIMIT\n=\n40\n\n\n\n\n\n\nIn the above configuration:\n\n\n\n\nThere would be no more than 80 transfers going at a time, across all users.\n\n\nBy default, any single user can have no more than 50 transfers at a time.\n\n\nThe \nUNIX USERNAME\n user has a more stringent limit of 40 transfers at a time.\n\n\n\n\n\n\nNote\n\n\nThis limits are per gridftp server. If you have several gridftp servers you may want to have this limits divided by the number of gridftp servers at your site.\n\n\n\n\nModifying the environment\n\n\nEnvironment variables are stored in \n/etc/sysconfig/globus-gridftp-server\n which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.\n\n\n#Uncomment and modify for firewalls\n\n\n#export GLOBUS_TCP_PORT_RANGE=min,max\n\n\n#export GLOBUS_TCP_SOURCE_RANGE=min,max\n\n\n\n\n\n\nNote that the variables \nGLOBUS_TCP_PORT_RANGE\n and \nGLOBUS_TCP_SOURCE_RANGE\n can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/globus-gridftp-server\n and choose a higher debug level:\n\n\n#\n \nlevel\n \n0\n:\n \nno\n \nmessages\n,\n \n1\n:\n \nerrors\n,\n \n2\n:\n \nalso\n \nwarnings\n,\n \n3\n:\n \nalso\n \nnotices\n,\n\n\n#\n  \n4\n:\n \nalso\n \ninfo\n,\n \n5\n:\n \nmaximum\n \ndebug\n\n\nLCMAPS_DEBUG_LEVEL\n=\n2\n\n\n\n\n\n\nOutput goes to \n/var/log/messages\n by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.\n\n\nConfiguring a multi-homed server\n\n\nThe GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file \n/etc/gridftp.conf\n.\n\n\nTo use a single interface you can set \nhostname\n to the Hostname or IP address to use:\n\n\nhostname IP-TO-USE\n\n\n\n\n\nYou can also set separately the \ncontrol_interface\n, \ndata_interface\n and \nipc_interface\n.  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting \ndata_interface\n:\n\n\ncontrol_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE\n\n\n\n\n\nFor more options available for the GridFTP server, read the comments in the configuration file (\n/etc/gridftp.conf\n) or\nsee the \nGridFTP manual\n.\n\n\nManaging GridFTP\n\n\nIn addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nGridFTP\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\n\n\nValidating GridFTP\n\n\nThe GridFTP service can be validated by using globus-url-copy. You will need to run \ngrid-proxy-init\n or \nvoms-proxy-init\n in order to get a valid user proxy in order to communicate with the GridFTP server.\n\n\nroot@host #\n globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero\n\nroot@host #\n \necho\n \n$?\n\n\n0\n\n\n\n\n\n\nRun the validation as an unprivileged user; when invoked as root, \nglobus-url-copy\n will attempt to use the host certificate instead of your user certificate, with confusing results.\n\n\nGetting Help\n\n\nFor assistance, please use \nthis page\n.\n\n\nReference\n\n\n\n\nGridFTP administration manual\n\n\nGridFTP tutorial\n\n\n\n\nConfiguration and Log Files\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/etc/sysconfig/globus-gridftp-server\n\n\nEnvironment variables for GridFTP and LCMAPS\n\n\n\n\n\n\n\n\n/usr/share/osg/sysconfig/globus-gridftp-server-plugin\n\n\nWhere environment variables for GridFTP plugin are included\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/gratia/gridftp-transfer/ProbeConfig\n\n\nGridFTP Gratia Probe configuration\n\n\n\n\n\n\nGratia Probe\n\n\n/etc/cron.d/gratia-probe-gridftp-transfer.cron\n\n\nCron tab file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nGridFTP\n\n\n/var/log/gridftp.log\n\n\nGridFTP transfer log\n\n\n\n\n\n\n\n\n/var/log/gridftp-auth.log\n\n\nGridFTP authorization log\n\n\n\n\n\n\nGratia probe\n\n\n/var/logs/gratia\n\n\n\n\n\n\n\n\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nInstructions\n to request a service certificate.\n\n\nYou will also need a copy of CA certificates.\n\n\nUsers\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS should be created on the GridFTP host.\n\n\nFor example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_PORT_RANGE\n\n\nX\n\n\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP data channels\n\n\ntcp\n\n\nGLOBUS_TCP_SOURCE_RANGE\n\n\n\n\nX\n\n\ncontiguous range of ports is necessary.\n\n\n\n\n\n\nGridFTP control channel\n\n\ntcp\n\n\n2811\n\n\nX", 
            "title": "Install GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#installing-and-maintaining-a-gridftp-server", 
            "text": "", 
            "title": "Installing and Maintaining a GridFTP Server"
        }, 
        {
            "location": "/data/gridftp/#about-this-guide", 
            "text": "This page explains how to install the stand-alone Globus GridFTP server.  The GridFTP package contains components necessary to set up a stand-alone gsiftp server and tools used to monitor and report its performance. A stand-alone GridFTP server might be used under the following circumstances:   You are serving VOs that use storage heavily (CMS, ATLAS, CDF, and D0) and your site has more than 250 cores  Your site will be managing more than 50 TB of disk space  A simple front-end to a filesystem allowing access over WAN - for example NFS.    Note  This document is for a standalone GridFTP server on top of POSIX storage.  We have two specialized documents\nfor Hadoop Distributed File System (HDFS) and XRootD based storage:   Install and configure a GridFTP server on top of HDFS.  Install and configure a GridFTP server on top of XRootD.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/data/gridftp/#before-starting", 
            "text": "Before starting the installation process you will need to fulfill these prerequisites.   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates  Service certificate: The GridFTP service uses a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports: GridFTP listens on TCP port 2811 and the list of ports configured by the  GLOBUS_TCP_SOURCE_RANGE  environment variable.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/gridftp/#installing-gridftp", 
            "text": "First, you will need to install the GridFTP meta-package:  root@host #  yum install osg-gridftp", 
            "title": "Installing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-gridftp", 
            "text": "", 
            "title": "Configuring GridFTP"
        }, 
        {
            "location": "/data/gridftp/#configuring-authentication", 
            "text": "To configure which virtual organizations and users are allowed to use your GridFTP server, follow the instructions in the LCMAPS VOMS plugin document .", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/data/gridftp/#enabling-gridftp-transfer-probe", 
            "text": "The Gratia probe requires the file  user-vo-map  to exist and be up to date.\nAssuming you installed GridFTP using the  osg-se-hadoop-gridftp  rpm, the Gratia Transfer Probe will already be installed.  Here are the most relevant file and directory locations:     Purpose  Needs Editing?  Location      Probe Configuration  Yes  /etc/gratia/gridftp-transfer/ProbeConfig    Probe Executables  No  /usr/share/gratia/gridftp-transfer    Log files  No  /var/log/gratia    Temporary files  No  /var/lib/gratia/tmp     The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file  / etc / gratia / gridftp - transfer / ProbeConfig   This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:     Attribute  Needs Editing  Value      ProbeName  Maybe  This should be set to \"gridftp-transfer: \", where   is the fully-qualified domain name of your gridftp host.    CollectorHost  Maybe  Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below.    SiteName  Yes  Set to the resource group name of your site as registered in OIM.    GridftpLogDir  Yes  Set to /var/log, or wherever your current gridftp logs are located    Grid  Maybe  Set to \"ITB\" if this is a test resource; otherwise, leave as OSG.    UserVOMapFile  No  This should be set to /var/lib/osg/user-vo-map; see below for information about this file.    SuppressUnknownVORecords  Maybe  Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended.    SuppressNoDNRecords  Maybe  Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.    EnableProbe  Yes  Set to 1 to enable the probe.", 
            "title": "Enabling GridFTP transfer probe"
        }, 
        {
            "location": "/data/gridftp/#selecting-a-collector-host", 
            "text": "The collector is the central server which logs the GridFTP transfers into a database. There are usually three options:   OSG Transfer Collector : This is the primary collector for transfers in the OSG. Use  CollectorHost=\"gratia-osg-prod.opensciencegrid.org:80\" .  OSG-ITB Transfer Collector : This is the test collector for transfers in the OSG. Use  CollectorHost=\" gratia-osg-itb.opensciencegrid.org:80\" .", 
            "title": "Selecting a collector host"
        }, 
        {
            "location": "/data/gridftp/#validation", 
            "text": "Run the Gratia probe once by hand to check for functionality:  root@host #  /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver  Look for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in  /var/log/gratia/ date .log  and make sure there are no error messages printed.", 
            "title": "Validation"
        }, 
        {
            "location": "/data/gridftp/#optional-configuration", 
            "text": "", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/gridftp/#setting-transfer-limits-for-gridftp-hdfs", 
            "text": "To set a limit on the total or per-user number of transfers, create  /etc/sysconfig/gridftp-hdfs  and set the following configuration:  export   GRIDFTP_TRANSFER_LIMIT = 80  export   GRIDFTP_DEFAULT_USER_TRANSFER_LIMIT = 50  export   GRIDFTP_ % RED % UNIX   USERNAME % ENDCOLOR % _USER_TRANSFER_LIMIT = 40   In the above configuration:   There would be no more than 80 transfers going at a time, across all users.  By default, any single user can have no more than 50 transfers at a time.  The  UNIX USERNAME  user has a more stringent limit of 40 transfers at a time.    Note  This limits are per gridftp server. If you have several gridftp servers you may want to have this limits divided by the number of gridftp servers at your site.", 
            "title": "Setting transfer limits for GridFTP-HDFS"
        }, 
        {
            "location": "/data/gridftp/#modifying-the-environment", 
            "text": "Environment variables are stored in  /etc/sysconfig/globus-gridftp-server  which is sourced on service startup.  If you want to change LCMAPS log levels, or GridFTP port ranges, you can edit them there.  #Uncomment and modify for firewalls  #export GLOBUS_TCP_PORT_RANGE=min,max  #export GLOBUS_TCP_SOURCE_RANGE=min,max   Note that the variables  GLOBUS_TCP_PORT_RANGE  and  GLOBUS_TCP_SOURCE_RANGE  can be set here to allow GridFTP to navigate around firewall rules (these affect the inbound and outbound ports, respectively).  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/globus-gridftp-server  and choose a higher debug level:  #   level   0 :   no   messages ,   1 :   errors ,   2 :   also   warnings ,   3 :   also   notices ,  #    4 :   also   info ,   5 :   maximum   debug  LCMAPS_DEBUG_LEVEL = 2   Output goes to  /var/log/messages  by default. Do not set logging to 5 on any production systems as that may cause systems to slow down significantly or become unresponsive.", 
            "title": "Modifying the environment"
        }, 
        {
            "location": "/data/gridftp/#configuring-a-multi-homed-server", 
            "text": "The GridFTP uses control connections, data connections and IPC connections. By default it listens in all interfaces but this can be changed by editing the configuration file  /etc/gridftp.conf .  To use a single interface you can set  hostname  to the Hostname or IP address to use:  hostname IP-TO-USE  You can also set separately the  control_interface ,  data_interface  and  ipc_interface .  On systems that have multiple network interfaces, you may want to associate data transfers with the fastest possible NIC available. This can be done in the GridFTP server by setting  data_interface :  control_interface IP-TO-USE\ndata_interface IP-TO-USE\nipc_interface IP-TO-USE  For more options available for the GridFTP server, read the comments in the configuration file ( /etc/gridftp.conf ) or\nsee the  GridFTP manual .", 
            "title": "Configuring a multi-homed server"
        }, 
        {
            "location": "/data/gridftp/#managing-gridftp", 
            "text": "In addition to the GridFTP service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    GridFTP  globus-gridftp-server", 
            "title": "Managing GridFTP"
        }, 
        {
            "location": "/data/gridftp/#validating-gridftp", 
            "text": "The GridFTP service can be validated by using globus-url-copy. You will need to run  grid-proxy-init  or  voms-proxy-init  in order to get a valid user proxy in order to communicate with the GridFTP server.  root@host #  globus-url-copy file:///tmp/zero.source gsiftp://yourhost.yourdomain/tmp/zero root@host #   echo   $?  0   Run the validation as an unprivileged user; when invoked as root,  globus-url-copy  will attempt to use the host certificate instead of your user certificate, with confusing results.", 
            "title": "Validating GridFTP"
        }, 
        {
            "location": "/data/gridftp/#getting-help", 
            "text": "For assistance, please use  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/gridftp/#reference", 
            "text": "GridFTP administration manual  GridFTP tutorial", 
            "title": "Reference"
        }, 
        {
            "location": "/data/gridftp/#configuration-and-log-files", 
            "text": "Service/Process  Configuration File  Description      GridFTP  /etc/sysconfig/globus-gridftp-server  Environment variables for GridFTP and LCMAPS     /usr/share/osg/sysconfig/globus-gridftp-server-plugin  Where environment variables for GridFTP plugin are included    Gratia Probe  /etc/gratia/gridftp-transfer/ProbeConfig  GridFTP Gratia Probe configuration    Gratia Probe  /etc/cron.d/gratia-probe-gridftp-transfer.cron  Cron tab file        Service/Process  Log File  Description      GridFTP  /var/log/gridftp.log  GridFTP transfer log     /var/log/gridftp-auth.log  GridFTP authorization log    Gratia probe  /var/logs/gratia", 
            "title": "Configuration and Log Files"
        }, 
        {
            "location": "/data/gridftp/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem     Instructions  to request a service certificate.  You will also need a copy of CA certificates.", 
            "title": "Certificates"
        }, 
        {
            "location": "/data/gridftp/#users", 
            "text": "For this package to function correctly, you will have to create the users needed for grid operation. Any Unix username that can be mapped by LCMAPS VOMS should be created on the GridFTP host.  For example, VOs newly-added to the LCMAPS VOMS configuration will not be able to transfer files until the corresponding Unix user account is created.", 
            "title": "Users"
        }, 
        {
            "location": "/data/gridftp/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      GridFTP data channels  tcp  GLOBUS_TCP_PORT_RANGE  X   contiguous range of ports is necessary.    GridFTP data channels  tcp  GLOBUS_TCP_SOURCE_RANGE   X  contiguous range of ports is necessary.    GridFTP control channel  tcp  2811  X", 
            "title": "Networking"
        }, 
        {
            "location": "/data/load-balanced-gridftp/", 
            "text": "Load Balancing GridFTP\n\n\nGridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a \nload balancer\n to distribute requests and present the appearance of a single high-throughput GridFTP server.\n\n\nOne general-purpose technology for implementing a load balancer on Linux is \nLinux Virtual Server\n (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the \nIP Virtual Server\n kernel module, which can be managed by userspace services on the load balancers such as \nkeepalived\n.\n\n\nThis guide explains how to install, configure, run, test, and troubleshoot the \nkeepalived\n service on a load balancing host for a set of \nGridFTP\n servers.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following requirements:\n\n\n\n\nThere must be a shared file system for file propagation across GridFTP servers\n\n\nYou must have reserved a virtual IP address and associated virtual hostname\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to each host\n\n\n\n\nDesigning Your Load-Balanced GridFTP System\n\n\nBefore beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements.\n\n\nGridFTP servers\n\n\nThe number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth.\n\n\nShared file system\n\n\nThe number of GridFTP servers can also be determined by your hardware needs and by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running \nGridFTP with Hadoop\n, plan for machines with more memory, or more GridFTP hosts to distribute the memory load.\n\n\n\n\nNote\n\n\nIf you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the \nstandalone-GridFTP installation guide\n.\n\n\n\n\nLoad balancer(s)\n\n\nIn the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, at least one additional load balancer will be needed as a backup.\n\n\nPreparing the GridFTP Servers\n\n\nBefore adding your GridFTP hosts to the load-balanced system, each host requires the following:\n\n\n\n\nGridFTP software\n\n\nSpecial host certificates\n\n\nLoad-balancing configuration\n\n\n\n\nAcquiring host certificate(s)\n\n\nWhen authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the \nOSG host certificate reference\n for more information on how to request these types of certificates.\n\n\nIf your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers.\n\n\nWithout XRootD\n\n\nThe single shared certificate must have the hostname associated with the load-balanced GridFTP system as its \ncommon name\n and each GridFTP servers hostname listed as \nsubject alternative names\n.\n\n\n\n\n\n\nRequest and generate the shared certificate:\n\n\nuser@host $\n osg-cert-request --hostname \nVIRTUAL-HOSTNAME\n \n\\\n\n            --country \nCOUNTRY\n \n\\\n\n            --state \nSTATE\n \n\\\n\n            --locality \nLOCALITY\n \n\\\n\n            --organization \nORGANIZATION\n\n\n            --altname \nGRIDFTP-SERVER-#1-HOSTNAME\n \\\n\n\n            --altname \nGRIDFTP-SERVER-#2-HOSTNAME\n\n\n\n\n\n\n\n\n\n\nTake the resulting CSR and get it signed by the appropriate authority.\n    Most institutions can use InCommon as outlined \nhere\n.\n\n\n\n\n\n\nCreate a directory to contain the shared certificate:\n\n\nroot@host #\n mkdir /etc/grid-security/gridftp\n\n\n\n\n\n\n\n\n\nPlace the shared certificate-key pair in the newly created directory:\n\n\nroot@host #\n mv \nPATH-TO-SERVICE-CERT\n /etc/grid-security/gridftp/gridftp-hostcert.pem\n\nroot@host #\n mv \nPATH-TO-SERVICE-KEY\n /etc/grid-security/gridftp/gridftp-hostkey.pem\n\n\n\n\n\n\n\n\n\nEdit \n/etc/sysconfig/globus-gridftp-server\n to identify the shared certificate-key pair:\n\n\nexport\n \nX509_USER_CERT\n=/\netc\n/\ngrid\n-\nsecurity\n/\ngridftp\n/\ngridftp\n-\nhostcert\n.\npem\n\n\nexport\n \nX509_USER_KEY\n=/\netc\n/\ngrid\n-\nsecurity\n/\ngridftp\n/\ngridftp\n-\nhostkey\n.\npem\n\n\n\n\n\n\n\n\n\n\nWith XRootD\n\n\nXRootD requires that the certificate's \ncommon name\n refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the \nsubject alternative name\n for each certificate.\n\n\n\n\n\n\nCreate a list of GridFTP server hostnames in \nload-balanced-hosts.txt\n:\n\n\nGRIDFTP\n-\nSERVER\n-#\n1\n-\nHOSTNAME\n \nVIRTUAL\n-\nHOSTNAME\n\n\nGRIDFTP\n-\nSERVER\n-#\n2\n-\nHOSTNAME\n \nVIRTUAL\n-\nHOSTNAME\n\n\n[...]\n\n\n\n\n\n\n\n\n\n\nSubmit a batch request for the per-GridFTP server certificates:\n\n\n::: console\n\n\nuser@host $\n osg-gridadmin-cert-request -f load-balanced-hosts.txt\n\n\n\n\n\n\n\n\n\nCopy the resulting certificates and keys to their corresponding GridFTP servers in \n/etc/grid-security/hostcert.pem\n and \n/etc/grid-security/hostkey.pem\n, respectively.\n\n\n\n\n\n\nInstalling GridFTP\n\n\nWhether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for \ninstalling a standalone GridFTP server\n for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section.\n\n\nConfiguring your GridFTP servers\n\n\nEach GridFTP server requires changes to its IP configuration and potentially its arptables:\n\n\n\n\nAdding your virtual IP address\n\n\nDisabling ARP\n \u2212 if your GridFTP servers are on the same network segment as the virtual IP\n\n\n\n\nAdding your virtual IP address\n\n\nUse the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers.\n\n\n\n\n\n\nAdd the virtual IP using the \nip\n tool:\n\n\nroot@host #\n ip addr add \nVIRTUAL-IP-ADDRESS\n/\nSUBNET-MASK\n dev \nNETWORK-INTERFACE\n\n\n\n\n\n\n\n\n\n\nTo persist the virtual IP changes across reboots, edit \n/etc/rc.d/rc.local\n, and add the same command as used above.\n\n\n\n\nMake sure that \n/etc/rc.d/rc.local\n is executable:\nroot@host #\n chmod u+x /etc/rc.d/rc.local\n\n\n\n\n\n\n\n\n\nDisabling ARP\n\n\nIf your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid \nARP race conditions\n. Otherwise, skip to \nthe section on preparing keepalived\n.\n\n\n\n\n\n\nSelect the appropriate RPM:\n\n\n\n\n\n\n\n\nIf your operating system version is...\n\n\nThen use the following package(s)...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 6\n\n\narptables_jf\n\n\n\n\n\n\nEnterprise Linux 7\n\n\narptables\n\n\n\n\n\n\n\n\n\n\n\n\nInstall the arptables software:\n\n\nroot@host #\n yum install \nPACKAGE\n\n\n\n\n\n\n\n\n\n\nDisable ARP:\n\n\nroot@host #\n arptables -F \n\nroot@host #\n arptables -A IN -d \nVIRTUAL-IP-ADDRESS\n -j DROP \n\nroot@host #\n arptables -A OUT -s \nVIRTUAL-IP-ADDRESS\n -j mangle --mangle-ip-s \nGRIDFTP-REAL-IP-ADDRESS\n\n\n\n\n\n\n\n\n\n\nSave ARP tables to survive reboots:\n\n\nroot@host #\n arptables-save \n /etc/sysconfig/arptables\n\n\n\n\n\n\n\n\n\nPreparing Keepalived Load Balancer(s)\n\n\nInstalling Keepalived\n\n\nWhether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the \nkeepalived\n software installed, configured, and running.\n\n\n\n\nNote\n\n\nDo not install \nkeepalived\n on the GridFTP servers themselves.\n\n\n\n\nThe \nkeepalived\n package is available from standard operating system repositories. Install it on each load balancer host using the following commands:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall the \nkeepalived\n package:\n\n\nroot\n@host\n \n#\n \nyum\n \ninstall\n \nkeepalived\n\n\n\n\n\n\n\n\n\n\nRequired configuration\n\n\nOn the primary load balancer, edit \n/etc/keepalived/keepalived.conf\n:\n\n\nglobal_defs\n {\n   \nrouter_id\n \nSTRING\n-\nLABEL\n-\nFOR\n-\nYOUR\n-\nLOAD\n-\nBALANCED\n-\nSYSTEM\n\n}\n\n\nvrrp_instance\n \nVI_gridftp\n {\n    \nstate\n \nMASTER\n\n    \ninterface\n \nNETWORK\n-\nINTERFACE\n\n    \nvirtual_router_id\n \nINTEGER\n-\nBETWEEN\n-\n0\n-\nAND\n-\n255\n\n    \npriority\n \n100\n\n    \nvirtual_ipaddress\n {\n        \nVIRTUAL\n-\nIP\n-\nADDRESS\n/\nSUBNET\n-\nMASK\n \ndev\n \nNETWORK\n-\nINTERFACE\n\n    }\n}\n\n\nvirtual_server\n \nVIRTUAL\n-\nIP\n-\nADDRESS\n \n2811\n {\n    \ndelay_loop\n \n10\n\n    \nlb_algo\n \nwlc\n\n    \nlb_kind\n \nDR\n\n    \nprotocol\n \ntcp\n\n\n    \nreal_server\n \nGRIDFTP\n-\nSERVER\n-\n#1\n-\nIP\n \nADDRESS\n {\n        \nTCP_CHECK\n {\n            \nconnect_timeout\n \n3\n\n            \nconnect_port\n \n2811\n\n        }\n    }\n    \nreal_server\n \nGRIDFTP\n-\nSERVER\n-\n#2\n-\nIP\n-\nADDRESS\n {\n        \nTCP_CHECK\n {\n            \nconnect_timeout\n \n3\n\n            \nconnect_port\n \n2811\n\n        }\n    }\n    [...]\n}\n\n\n\n\n\n\n\nNote\n\n\nUse the same \nVIRTUAL-IP-ADDRESS\n throughout the configuration of your load-balanced GridFTP system.\n\n\n\n\n\n\nNote\n\n\nIn the \nvirtual_server\n section, write one \nreal_server\n subsection for each GridFTP server behind the load balancer.\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to \nthe section on using keepalived\n.\n\n\n\n\nAdding backup load balancers\n\n\nEnabling e-mail notifications\n\n\n\n\nAdding backup load balancers\n\n\nIf you need to add backup load balancers, copy \n/etc/keepalived/keepalived.conf\n from your primary load balancer and change the \nstate\n and \npriority\n attributes under your \nvrrp_instance VI_gridftp\n section:\n\n\n\n\nNote\n\n\nPriority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference.\n\n\n\n\nvrrp_instance\n \nVI_gridftp\n \n{\n\n    \nstate\n \nBACKUP\n\n    \ninterface\n \nNETWORK\n-\nINTERFACE\n\n    \nvirtual_router_id\n \nSAME\n-\nID\n-\nAS\n-\nMASTER\n-\nLOAD\n-\nBALANCER\n\n    \npriority\n \nPRIORITY\n-\nINTEGER\n\n    \nvirtual_ipaddress\n \n{\n\n        \nVIRTUAL\n-\nIP\n-\nADDRESS\n/\nSUBNET\n-\nMASK\n \ndev\n \nNETWORK\n-\nINTERFACE\n\n    \n}\n\n\n}\n\n\n\n\n\n\nEnabling e-mail notifications\n\n\nTo receive e-mails when the state of your load-balanced system changes, update the \nglobal_defs\n section of \n/etc/keepalived/keepalived.conf\n for each of your load balancer nodes:\n\n\nnotification_email\n \n{\n\n\nNOTIFY\n-\nEMAIL\n-\nADDRESS\n-#\n1\n\n\nNOTIFY\n-\nEMAIL\n-\nADDRESS\n-#\n2\n\n\n[...]\n\n\n}\n\n\nnotification_email_from\n \nFROM\n-\nEMAIL\n-\nADDRESS\n\n\nsmtp_server\n \nSMTP\n-\nSERVER\n-\nIP\n-\nADDRESS\n\n\nsmtp_connect_timeout\n \n60\n\n\nrouter_id\n \nMACHINE\n-\nIDENTIFYING\n-\nSTRING\n\n\n\n\n\n\nUsing Your Load Balanced GridFTP System\n\n\nUsing GridFTP\n\n\nOn the GridFTP servers, arptables is the only additional service required for running a load-balanced GridFTP system. The name of the arptables service depends on the version of your host OS:\n\n\n\n\n\n\nSelect the appropriate RPM:\n\n\n\n\n\n\n\n\nIf your operating system version is...\n\n\nThen use the following package(s)...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 6\n\n\narptables_jf\n\n\n\n\n\n\nEnterprise Linux 7\n\n\narptables\n\n\n\n\n\n\n\n\n\n\n\n\nManage the service with the following commands:\n\n\n\n\n\n\n\n\nTo ...\n\n\nOn EL\u00a06, run the command...\n\n\nOn EL\u00a07, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\nsystemctl start SERVICE-NAME\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\nsystemctl stop SERVICE-NAME\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\nsystemctl enable SERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\nsystemctl disable SERVICE-NAME\n\n\n\n\n\n\n\n\n\n\n\n\nFor information on how to use your individual GridFTP servers, please refer to the \nManaging GridFTP section\n of the GridFTP installation guide.\n\n\nUsing Keepalived\n\n\nOn the load balancer nodes, \nkeepalived\n is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL\u00a06, run the command...\n\n\nOn EL\u00a07, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice keepalived start\n\n\nsystemctl start keepalived\n\n\n\n\n\n\nStop a service\n\n\nservice keepalived stop\n\n\nsystemctl stop keepalived\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig keepalived on\n\n\nsystemctl enable keepalived\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig keepalived off\n\n\nsystemctl disable keepalived\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance with \nkeepalived\n in front of OSG Software services, please use the \nthis page\n.\n\n\n\n\n\n\nLinux Virtual Server homepage\n\n\nKeepalived homepage\n\n\nRHEL 7 Load Balancer Administration Guide\n\n\nRHEL 6 Load Balancer Administration Guide\n\n\nT2 Nebraska LVS installation notes", 
            "title": "Install Load Balanced GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#load-balancing-gridftp", 
            "text": "GridFTP is designed for high throughput data transfers and in many cases can handle all of the transfers for a site. However, in some cases it may be useful to run multiple GridFTP servers to distribute the load. For such sites, we recommend using a  load balancer  to distribute requests and present the appearance of a single high-throughput GridFTP server.  One general-purpose technology for implementing a load balancer on Linux is  Linux Virtual Server  (LVS). To use it with GridFTP, a single load balancer listens on a virtual IP address, monitors the health of the set of real GridFTP servers, and forwards requests to available ones. Optionally, there can be one or more inactive, backup load balancers that can activate and take over the virtual IP address in case the primary load balancer fails, resulting in a system that is more resilient to failure. LVS is implemented by the  IP Virtual Server  kernel module, which can be managed by userspace services on the load balancers such as  keepalived .  This guide explains how to install, configure, run, test, and troubleshoot the  keepalived  service on a load balancing host for a set of  GridFTP  servers.", 
            "title": "Load Balancing GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#before-starting", 
            "text": "Before starting the installation process, consider the following requirements:   There must be a shared file system for file propagation across GridFTP servers  You must have reserved a virtual IP address and associated virtual hostname   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to each host", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#designing-your-load-balanced-gridftp-system", 
            "text": "Before beginning the installation process, you will need to plan the overall architecture of your load-balanced GridFTP system: the number of GridFTP servers, the type of shared file system to run on the GridFTP servers, whether or not backup load balancers are required, and hardware requirements.", 
            "title": "Designing Your Load-Balanced GridFTP System"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#gridftp-servers", 
            "text": "The number of GridFTP servers that you should run is determined first and foremost by the expected GridFTP transfer load at your site and the speed of the links available to each server. For example, if you expect a 20Gbps peak transfer load and have 10Gb links with 80\u201390% efficiency, you would need a minimum of 4 GridFTP servers: 3 to satisfy your desired throughput + 1 for failover or growth.", 
            "title": "GridFTP servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#shared-file-system", 
            "text": "The number of GridFTP servers can also be determined by your hardware needs and by your choice of shared file system. If you choose a POSIX-based shared file system, plan for machines with more cores, or more GridFTP hosts to distribute the CPU load. If you are running  GridFTP with Hadoop , plan for machines with more memory, or more GridFTP hosts to distribute the memory load.   Note  If you determine that you need only a single GridFTP host, you do not need load balancing. Instead, follow the  standalone-GridFTP installation guide .", 
            "title": "Shared file system"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#load-balancers", 
            "text": "In the recommended direct routing mode, load balancers simply rewrite initial packets from a given request so the hardware requirements are minimal. When choosing load balancer hosts, aim for stability. If your chosen host is unstable or if you do not want to introduce downtime for operating system or hardware updates, at least one additional load balancer will be needed as a backup.", 
            "title": "Load balancer(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#preparing-the-gridftp-servers", 
            "text": "Before adding your GridFTP hosts to the load-balanced system, each host requires the following:   GridFTP software  Special host certificates  Load-balancing configuration", 
            "title": "Preparing the GridFTP Servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#acquiring-host-certificates", 
            "text": "When authenticating with a GridFTP server, clients verify that the server's host certificate matches the hostname of the server. In the case of a load-balanced GridFTP system, clients contact the GridFTP server through the virtual hostname, so the GridFTP server will have to present a certificate containing the virtual hostname as well as the GridFTP server's hostname. Use the  OSG host certificate reference  for more information on how to request these types of certificates.  If your GridFTP servers are also running XRootD, you will need unique certificates for each GridFTP server. Otherwise, you can request a single certificate that can be shared among the GridFTP servers.", 
            "title": "Acquiring host certificate(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#without-xrootd", 
            "text": "The single shared certificate must have the hostname associated with the load-balanced GridFTP system as its  common name  and each GridFTP servers hostname listed as  subject alternative names .    Request and generate the shared certificate:  user@host $  osg-cert-request --hostname  VIRTUAL-HOSTNAME   \\ \n            --country  COUNTRY   \\ \n            --state  STATE   \\ \n            --locality  LOCALITY   \\ \n            --organization  ORGANIZATION              --altname  GRIDFTP-SERVER-#1-HOSTNAME  \\              --altname  GRIDFTP-SERVER-#2-HOSTNAME     Take the resulting CSR and get it signed by the appropriate authority.\n    Most institutions can use InCommon as outlined  here .    Create a directory to contain the shared certificate:  root@host #  mkdir /etc/grid-security/gridftp    Place the shared certificate-key pair in the newly created directory:  root@host #  mv  PATH-TO-SERVICE-CERT  /etc/grid-security/gridftp/gridftp-hostcert.pem root@host #  mv  PATH-TO-SERVICE-KEY  /etc/grid-security/gridftp/gridftp-hostkey.pem    Edit  /etc/sysconfig/globus-gridftp-server  to identify the shared certificate-key pair:  export   X509_USER_CERT =/ etc / grid - security / gridftp / gridftp - hostcert . pem  export   X509_USER_KEY =/ etc / grid - security / gridftp / gridftp - hostkey . pem", 
            "title": "Without XRootD"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#with-xrootd", 
            "text": "XRootD requires that the certificate's  common name  refers specifically to the host it resides on. To ensure each GridFTP server can authenticate using the virtual hostname, add it as the  subject alternative name  for each certificate.    Create a list of GridFTP server hostnames in  load-balanced-hosts.txt :  GRIDFTP - SERVER -# 1 - HOSTNAME   VIRTUAL - HOSTNAME  GRIDFTP - SERVER -# 2 - HOSTNAME   VIRTUAL - HOSTNAME  [...]     Submit a batch request for the per-GridFTP server certificates:  ::: console  user@host $  osg-gridadmin-cert-request -f load-balanced-hosts.txt    Copy the resulting certificates and keys to their corresponding GridFTP servers in  /etc/grid-security/hostcert.pem  and  /etc/grid-security/hostkey.pem , respectively.", 
            "title": "With XRootD"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#installing-gridftp", 
            "text": "Whether you are starting from scratch or adding more GridFTP servers to your load-balanced GridFTP system, follow the documentation for  installing a standalone GridFTP server  for each of your intended GridFTP servers (skip section 2.2, requesting a certificate). For hosts with GridFTP already installed, skip this section.", 
            "title": "Installing GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#configuring-your-gridftp-servers", 
            "text": "Each GridFTP server requires changes to its IP configuration and potentially its arptables:   Adding your virtual IP address  Disabling ARP  \u2212 if your GridFTP servers are on the same network segment as the virtual IP", 
            "title": "Configuring your GridFTP servers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#adding-your-virtual-ip-address", 
            "text": "Use the virtual IP address of your load balancer(s) as the secondary IPs of each of your GridFTP servers.    Add the virtual IP using the  ip  tool:  root@host #  ip addr add  VIRTUAL-IP-ADDRESS / SUBNET-MASK  dev  NETWORK-INTERFACE     To persist the virtual IP changes across reboots, edit  /etc/rc.d/rc.local , and add the same command as used above.   Make sure that  /etc/rc.d/rc.local  is executable: root@host #  chmod u+x /etc/rc.d/rc.local", 
            "title": "Adding your virtual IP address"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#disabling-arp", 
            "text": "If your GridFTP servers and load balancer(s) are on the same network segment, you will have to disable ARP on the GridFTP servers to avoid  ARP race conditions . Otherwise, skip to  the section on preparing keepalived .    Select the appropriate RPM:     If your operating system version is...  Then use the following package(s)...      Enterprise Linux 6  arptables_jf    Enterprise Linux 7  arptables       Install the arptables software:  root@host #  yum install  PACKAGE     Disable ARP:  root@host #  arptables -F  root@host #  arptables -A IN -d  VIRTUAL-IP-ADDRESS  -j DROP  root@host #  arptables -A OUT -s  VIRTUAL-IP-ADDRESS  -j mangle --mangle-ip-s  GRIDFTP-REAL-IP-ADDRESS     Save ARP tables to survive reboots:  root@host #  arptables-save   /etc/sysconfig/arptables", 
            "title": "Disabling ARP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#preparing-keepalived-load-balancers", 
            "text": "", 
            "title": "Preparing Keepalived Load Balancer(s)"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#installing-keepalived", 
            "text": "Whether you run a single load balancer, or have one active load balancer and some inactive backups, each load balancer host must have the  keepalived  software installed, configured, and running.   Note  Do not install  keepalived  on the GridFTP servers themselves.   The  keepalived  package is available from standard operating system repositories. Install it on each load balancer host using the following commands:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install the  keepalived  package:  root @host   #   yum   install   keepalived", 
            "title": "Installing Keepalived"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#required-configuration", 
            "text": "On the primary load balancer, edit  /etc/keepalived/keepalived.conf :  global_defs  {\n    router_id   STRING - LABEL - FOR - YOUR - LOAD - BALANCED - SYSTEM \n} vrrp_instance   VI_gridftp  {\n     state   MASTER \n     interface   NETWORK - INTERFACE \n     virtual_router_id   INTEGER - BETWEEN - 0 - AND - 255 \n     priority   100 \n     virtual_ipaddress  {\n         VIRTUAL - IP - ADDRESS / SUBNET - MASK   dev   NETWORK - INTERFACE \n    }\n} virtual_server   VIRTUAL - IP - ADDRESS   2811  {\n     delay_loop   10 \n     lb_algo   wlc \n     lb_kind   DR \n     protocol   tcp \n\n     real_server   GRIDFTP - SERVER - #1 - IP   ADDRESS  {\n         TCP_CHECK  {\n             connect_timeout   3 \n             connect_port   2811 \n        }\n    }\n     real_server   GRIDFTP - SERVER - #2 - IP - ADDRESS  {\n         TCP_CHECK  {\n             connect_timeout   3 \n             connect_port   2811 \n        }\n    }\n    [...]\n}   Note  Use the same  VIRTUAL-IP-ADDRESS  throughout the configuration of your load-balanced GridFTP system.    Note  In the  virtual_server  section, write one  real_server  subsection for each GridFTP server behind the load balancer.", 
            "title": "Required configuration"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small cluster of GridFTP hosts. If you do not need any of the following special configurations, skip to  the section on using keepalived .   Adding backup load balancers  Enabling e-mail notifications", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#adding-backup-load-balancers", 
            "text": "If you need to add backup load balancers, copy  /etc/keepalived/keepalived.conf  from your primary load balancer and change the  state  and  priority  attributes under your  vrrp_instance VI_gridftp  section:   Note  Priority specifies the order of preferred load balancer fallback, where larger values corresponds to a higher preference.   vrrp_instance   VI_gridftp   { \n     state   BACKUP \n     interface   NETWORK - INTERFACE \n     virtual_router_id   SAME - ID - AS - MASTER - LOAD - BALANCER \n     priority   PRIORITY - INTEGER \n     virtual_ipaddress   { \n         VIRTUAL - IP - ADDRESS / SUBNET - MASK   dev   NETWORK - INTERFACE \n     }  }", 
            "title": "Adding backup load balancers"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#enabling-e-mail-notifications", 
            "text": "To receive e-mails when the state of your load-balanced system changes, update the  global_defs  section of  /etc/keepalived/keepalived.conf  for each of your load balancer nodes:  notification_email   {  NOTIFY - EMAIL - ADDRESS -# 1  NOTIFY - EMAIL - ADDRESS -# 2  [...]  }  notification_email_from   FROM - EMAIL - ADDRESS  smtp_server   SMTP - SERVER - IP - ADDRESS  smtp_connect_timeout   60  router_id   MACHINE - IDENTIFYING - STRING", 
            "title": "Enabling e-mail notifications"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-your-load-balanced-gridftp-system", 
            "text": "", 
            "title": "Using Your Load Balanced GridFTP System"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-gridftp", 
            "text": "On the GridFTP servers, arptables is the only additional service required for running a load-balanced GridFTP system. The name of the arptables service depends on the version of your host OS:    Select the appropriate RPM:     If your operating system version is...  Then use the following package(s)...      Enterprise Linux 6  arptables_jf    Enterprise Linux 7  arptables       Manage the service with the following commands:     To ...  On EL\u00a06, run the command...  On EL\u00a07, run the command...      Start a service  service SERVICE-NAME start  systemctl start SERVICE-NAME    Stop a service  service SERVICE-NAME stop  systemctl stop SERVICE-NAME    Enable a service to start during boot  chkconfig SERVICE-NAME on  systemctl enable SERVICE-NAME    Disable a service from starting during boot  chkconfig SERVICE-NAME off  systemctl disable SERVICE-NAME       For information on how to use your individual GridFTP servers, please refer to the  Managing GridFTP section  of the GridFTP installation guide.", 
            "title": "Using GridFTP"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#using-keepalived", 
            "text": "On the load balancer nodes,  keepalived  is the only additional service required for running a load-balanced GridFTP system. As a reminder, here are common service commands (all run as  root ):     To...  On EL\u00a06, run the command...  On EL\u00a07, run the command...      Start a service  service keepalived start  systemctl start keepalived    Stop a service  service keepalived stop  systemctl stop keepalived    Enable a service to start during boot  chkconfig keepalived on  systemctl enable keepalived    Disable a service from starting during boot  chkconfig keepalived off  systemctl disable keepalived", 
            "title": "Using Keepalived"
        }, 
        {
            "location": "/data/load-balanced-gridftp/#getting-help", 
            "text": "To get assistance with  keepalived  in front of OSG Software services, please use the  this page .    Linux Virtual Server homepage  Keepalived homepage  RHEL 7 Load Balancer Administration Guide  RHEL 6 Load Balancer Administration Guide  T2 Nebraska LVS installation notes", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/xrootd/overview/", 
            "text": "XRootD Overview\n\n\nXRootD\n is a highly-configurable data server used by sites in the OSG to support VO-specific\nstorage needs.\nThe software can be used to create a export an existing file system through multiple protocols, participate in a data\nfederation, or act as a caching service.\nXRootD data servers can stream data directly to client applications or support experiment-wide data management by\nperforming bulk data transfer via \"third-party-copy\" between distinct sites.\nThe OSG currently supports two different configurations of XRootD:\n\n\nXCache\n\n\nPreviously known as the \"XRootD proxy cache\", XCache provides a caching service for data federations that serve one or\nmore VOs.\nIn the OSG, there are three data federations based on XCache: ATLAS XCache, CMS XCache, and\n\nStashCache\n.\n\n\nIf you are affiliated with a site or VO interested in contributing to or starting a data federation, contact us at\n\n.\n\n\nXRootD Standalone\n\n\nAn \nXRootD standalone server\n exports an existing filesystem, such as HDFS or Lustre, \nusing both the XRootD and WebDAV protocols.\nGenerally, only sites affiliated with large VOs would need to install an XRootD standalone server so consult your VO if\nyou are interested in contributing storage.", 
            "title": "XRootD Overview"
        }, 
        {
            "location": "/data/xrootd/overview/#xrootd-overview", 
            "text": "XRootD  is a highly-configurable data server used by sites in the OSG to support VO-specific\nstorage needs.\nThe software can be used to create a export an existing file system through multiple protocols, participate in a data\nfederation, or act as a caching service.\nXRootD data servers can stream data directly to client applications or support experiment-wide data management by\nperforming bulk data transfer via \"third-party-copy\" between distinct sites.\nThe OSG currently supports two different configurations of XRootD:", 
            "title": "XRootD Overview"
        }, 
        {
            "location": "/data/xrootd/overview/#xcache", 
            "text": "Previously known as the \"XRootD proxy cache\", XCache provides a caching service for data federations that serve one or\nmore VOs.\nIn the OSG, there are three data federations based on XCache: ATLAS XCache, CMS XCache, and StashCache .  If you are affiliated with a site or VO interested in contributing to or starting a data federation, contact us at .", 
            "title": "XCache"
        }, 
        {
            "location": "/data/xrootd/overview/#xrootd-standalone", 
            "text": "An  XRootD standalone server  exports an existing filesystem, such as HDFS or Lustre, \nusing both the XRootD and WebDAV protocols.\nGenerally, only sites affiliated with large VOs would need to install an XRootD standalone server so consult your VO if\nyou are interested in contributing storage.", 
            "title": "XRootD Standalone"
        }, 
        {
            "location": "/data/xrootd/install-standalone/", 
            "text": "Install XRootD Standalone\n\n\nXRootD\n is a hierarchical storage system that can be used in a variety of ways to access data,\ntypically distributed among actual storage resources. In this document we focus on using XRootD as a simple layer\nexporting an underlying storage system (e.g., \nHDFS\n) to the outside world.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the Linux user ID \nxrootd\n\n\nService certificate:\n The XRootD service uses a host certificate and key pair at\n    \n/etc/grid-security/xrd/xrdcert.pem\n and \n/etc/grid-security/xrd/xrdkey.pem\n\n\nNetworking:\n The XRootD service uses port 1094 by default\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling XRootD\n\n\nTo install XRootD, run the following Yum command:\n\n\nroot@xrootd-standalone #\n yum install xrootd\n\n\n\n\n\nConfiguring XRootD\n\n\nTo configure XRootD as a standalone server, replace the contents of \n/etc/xrootd/xrootd-standalone.cfg\n as follows:\n\n\n\n\n\n\nAdd an \nall.export\n directive for each directory that you wish to serve via XRootD.\n    For example, to serve the contents of \n/store\n and \n/public\n:\n\n\nall\n.\nexport\n \n/\nstore\n/\n\n\nall\n.\nexport\n \n/\npublic\n/\n\n\n\n\n\n\n\n\nNote\n\n\nThe directories specified this way are writable by default.\nAccess controls should be managed via \nauthorization configuration\n.\n\n\n\n\n\n\n\n\nAdd an \nall.sitename\n directive set to the \nresource name\n of your\n   XRootD service.\n   For example, the XRootD service registered at the\n   \nFermiGrid site\n\n   should set the following configuration:\n\n\nall\n.\nsitename\n   \nFermilab\n \nPublic\n \nDCache\n\n\n\n\n\n\n\n\nNote\n\n\nCMS sites should follow CMS policy for \nall.sitename\n\n\n\n\n\n\n\n\nAppend the following configuration to the end of \n/etc/xrootd/xrootd-standalone.cfg\n\n\nxrd\n.\nport\n \n1094\n\n\nall\n.\nrole\n \nserver\n\n\n\ncms\n.\nallow\n \nhost\n \n*\n\n\n#\n \nLogging\n \nverbosity\n\n\nxrootd\n.\ntrace\n \nemsg\n \nlogin\n \nstall\n \nredirect\n\n\nofs\n.\ntrace\n \n-\nall\n\n\nxrd\n.\ntrace\n \nconn\n\n\ncms\n.\ntrace\n \nall\n\n\n\nxrd\n.\nreport\n \nxrd\n-\nreport\n.\nosgstorage\n.\norg\n:\n9931\n\n\nxrootd\n.\nmonitor\n \nall\n \n\\\n\n               \nauth\n \n\\\n\n               \nflush\n \n30\ns\n \n\\\n\n               \nwindow\n \n5\ns\n \nfstat\n \n60\n \nlfn\n \nops\n \nxfr\n \n5\n \n\\\n\n               \ndest\n \nredir\n \nfstat\n \ninfo\n \nuser\n \nxrd\n-\nreport\n.\nosgstorage\n.\norg\n:\n9930\n \n\\\n\n               \ndest\n \nfstat\n \ninfo\n \nuser\n \nxrd\n-\nmon\n.\nosgstorage\n.\norg\n:\n9930\n\n\n\nxrd\n.\nnetwork\n \nkeepalive\n \nkaparms\n \n10\nm\n,\n1\nm\n,\n5\n\n\nxrd\n.\ntimeout\n \nidle\n \n60\nm\n\n\n\n\n\n\n\n\n\n\nOn EL 6, set the default options to use the standalone configuration in the \n/etc/sysconfig/xrootd\n file.\n\n\nXROOTD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-standalone.cfg -k fifo\n\n\n\n\n\n\n\n\n\n\nConfiguring authorization\n\n\nTo configure XRootD authorization please follow the documentation \nhere\n.\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small site.\nIf you do not need any of the following special configurations, skip to\n\nthe section on using XRootD\n.\n\n\nEnabling HTTP support\n\n\nIn order to enable XRootD HTTP support please follow the instructions\n\nhere\n\n\nEnabling Hadoop support (EL 7 Only)\n\n\nFor documentation on how to export your Hadoop storage using XRootD please see\n\nthis documentation\n\n\nEnabling CMS TFC support (CMS sites only)\n\n\nFor CMS users, there is a package available to integrate rule-based name lookup using a \nstorage.xml\n file.\nSee \nthis documentation\n.\n\n\nUsing XRootD\n\n\nIn addition to the XRootD service itself, there are a number of supporting services in your installation.\nThe specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService Name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nXRootD\n\n\nEL 7:\nxrootd@standalone\n, EL 6:\nxrootd\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nOn EL 7, run the command\u2026\n\n\nOn EL 6, run the command\u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nsystemctl start SERVICE-NAME\n\n\nservice SERVICE-NAME start\n\n\n\n\n\n\nStop a service\n\n\nsystemctl stop SERVICE-NAME\n\n\nservice SERVICE-NAME stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nsystemctl enable SERVICE-NAME\n\n\nchkconfig SERVICE-NAME on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nsystemctl disable SERVICE-NAME\n\n\nchkconfig SERVICE-NAME off\n\n\n\n\n\n\n\n\nValidating XRootD\n\n\nTo validate an XRootD installation, perform the following verification steps:\n\n\n\n\n\n\nVerify file transfer over the XRootD protocol using XRootD client tools:\n\n\n\n\n\n\nInstall the client tools:\n\n\nroot@xrootd-standalone #\n yum install xrootd-client\n\n\n\n\n\n\n\n\n\nCopy a file to a directory for which you have write access:\n\n\nroot@xrootd-standalone #\n xrdcp /bin/sh root://localhost:1094//tmp/first_test\n\n[xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]\n\n\n\n\n\n\n\n\n\n\nVerify that the file has been copied over:\n\n\nroot@xrootd-standalone #\n ls -l /tmp/first_test\n\n-rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you have enabled \nHTTP support\n, verify file transfer over HTTP using GFAL2 client\n   tools:\n\n\n\n\n\n\nInstall the GFAL2 client tools:\n\n\nroot@xrootd-standalone #\n yum install gfal2-util gfal2-plugin-http\n\n\n\n\n\n\n\n\n\nCopy a file to a directory for which you have write access:\n\n\nroot@xrootd-standalone #\n gfal-copy /bin/sh http://localhost:1094//tmp/first_test\n\n\n\n\n\n\n\n\n\nVerify that the file has been copied over:\n\n\nroot@xrootd-standalone #\n ls -l /tmp/first_test\n\n-rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance. please use the \nHelp Procedure\n page.\n\n\nReference\n\n\n\n\nXRootD documentation\n\n\nExport directive\n in the XRootD configuration and\n  \nrelevant options\n\n\n\n\nService Configuration\n\n\nOn EL 6, which config to use is set in the file \n/etc/sysconfig/xrootd\n.\n\n\nTo use the standalone config, you would use:\n\n\nXROOTD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-\nstandalone\n.cfg -k fifo\n\n\n\n\n\n\nOn EL 7, which config to use is determined by the service name given to \nsystemctl\n.\nTo use the standalone config, you would use:\n\n\nroot@host #\n systemctl start xrootd@\nstandalone\n\n\n\n\n\n\nFile locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/etc/xrootd/xrootd-standalone.cfg\n\n\nMain XRootD configuration\n\n\n\n\n\n\n\n\n/etc/xrootd/auth_file\n\n\nAuthorized users file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/var/log/xrootd/server/xrootd.log\n\n\nXRootD server daemon log\n\n\n\n\n\n\ncmsd\n\n\n/var/log/xrootd/server/cmsd.log\n\n\nCluster management log", 
            "title": "Install XRootD Standalone"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#install-xrootd-standalone", 
            "text": "XRootD  is a hierarchical storage system that can be used in a variety of ways to access data,\ntypically distributed among actual storage resources. In this document we focus on using XRootD as a simple layer\nexporting an underlying storage system (e.g.,  HDFS ) to the outside world.", 
            "title": "Install XRootD Standalone"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   User IDs:  If it does not exist already, the installation will create the Linux user ID  xrootd  Service certificate:  The XRootD service uses a host certificate and key pair at\n     /etc/grid-security/xrd/xrdcert.pem  and  /etc/grid-security/xrd/xrdkey.pem  Networking:  The XRootD service uses port 1094 by default   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#installing-xrootd", 
            "text": "To install XRootD, run the following Yum command:  root@xrootd-standalone #  yum install xrootd", 
            "title": "Installing XRootD"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#configuring-xrootd", 
            "text": "To configure XRootD as a standalone server, replace the contents of  /etc/xrootd/xrootd-standalone.cfg  as follows:    Add an  all.export  directive for each directory that you wish to serve via XRootD.\n    For example, to serve the contents of  /store  and  /public :  all . export   / store /  all . export   / public /    Note  The directories specified this way are writable by default.\nAccess controls should be managed via  authorization configuration .     Add an  all.sitename  directive set to the  resource name  of your\n   XRootD service.\n   For example, the XRootD service registered at the\n    FermiGrid site \n   should set the following configuration:  all . sitename     Fermilab   Public   DCache    Note  CMS sites should follow CMS policy for  all.sitename     Append the following configuration to the end of  /etc/xrootd/xrootd-standalone.cfg  xrd . port   1094  all . role   server  cms . allow   host   *  #   Logging   verbosity  xrootd . trace   emsg   login   stall   redirect  ofs . trace   - all  xrd . trace   conn  cms . trace   all  xrd . report   xrd - report . osgstorage . org : 9931  xrootd . monitor   all   \\ \n                auth   \\ \n                flush   30 s   \\ \n                window   5 s   fstat   60   lfn   ops   xfr   5   \\ \n                dest   redir   fstat   info   user   xrd - report . osgstorage . org : 9930   \\ \n                dest   fstat   info   user   xrd - mon . osgstorage . org : 9930  xrd . network   keepalive   kaparms   10 m , 1 m , 5  xrd . timeout   idle   60 m     On EL 6, set the default options to use the standalone configuration in the  /etc/sysconfig/xrootd  file.  XROOTD_DEFAULT_OPTIONS = -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-standalone.cfg -k fifo", 
            "title": "Configuring XRootD"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#configuring-authorization", 
            "text": "To configure XRootD authorization please follow the documentation  here .", 
            "title": "Configuring authorization"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small site.\nIf you do not need any of the following special configurations, skip to the section on using XRootD .", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#enabling-http-support", 
            "text": "In order to enable XRootD HTTP support please follow the instructions here", 
            "title": "Enabling HTTP support"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#enabling-hadoop-support-el-7-only", 
            "text": "For documentation on how to export your Hadoop storage using XRootD please see this documentation", 
            "title": "Enabling Hadoop support (EL 7 Only)"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#enabling-cms-tfc-support-cms-sites-only", 
            "text": "For CMS users, there is a package available to integrate rule-based name lookup using a  storage.xml  file.\nSee  this documentation .", 
            "title": "Enabling CMS TFC support (CMS sites only)"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#using-xrootd", 
            "text": "In addition to the XRootD service itself, there are a number of supporting services in your installation.\nThe specific services are:     Software  Service Name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    XRootD  EL 7: xrootd@standalone , EL 6: xrootd      Start the services in the order listed and stop them in reverse order.\nAs a reminder, here are common service commands (all run as  root ):     To \u2026  On EL 7, run the command\u2026  On EL 6, run the command\u2026      Start a service  systemctl start SERVICE-NAME  service SERVICE-NAME start    Stop a service  systemctl stop SERVICE-NAME  service SERVICE-NAME stop    Enable a service to start during boot  systemctl enable SERVICE-NAME  chkconfig SERVICE-NAME on    Disable a service from starting during boot  systemctl disable SERVICE-NAME  chkconfig SERVICE-NAME off", 
            "title": "Using XRootD"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#validating-xrootd", 
            "text": "To validate an XRootD installation, perform the following verification steps:    Verify file transfer over the XRootD protocol using XRootD client tools:    Install the client tools:  root@xrootd-standalone #  yum install xrootd-client    Copy a file to a directory for which you have write access:  root@xrootd-standalone #  xrdcp /bin/sh root://localhost:1094//tmp/first_test [xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]     Verify that the file has been copied over:  root@xrootd-standalone #  ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test       If you have enabled  HTTP support , verify file transfer over HTTP using GFAL2 client\n   tools:    Install the GFAL2 client tools:  root@xrootd-standalone #  yum install gfal2-util gfal2-plugin-http    Copy a file to a directory for which you have write access:  root@xrootd-standalone #  gfal-copy /bin/sh http://localhost:1094//tmp/first_test    Verify that the file has been copied over:  root@xrootd-standalone #  ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test", 
            "title": "Validating XRootD"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#getting-help", 
            "text": "To get assistance. please use the  Help Procedure  page.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#reference", 
            "text": "XRootD documentation  Export directive  in the XRootD configuration and\n   relevant options", 
            "title": "Reference"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#service-configuration", 
            "text": "On EL 6, which config to use is set in the file  /etc/sysconfig/xrootd .  To use the standalone config, you would use:  XROOTD_DEFAULT_OPTIONS = -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd- standalone .cfg -k fifo   On EL 7, which config to use is determined by the service name given to  systemctl .\nTo use the standalone config, you would use:  root@host #  systemctl start xrootd@ standalone", 
            "title": "Service Configuration"
        }, 
        {
            "location": "/data/xrootd/install-standalone/#file-locations", 
            "text": "Service/Process  Configuration File  Description      xrootd  /etc/xrootd/xrootd-standalone.cfg  Main XRootD configuration     /etc/xrootd/auth_file  Authorized users file        Service/Process  Log File  Description      xrootd  /var/log/xrootd/server/xrootd.log  XRootD server daemon log    cmsd  /var/log/xrootd/server/cmsd.log  Cluster management log", 
            "title": "File locations"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/", 
            "text": "Installing and Maintaining XRootD\n\n\nXRootD\n is a hierarchical storage system that can be used in a variety of ways to access data,\ntypically distributed among actual storage resources. \nOne way to use XRootD is to have it refer to many data resources at a single site, and another way to use it is to refer\nto many storage systems, most likely distributed among sites.  An XRootD system includes a \nredirector\n, which accepts\nrequests for data and finds a storage repository\u00a0\u2014 locally or\notherwise\u00a0\u2014 that can provide the data to the requestor.\n\n\nUse this page to learn how to install, configure, and use an XRootD redirector as part of a Storage Element (SE) or as\npart of a global namespace.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nUser IDs:\n If it does not exist already, the installation will create the Linux user ID \nxrootd\n\n\nService certificate:\n The XRootD service uses a host certificate at \n/etc/grid-security/host*.pem\n\n\nNetworking:\n The XRootD service uses port 1094 by default\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling an XRootD Server\n\n\nAn installation of the XRootD server consists of the server itself and its dependencies. \nInstall these with Yum:\n\n\nroot@host #\n yum install xrootd\n\n\n\n\n\nConfiguring an XRootD Server\n\n\nMinimal configuration\n\n\nA new installation of XRootD is already configured to run a standalone server that serves files from \n/tmp\n on the local\nfile system. \nThis configuration is useful to verify basic connectivity between your clients and your server. \nTo do this, start the \nxrootd\n service with standalone config as described in the \nmanaging services\nsection\n.\n\n\nYou should be able now to copy a file such as \n/bin/sh\n using \nxrdcp\n command into \n/tmp\n. \nTo test, do:\n\n\nroot@host #\n yum install xrootd-client\n\nroot@host #\n xrdcp /bin/sh root://localhost:1094//tmp/first_test\n\n[xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]\n\n\nroot@host #\n ls -l /tmp/first_test\n\n-rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test\n\n\n\n\n\n\nOther than for testing, a standalone server is useful when you want to serve files off of a single host with lots of\nlarge disks. \nIf your storage capacity is spread out over multiple hosts, you will need to set up an XRootD cluster.\n\n\nAdvanced configuration\n\n\nAn advanced XRootD setup has multiple components; it is important to validate that each additional component that you\nset up is working before moving on to the next component. \nWe have included validation instructions after each component below.\n\n\nCreating an XRootD cluster\n\n\n\n\nIf your storage is spread out over multiple hosts, you will need to set up an XRootD \ncluster\n. \nThe cluster uses one \"redirector\" node as a frontend for user accesses, and multiple data nodes that have the data that\nusers request. \nTwo daemons will run on each node:\n\n\nxrootd\n\nThe eXtended Root Daemon controls file access and storage.\n\n\ncmsd\n\nThe Cluster Management Services Daemon controls communication between nodes.\n\n\nNote that for large virtual organizations, a site-level redirector may actually also communicate upwards to a regional\nor global redirector that handles access to a multi-level hierarchy. \nThis section will only cover handling one level of XRootD hierarchy.\n\n\nIn the instructions below, \nRDRNODE\n will refer to the redirector host and \nDATANODE\n will\nrefer to the data node host. \nThese should be replaced with the fully-qualified domain name of the host in question.\n\n\nModify /etc/xrootd/xrootd-clustered.cfg\n\n\nYou will need to modify the \nxrootd-clustered.cfg\n on the redirector node and each data node. \nThe following example should serve as a base configuration for clustering. Further customizations are detailed below.\n\n\nall\n.\nexport\n \n%\nRED\n%/\ntmp\n%\nENDCOLOR\n%\n \nstage\n\n\nset\n \nxrdr\n \n=\n \n%\nRED\n%\nRDRNODE\n%\nENDCOLOR\n%\n\n\nall\n.\nmanager\n $\n(\nxrdr\n)\n:\n3121\n\n\n\nif\n $\n(\nxrdr\n)\n\n  # \nLines\n \nin\n \nthis\n \nblock\n \nare\n \nonly\n \nexecuted\n \non\n \nthe\n \nredirector\n \nnode\n\n  \nall\n.\nrole\n \nmanager\n\n\nelse\n\n  # \nLines\n \nin\n \nthis\n \nblock\n \nare\n \nexecuted\n \non\n \nall\n \nnodes\n \nbut\n \nthe\n \nredirector\n \nnode\n\n  \nall\n.\nrole\n \nserver\n\n  \ncms\n.\nspace\n \nmin\n \n%\nRED\n%\n2\ng\n \n5\ng\n%\nENDCOLOR\n%\n\n\nfi\n\n\n\n\n\n\nYou will need to customize the following lines:\n\n\n\n\n\n\n\n\nConfiguration Line\n\n\nChanges Needed\n\n\n\n\n\n\n\n\n\n\nall.export \n/tmp\n stage\n\n\nChange \n/tmp\n to the directory to allow XRootD access to\n\n\n\n\n\n\nset xrdr=\nRDRNODE\n\n\nChange to the hostname of the redirector\n\n\n\n\n\n\ncms.space min \n2g 5g\n\n\nReserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use \nk\n, \nm\n, \ng\n, or \nt\n to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively.\n\n\n\n\n\n\n\n\nFurther information can be found at \nhttp://xrootd.slac.stanford.edu/doc\n\n\nVerifying the clustered config\n\n\nStart both \nxrootd\n and \ncmsd\n on all nodes according to the instructions in the \nmanaging services\nsection\n.\n\n\nVerify that you can copy a file such as \n/bin/sh\n to \n/tmp\n on the server data via the redirector:\n\n\nroot@host #\n xrdcp /bin/sh  root://\nRDRNODE\n:1094///tmp/second_test\n\n[xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]\n\n\n\n\n\n\nCheck that the \n/tmp/second_test\n is located on data server \nDATANODE\n.\n\n\n(Optional) Adding Simple Server Inventory to your cluster\n\n\nThe Simple Server Inventory (SSI) provide means to have an inventory for each data server. \nSSI requires:\n\n\n\n\nA second instance of the \nxrootd\n daemon on the redirector\n\n\nA \"composite name space daemon\" (\nXrdCnsd\n) on each data server; this daemon handles the inventory\n\n\n\n\nAs an example, we will set up a two-node XRootD cluster with SSI.\n\n\nHost A is a redirector node that is running the following daemons:\n\n\n\n\nxrootd redirector\n\n\ncmsd\n\n\nxrootd - second instance that required for SSI\n\n\n\n\nHost B is a data server that is running the following daemons:\n\n\n\n\nxrootd data server\n\n\ncmsd\n\n\nXrdCnsd - started automatically by xrootd\n\n\n\n\nWe will need to create a directory on the redirector node for Inventory files.\n\n\nroot@host #\n mkdir -p /data/inventory\n\nroot@host #\n chown xrootd:xrootd /data/inventory\n\n\n\n\n\nOn the data server (host B) let's use a storage cache that will be at a different location from \n/tmp\n. \n\n\nroot@host #\n mkdir -p  /local/xrootd\n\nroot@host #\n chown xrootd:xrootd /local/xrootd\n\n\n\n\n\nWe will be running two instances of XRootD on \nhostA\n. \nModify \n/etc/xrootd/xrootd-clustered.cfg\n to give the two instances different behavior, as such:\n\n\nall\n.\nexport\n \n/\ndata\n/\nxrootdfs\n\n\nset\n \nxrdr\n=%\nRED\n%\nhostA\n%\nENDCOLOR\n%\n\n\nall\n.\nmanager\n $\n(\nxrdr\n)\n:\n3121\n\n\nif\n $\n(\nxrdr\n)\n \n \nnamed\n \ncns\n\n      \nall\n.\nexport\n \n/\ndata\n/\ninventory\n\n      \nxrd\n.\nport\n \n1095\n\n\nelse\n \nif\n $\n(\nxrdr\n)\n\n      \nall\n.\nrole\n \nmanager\n\n      \nxrd\n.\nport\n \n1094\n\n\nelse\n\n      \nall\n.\nrole\n \nserver\n\n      \noss\n.\nlocalroot\n \n/\nlocal\n/\nxrootd\n\n      \nofs\n.\nnotify\n \nclosew\n \ncreate\n \nmkdir\n \nmv\n \nrm\n \nrmdir\n \ntrunc\n \n|\n \n/\nusr\n/\nbin\n/\nXrdCnsd\n \n-\nd\n \n-\nD\n \n2\n \n-\ni\n \n90\n \n-\nb\n $\n(\nxrdr\n)\n:\n1095\n:\n/\ndata\n/\ninventory\n\n      #\nadd\n \ncms\n.\nspace\n \nif\n \nyou\n \nhave\n \nless\n \nthe\n \n11\nGB\n\n      # \ncms\n.\nspace\n \noptions\n \nhttp\n:\n//\nxrootd\n.\nslac\n.\nstanford\n.\nedu\n/\ndoc\n/\ndev\n/\ncms_config\n.\nhtm\n\n      \ncms\n.\nspace\n \nmin\n \n2\ng\n \n5\ng\n\n\nfi\n\n\n\n\n\n\nThe value of \noss.localroot\n will be prepended to any file access.\n\nE.g. accessing \nroot://\nRDRNODE\n:1094//data/xrootdfs/test1\n will actually go to\n\n/local/xrootd/data/xrootdfs/test1\n.\n\n\nStarting a second instance of XRootD on EL 6\n\n\nThe procedure for starting a second instance differs between EL 6 and EL 7. \nThis section is the procedure for EL 6.\n\n\nNow, we have to change \n/etc/sysconfig/xrootd\n on the redirector node (\nhostA\n) to run multiple instances\nof XRootD. \nThe second instance of XRootD will be named \"cns\" and will be used for SSI.\n\n\nXROOTD_USER\n=\nxrootd\n \n\nXROOTD_GROUP\n=\nxrootd\n \n\nXROOTD_DEFAULT_OPTIONS\n=\n-k 7\n -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n\n\n%\nRED\n%\nXROOTD_CNS_OPTIONS\n=\n-k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n%\nENDCOLOR\n%\n \n\nCMSD_DEFAULT_OPTIONS\n=\n-k 7\n -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\nFRMD_DEFAULT_OPTIONS\n=\n-k 7\n -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\n%\nRED\n%\nXROOTD_INSTANCES\n=\ndefault cns\n%\nENDCOLOR\n%\n \n\nCMSD_INSTANCES\n=\ndefault\n \n\nFRMD_INSTANCES\n=\ndefault\n \n\n\n\n\n\nNow, we can start XRootD cluster executing the following commands. \nOn redirector you will see:\n\n\nroot@host #\n service xrootd start \n\nStarting xrootd (xrootd, default): \n[ OK ]\n \n\n\nStarting xrootd (xrootd, cns): \n[ OK ]\n \n\n\nroot@host #\n service cmsd start \n\nStarting xrootd (cmsd, default): \n[ OK ]\n \n\n\n\n\n\n\nOn redirector node you should see two instances of xrootd running:\n\n\nroot@host #\n ps auxww\n|\ngrep xrootd\n\nxrootd 29036 0.0 0.0 44008 3172 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default\n\n\nxrootd 29108 0.0 0.0 43868 3016 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-cns.pid -n cns\n\n\nxrootd 29196 0.0 0.0 51420 3692 ? Sl Apr11 0:00 /usr/bin/cmsd -k 7 -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default\n\n\n\n\n\n\nWarning\n the log file for second named instance of xrootd with be\nplaced in \n/var/log/xrootd/cns/xrootd.log\n\n\nOn data server node you should that XrdCnsd process has been started:\n\n\nroot@host #\n ps auxww\n|\ngrep xrootd\n\nxrootd 19156 0.0 0.0 48096 3256 ? Sl 07:37 0:00 /usr/bin/cmsd -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default\n\n\nxrootd 19880 0.0 0.0 46124 2916 ? Sl 08:33 0:00 /usr/bin/xrootd -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default\n\n\nxrootd 19894 0.0 0.1 71164 6960 ? Sl 08:33 0:00 /usr/bin/XrdCnsd -d -D 2 -i 90 -b fermicloud053.fnal.gov:1095:/data/inventory\n\n\n\n\n\n\nStarting a second instance of XRootD on EL 7\n\n\nThe procedure for starting a second instance differs between EL 6 and EL 7. \nThis section is the procedure for EL 7.\n\n\n\n\nCreate a symlink pointing to \n/etc/xrootd/xrootd-clustered.cfg\n at \n/etc/xrootd/xrootd-cns.cfg\n:\n\n\n\n\nroot@host #\n ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg\n\n\n\n\n\n\n\nStart an instance of the \nxrootd\n service named \ncns\n using the syntax in the \nmanaging services section\n:\n\n\n\n\nroot@host #\n systemctl start xrootd@cns\n\n\n\n\n\nTesting an XRootD cluster with SSI\n\n\n\n\nCopy file to redirector node specifying storage path (/data/xrootdfs instead of /tmp):\n\n\n\n\nroot@host #\n xrdcp /bin/sh root://\nRDRNODE\n:1094//data/xrootdfs/test1 \n\n[xrootd] Total 0.00 MB [================] 100.00 % [inf MB/s] \n\n\n\n\n\n\n\n\nTo verify that SSI is working execute \ncns_ssi\n command on the redirector node:\n\n\n\n\nroot@host #\n cns_ssi list /data/inventory \n\nfermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011 \n\n\nroot@host #\n cns_ssi updt /data/inventory \n\ncns_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors. \n\n\nroot@host #\n cns_ssi list /data/inventory \n\nfermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1 \n\n\n\n\n\n\nNote\n: In this example, \nfermicloud53.fnal.gov\n is a redirector node and \nfermicloud054.fnal.gov\n is a data node.\n\n\n(Optional) Enabling Xrootd over HTTP\n\n\nXRootD can be accessed using the HTTP protocol. To do that:\n\n\n\n\n\n\nModify \n/etc/xrootd/xrootd-clustered.cfg\n and add the following lines. You will also need to add the configuration regarding \nlcmaps authorization\n.\n\n\n   \nif\n \nexec\n \nxrootd\n\n    \nxrd\n.\nprotocol\n \nhttp\n:\n1094\n \nlibXrdHttp\n.\nso\n\n    \nhttp\n.\ncadir\n \n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n\n    \nhttp\n.\ncert\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdcert\n.\npem\n\n    \nhttp\n.\nkey\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdkey\n.\npem\n\n    \nhttp\n.\nsecxtractor\n \n/\nusr\n/\nlib64\n/\nlibXrdLcmaps\n.\nso\n\n    \nhttp\n.\nlistingdeny\n \nyes\n\n    \nhttp\n.\nstaticpreload\n \nhttp\n:\n//\nstatic\n/\nrobots\n.\ntxt\n \n/\netc\n/\nxrootd\n/\nrobots\n.\ntxt\n\n    \nhttp\n.\ndesthttps\n \nyes\n\n   \nfi\n\n\n\n\n\n\n\n\n\n\nCreate robots.txt. Add file \n/etc/xrootd/robots.txt\n with these contents:\n\n\n   \nUser\n-\nagent\n:\n \n*\n\n   \nDisallow\n:\n \n/\n\n\n\n\n\n\n\n\n\n\nTesting the configuration\n\n\nFrom the terminal, generate a proxy and attempt to use davix-get to copy from your XRootD host (the XRootD service needs running; see the \nservices section\n).  For example, if your server has a file named \n/store/user/test.root\n:\n\n\n   davix-get https://\nyourHostname\n:1094/store/user/test.root -E /tmp/x509up_u`id -u` --capath /etc/grid-security/certificates\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nFor clients to successfully read from the regional redirector, HTTPS must be enabled for the data servers and the site-level redirector.\n\n\n\n\n\n\nWarning\n\n\nIf you have \nu *\n in your Authfile, recall this provides an authorization to ALL users, including unauthenticated. This includes random web spiders!\n\n\n\n\n(Optional) Enable HTTP based Writes\n\n\nNo changes to the HTTP module is needed to enable HTTP-based writes.  The HTTP protocol uses the same authorization setup as the XRootD protocol.  For example, you may need to provide \na\n (all) style authorizations to allow users authorization to write. See the \nAuthentication File section\n for more details.\n\n\n(Optional) Enabling a FUSE mount\n\n\nXRootD storage can be mounted as a standard POSIX filesystem via FUSE, providing users with a more familiar interface..\n\n\nModify \n/etc/fstab\n by adding the following entries:\n\n\n....\n\n\nxrootdfs\n                \n%\nRED\n%/\nmnt\n/\nxrootd\n%\nENDCOLOR\n%\n              \nfuse\n    \nrdr\n=\nxroot\n:\n//%\nRED\n%\nredirector1\n.\ndomain\n.\ncom\n%\nENDCOLOR\n%\n:\n1094\n/%\nRED\n%/\npath\n/%\nENDCOLOR\n%\n,\nuid\n=\nxrootd\n \n0\n \n0\n\n\n\n\n\n\nReplace \n/mnt/xrootd\n with the path that you would like to access with. \nThis should also match the GridFTP settings for the \nXROOTD_VMP\n local path. \nCreate \n/mnt/xrootd\n directory. Make sure the xrootd user exists on the system. Once you are finished, you can mount it:\n\n\nmount\n \n/\nmnt\n/\nxrootd\n\n\n\n\n\n\nYou should now be able to run UNIX commands such as \nls /mnt/xrootd\n to see the contents of the XRootD server.\n\n\n(Optional) Authorization\n\n\nFor information on how to configure xrootd-lcmaps authorization, please refer to the\n\nConfiguring XRootD Authorization guide\n.\n\n\n(Optional) Adding CMS TFC support to XRootD (CMS sites only)\n\n\nFor CMS users, there is a package available to integrate rule-based name lookup using a \nstorage.xml\n file. \nIf you are not setting up a CMS site, you can skip this section.\n\n\nyum install --enablerepo=osg-contrib xrootd-cmstfc\n\n\n\n\n\n\nYou will need to add your \nstorage.xml\n to \n/etc/xrootd/storage.xml\n and then add the following line to your XRootD\nconfiguration:\n\n\n#\n \nIntegrate\n \nwith\n \nCMS\n \nTFC\n,\n \nplaced\n \nin\n \n/\netc\n/\nxrootd\n/\nstorage\n.\nxml\n\n\noss\n.\nnamelib\n \n/\nusr\n/\nlib64\n/\nlibXrdCmsTfc\n.\nso\n \nfile\n:\n/\netc\n/\nxrootd\n/\nstorage\n.\nxml\n%\nORANGE\n%?\nprotocol\n=\nhadoop\n%\nENDCOLOR\n%\n\n\n\n\n\n\nAdd the orange text only if you are running hadoop (see below).\n\n\nSee the CMS TWiki for more information:\n\n\n\n\nhttps://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges\n\n\nhttps://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall\n\n\n\n\n(Optional) Adding Hadoop support to XRootD\n\n\nHDFS-based sites should utilize the \nxrootd-hdfs\n plugin to allow XRootD to access their storage.\n\n\nroot@host #\n yum install xrootd-hdfs\n\n\n\n\n\nYou will then need to add the following lines to your\n\n/etc/xrootd/xrootd-clustered.cfg\n:\n\n\nxrootd\n.\nfslib\n \n/\nusr\n/\nlib64\n/\nlibXrdOfs\n.\nso\n\n\nofs\n.\nosslib\n \n/\nusr\n/\nlib64\n/\nlibXrdHdfs\n.\nso\n\n\n\n\n\n\nFor more information, see \nthe HDFS installation documents\n.\n\n\n(Optional) Adding File Residency Manager (FRM) to an XRootd cluster\n\n\nIf you have a multi-tiered storage system (e.g. some data is stored on SSDs and some on disks or tapes), then install\nthe File Residency Manager (FRM), so you can move data between tiers more easily. \nIf you do not have a multi-tiered storage system, then you do not need FRM and you can skip this section.\n\n\nThe FRM deals with two major mechanisms:\n\n\n\n\nlocal disk\n\n\nremote servers\n\n\n\n\nThe description of fully functional multiple XRootD clusters is beyond the scope of this document. \nIn order to have this fully functional system you will need a global redirector and at least one remote XRootD cluster\nfrom where files could be moved to the local cluster.\n\n\nBelow are the modifications you should make in order to enable FRM on your local cluster:\n\n\n\n\nMake sure that FRM is enabled in \n/etc/sysconfig/xrootd\n on your data sever:\n\n\n\n\nROOTD_USER\n=\nxrootd\n \n\nXROOTD_GROUP\n=\nxrootd\n \n\nXROOTD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\nCMSD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\nFRMD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg\n \n\nXROOTD_INSTANCES\n=\ndefault\n \n\nCMSD_INSTANCES\n=\ndefault\n \n\nFRMD_INSTANCES\n=\ndefault\n\n\n\n\n\n\n\n\nModify \n/etc/xrootd/xrootd-clustered.cfg\n on both nodes to specify options for \nfrm_xfrd\n (File Transfer Daemon) and \nfrm_purged\n (File Purging Daemon). For more information, you can visit the \nFRM Documentation\n\n\nStart frm daemons on data server:\n\n\n\n\nroot@host #\n service frm_xfrd start\n\nroot@host #\n service frm_purged start\n\n\n\n\n\n(Optional) Installing a GridFTP Server\n\n\nThe Globus GridFTP server can be installed alongside an XRootD storage element to provide GridFTP-based access to the\nstorage.\n\n\n\n\nSee Also\n\n\nOSG has extensive documentation on setting up a GridFTP server; this section is an\nabbreviated version documenting the special steps needed for XRootD integration.\nYou may also find the following useful:\n\n\n\n\nBasic GridFTP Install\n.  Additionally covers service planning topics.\n\n\nLoad-balanced GridFTP Install\n.  Covers the creation of\n    a load-balanced GridFTP service using multiple servers.\n\n\n\n\n\n\nPrior to following this installation guide, verify the host certificates and networking is configured correctly as in\nthe \nbasic GridFTP install\n.\n\n\nInstallation\n\n\nGridFTP support for XRootD-based storage is provided by the \nosg-gridftp-xrootd\n meta-package:\n\n\nroot@host #\n yum install osg-gridftp-xrootd\n\n\n\n\n\nConfiguration\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring\nauthentication section of the GridFTP guide\n.\n\n\nEdit \n/etc/sysconfig/xrootd-dsi\n to set \nXROOTD_VMP\n to use your XRootD redirector.\n\n\nexport\n \nXROOTD_VMP\n=\nredirector:1094:/local_path=/remote_path\n\n\n\n\n\n\n\n\nWarning\n\n\nThe syntax of \nXROOTD_VMP\n is tricky; make sure to use the following guidance:\n\n\n\n\nRedirector\n: The hostname and domain of the local XRootD redirector server.\n\n\nlocal_path\n: The path exported by the GridFTP server.\n\n\nremote_path\n: The XRootD path that will be mounted at \nlocal_path\n.\n\n\n\n\n\n\nWhen \nxrootd-dsi\n is enabled, GridFTP configuration changes should go into \n/etc/xrootd-dsi/gridftp-xrootd.conf\n, not\n\n/etc/gridftp.conf\n.\n\nSites should review any customizations made in the latter and copy them as necessary.\n\n\nYou can use the FUSE mount in order to test POSIX access to xrootd in the GridFTP server.\nYou should be able to run Unix commands such as \nls /mnt/xrootd\n and see the contents of the XRootD server.\n\n\nFor log / config file locations and system services to run, see the \nbasic GridFTP install\n.\n\n\nUsing XRootD\n\n\nManaging XRootD services\n\n\nStart services on the redirector node before starting any services on the data nodes. \nIf you installed only XRootD itself, you will only need to start the \nxrootd\n service. \nHowever, if you installed cluster management services, you will need to start \ncmsd\n as well.\n\n\nThe instructions for starting and stopping an XRootD service depend on whether the service is installed on an EL 6 or EL\n7 machine, and whether you are using a standalone or clustered configuration.\n\n\nOn EL 6, which config to use is set in the file \n/etc/sysconfig/xrootd\n. \nFor example, to have \nxrootd\n use the clustered config, you would have a line such as this:\n\n\nXROOTD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-\nclustered\n.cfg -k fifo\n\n\n\n\n\n\nTo use the standalone config instead, you would use:\n\n\nXROOTD_DEFAULT_OPTIONS\n=\n-l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-\nstandalone\n.cfg -k fifo\n\n\n\n\n\n\nOn EL 7, which config to use is determined by the service name given to \nsystemctl\n. \nFor example, to have \nxrootd\n use the clustered config, you would start up \nxrootd\n with this line:\n\n\nroot@host #\n systemctl start xrootd@\nclustered\n\n\n\n\n\n\nTo use the standalone config instead, you would use:\n\n\nroot@host #\n systemctl start xrootd@\nstandalone\n\n\n\n\n\n\nThe services are:\n\n\n\n\n\n\n\n\nService\n\n\nEL 6 service name\n\n\nEL 7 service name\n\n\n\n\n\n\n\n\n\n\nXRootD (standalone config)\n\n\nxrootd\n\n\nxrootd@standalone\n\n\n\n\n\n\nXRootD (clustered config)\n\n\nxrootd\n\n\nxrootd@clustered\n\n\n\n\n\n\nCMSD (clustered config)\n\n\ncmsd\n\n\ncmsd@clustered\n\n\n\n\n\n\n\n\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nOn EL\u00a06, run the command\u2026\n\n\nOn EL\u00a07, run the command\u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice SERVICE-NAME start\n\n\nsystemctl start SERVICE-NAME\n\n\n\n\n\n\nStop a service\n\n\nservice SERVICE-NAME stop\n\n\nsystemctl stop SERVICE-NAME\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig SERVICE-NAME on\n\n\nsystemctl enable SERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig SERVICE-NAME off\n\n\nsystemctl disable SERVICE-NAME\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance. please use the \nHelp Procedure\n page.\n\n\nReference\n\n\nFile locations\n\n\n\n\n\n\n\n\nService/Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/etc/xrootd/xrootd-clustered.cfg\n\n\nMain clustered mode XRootD configuration\n\n\n\n\n\n\n\n\n/etc/xrootd/auth_file\n\n\nAuthorized users file\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nService/Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nxrootd\n\n\n/var/log/xrootd/xrootd.log\n\n\nXRootD server daemon log\n\n\n\n\n\n\ncmsd\n\n\n/var/log/xrootd/cmsd.log\n\n\nCluster management log\n\n\n\n\n\n\ncns\n\n\n/var/log/xrootd/cns/xrootd.log\n\n\nServer inventory (composite name space) log\n\n\n\n\n\n\nfrm_xfrd\n, \nfrm_purged\n\n\n/var/log/xrootd/frmd.log\n\n\nFile Residency Manager log\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nXRootD documentation", 
            "title": "Install XRootD SE"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#installing-and-maintaining-xrootd", 
            "text": "XRootD  is a hierarchical storage system that can be used in a variety of ways to access data,\ntypically distributed among actual storage resources. \nOne way to use XRootD is to have it refer to many data resources at a single site, and another way to use it is to refer\nto many storage systems, most likely distributed among sites.  An XRootD system includes a  redirector , which accepts\nrequests for data and finds a storage repository\u00a0\u2014 locally or\notherwise\u00a0\u2014 that can provide the data to the requestor.  Use this page to learn how to install, configure, and use an XRootD redirector as part of a Storage Element (SE) or as\npart of a global namespace.", 
            "title": "Installing and Maintaining XRootD"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   User IDs:  If it does not exist already, the installation will create the Linux user ID  xrootd  Service certificate:  The XRootD service uses a host certificate at  /etc/grid-security/host*.pem  Networking:  The XRootD service uses port 1094 by default   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#installing-an-xrootd-server", 
            "text": "An installation of the XRootD server consists of the server itself and its dependencies. \nInstall these with Yum:  root@host #  yum install xrootd", 
            "title": "Installing an XRootD Server"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#configuring-an-xrootd-server", 
            "text": "", 
            "title": "Configuring an XRootD Server"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#minimal-configuration", 
            "text": "A new installation of XRootD is already configured to run a standalone server that serves files from  /tmp  on the local\nfile system. \nThis configuration is useful to verify basic connectivity between your clients and your server. \nTo do this, start the  xrootd  service with standalone config as described in the  managing services\nsection .  You should be able now to copy a file such as  /bin/sh  using  xrdcp  command into  /tmp . \nTo test, do:  root@host #  yum install xrootd-client root@host #  xrdcp /bin/sh root://localhost:1094//tmp/first_test [xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]  root@host #  ls -l /tmp/first_test -rw-r--r-- 1 xrootd xrootd 801512 Apr 11 10:48 /tmp/first_test   Other than for testing, a standalone server is useful when you want to serve files off of a single host with lots of\nlarge disks. \nIf your storage capacity is spread out over multiple hosts, you will need to set up an XRootD cluster.", 
            "title": "Minimal configuration"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#advanced-configuration", 
            "text": "An advanced XRootD setup has multiple components; it is important to validate that each additional component that you\nset up is working before moving on to the next component. \nWe have included validation instructions after each component below.", 
            "title": "Advanced configuration"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#creating-an-xrootd-cluster", 
            "text": "If your storage is spread out over multiple hosts, you will need to set up an XRootD  cluster . \nThe cluster uses one \"redirector\" node as a frontend for user accesses, and multiple data nodes that have the data that\nusers request. \nTwo daemons will run on each node:  xrootd \nThe eXtended Root Daemon controls file access and storage.  cmsd \nThe Cluster Management Services Daemon controls communication between nodes.  Note that for large virtual organizations, a site-level redirector may actually also communicate upwards to a regional\nor global redirector that handles access to a multi-level hierarchy. \nThis section will only cover handling one level of XRootD hierarchy.  In the instructions below,  RDRNODE  will refer to the redirector host and  DATANODE  will\nrefer to the data node host. \nThese should be replaced with the fully-qualified domain name of the host in question.", 
            "title": "Creating an XRootD cluster"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#modify-etcxrootdxrootd-clusteredcfg", 
            "text": "You will need to modify the  xrootd-clustered.cfg  on the redirector node and each data node. \nThe following example should serve as a base configuration for clustering. Further customizations are detailed below.  all . export   % RED %/ tmp % ENDCOLOR %   stage  set   xrdr   =   % RED % RDRNODE % ENDCOLOR %  all . manager  $ ( xrdr ) : 3121  if  $ ( xrdr ) \n  #  Lines   in   this   block   are   only   executed   on   the   redirector   node \n   all . role   manager  else \n  #  Lines   in   this   block   are   executed   on   all   nodes   but   the   redirector   node \n   all . role   server \n   cms . space   min   % RED % 2 g   5 g % ENDCOLOR %  fi   You will need to customize the following lines:     Configuration Line  Changes Needed      all.export  /tmp  stage  Change  /tmp  to the directory to allow XRootD access to    set xrdr= RDRNODE  Change to the hostname of the redirector    cms.space min  2g 5g  Reserve this amount of free space on the node. For this example, if space falls below 2GB, xrootd will not store further files on this node until space climbs above 5GB. You can use  k ,  m ,  g , or  t  to indicate kilobyte, megabytes, gigabytes, or terabytes, respectively.     Further information can be found at  http://xrootd.slac.stanford.edu/doc", 
            "title": "Modify /etc/xrootd/xrootd-clustered.cfg"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#verifying-the-clustered-config", 
            "text": "Start both  xrootd  and  cmsd  on all nodes according to the instructions in the  managing services\nsection .  Verify that you can copy a file such as  /bin/sh  to  /tmp  on the server data via the redirector:  root@host #  xrdcp /bin/sh  root:// RDRNODE :1094///tmp/second_test [xrootd] Total 0.76 MB  [====================] 100.00 % [inf MB/s]   Check that the  /tmp/second_test  is located on data server  DATANODE .", 
            "title": "Verifying the clustered config"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-adding-simple-server-inventory-to-your-cluster", 
            "text": "The Simple Server Inventory (SSI) provide means to have an inventory for each data server. \nSSI requires:   A second instance of the  xrootd  daemon on the redirector  A \"composite name space daemon\" ( XrdCnsd ) on each data server; this daemon handles the inventory   As an example, we will set up a two-node XRootD cluster with SSI.  Host A is a redirector node that is running the following daemons:   xrootd redirector  cmsd  xrootd - second instance that required for SSI   Host B is a data server that is running the following daemons:   xrootd data server  cmsd  XrdCnsd - started automatically by xrootd   We will need to create a directory on the redirector node for Inventory files.  root@host #  mkdir -p /data/inventory root@host #  chown xrootd:xrootd /data/inventory  On the data server (host B) let's use a storage cache that will be at a different location from  /tmp .   root@host #  mkdir -p  /local/xrootd root@host #  chown xrootd:xrootd /local/xrootd  We will be running two instances of XRootD on  hostA . \nModify  /etc/xrootd/xrootd-clustered.cfg  to give the two instances different behavior, as such:  all . export   / data / xrootdfs  set   xrdr =% RED % hostA % ENDCOLOR %  all . manager  $ ( xrdr ) : 3121  if  $ ( xrdr )     named   cns \n       all . export   / data / inventory \n       xrd . port   1095  else   if  $ ( xrdr ) \n       all . role   manager \n       xrd . port   1094  else \n       all . role   server \n       oss . localroot   / local / xrootd \n       ofs . notify   closew   create   mkdir   mv   rm   rmdir   trunc   |   / usr / bin / XrdCnsd   - d   - D   2   - i   90   - b  $ ( xrdr ) : 1095 : / data / inventory \n      # add   cms . space   if   you   have   less   the   11 GB \n      #  cms . space   options   http : // xrootd . slac . stanford . edu / doc / dev / cms_config . htm \n       cms . space   min   2 g   5 g  fi   The value of  oss.localroot  will be prepended to any file access. \nE.g. accessing  root:// RDRNODE :1094//data/xrootdfs/test1  will actually go to /local/xrootd/data/xrootdfs/test1 .", 
            "title": "(Optional) Adding Simple Server Inventory to your cluster"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#starting-a-second-instance-of-xrootd-on-el-6", 
            "text": "The procedure for starting a second instance differs between EL 6 and EL 7. \nThis section is the procedure for EL 6.  Now, we have to change  /etc/sysconfig/xrootd  on the redirector node ( hostA ) to run multiple instances\nof XRootD. \nThe second instance of XRootD will be named \"cns\" and will be used for SSI.  XROOTD_USER = xrootd   XROOTD_GROUP = xrootd   XROOTD_DEFAULT_OPTIONS = -k 7  -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg  % RED % XROOTD_CNS_OPTIONS = -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg % ENDCOLOR %   CMSD_DEFAULT_OPTIONS = -k 7  -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg   FRMD_DEFAULT_OPTIONS = -k 7  -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg   % RED % XROOTD_INSTANCES = default cns % ENDCOLOR %   CMSD_INSTANCES = default   FRMD_INSTANCES = default    Now, we can start XRootD cluster executing the following commands. \nOn redirector you will see:  root@host #  service xrootd start  Starting xrootd (xrootd, default):  [ OK ]    Starting xrootd (xrootd, cns):  [ OK ]    root@host #  service cmsd start  Starting xrootd (cmsd, default):  [ OK ]     On redirector node you should see two instances of xrootd running:  root@host #  ps auxww | grep xrootd xrootd 29036 0.0 0.0 44008 3172 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default  xrootd 29108 0.0 0.0 43868 3016 ? Sl Apr11 0:00 /usr/bin/xrootd -k 7 -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-cns.pid -n cns  xrootd 29196 0.0 0.0 51420 3692 ? Sl Apr11 0:00 /usr/bin/cmsd -k 7 -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default   Warning  the log file for second named instance of xrootd with be\nplaced in  /var/log/xrootd/cns/xrootd.log  On data server node you should that XrdCnsd process has been started:  root@host #  ps auxww | grep xrootd xrootd 19156 0.0 0.0 48096 3256 ? Sl 07:37 0:00 /usr/bin/cmsd -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/cmsd-default.pid -n default  xrootd 19880 0.0 0.0 46124 2916 ? Sl 08:33 0:00 /usr/bin/xrootd -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg -b -s /var/run/xrootd/xrootd-default.pid -n default  xrootd 19894 0.0 0.1 71164 6960 ? Sl 08:33 0:00 /usr/bin/XrdCnsd -d -D 2 -i 90 -b fermicloud053.fnal.gov:1095:/data/inventory", 
            "title": "Starting a second instance of XRootD on EL 6"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#starting-a-second-instance-of-xrootd-on-el-7", 
            "text": "The procedure for starting a second instance differs between EL 6 and EL 7. \nThis section is the procedure for EL 7.   Create a symlink pointing to  /etc/xrootd/xrootd-clustered.cfg  at  /etc/xrootd/xrootd-cns.cfg :   root@host #  ln -s /etc/xrootd/xrootd-clustered.cfg /etc/xrootd/xrootd-cns.cfg   Start an instance of the  xrootd  service named  cns  using the syntax in the  managing services section :   root@host #  systemctl start xrootd@cns", 
            "title": "Starting a second instance of XRootD on EL 7"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#testing-an-xrootd-cluster-with-ssi", 
            "text": "Copy file to redirector node specifying storage path (/data/xrootdfs instead of /tmp):   root@host #  xrdcp /bin/sh root:// RDRNODE :1094//data/xrootdfs/test1  [xrootd] Total 0.00 MB [================] 100.00 % [inf MB/s]     To verify that SSI is working execute  cns_ssi  command on the redirector node:   root@host #  cns_ssi list /data/inventory  fermicloud054.fnal.gov incomplete inventory as of Mon Apr 11 17:28:11 2011   root@host #  cns_ssi updt /data/inventory  cns_ssi: fermicloud054.fnal.gov inventory with 1 directory and 1 file updated with 0 errors.   root@host #  cns_ssi list /data/inventory  fermicloud054.fnal.gov complete inventory as of Tue Apr 12 07:38:29 2011 /data/xrootdfs/test1    Note : In this example,  fermicloud53.fnal.gov  is a redirector node and  fermicloud054.fnal.gov  is a data node.", 
            "title": "Testing an XRootD cluster with SSI"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-enabling-xrootd-over-http", 
            "text": "XRootD can be accessed using the HTTP protocol. To do that:    Modify  /etc/xrootd/xrootd-clustered.cfg  and add the following lines. You will also need to add the configuration regarding  lcmaps authorization .      if   exec   xrootd \n     xrd . protocol   http : 1094   libXrdHttp . so \n     http . cadir   / etc / grid - security / certificates \n     http . cert   / etc / grid - security / xrd / xrdcert . pem \n     http . key   / etc / grid - security / xrd / xrdkey . pem \n     http . secxtractor   / usr / lib64 / libXrdLcmaps . so \n     http . listingdeny   yes \n     http . staticpreload   http : // static / robots . txt   / etc / xrootd / robots . txt \n     http . desthttps   yes \n    fi     Create robots.txt. Add file  /etc/xrootd/robots.txt  with these contents:      User - agent :   * \n    Disallow :   /     Testing the configuration  From the terminal, generate a proxy and attempt to use davix-get to copy from your XRootD host (the XRootD service needs running; see the  services section ).  For example, if your server has a file named  /store/user/test.root :     davix-get https:// yourHostname :1094/store/user/test.root -E /tmp/x509up_u`id -u` --capath /etc/grid-security/certificates      Note  For clients to successfully read from the regional redirector, HTTPS must be enabled for the data servers and the site-level redirector.    Warning  If you have  u *  in your Authfile, recall this provides an authorization to ALL users, including unauthenticated. This includes random web spiders!", 
            "title": "(Optional) Enabling Xrootd over HTTP"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-enable-http-based-writes", 
            "text": "No changes to the HTTP module is needed to enable HTTP-based writes.  The HTTP protocol uses the same authorization setup as the XRootD protocol.  For example, you may need to provide  a  (all) style authorizations to allow users authorization to write. See the  Authentication File section  for more details.", 
            "title": "(Optional) Enable HTTP based Writes"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-enabling-a-fuse-mount", 
            "text": "XRootD storage can be mounted as a standard POSIX filesystem via FUSE, providing users with a more familiar interface..  Modify  /etc/fstab  by adding the following entries:  ....  xrootdfs                  % RED %/ mnt / xrootd % ENDCOLOR %                fuse      rdr = xroot : //% RED % redirector1 . domain . com % ENDCOLOR % : 1094 /% RED %/ path /% ENDCOLOR % , uid = xrootd   0   0   Replace  /mnt/xrootd  with the path that you would like to access with. \nThis should also match the GridFTP settings for the  XROOTD_VMP  local path. \nCreate  /mnt/xrootd  directory. Make sure the xrootd user exists on the system. Once you are finished, you can mount it:  mount   / mnt / xrootd   You should now be able to run UNIX commands such as  ls /mnt/xrootd  to see the contents of the XRootD server.", 
            "title": "(Optional) Enabling a FUSE mount"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-authorization", 
            "text": "For information on how to configure xrootd-lcmaps authorization, please refer to the Configuring XRootD Authorization guide .", 
            "title": "(Optional) Authorization"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-adding-cms-tfc-support-to-xrootd-cms-sites-only", 
            "text": "For CMS users, there is a package available to integrate rule-based name lookup using a  storage.xml  file. \nIf you are not setting up a CMS site, you can skip this section.  yum install --enablerepo=osg-contrib xrootd-cmstfc   You will need to add your  storage.xml  to  /etc/xrootd/storage.xml  and then add the following line to your XRootD\nconfiguration:  #   Integrate   with   CMS   TFC ,   placed   in   / etc / xrootd / storage . xml  oss . namelib   / usr / lib64 / libXrdCmsTfc . so   file : / etc / xrootd / storage . xml % ORANGE %? protocol = hadoop % ENDCOLOR %   Add the orange text only if you are running hadoop (see below).  See the CMS TWiki for more information:   https://twiki.cern.ch/twiki/bin/view/Main/XrootdTfcChanges  https://twiki.cern.ch/twiki/bin/view/Main/HdfsXrootdInstall", 
            "title": "(Optional) Adding CMS TFC support to XRootD (CMS sites only)"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-adding-hadoop-support-to-xrootd", 
            "text": "HDFS-based sites should utilize the  xrootd-hdfs  plugin to allow XRootD to access their storage.  root@host #  yum install xrootd-hdfs  You will then need to add the following lines to your /etc/xrootd/xrootd-clustered.cfg :  xrootd . fslib   / usr / lib64 / libXrdOfs . so  ofs . osslib   / usr / lib64 / libXrdHdfs . so   For more information, see  the HDFS installation documents .", 
            "title": "(Optional) Adding Hadoop support to XRootD"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-adding-file-residency-manager-frm-to-an-xrootd-cluster", 
            "text": "If you have a multi-tiered storage system (e.g. some data is stored on SSDs and some on disks or tapes), then install\nthe File Residency Manager (FRM), so you can move data between tiers more easily. \nIf you do not have a multi-tiered storage system, then you do not need FRM and you can skip this section.  The FRM deals with two major mechanisms:   local disk  remote servers   The description of fully functional multiple XRootD clusters is beyond the scope of this document. \nIn order to have this fully functional system you will need a global redirector and at least one remote XRootD cluster\nfrom where files could be moved to the local cluster.  Below are the modifications you should make in order to enable FRM on your local cluster:   Make sure that FRM is enabled in  /etc/sysconfig/xrootd  on your data sever:   ROOTD_USER = xrootd   XROOTD_GROUP = xrootd   XROOTD_DEFAULT_OPTIONS = -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd-clustered.cfg   CMSD_DEFAULT_OPTIONS = -l /var/log/xrootd/cmsd.log -c /etc/xrootd/xrootd-clustered.cfg   FRMD_DEFAULT_OPTIONS = -l /var/log/xrootd/frmd.log -c /etc/xrootd/xrootd-clustered.cfg   XROOTD_INSTANCES = default   CMSD_INSTANCES = default   FRMD_INSTANCES = default    Modify  /etc/xrootd/xrootd-clustered.cfg  on both nodes to specify options for  frm_xfrd  (File Transfer Daemon) and  frm_purged  (File Purging Daemon). For more information, you can visit the  FRM Documentation  Start frm daemons on data server:   root@host #  service frm_xfrd start root@host #  service frm_purged start", 
            "title": "(Optional) Adding File Residency Manager (FRM) to an XRootd cluster"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#optional-installing-a-gridftp-server", 
            "text": "The Globus GridFTP server can be installed alongside an XRootD storage element to provide GridFTP-based access to the\nstorage.   See Also  OSG has extensive documentation on setting up a GridFTP server; this section is an\nabbreviated version documenting the special steps needed for XRootD integration.\nYou may also find the following useful:   Basic GridFTP Install .  Additionally covers service planning topics.  Load-balanced GridFTP Install .  Covers the creation of\n    a load-balanced GridFTP service using multiple servers.    Prior to following this installation guide, verify the host certificates and networking is configured correctly as in\nthe  basic GridFTP install .", 
            "title": "(Optional) Installing a GridFTP Server"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#installation", 
            "text": "GridFTP support for XRootD-based storage is provided by the  osg-gridftp-xrootd  meta-package:  root@host #  yum install osg-gridftp-xrootd", 
            "title": "Installation"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#configuration", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring\nauthentication section of the GridFTP guide .  Edit  /etc/sysconfig/xrootd-dsi  to set  XROOTD_VMP  to use your XRootD redirector.  export   XROOTD_VMP = redirector:1094:/local_path=/remote_path    Warning  The syntax of  XROOTD_VMP  is tricky; make sure to use the following guidance:   Redirector : The hostname and domain of the local XRootD redirector server.  local_path : The path exported by the GridFTP server.  remote_path : The XRootD path that will be mounted at  local_path .    When  xrootd-dsi  is enabled, GridFTP configuration changes should go into  /etc/xrootd-dsi/gridftp-xrootd.conf , not /etc/gridftp.conf . \nSites should review any customizations made in the latter and copy them as necessary.  You can use the FUSE mount in order to test POSIX access to xrootd in the GridFTP server.\nYou should be able to run Unix commands such as  ls /mnt/xrootd  and see the contents of the XRootD server.  For log / config file locations and system services to run, see the  basic GridFTP install .", 
            "title": "Configuration"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#using-xrootd", 
            "text": "", 
            "title": "Using XRootD"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#managing-xrootd-services", 
            "text": "Start services on the redirector node before starting any services on the data nodes. \nIf you installed only XRootD itself, you will only need to start the  xrootd  service. \nHowever, if you installed cluster management services, you will need to start  cmsd  as well.  The instructions for starting and stopping an XRootD service depend on whether the service is installed on an EL 6 or EL\n7 machine, and whether you are using a standalone or clustered configuration.  On EL 6, which config to use is set in the file  /etc/sysconfig/xrootd . \nFor example, to have  xrootd  use the clustered config, you would have a line such as this:  XROOTD_DEFAULT_OPTIONS = -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd- clustered .cfg -k fifo   To use the standalone config instead, you would use:  XROOTD_DEFAULT_OPTIONS = -l /var/log/xrootd/xrootd.log -c /etc/xrootd/xrootd- standalone .cfg -k fifo   On EL 7, which config to use is determined by the service name given to  systemctl . \nFor example, to have  xrootd  use the clustered config, you would start up  xrootd  with this line:  root@host #  systemctl start xrootd@ clustered   To use the standalone config instead, you would use:  root@host #  systemctl start xrootd@ standalone   The services are:     Service  EL 6 service name  EL 7 service name      XRootD (standalone config)  xrootd  xrootd@standalone    XRootD (clustered config)  xrootd  xrootd@clustered    CMSD (clustered config)  cmsd  cmsd@clustered     As a reminder, here are common service commands (all run as  root ):     To \u2026  On EL\u00a06, run the command\u2026  On EL\u00a07, run the command\u2026      Start a service  service SERVICE-NAME start  systemctl start SERVICE-NAME    Stop a service  service SERVICE-NAME stop  systemctl stop SERVICE-NAME    Enable a service to start during boot  chkconfig SERVICE-NAME on  systemctl enable SERVICE-NAME    Disable a service from starting during boot  chkconfig SERVICE-NAME off  systemctl disable SERVICE-NAME", 
            "title": "Managing XRootD services"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#getting-help", 
            "text": "To get assistance. please use the  Help Procedure  page.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#file-locations", 
            "text": "Service/Process  Configuration File  Description      xrootd  /etc/xrootd/xrootd-clustered.cfg  Main clustered mode XRootD configuration     /etc/xrootd/auth_file  Authorized users file        Service/Process  Log File  Description      xrootd  /var/log/xrootd/xrootd.log  XRootD server daemon log    cmsd  /var/log/xrootd/cmsd.log  Cluster management log    cns  /var/log/xrootd/cns/xrootd.log  Server inventory (composite name space) log    frm_xfrd ,  frm_purged  /var/log/xrootd/frmd.log  File Residency Manager log", 
            "title": "File locations"
        }, 
        {
            "location": "/data/xrootd/install-storage-element/#links", 
            "text": "XRootD documentation", 
            "title": "Links"
        }, 
        {
            "location": "/data/xrootd/xrootd-authorization/", 
            "text": "Configuring XRootD Authorization\n\n\nThere are several authorization options in XRootD available through the security plugins. \nIn this document, we will cover the \nxrootd-lcmaps\n security option supported\nin the OSG.\n\n\n\n\nNote\n\n\nOn the data nodes, the files will actually be owned by unix user \nxrootd\n (or other daemon user), not as the user\nauthenticated to, under most circumstances.\nXRootD will verify the permissions and authorization based on the user that the security plugin authenticates you\nto, but, internally, the data node files will be owned by the \nxrootd\n user.\n\n\n\n\nAuthorization file\n\n\nXRootD allows configuring fine-grained file access permissions based on usernames and paths.\nThis is configured in the authorization file \n/etc/xrootd/auth_file\n on the data server node, which should be writable\nonly by the xrootd user, optionally readable by others.\n\n\n(The path \n/etc/xrootd/auth_file\n corresponds to the\n\nacc.authdb\n parameter in your xrootd config.)\n\n\nHere is an example \n/etc/xrootd/auth_file\n :\n\n\n# \nThis\n \nmeans\n \nthat\n \nall\n \nthe\n \nusers\n \nhave\n \nread\n \naccess\n \nto\n \nthe\n \ndatasets\n, \n_except_\n \nunder\n \n/\nprivate\n\n\nu\n \n*\n \n-\nrl\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%/\nprivate\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%\n \nrl\n\n\n# \nOr\n \nthe\n \nfollowing\n, \nwithout\n \na\n \nrestricted\n \n/\nprivate\n \ndir\n\n# \nu\n \n*\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%\n \nrl\n\n\n# \nThis\n \nmeans\n \nthat\n \nall\n \nthe\n \nusers\n \nhave\n \nfull\n \naccess\n \nto\n \ntheir\n \nprivate\n \ndirs\n\n\nu\n \n=\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%/\nhome\n/\n@\n=/\n \na\n\n\n# \nThis\n \nmeans\n \nthat\n \nthis\n \nprivileged\n \nuser\n \ncan\n \ndo\n \neverything\n\n# \nYou\n \nneed\n \nat\n \nleast\n \none\n \nuser\n \nlike\n \nthat\n, \nin\n \norder\n \nto\n \ncreate\n \nthe\n\n# \nprivate\n \ndirs\n \nfor\n \nusers\n \nwilling\n \nto\n \nstore\n \ntheir\n \ndata\n \nin\n \nthe\n \nfacility\n\n\nu\n \nxrootd\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%\n \na\n\n\n# \nThis\n \nmeans\n \nthat\n \nusers\n \nin\n \ngroup\n \nbiology\n \ncan\n \ndo\n \nanything\n \nunder\n \nthis\n \npath\n\n\ng\n \nbiology\n \n%\nRED\n%/\ndata\n/\nxrootdfs\n%\nENDCOLOR\n%/\nbiology\n \na\n\n\n\n\n\n\nHere we assume that your storage path is \n/data/xrootdfs\n.\nThis path is relative to the \noss.localroot\n or \nall.localroot\n configuration values, if either one is defined in the\nxrootd config file.\n\n\n\n\nNote\n\n\nSpecific paths need to be specified \nbefore\n generic paths; e.g., this does not work:\n\n\n    \nu\n \n*\n \nrl\n \n/\ndata\n/\nxrootdfs\n \n-\nrl\n \n/\ndata\n/\nxrootdfs\n/\nprivate\n\n\n\n\n\n\n\n\nMore generally, each configuration line of the auth file has the following form:\n\n\nidtype\n \nid\n \npath\n \nprivs\n\n\n\n\n\n\n\n\n\n\n\n\nField\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nidtype\n\n\nType of id. Use \nu\n for username, \ng\n for group, etc.\n\n\n\n\n\n\nid\n\n\nUsername (or groupname). Use \n*\n for all users or \n=\n for user-specific capabilities, like home directories\n\n\n\n\n\n\npath\n\n\nThe path prefix to be used for matching purposes.  \n@=\n expands to the current user name before a path prefix match is attempted\n\n\n\n\n\n\nprivs\n\n\nLetter list of privileges: \na\n - all ; \nl\n - lookup ; \nd\n - delete ; \nn\n - rename ; \ni\n - insert ; \nr\n - read ; \nk\n - lock (not used) ; \nw\n - write ; \n-\n - prefix to remove specified privileges\n\n\n\n\n\n\n\n\nFor more details or examples on how to use templated user options, see\n\nXRootd Authorization Database File\n.\n\n\nEnsure the auth file is owned by \nxrootd\n (if you have created file as root), and that it is not writable by others.\n\n\nroot@host #\n chown xrootd:xrootd /etc/xrootd/auth_file\n\nroot@host #\n chmod \n0640\n /etc/xrootd/auth_file  \n# or 0644\n\n\n\n\n\n\nEnabling xrootd-lcmaps authorization\n\n\nThe xrootd-lcmaps security plugin uses the \nlcmaps\n library and the \nLCMAPS VOMS plugin\n\nto authenticate and authorize users based on X509 certificates and VOMS attributes. Perform the following instructions\non all data nodes:\n\n\n\n\n\n\nInstall \nCA certificates\n and \nmanage CRLs\n\n\n\n\n\n\nFollow the instructions for requesting a \nservice certificate\n,\n   using \nxrootd\n for both the \nSERVICE\n and \nOWNER\n, resulting in a certificate and key in \n/etc/grid-security/xrd/xrdcert.pem\n\n   and \n/etc/grid-security/xrd/xrdkey.pem\n, respectively.\n\n\n\n\n\n\nInstall and configure the \nLCMAPS VOMS plugin\n\n\n\n\n\n\nInstall \nxrootd-lcmaps\n and necessary configuration:\n\n\nroot@host #\n yum install xrootd-lcmaps vo-client\n\n\n\n\n\n\n\n\n\nConfigure access rights for mapped users by creating and modifying the XRootD \nauthorization file\n\n\n\n\n\n\nModify your XRootD configuration:\n\n\n\n\n\n\nChoose the configuration file to edit based on the following table:\n\n\n\n\n\n\n\n\nIf you are running XRootD in...\n\n\nThen modify the following file...\n\n\n\n\n\n\n\n\n\n\nStandalone mode\n\n\n/etc/xrootd/xrootd-standalone.cfg\n\n\n\n\n\n\nClustered mode\n\n\n/etc/xrootd/xrootd-clustered.cfg\n\n\n\n\n\n\n\n\n\n\n\n\nAdd the following lines to the configuration that you chose above:\n\n\nxrootd\n.\nseclib\n \n/\nusr\n/\nlib64\n/\nlibXrdSec\n-\n4.\nso\n\n\nsec\n.\nprotocol\n \n/\nusr\n/\nlib64\n \ngsi\n \n-\ncertdir\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n \n\\\n\n                    \n-\ncert\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdcert\n.\npem\n \n\\\n\n                    \n-\nkey\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdkey\n.\npem\n \n-\ncrl\n:\n1\n \n\\\n\n                    \n-\nauthzfun\n:\nlibXrdLcmaps\n.\nso\n \n-\nauthzfunparms\n:\n--loglevel\n,\n0\n,\n--policy\n,\nauthorize_only\n \n\\\n\n                    \n-\ngmapopt\n:\n10\n \n-\ngmapto\n:\n0\n\n\nacc\n.\nauthdb\n \n/\netc\n/\nxrootd\n/\nauth_file\n\n\nofs\n.\nauthorize\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRestart the \nrelevant services\n\n\n\n\n\n\nTo verify the LCMAPS security, run the following commands from a machine with your user certificate/key pair,\n\nxrootd-client\n, and \nvoms-clients-cpp\n installed:\n\n\n\n\n\n\nDestroy any pre-existing proxies and attempt a copy to a directory (which we will refer to as \nDESTINATION PATH\n) on the \nXROOTD HOST\n to verify failure:\n\n\nuser@client $\n voms-proxy-destroy\n\nuser@client $\n xrdcp /bin/bash root://\nXROOTD HOST\n/\nDESTINATION PATH\n\n\n180213 13:56:49 396570 cryptossl_X509CreateProxy: EEC certificate has expired\n\n\n[0B/0B][100%][==================================================][0B/s]\n\n\nRun: [FATAL] Auth failed\n\n\n\n\n\n\n\n\n\n\nOn the XRootD host, add your DN to \n/etc/grid-security/grid-mapfile\n\n\n\n\n\n\nAdd a line to \n/etc/xrootd/auth_file\n to ensure the mapped user can write to \nDESTINATION PATH\n\n\n\n\n\n\nRestart the xrootd service. (See \nthis section\n for more information\n   of managing XRootD services.)\n\n\n\n\n\n\nGenerate your proxy and verify that you can successfully transfer files:\n\n\nuser@client $\n voms-proxy-init\n\nuser@client $\n xrdcp  /bin/sh root://\nXROOTD HOST\n/\nDESTINATION PATH\n\n\n[938.1kB/938.1kB][100%][==================================================][938.1kB/s]\n\n\n\n\n\n\nIf your transfer does not succeed, run the previous command with \n--debug 2\n for more information.", 
            "title": "Configure Authorization"
        }, 
        {
            "location": "/data/xrootd/xrootd-authorization/#configuring-xrootd-authorization", 
            "text": "There are several authorization options in XRootD available through the security plugins. \nIn this document, we will cover the  xrootd-lcmaps  security option supported\nin the OSG.   Note  On the data nodes, the files will actually be owned by unix user  xrootd  (or other daemon user), not as the user\nauthenticated to, under most circumstances.\nXRootD will verify the permissions and authorization based on the user that the security plugin authenticates you\nto, but, internally, the data node files will be owned by the  xrootd  user.", 
            "title": "Configuring XRootD Authorization"
        }, 
        {
            "location": "/data/xrootd/xrootd-authorization/#authorization-file", 
            "text": "XRootD allows configuring fine-grained file access permissions based on usernames and paths.\nThis is configured in the authorization file  /etc/xrootd/auth_file  on the data server node, which should be writable\nonly by the xrootd user, optionally readable by others.  (The path  /etc/xrootd/auth_file  corresponds to the acc.authdb  parameter in your xrootd config.)  Here is an example  /etc/xrootd/auth_file  :  #  This   means   that   all   the   users   have   read   access   to   the   datasets ,  _except_   under   / private  u   *   - rl   % RED %/ data / xrootdfs % ENDCOLOR %/ private   % RED %/ data / xrootdfs % ENDCOLOR %   rl \n\n#  Or   the   following ,  without   a   restricted   / private   dir \n#  u   *   % RED %/ data / xrootdfs % ENDCOLOR %   rl \n\n#  This   means   that   all   the   users   have   full   access   to   their   private   dirs  u   =   % RED %/ data / xrootdfs % ENDCOLOR %/ home / @ =/   a \n\n#  This   means   that   this   privileged   user   can   do   everything \n#  You   need   at   least   one   user   like   that ,  in   order   to   create   the \n#  private   dirs   for   users   willing   to   store   their   data   in   the   facility  u   xrootd   % RED %/ data / xrootdfs % ENDCOLOR %   a \n\n#  This   means   that   users   in   group   biology   can   do   anything   under   this   path  g   biology   % RED %/ data / xrootdfs % ENDCOLOR %/ biology   a   Here we assume that your storage path is  /data/xrootdfs .\nThis path is relative to the  oss.localroot  or  all.localroot  configuration values, if either one is defined in the\nxrootd config file.   Note  Specific paths need to be specified  before  generic paths; e.g., this does not work:       u   *   rl   / data / xrootdfs   - rl   / data / xrootdfs / private    More generally, each configuration line of the auth file has the following form:  idtype   id   path   privs      Field  Description      idtype  Type of id. Use  u  for username,  g  for group, etc.    id  Username (or groupname). Use  *  for all users or  =  for user-specific capabilities, like home directories    path  The path prefix to be used for matching purposes.   @=  expands to the current user name before a path prefix match is attempted    privs  Letter list of privileges:  a  - all ;  l  - lookup ;  d  - delete ;  n  - rename ;  i  - insert ;  r  - read ;  k  - lock (not used) ;  w  - write ;  -  - prefix to remove specified privileges     For more details or examples on how to use templated user options, see XRootd Authorization Database File .  Ensure the auth file is owned by  xrootd  (if you have created file as root), and that it is not writable by others.  root@host #  chown xrootd:xrootd /etc/xrootd/auth_file root@host #  chmod  0640  /etc/xrootd/auth_file   # or 0644", 
            "title": "Authorization file"
        }, 
        {
            "location": "/data/xrootd/xrootd-authorization/#enabling-xrootd-lcmaps-authorization", 
            "text": "The xrootd-lcmaps security plugin uses the  lcmaps  library and the  LCMAPS VOMS plugin \nto authenticate and authorize users based on X509 certificates and VOMS attributes. Perform the following instructions\non all data nodes:    Install  CA certificates  and  manage CRLs    Follow the instructions for requesting a  service certificate ,\n   using  xrootd  for both the  SERVICE  and  OWNER , resulting in a certificate and key in  /etc/grid-security/xrd/xrdcert.pem \n   and  /etc/grid-security/xrd/xrdkey.pem , respectively.    Install and configure the  LCMAPS VOMS plugin    Install  xrootd-lcmaps  and necessary configuration:  root@host #  yum install xrootd-lcmaps vo-client    Configure access rights for mapped users by creating and modifying the XRootD  authorization file    Modify your XRootD configuration:    Choose the configuration file to edit based on the following table:     If you are running XRootD in...  Then modify the following file...      Standalone mode  /etc/xrootd/xrootd-standalone.cfg    Clustered mode  /etc/xrootd/xrootd-clustered.cfg       Add the following lines to the configuration that you chose above:  xrootd . seclib   / usr / lib64 / libXrdSec - 4. so  sec . protocol   / usr / lib64   gsi   - certdir : / etc / grid - security / certificates   \\ \n                     - cert : / etc / grid - security / xrd / xrdcert . pem   \\ \n                     - key : / etc / grid - security / xrd / xrdkey . pem   - crl : 1   \\ \n                     - authzfun : libXrdLcmaps . so   - authzfunparms : --loglevel , 0 , --policy , authorize_only   \\ \n                     - gmapopt : 10   - gmapto : 0  acc . authdb   / etc / xrootd / auth_file  ofs . authorize       Restart the  relevant services    To verify the LCMAPS security, run the following commands from a machine with your user certificate/key pair, xrootd-client , and  voms-clients-cpp  installed:    Destroy any pre-existing proxies and attempt a copy to a directory (which we will refer to as  DESTINATION PATH ) on the  XROOTD HOST  to verify failure:  user@client $  voms-proxy-destroy user@client $  xrdcp /bin/bash root:// XROOTD HOST / DESTINATION PATH  180213 13:56:49 396570 cryptossl_X509CreateProxy: EEC certificate has expired  [0B/0B][100%][==================================================][0B/s]  Run: [FATAL] Auth failed     On the XRootD host, add your DN to  /etc/grid-security/grid-mapfile    Add a line to  /etc/xrootd/auth_file  to ensure the mapped user can write to  DESTINATION PATH    Restart the xrootd service. (See  this section  for more information\n   of managing XRootD services.)    Generate your proxy and verify that you can successfully transfer files:  user@client $  voms-proxy-init user@client $  xrdcp  /bin/sh root:// XROOTD HOST / DESTINATION PATH  [938.1kB/938.1kB][100%][==================================================][938.1kB/s]   If your transfer does not succeed, run the previous command with  --debug 2  for more information.", 
            "title": "Enabling xrootd-lcmaps authorization"
        }, 
        {
            "location": "/data/xrootd/install-client/", 
            "text": "Using XRootD\n\n\nXRootD is a high performance data system widely used by several science VOs on OSG to store and to distribute data to\njobs.\nIt can be used to create a data store from distributed data nodes or to serve data to systems using a distributed\ncaching architecture.\nEither mode of operation requires you to install the XRootD client software.\n\n\nThis page provides instructions for accessing data on XRootD data systems using a variety of methods.\n\n\nAs a user you have three different ways to interact with XRootD: \n\n\n\n\nUsing the XRootD clients\n\n\nUsing a XRootDFS FUSE mount to access a local XRootD data store\n\n\nUsing LD_PRELOAD to use XRootD libraries with Unix tools\n\n\n\n\nWe'll show how to install the XRootD client software and use all three mechanisms to access data.\n\n\n\n\nNote\n\n\nOnly the client tools method should be used to access XRootD systems across a WAN link.\n\n\n\n\nBefore Starting\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nIf you are using the FUSE mount, you should also consider the following requirement:\n\n\n\n\nUser IDs:\n If it does not exist already, you will need to create a \nxrootd\n user\n\n\n\n\nUsing the XRootD client software\n\n\nInstalling the XRootD Client\n\n\nIf you are planning on interacting with XRootD using the XRootD client, then you'll need to install the XRootD\nclient RPM.\n\n\nInstalling the XRootD Client RPM\n\n\nThe following steps will install the rpm on your system.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@client $\n yum clean all --enablerepo\n=\n\\*\n\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@client $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall XRootD Client rpm:\n\n\nroot@client $\n yum install xrootd-client\n\n\n\n\n\n\n\n\n\nUsing the XRootD Client\n\n\nOnce the \nxrootd-client\n rpm is installed, you should be able to use the \nxrdcp\n command to copy files to and from\nXRootD systems and the local file system.\nFor example:\n\n\nuser@client $\n \necho\n \nThis is a test\n \n/tmp/test \n\nuser@client $\n xrdcp /tmp/test xroot://redirector.domain.org:1094//storage/path/test \n\nuser@client $\n xrdcp xroot://redirector.domain.org:1094//storage/path/test /tmp/test1 \n\nuser@client $\n diff /tmp/test1 /tmp/test \n\n\n\n\n\nFor other operations, you'll need to use the \nxrdfs\n command.\nThis command allows you to do file operations such as creating directories, removing directories, deleting files, and\nmoving files on a XRootD system, provided you have the appropriate authorization.\nThe \nxrdfs\n command can be used interactively by running \nxrdfs xroot://redirector.domain.org:1094/\n.\nAlternatively, you can use it in batch mode by adding the \nxrdfs\n command after the xroot URI.  For example:\n\n\nuser@client $\n \necho\n \nThis is a test\n \n/tmp/test \n\nuser@client $\n xrdfs xroot://redirector.domain.org:1094/  mkdir  /storage/path/test\n\nuser@client $\n xrdcp xroot://redirector.domain.org:1094//storage/path/test/test1 /tmp/test1 \n\nuser@client $\n xrdfs xroot://redirector.domain.org:1094/  ls  /storage/path/test/test1\n\nuser@client $\n xrdfs xroot://redirector.domain.org:1094/  rm  /storage/path/test/test1\n\nuser@client $\n xrdfs xroot://redirector.domain.org:1094/  rmdir  /storage/path/test\n\n\n\n\n\n\n\nNote\n\n\nTo access remote XRootD resources, you will may need to use a VOMS proxy in order to authenticate successfully.  The\nXRootD client tools will automatically locate your proxy if you generate it using \nvoms-proxy-init\n, otherwise you\ncan set the \nX509_USER_PROXY\n environment variable to the location of the proxy XRootD should use.\n\n\n\n\nValidation\n\n\nAssuming that there is a file called \ntest_file\n in your XRootD data store, you can do the following to validate your\ninstallation.  Here we assume that there is a file on your XRootD system at \n/storage/path/test_file\n.\n\n\nuser@client $\n xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1\n\n\n\n\n\nUsing XRootDFS FUSE mount\n\n\nThis section will explain how to install, setup, and interact with XRootD using a FUSE mount.\nThis method of accessing XRootD only works when accessing a local XRootD system.\n\n\nInstalling the XRootD FUSE RPM\n\n\nIf you are planning on using a FUSE mount, you'll need to install the xrootd-fuse rpm by running the following commands:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@client $\n yum clean all --enablerepo\n=\n\\*\n\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@client $\n yum update\n\n\n\n\n\n\n\n\n\nInstall XRootD FUSE rpm:\n\n\nroot@client $\n yum install xrootd-fuse\n\n\n\n\n\n\n\n\n\nConfiguring the FUSE Mount\n\n\nOnce the appropriate rpms are installed, the FUSE setup will need further configuration.\nSee \nthis\n for instructions on updating your\n\nfstab\n file.\n\n\nUsing the XRootDFS FUSE Mount\n\n\nThe directory mounted using XRootDFS can be used as any other directory mounted on your file system. \nAll the normal Unix commands should work out of the box.\nTry using \ncp\n, \nrm\n, \nmv\n, \nmkdir\n, \nrmdir\n.\n\n\nAssuming your mount is \n/mnt/xrootd\n:\n\n\nuser@client $\n \necho\n \nThis is a new test\n \n/tmp/test \n\nuser@client $\n mkdir -p /mnt/xrootd/subdir/sub2\n\nuser@client $\n cp /tmp/test /mnt/xrootd/subdir/sub2/test \n\nuser@client $\n cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1 \n\nuser@client $\n cp /mnt/xuserd/subdir/sub2/test1 /tmp/test1 \n\nuser@client $\n diff /tmp/test1 /tmp/test \n\nuser@client $\n rm -r /mnt/xrootd/subdir\n\n\n\n\n\nValidation\n\n\nAssuming your mount is \n/mnt/xrootd\n and that there is a file called \ntest_file\n in your XRootD data store:\n\n\nuser@client $\n cp /mnt/xrootd/test_file /tmp/test1\n\n\n\n\n\nUsing LD_PRELOAD to access XRootD\n\n\nInstalling XRootD Libraries For LD_PRELOAD\n\n\nIn order to use LD_PRELOAD to access XRootD, you'll need to install the XRootD client libraries.\n\nThe following steps will install them on your system:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@client $\n yum clean all --enablerepo\n=\n\\*\n\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@client $\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall XRootD Client rpm:\n\n\nroot@client $\n yum install xrootd-client\n\n\n\n\n\n\n\n\n\nUsing LD_PRELOAD method\n\n\nIn order to use the LD_PRELOAD method to access a XRootD data store, you'll need to change your environment to use the\nXRootD libraries in conjunction with the standard Unix binaries.\nThis is done by setting the \nLD_PRELOAD\n environment variable.\nOnce this is done, the standard unix commands like \nmkdir\n, \nrm\n, \ncp\n, etc. will work with xroot URIs.\nFor example:\n\n\nuser@client $\n \nexport\n \nLD_PRELOAD\n=\n/usr/lib64/libXrdPosixPreload.so \n\nuser@client $\n \necho\n \nThis is a new test\n \n/tmp/test \n\nuser@client $\n mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir\n\nuser@client $\n cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test \n\nuser@client $\n cp xuser://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1 \n\nuser@client $\n diff /tmp/test1 /tmp/test \n\nuser@client $\n rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test \n\nuser@client $\n rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir\n\n\n\n\n\nValidation\n\n\nAssuming that there is a file called \ntest_file\n in your XRootD data store, the following steps will validate your\ninstallation:\n\n\nuser@client $\n \nexport\n \nLD_PRELOAD\n=\n/usr/lib64/libXrdPosixPreload.so\n\nuser@client $\n cp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1\n\n\n\n\n\nHow to get Help?\n\n\nIf you cannot resolve the problem, the best way to get help is by contacting \n.\nFor a full set of help options, see \nHelp Procedure\n.", 
            "title": "Using XRootD"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-xrootd", 
            "text": "XRootD is a high performance data system widely used by several science VOs on OSG to store and to distribute data to\njobs.\nIt can be used to create a data store from distributed data nodes or to serve data to systems using a distributed\ncaching architecture.\nEither mode of operation requires you to install the XRootD client software.  This page provides instructions for accessing data on XRootD data systems using a variety of methods.  As a user you have three different ways to interact with XRootD:    Using the XRootD clients  Using a XRootDFS FUSE mount to access a local XRootD data store  Using LD_PRELOAD to use XRootD libraries with Unix tools   We'll show how to install the XRootD client software and use all three mechanisms to access data.   Note  Only the client tools method should be used to access XRootD systems across a WAN link.", 
            "title": "Using XRootD"
        }, 
        {
            "location": "/data/xrootd/install-client/#before-starting", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates   If you are using the FUSE mount, you should also consider the following requirement:   User IDs:  If it does not exist already, you will need to create a  xrootd  user", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-the-xrootd-client-software", 
            "text": "", 
            "title": "Using the XRootD client software"
        }, 
        {
            "location": "/data/xrootd/install-client/#installing-the-xrootd-client", 
            "text": "If you are planning on interacting with XRootD using the XRootD client, then you'll need to install the XRootD\nclient RPM.", 
            "title": "Installing the XRootD Client"
        }, 
        {
            "location": "/data/xrootd/install-client/#installing-the-xrootd-client-rpm", 
            "text": "The following steps will install the rpm on your system.    Clean yum cache:  root@client $  yum clean all --enablerepo = \\*     Update software:  root@client $  yum update  This command will update  all  packages    Install XRootD Client rpm:  root@client $  yum install xrootd-client", 
            "title": "Installing the XRootD Client RPM"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-the-xrootd-client", 
            "text": "Once the  xrootd-client  rpm is installed, you should be able to use the  xrdcp  command to copy files to and from\nXRootD systems and the local file system.\nFor example:  user@client $   echo   This is a test   /tmp/test  user@client $  xrdcp /tmp/test xroot://redirector.domain.org:1094//storage/path/test  user@client $  xrdcp xroot://redirector.domain.org:1094//storage/path/test /tmp/test1  user@client $  diff /tmp/test1 /tmp/test   For other operations, you'll need to use the  xrdfs  command.\nThis command allows you to do file operations such as creating directories, removing directories, deleting files, and\nmoving files on a XRootD system, provided you have the appropriate authorization.\nThe  xrdfs  command can be used interactively by running  xrdfs xroot://redirector.domain.org:1094/ .\nAlternatively, you can use it in batch mode by adding the  xrdfs  command after the xroot URI.  For example:  user@client $   echo   This is a test   /tmp/test  user@client $  xrdfs xroot://redirector.domain.org:1094/  mkdir  /storage/path/test user@client $  xrdcp xroot://redirector.domain.org:1094//storage/path/test/test1 /tmp/test1  user@client $  xrdfs xroot://redirector.domain.org:1094/  ls  /storage/path/test/test1 user@client $  xrdfs xroot://redirector.domain.org:1094/  rm  /storage/path/test/test1 user@client $  xrdfs xroot://redirector.domain.org:1094/  rmdir  /storage/path/test   Note  To access remote XRootD resources, you will may need to use a VOMS proxy in order to authenticate successfully.  The\nXRootD client tools will automatically locate your proxy if you generate it using  voms-proxy-init , otherwise you\ncan set the  X509_USER_PROXY  environment variable to the location of the proxy XRootD should use.", 
            "title": "Using the XRootD Client"
        }, 
        {
            "location": "/data/xrootd/install-client/#validation", 
            "text": "Assuming that there is a file called  test_file  in your XRootD data store, you can do the following to validate your\ninstallation.  Here we assume that there is a file on your XRootD system at  /storage/path/test_file .  user@client $  xrdcp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1", 
            "title": "Validation"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-xrootdfs-fuse-mount", 
            "text": "This section will explain how to install, setup, and interact with XRootD using a FUSE mount.\nThis method of accessing XRootD only works when accessing a local XRootD system.", 
            "title": "Using XRootDFS FUSE mount"
        }, 
        {
            "location": "/data/xrootd/install-client/#installing-the-xrootd-fuse-rpm", 
            "text": "If you are planning on using a FUSE mount, you'll need to install the xrootd-fuse rpm by running the following commands:    Clean yum cache:  root@client $  yum clean all --enablerepo = \\*     Update software:  root@client $  yum update    Install XRootD FUSE rpm:  root@client $  yum install xrootd-fuse", 
            "title": "Installing the XRootD FUSE RPM"
        }, 
        {
            "location": "/data/xrootd/install-client/#configuring-the-fuse-mount", 
            "text": "Once the appropriate rpms are installed, the FUSE setup will need further configuration.\nSee  this  for instructions on updating your fstab  file.", 
            "title": "Configuring the FUSE Mount"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-the-xrootdfs-fuse-mount", 
            "text": "The directory mounted using XRootDFS can be used as any other directory mounted on your file system. \nAll the normal Unix commands should work out of the box.\nTry using  cp ,  rm ,  mv ,  mkdir ,  rmdir .  Assuming your mount is  /mnt/xrootd :  user@client $   echo   This is a new test   /tmp/test  user@client $  mkdir -p /mnt/xrootd/subdir/sub2 user@client $  cp /tmp/test /mnt/xrootd/subdir/sub2/test  user@client $  cp /mnt/xrootd/subdir/sub2/test /mnt/xrootd/subdir/sub2/test1  user@client $  cp /mnt/xuserd/subdir/sub2/test1 /tmp/test1  user@client $  diff /tmp/test1 /tmp/test  user@client $  rm -r /mnt/xrootd/subdir", 
            "title": "Using the XRootDFS FUSE Mount"
        }, 
        {
            "location": "/data/xrootd/install-client/#validation_1", 
            "text": "Assuming your mount is  /mnt/xrootd  and that there is a file called  test_file  in your XRootD data store:  user@client $  cp /mnt/xrootd/test_file /tmp/test1", 
            "title": "Validation"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-ld95preload-to-access-xrootd", 
            "text": "", 
            "title": "Using LD_PRELOAD to access XRootD"
        }, 
        {
            "location": "/data/xrootd/install-client/#installing-xrootd-libraries-for-ld95preload", 
            "text": "In order to use LD_PRELOAD to access XRootD, you'll need to install the XRootD client libraries. \nThe following steps will install them on your system:    Clean yum cache:  root@client $  yum clean all --enablerepo = \\*     Update software:  root@client $  yum update  This command will update  all  packages    Install XRootD Client rpm:  root@client $  yum install xrootd-client", 
            "title": "Installing XRootD Libraries For LD_PRELOAD"
        }, 
        {
            "location": "/data/xrootd/install-client/#using-ld95preload-method", 
            "text": "In order to use the LD_PRELOAD method to access a XRootD data store, you'll need to change your environment to use the\nXRootD libraries in conjunction with the standard Unix binaries.\nThis is done by setting the  LD_PRELOAD  environment variable.\nOnce this is done, the standard unix commands like  mkdir ,  rm ,  cp , etc. will work with xroot URIs.\nFor example:  user@client $   export   LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so  user@client $   echo   This is a new test   /tmp/test  user@client $  mkdir xroot://redirector.yourdomain.org:1094//storage/path/subdir user@client $  cp /tmp/test xroot://redirector.yourdomain.org:1094//storage/path/subdir/test  user@client $  cp xuser://redirector.yourdomain.org:1094//storage/path/subdir/test /tmp/test1  user@client $  diff /tmp/test1 /tmp/test  user@client $  rm xroot://redirector.yourdomain.org:1094//storage/path/subdir/test  user@client $  rmdir xroot://redirector.yourdomain.org:1094//storage/path/subdir", 
            "title": "Using LD_PRELOAD method"
        }, 
        {
            "location": "/data/xrootd/install-client/#validation_2", 
            "text": "Assuming that there is a file called  test_file  in your XRootD data store, the following steps will validate your\ninstallation:  user@client $   export   LD_PRELOAD = /usr/lib64/libXrdPosixPreload.so user@client $  cp xroot://redirector.yourdomain.org:1094//storage/path/test_file /tmp/test1", 
            "title": "Validation"
        }, 
        {
            "location": "/data/xrootd/install-client/#how-to-get-help", 
            "text": "If you cannot resolve the problem, the best way to get help is by contacting  .\nFor a full set of help options, see  Help Procedure .", 
            "title": "How to get Help?"
        }, 
        {
            "location": "/data/stashcache/overview/", 
            "text": "Overview of StashCache\n\n\nThe OSG operates the \nStashCache data federation\n, which provides organizations with a method to distribute their data\nin a scalable manner to thousands of jobs without needing to pre-stage data at each site.\nThe StashCache data federation is best suited for per-job data set sizes between 1 and 50 GB,\nwith no more than 1 TB for a complete workflow.\n\n\nOrganizations export their data from an \norigin server\n,\nand the data is streamed to jobs from a set of \ncache servers\n distributed throughout the OSG and Internet2.\nJobs can achieve lower latency and higher throughput for data transfer by using a nearby cache\ninstead of accessing the origin directly.\n\n\nThe map below shows the location of the current caches in the StashCache federation:\n\n\n\n\nArchitecture\n\n\nThe StashCache federation consists of three service types:\n\n\n\n\nOrigin\n: Keeps the authoritative copy of an organization's data.\n    Each origin is operated by the organization that wants to distribute its data within the StashCache federation.\n\n\nCache\n: Transfers data to clients such as jobs or users.\n    A set of caches are operated across the OSG for the benefit of nearby sites;\n    in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN.\n\n\nRedirector\n: Tells a cache service which origin to download data from.\n    The redirectors are run centrally by OSG Operations.\n\n\n\n\nThe structure of the federation is illustrated below:\n\n\n\n\nA job will request data from a cache.\nThe cache will serve the requested data from local disk if possible.\nOtherwise, it will query a redirector for the origin server that provides the data,\ndownload the data from that origin server, and then serve the data to the job.\nA copy of the data will be kept on the cache for use by future jobs.\n\n\nJoining and Using StashCache\n\n\n\n\nOrganizations can export their data to StashCache by \ninstalling the StashCache \nOrigin\n\n\nSites can reduce data transfer via the WAN by \ninstalling the StashCache \nCache\n\n\nUsers can access StashCache via the\n  \nstashcp client\n\n  or from CVMFS via the \n/cvmfs/stash.osgstorage.org\n directory tree.", 
            "title": "StashCache Overview"
        }, 
        {
            "location": "/data/stashcache/overview/#overview-of-stashcache", 
            "text": "The OSG operates the  StashCache data federation , which provides organizations with a method to distribute their data\nin a scalable manner to thousands of jobs without needing to pre-stage data at each site.\nThe StashCache data federation is best suited for per-job data set sizes between 1 and 50 GB,\nwith no more than 1 TB for a complete workflow.  Organizations export their data from an  origin server ,\nand the data is streamed to jobs from a set of  cache servers  distributed throughout the OSG and Internet2.\nJobs can achieve lower latency and higher throughput for data transfer by using a nearby cache\ninstead of accessing the origin directly.  The map below shows the location of the current caches in the StashCache federation:", 
            "title": "Overview of StashCache"
        }, 
        {
            "location": "/data/stashcache/overview/#architecture", 
            "text": "The StashCache federation consists of three service types:   Origin : Keeps the authoritative copy of an organization's data.\n    Each origin is operated by the organization that wants to distribute its data within the StashCache federation.  Cache : Transfers data to clients such as jobs or users.\n    A set of caches are operated across the OSG for the benefit of nearby sites;\n    in addition, each site may run its own cache in order to reduce the amount of data transferred over the WAN.  Redirector : Tells a cache service which origin to download data from.\n    The redirectors are run centrally by OSG Operations.   The structure of the federation is illustrated below:   A job will request data from a cache.\nThe cache will serve the requested data from local disk if possible.\nOtherwise, it will query a redirector for the origin server that provides the data,\ndownload the data from that origin server, and then serve the data to the job.\nA copy of the data will be kept on the cache for use by future jobs.", 
            "title": "Architecture"
        }, 
        {
            "location": "/data/stashcache/overview/#joining-and-using-stashcache", 
            "text": "Organizations can export their data to StashCache by  installing the StashCache  Origin  Sites can reduce data transfer via the WAN by  installing the StashCache  Cache  Users can access StashCache via the\n   stashcp client \n  or from CVMFS via the  /cvmfs/stash.osgstorage.org  directory tree.", 
            "title": "Joining and Using StashCache"
        }, 
        {
            "location": "/data/stashcache/install-cache/", 
            "text": "Installing the StashCache Cache\n\n\nThis document describes how to install a StashCache cache service.  This service allows a site or regional\nnetwork to cache data frequently used on the OSG, reducing data transfer over the wide-area network and\ndecreasing access latency.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following requirements:\n\n\n\n\nOperating system:\n A RHEL 7 or compatible operating systems.\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux user IDs \ncondor\n and\n  \nxrootd\n\n\nHost certificate:\n Required for reporting and authenticated StashCache.\n  Authenticated StashCache is an optional feature.\n  See our \ndocumentation\n for instructions on how to request and install host certificates.\n\n\nNetwork ports:\n The cache service requires the following ports open:\n\n\nInbound TCP port 1094 for file access via the XRootD protocol\n\n\nInbound TCP port 8000 for file access via HTTP\n\n\nInbound TCP port 8443 for authenticated file access via HTTPS (optional)\n\n\nOutbound UDP port 9930 for reporting to \nxrd-report.osgstorage.org\n and \nxrd-mon.osgstorage.org\n for monitoring\n\n\n\n\n\n\nHardware requirements:\n We recommend that a cache has at least 10Gbps connectivity, 1TB of\n disk space for the cache directory, and 8GB of RAM.\n\n\n\n\nAs with all OSG software installations, there are some one-time steps to prepare in advance:\n\n\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nRegistering the Cache\n\n\nTo be part of the OSG StashCache Federation, your cache must be registered with the OSG.\nYou will need basic information like the resource name and hostname,\nand the administrative and security contacts.\n\n\nInitial registration\n\n\nTo register your cache host, follow the general registration instructions \nhere\n.\nThe service type is \nXRootD cache server\n.\n\n\n\n\nInfo\n\n\nThis step must be completed before installation.\n\n\n\n\nIn your registration, you must specify which VOs your cache will serve by adding an \nAllowedVOs\n list, with each line specifying a VO whose data you are willing to cache.\n\n\nThere are special values you may use in \nAllowedVOs\n:\n\n\n\n\nANY_PUBLIC\n indicates that the cache is willing to serve public data from any VO.\n\n\nANY\n indicates that the cache is willing to serve data from any VO,\n  both public and non-public.\n  \nANY\n implies \nANY_PUBLIC\n.\n\n\n\n\nThere are extra requirements for serving non-public data:\n\n\n\n\nIn addition to the cache allowing a VO in the \nAllowedVOs\n list,\n  that VO must also allow the cache in its \nAllowedCaches\n list.\n  See the page on \ngetting your VO's data into StashCache\n.\n\n\nThere must be an authenticated XRootD instance on the cache server.\n\n\nThere must be a \nDN\n attribute in the resource registration\n  with the \nsubject DN\n of the host certificate\n\n\n\n\nThis is an example registration for a cache server that serves all public data:\n\n\n  \nMY_STASHCACHE_CACHE\n:\n\n    \nFQDN\n:\n \nmy-cache.example.net\n\n    \nService\n:\n \nXRootD cache server\n\n      \nDescription\n:\n \nStashCache cache server\n\n    \nAllowedVOs\n:\n\n      \n-\n \nANY_PUBLIC\n\n\n\n\n\n\nThis is an example registration for a cache server that only serves authenticated data from the OSG VO:\n\n\n  \nMY_AUTH_STASHCACHE_CACHE\n:\n\n    \nFQDN\n:\n \nmy-auth-cache.example.net\n\n    \nService\n:\n \nXRootD cache server\n\n      \nDescription\n:\n \nStashCache cache server\n\n    \nAllowedVOs\n:\n\n      \n-\n \nOSG\n\n    \nDN\n:\n \n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-auth-cache.example.net\n\n\n\n\n\n\nThis is an example registration for a cache server that serves all public data \nand\n authenticated data from the OSG VO:\n\n\n  \nMY_COMBO_STASHCACHE_CACHE\n:\n\n    \nFQDN\n:\n \nmy-combo-cache.example.net\n\n    \nService\n:\n \nXRootD cache server\n\n      \nDescription\n:\n \nStashCache cache server\n\n    \nAllowedVOs\n:\n\n      \n-\n \nOSG\n\n      \n-\n \nANY_PUBLIC\n\n    \nDN\n:\n \n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-combo-cache.example.net\n\n\n\n\n\n\nFinalizing registration\n\n\nOnce initial registration is complete, you may start the installation process.\nIn the meantime, open a \nhelp ticket\n with your cache name.\nMention in your ticket that you would like to \"Finalize the cache registration.\"\n\n\nInstalling the Cache\n\n\nThe StashCache software consists of an XRootD server with special configuration and supporting services.\nTo simplify installation, OSG provides convenience RPMs that install all required\npackages with a single command:\n\n\nroot@host #\n yum install stash-cache\n\n\n\n\n\nConfiguring the Cache\n\n\nFirst, you must create a \"cache directory\", which will be used to store downloaded files.\nBy default this is \n/mnt/stash\n.\nWe recommend using a separate file system for the cache directory,\nwith at least 1 TB of storage available.\n\n\n\n\nNote\n\n\nThe cache directory must be writable by the \nxrootd:xrootd\n user and group.\n\n\n\n\nThe \nstash-cache\n package provides default configuration files in \n/etc/xrootd/xrootd-stash-cache.cfg\n and \n/etc/xrootd/config.d/\n.\nAdministrators may provide additional configuration by placing files in \n/etc/xrootd/config.d/1*.cfg\n (for files that need to be processed BEFORE the OSG configuration) or \n/etc/xrootd/config.d/9*.cfg\n (for files that need to be processed AFTER the OSG configuration).\n\n\nYou \nmust\n configure every variable in \n/etc/xrootd/10-common-site-local.cfg\n.\n\n\nThe mandatory variables to configure are:\n\n\n\n\nset rootdir = /mnt/stash\n: the mounted filesystem path to export.  This document refers to this as \n/mnt/stash\n.\n\n\nset resourcename = YOUR_RESOURCE_NAME\n: the resource name registered with the OSG.\n\n\n\n\nEnsure the xrootd service has a certificate\n\n\nThe service will need a certificate for reporting and to authenticate to StashCache origins.\nThe easiest solution for this is to use your host certificate and key as follows:\n\n\n\n\nCopy the host certificate to \n/etc/grid-security/xrd/xrd{cert,key}.pem\n\n\nSet the owner of the directory and contents \n/etc/grid-security/xrd/\n to \nxrootd:xrootd\n:\nroot@host #\n chown -R xrootd:xrootd /etc/grid-security/xrd/\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nYou must repeat the above steps whenever you renew your host certificate.\nIf you automate certificate renewal, you should automate copying as well.\nFor example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented\n\non the Certbot site\n.\n\n\n\n\nConfiguring Optional Features\n\n\nAdjust disk utilization\n\n\nTo adjust the disk utilization of your cache, create or edit a file named \n/etc/xrootd/config.d/90-local.cfg\n\nand set the values of \npfc.diskusage\n.\n\n\npfc\n.\ndiskusage\n \n0\n.\n90\n \n0\n.\n95\n\n\n\n\n\n\nThe two values correspond to the low and high usage water marks, respectively.\nWhen usage goes above the high water mark,\nthe XRootD service will delete cached files until usage goes below the low water mark.\n\n\nEnable remote debugging\n\n\nXRootD provides remote debugging via a read-only file system named digFS.\nThis feature is disabled by default, but you may enable it if you need help troubleshooting your server.\n\n\nTo enable remote debugging, edit \n/etc/xrootd/digauth.cfg\n and specify the authorizations for reading digFS.\nAn example of authorizations:\n\n\nall\n \nallow\n \ngsi\n \ng\n=/\nglow\n \nh\n=*\n.\ncs\n.\nwisc\n.\nedu\n\n\n\n\n\n\nThis gives access to the config file, log files, core files, and process information\nto anyone from \n*.cs.wisc.edu\n in the \n/glow\n VOMS group.\n\n\nSee \nthe XRootD manual\n for the full syntax.\n\n\nRemote debugging should only be enabled for as long as you need assistance.\nAs soon as your issue has been resolved, revert any changes you have made to \n/etc/xrootd/digauth.cfg\n.\n\n\nEnable HTTPS on the unauthenticated cache\n\n\nBy default, the unauthenticated stash-cache instance uses plain HTTP, not HTTPS.\nTo use HTTPS:\n\n\n\n\n\n\nAdd a certificate according to the \ninstructions above\n\n\n\n\n\n\nCreate a file named \n/etc/xrootd/config.d/11-cache-https.cfg\n with the following contents:\n\n\n# \nSupport\n \nHTTPS\n \naccess\n \nto\n \nunauthenticated\n \ncache\n\n\nif\n \nnamed\n \nstash\n-\ncache\n\n  \nhttp\n.\ncadir\n \n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n\n  \nhttp\n.\ncert\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdcert\n.\npem\n\n  \nhttp\n.\nkey\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nxrd\n/\nxrdkey\n.\npem\n\n  \nhttp\n.\nsecxtractor\n \n/\nusr\n/\nlib64\n/\nlibXrdLcmaps\n.\nso\n\n\nfi\n\n\n\n\n\n\n\n\n\n\nManaging StashCache and associated services\n\n\nThese services must be managed by \nsystemctl\n and may start additional services as dependencies.\nAs a reminder, here are common service commands (all run as \nroot\n) for EL7:\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a service\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nPublic cache services\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nXRootD\n\n\nxrootd@stash-cache.service\n\n\nThe XRootD daemon, which performs the data transfers\n\n\n\n\n\n\nXCache\n\n\nxcache-reporter.timer\n\n\nReports usage information to collector.opensciencegrid.org\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nRequired to authenticate monitoring services.  See \nCA documentation\n for more info\n\n\n\n\n\n\n\n\nAuthenticated cache services (optional)\n\n\nIn addition to the public cache services, there are three systemd units specific to the authenticated cache.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nXRootD\n\n\nxrootd@stash-cache-auth.service\n\n\nThe xrootd daemon which performs authenticated data transfers\n\n\n\n\n\n\n\n\nxrootd-renew-proxy.service\n\n\nRenew a proxy for authenticated downloads to the cache\n\n\n\n\n\n\n\n\nxrootd-renew-proxy.timer\n\n\nTrigger daily proxy renewal\n\n\n\n\n\n\n\n\nValidating the Cache\n\n\nThe cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as \ncurl\n.\n\n\nuser@host $\n curl -O http://cache_host:8000/user/dweitzel/public/blast/queries/query1\n\n\n\n\n\ncurl\n may not correctly report a failure, so verify that the contents of the file are:\n\n\nDerek\ns\n \nfirst\n \nquery\n!\n\n\nMPVSDSGFDNSSKTMKDDTIPTEDYEEITKESEMGDATKITSKIDANVIEKKDTDSENNITIAQDDEKVSWLQRVVEFFE\n\n\n\n\n\n\nTest cache server reporting to the central collector\n\n\nTo verify the cache is reporting to the central collector, run the following command from the cache server:\n\n\nuser@host $\n condor_status -any -pool collector.opensciencegrid.org:9619 \n\\\n\n                          -l -const \nName==\\\nxrootd@`hostname`\\\n\n\n\n\n\n\nThe output of the above command should detail what the collector knows about the status of your cache.\nHere is an example snippet of the output:\n\n\nAuthenticatedIdentity\n \n=\n \nsc-cache.chtc.wisc.edu@daemon.opensciencegrid.org\n\n\nAuthenticationMethod\n \n=\n \nGSI\n\n\nfree_cache_bytes\n \n=\n \n868104454144\n\n\nfree_cache_fraction\n \n=\n \n0.8022261674321525\n\n\nLastHeardFrom\n \n=\n \n1552002482\n\n\nmost_recent_access_time\n \n=\n \n1551997049\n\n\nMyType\n \n=\n \nMachine\n\n\nName\n \n=\n \nxrootd@sc-cache.chtc.wisc.edu\n\n\nping_elapsed_time\n \n=\n \n0.00763392448425293\n\n\nping_response_code\n \n=\n \n0\n\n\nping_response_message\n \n=\n \n[SUCCESS] \n\n\nping_response_status\n \n=\n \nok\n\n\nSTASHCACHE_DaemonVersion\n \n=\n \n1.0.0\n\n\n...\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n or contact \n directly.", 
            "title": "Install StashCache Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#installing-the-stashcache-cache", 
            "text": "This document describes how to install a StashCache cache service.  This service allows a site or regional\nnetwork to cache data frequently used on the OSG, reducing data transfer over the wide-area network and\ndecreasing access latency.", 
            "title": "Installing the StashCache Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#before-starting", 
            "text": "Before starting the installation process, consider the following requirements:   Operating system:  A RHEL 7 or compatible operating systems.  User IDs:  If they do not exist already, the installation will create the Linux user IDs  condor  and\n   xrootd  Host certificate:  Required for reporting and authenticated StashCache.\n  Authenticated StashCache is an optional feature.\n  See our  documentation  for instructions on how to request and install host certificates.  Network ports:  The cache service requires the following ports open:  Inbound TCP port 1094 for file access via the XRootD protocol  Inbound TCP port 8000 for file access via HTTP  Inbound TCP port 8443 for authenticated file access via HTTPS (optional)  Outbound UDP port 9930 for reporting to  xrd-report.osgstorage.org  and  xrd-mon.osgstorage.org  for monitoring    Hardware requirements:  We recommend that a cache has at least 10Gbps connectivity, 1TB of\n disk space for the cache directory, and 8GB of RAM.   As with all OSG software installations, there are some one-time steps to prepare in advance:   Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/stashcache/install-cache/#registering-the-cache", 
            "text": "To be part of the OSG StashCache Federation, your cache must be registered with the OSG.\nYou will need basic information like the resource name and hostname,\nand the administrative and security contacts.", 
            "title": "Registering the Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#initial-registration", 
            "text": "To register your cache host, follow the general registration instructions  here .\nThe service type is  XRootD cache server .   Info  This step must be completed before installation.   In your registration, you must specify which VOs your cache will serve by adding an  AllowedVOs  list, with each line specifying a VO whose data you are willing to cache.  There are special values you may use in  AllowedVOs :   ANY_PUBLIC  indicates that the cache is willing to serve public data from any VO.  ANY  indicates that the cache is willing to serve data from any VO,\n  both public and non-public.\n   ANY  implies  ANY_PUBLIC .   There are extra requirements for serving non-public data:   In addition to the cache allowing a VO in the  AllowedVOs  list,\n  that VO must also allow the cache in its  AllowedCaches  list.\n  See the page on  getting your VO's data into StashCache .  There must be an authenticated XRootD instance on the cache server.  There must be a  DN  attribute in the resource registration\n  with the  subject DN  of the host certificate   This is an example registration for a cache server that serves all public data:     MY_STASHCACHE_CACHE : \n     FQDN :   my-cache.example.net \n     Service :   XRootD cache server \n       Description :   StashCache cache server \n     AllowedVOs : \n       -   ANY_PUBLIC   This is an example registration for a cache server that only serves authenticated data from the OSG VO:     MY_AUTH_STASHCACHE_CACHE : \n     FQDN :   my-auth-cache.example.net \n     Service :   XRootD cache server \n       Description :   StashCache cache server \n     AllowedVOs : \n       -   OSG \n     DN :   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-auth-cache.example.net   This is an example registration for a cache server that serves all public data  and  authenticated data from the OSG VO:     MY_COMBO_STASHCACHE_CACHE : \n     FQDN :   my-combo-cache.example.net \n     Service :   XRootD cache server \n       Description :   StashCache cache server \n     AllowedVOs : \n       -   OSG \n       -   ANY_PUBLIC \n     DN :   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=my-combo-cache.example.net", 
            "title": "Initial registration"
        }, 
        {
            "location": "/data/stashcache/install-cache/#finalizing-registration", 
            "text": "Once initial registration is complete, you may start the installation process.\nIn the meantime, open a  help ticket  with your cache name.\nMention in your ticket that you would like to \"Finalize the cache registration.\"", 
            "title": "Finalizing registration"
        }, 
        {
            "location": "/data/stashcache/install-cache/#installing-the-cache", 
            "text": "The StashCache software consists of an XRootD server with special configuration and supporting services.\nTo simplify installation, OSG provides convenience RPMs that install all required\npackages with a single command:  root@host #  yum install stash-cache", 
            "title": "Installing the Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#configuring-the-cache", 
            "text": "First, you must create a \"cache directory\", which will be used to store downloaded files.\nBy default this is  /mnt/stash .\nWe recommend using a separate file system for the cache directory,\nwith at least 1 TB of storage available.   Note  The cache directory must be writable by the  xrootd:xrootd  user and group.   The  stash-cache  package provides default configuration files in  /etc/xrootd/xrootd-stash-cache.cfg  and  /etc/xrootd/config.d/ .\nAdministrators may provide additional configuration by placing files in  /etc/xrootd/config.d/1*.cfg  (for files that need to be processed BEFORE the OSG configuration) or  /etc/xrootd/config.d/9*.cfg  (for files that need to be processed AFTER the OSG configuration).  You  must  configure every variable in  /etc/xrootd/10-common-site-local.cfg .  The mandatory variables to configure are:   set rootdir = /mnt/stash : the mounted filesystem path to export.  This document refers to this as  /mnt/stash .  set resourcename = YOUR_RESOURCE_NAME : the resource name registered with the OSG.", 
            "title": "Configuring the Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#ensure-the-xrootd-service-has-a-certificate", 
            "text": "The service will need a certificate for reporting and to authenticate to StashCache origins.\nThe easiest solution for this is to use your host certificate and key as follows:   Copy the host certificate to  /etc/grid-security/xrd/xrd{cert,key}.pem  Set the owner of the directory and contents  /etc/grid-security/xrd/  to  xrootd:xrootd : root@host #  chown -R xrootd:xrootd /etc/grid-security/xrd/     Note  You must repeat the above steps whenever you renew your host certificate.\nIf you automate certificate renewal, you should automate copying as well.\nFor example, if you are using Certbot for Let's Encrypt, you should write a \"deploy hook\" as documented on the Certbot site .", 
            "title": "Ensure the xrootd service has a certificate"
        }, 
        {
            "location": "/data/stashcache/install-cache/#configuring-optional-features", 
            "text": "", 
            "title": "Configuring Optional Features"
        }, 
        {
            "location": "/data/stashcache/install-cache/#adjust-disk-utilization", 
            "text": "To adjust the disk utilization of your cache, create or edit a file named  /etc/xrootd/config.d/90-local.cfg \nand set the values of  pfc.diskusage .  pfc . diskusage   0 . 90   0 . 95   The two values correspond to the low and high usage water marks, respectively.\nWhen usage goes above the high water mark,\nthe XRootD service will delete cached files until usage goes below the low water mark.", 
            "title": "Adjust disk utilization"
        }, 
        {
            "location": "/data/stashcache/install-cache/#enable-remote-debugging", 
            "text": "XRootD provides remote debugging via a read-only file system named digFS.\nThis feature is disabled by default, but you may enable it if you need help troubleshooting your server.  To enable remote debugging, edit  /etc/xrootd/digauth.cfg  and specify the authorizations for reading digFS.\nAn example of authorizations:  all   allow   gsi   g =/ glow   h =* . cs . wisc . edu   This gives access to the config file, log files, core files, and process information\nto anyone from  *.cs.wisc.edu  in the  /glow  VOMS group.  See  the XRootD manual  for the full syntax.  Remote debugging should only be enabled for as long as you need assistance.\nAs soon as your issue has been resolved, revert any changes you have made to  /etc/xrootd/digauth.cfg .", 
            "title": "Enable remote debugging"
        }, 
        {
            "location": "/data/stashcache/install-cache/#enable-https-on-the-unauthenticated-cache", 
            "text": "By default, the unauthenticated stash-cache instance uses plain HTTP, not HTTPS.\nTo use HTTPS:    Add a certificate according to the  instructions above    Create a file named  /etc/xrootd/config.d/11-cache-https.cfg  with the following contents:  #  Support   HTTPS   access   to   unauthenticated   cache  if   named   stash - cache \n   http . cadir   / etc / grid - security / certificates \n   http . cert   / etc / grid - security / xrd / xrdcert . pem \n   http . key   / etc / grid - security / xrd / xrdkey . pem \n   http . secxtractor   / usr / lib64 / libXrdLcmaps . so  fi", 
            "title": "Enable HTTPS on the unauthenticated cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#managing-stashcache-and-associated-services", 
            "text": "These services must be managed by  systemctl  and may start additional services as dependencies.\nAs a reminder, here are common service commands (all run as  root ) for EL7:     To...  On EL7, run the command...      Start a service  systemctl start  SERVICE-NAME    Stop a service  systemctl stop  SERVICE-NAME    Enable a service to start on boot  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  systemctl disable  SERVICE-NAME", 
            "title": "Managing StashCache and associated services"
        }, 
        {
            "location": "/data/stashcache/install-cache/#public-cache-services", 
            "text": "Software  Service name  Notes      XRootD  xrootd@stash-cache.service  The XRootD daemon, which performs the data transfers    XCache  xcache-reporter.timer  Reports usage information to collector.opensciencegrid.org    Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  Required to authenticate monitoring services.  See  CA documentation  for more info", 
            "title": "Public cache services"
        }, 
        {
            "location": "/data/stashcache/install-cache/#authenticated-cache-services-optional", 
            "text": "In addition to the public cache services, there are three systemd units specific to the authenticated cache.     Software  Service name  Notes      XRootD  xrootd@stash-cache-auth.service  The xrootd daemon which performs authenticated data transfers     xrootd-renew-proxy.service  Renew a proxy for authenticated downloads to the cache     xrootd-renew-proxy.timer  Trigger daily proxy renewal", 
            "title": "Authenticated cache services (optional)"
        }, 
        {
            "location": "/data/stashcache/install-cache/#validating-the-cache", 
            "text": "The cache server functions as a normal HTTP server and can interact with typical HTTP clients, such as  curl .  user@host $  curl -O http://cache_host:8000/user/dweitzel/public/blast/queries/query1  curl  may not correctly report a failure, so verify that the contents of the file are:  Derek s   first   query !  MPVSDSGFDNSSKTMKDDTIPTEDYEEITKESEMGDATKITSKIDANVIEKKDTDSENNITIAQDDEKVSWLQRVVEFFE", 
            "title": "Validating the Cache"
        }, 
        {
            "location": "/data/stashcache/install-cache/#test-cache-server-reporting-to-the-central-collector", 
            "text": "To verify the cache is reporting to the central collector, run the following command from the cache server:  user@host $  condor_status -any -pool collector.opensciencegrid.org:9619  \\ \n                          -l -const  Name==\\ xrootd@`hostname`\\   The output of the above command should detail what the collector knows about the status of your cache.\nHere is an example snippet of the output:  AuthenticatedIdentity   =   sc-cache.chtc.wisc.edu@daemon.opensciencegrid.org  AuthenticationMethod   =   GSI  free_cache_bytes   =   868104454144  free_cache_fraction   =   0.8022261674321525  LastHeardFrom   =   1552002482  most_recent_access_time   =   1551997049  MyType   =   Machine  Name   =   xrootd@sc-cache.chtc.wisc.edu  ping_elapsed_time   =   0.00763392448425293  ping_response_code   =   0  ping_response_message   =   [SUCCESS]   ping_response_status   =   ok  STASHCACHE_DaemonVersion   =   1.0.0  ...", 
            "title": "Test cache server reporting to the central collector"
        }, 
        {
            "location": "/data/stashcache/install-cache/#getting-help", 
            "text": "To get assistance, please use the  this page  or contact   directly.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/stashcache/install-origin/", 
            "text": "Installing the StashCache Origin\n\n\nThis document describes how to install a StashCache origin service.  This service allows an organization\nto export its data to the StashCache data federation.\n\n\n\n\nNote\n\n\nThe \norigin\n must be registered with the OSG prior to joining the data federation. You may start the\nregistration process prior to finishing the installation by \nusing this link\n \nalong with information like:\n\n\n\n\nResource name and hostname\n\n\nVO associated with this origin server (which will be used to determine the origin's namespace prefix)\n\n\nAdministrative and security contact(s)\n\n\nWho (or what) will be allowed to access the VO's data\n\n\nWhich caches will be allowed to cache the VO data\n\n\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nOperating system:\n A RHEL 7 or compatible operating system.\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux user IDs \ncondor\n and \nxrootd\n;\n  only the \nxrootd\n user is utilized for the running daemons.\n\n\nHost certificate:\n The origin service uses a host certificate to authenticate with the caches it serves.\n  The \nhost certificate documentation\n provides more information on setting up host\n  certificates.\n\n\nNetwork ports:\n The origin service requires the following ports open:\n\n\nInbound TCP port 1094 for file access via the XRootD protocol\n\n\nOutbound TCP port 1213 to \nredirector.osgstorage.org\n for connecting to the data federation\n\n\nOutbound UDP port 9930 for reporting to \nxrd-report.osgstorage.org\n and \nxrd-mon.osgstorage.org\n for monitoring.\n\n\nHardware requirements:\n We recommend that an origin has at least 1Gbps connectivity and 8GB of RAM.\n  We suggest that several gigabytes of local disk space be available for log files,\n  although some logging verbosity can be reduced.\n\n\n\n\nAs with all OSG software installations, there are some one-time steps to prepare in advance:\n\n\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling the Origin\n\n\nThe origin service consists of one or more XRootD daemons and their dependencies for the authentication infrastructure.\nTo simplify installation, OSG provides convenience RPMs that install all required\nsoftware with a single command:\n\n\nroot@host #\n yum install stash-origin\n\n\n\n\n\nFor this installation guide, we assume that the data to be exported to the federation is mounted at \n/mnt/stash\n\nand owned by the \nxrootd:xrootd\n user.\n\n\nConfiguring the Origin Server\n\n\nThe \nstash-origin\n package provides a default configuration files in\n\n/etc/xrootd/xrootd-stash-origin.cfg\n and \n/etc/xrootd/config.d\n.\nAdministrators may provide additional configuration by placing files in \n/etc/xrootd/config.d\n\nof the form \n/etc/xrootd/config.d/1*.cfg\n (for directives that need to be processed BEFORE the OSG configuration)\nor \n/etc/xrootd/config.d/9*.cfg\n (for directives that are processed AFTER the OSG configuration).\n\n\nYou \nmust\n configure every variable in \n/etc/xrootd/10-common-site-local.cfg\n and \n/etc/xrootd/10-origin-site-local.cfg\n.\n\n\nThe mandatory variables to configure are:\n\n\n\n\n\n\n\n\nFile\n\n\nConfig line\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n10-common-site-local.cfg\n\n\nset rootdir = /mnt/stash\n\n\nThe mounted filesystem path to export; this document calls it \n/mnt/stash\n\n\n\n\n\n\n10-common-site-local.cfg\n\n\nset resourcename = YOUR_RESOURCE_NAME\n\n\nThe resource name registered with OSG\n\n\n\n\n\n\n10-origin-site-local.cfg\n\n\nset originexport = /VO\n\n\nThe directory relative to \nrootdir\n that is the top of the exported namespace for the origin services\n\n\n\n\n\n\n\n\nFor example, if the HCC VO would like to set up an origin server exporting from the mount point \n/mnt/stash\n,\nand HCC's registered namespace is \n/hcc\n, then the following would be set in \n10-common-site-local.cfg\n:\n\n\nset\n \nrootdir\n \n=\n \n/\nmnt\n/\nstash\n\n\nset\n \nresourcename\n \n=\n \nHCC_STASH_ORIGIN\n\n\n\n\n\n\nAnd the following would be set in \n10-origin-site-local.cfg\n:\n\n\nset\n \noriginexport\n \n=\n \n/\nhcc\n\n\n\n\n\n\nWith this configuration, the data under \n/mnt/stash/hcc/bio/datasets\n would be available under the StashCache path\n\n/hcc/bio/datasets\n and the data under \n/mnt/stash/hcc/hep/generators\n would be available under the StashCache path\n\n/hcc/hep/generators\n.\n\n\n\n\nWarning\n\n\nIf you want to run origins for authenticated and unauthenticated data,\nyou \nmust\n run them on separate hosts.\nThis requires registering a resource for each host.\nThis requirement will be removed in a future version of StashCache.\n\n\n\n\n\n\nWarning\n\n\nThe StashCache namespace is \nglobal\n within a data federation.\nDirectories you export \nmust not\n collide with directories provided by other origin servers; this is\nwhy the explicit registration is required.\n\n\n\n\nManaging the Origin Service\n\n\nThe origin service consists of the following SystemD units that you must directly manage:\n\n\n\n\n\n\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nxrootd@stash-origin.service\n\n\nPerforms data transfers (unauthenticated instance)\n\n\n\n\n\n\nxrootd@stash-origin-auth.service\n\n\nPerforms data transfers (authenticated instance)\n\n\n\n\n\n\n\n\nThese services must be managed with \nsystemctl\n and may start additional services as dependencies.\nAs a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a service\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nIn addition, the origin service automatically uses the following SystemD units:\n\n\n\n\n\n\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\ncmsd@stash-origin.service\n\n\nIntegrates the origin into the data federation (unauthenticated instance)\n\n\n\n\n\n\ncmsd@stash-origin-auth.service\n\n\nIntegrates the origin into the data federation (authenticated instance)\n\n\n\n\n\n\nstash-origin-authfile.timer\n\n\nUpdates the authorization files periodically\n\n\n\n\n\n\nxcache-reporter.timer\n\n\nReports usage stats periodically\n\n\n\n\n\n\nxrootd-renew-proxy.timer\n\n\nRenews grid proxy periodically\n\n\n\n\n\n\n\n\nVerifying the Origin Server\n\n\nOnce your server has been registered with the OSG and started,\nperform the following steps to verify that it is functional.\n\n\nTesting availability\n\n\nTo verify that your origin is correctly advertising its availability, run the following command from the origin server:\n\n\n[user@server ~]$\n xrdmapc -r --list s redirector.osgstorage.org:1094\n\n0**** redirector.osgstorage.org:1094\n\n\n      Srv ceph-gridftp1.grid.uchicago.edu:1094\n\n\n      Srv stashcache.fnal.gov:1094\n\n\n      Srv stash.osgconnect.net:1094\n\n\n      Srv origin.ligo.caltech.edu:1094\n\n\n      Srv csiu.grid.iu.edu:1094\n\n\n\n\n\n\nThe output should list the hostname of your origin server.\n\n\nTesting directory export\n\n\nTo verify that the directories you are exporting are visible from the redirector,\nrun the following command from the origin server:\n\n\n[user@server ~]$\n xrdmapc -r --verify --list s redirector.osgstorage.org:1094 \nexported dir\n\n\n0*rv* redirector.osgstorage.org:1094\n\n\n  \n+  Srv ceph-gridftp1.grid.uchicago.edu:1094\n\n\n   ?  Srv stashcache.fnal.gov:1094 [not authorized]\n\n\n  \n+  Srv stash.osgconnect.net:1094\n\n\n   -  Srv origin.ligo.caltech.edu:1094\n\n\n   ?  Srv csiu.grid.iu.edu:1094 [connect error]\n\n\n\n\n\n\nYour server should be marked with a \n+\n to indicate that it contains the given path and the path was accessible.\n\n\nTesting file access\n\n\nTo verify that you can download a file from the origin server, use the \nstashcp\n tool.\nPlace a test file in the exported dir.\n\nstashcp\n is available in the \nstashcache-client\n RPM.\nRun the following command:\n\n\n[user@host]$\n stashcp \ntest\u00a0file\n /tmp/testfile\n\n\n\n\n\n\n\n\nIf successful, there should be a file at \n/tmp/testfile\n with the contents of the test file on your origin server.\nIf unsuccessful, you can pass the \n-d\n flag to \nstashcp\n for debug info.\n\n\nYou can also test directly downloading from the origin via \nxrdcp\n, which is available in the \nxrootd-client\n RPM.\nRun the following command:\n\n\n[user@host]$\n xrdcp xroot://\norigin server\n:1094/\ntestfile\n /tmp/testfile\n\n\n\n\n\nRegistering the Origin\n\n\nTo be part of the OSG StashCache Federation, your origin must be\n\nregistered with the OSG\n.  The service type is \nXRootD origin server\n.\n\n\nThe resource must also specify which VOs it will serve data from.\nTo do this, add an \nAllowedVOs\n list, with each line specifying a VO whose StashCache data the resource is willing to host.\nFor example:\n\n\n  \nMY_STASHCACHE_ORIGIN\n:\n\n    \nService\n:\n \nXRootD origin server\n\n      \nDescription\n:\n \nStashCache origin server\n\n    \nAllowedVOs\n:\n\n      \n-\n \nGLOW\n\n      \n-\n \nOSG\n\n\n\n\n\n\nYou can use the special value \nANY\n to indicate that the origin will serve data from any VO that puts data on it.\n\n\nIn addition to the origin allowing a VOs via the \nAllowedVOs\n list,\nthat VO must also allow the origin in its \nDataFederations/StashCache/AllowedOrigins\n list.\nSee the page on \ngetting your VO's data into StashCache\n.\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n or contact \n directly.", 
            "title": "Install StashCache Origin"
        }, 
        {
            "location": "/data/stashcache/install-origin/#installing-the-stashcache-origin", 
            "text": "This document describes how to install a StashCache origin service.  This service allows an organization\nto export its data to the StashCache data federation.   Note  The  origin  must be registered with the OSG prior to joining the data federation. You may start the\nregistration process prior to finishing the installation by  using this link  \nalong with information like:   Resource name and hostname  VO associated with this origin server (which will be used to determine the origin's namespace prefix)  Administrative and security contact(s)  Who (or what) will be allowed to access the VO's data  Which caches will be allowed to cache the VO data", 
            "title": "Installing the StashCache Origin"
        }, 
        {
            "location": "/data/stashcache/install-origin/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   Operating system:  A RHEL 7 or compatible operating system.  User IDs:  If they do not exist already, the installation will create the Linux user IDs  condor  and  xrootd ;\n  only the  xrootd  user is utilized for the running daemons.  Host certificate:  The origin service uses a host certificate to authenticate with the caches it serves.\n  The  host certificate documentation  provides more information on setting up host\n  certificates.  Network ports:  The origin service requires the following ports open:  Inbound TCP port 1094 for file access via the XRootD protocol  Outbound TCP port 1213 to  redirector.osgstorage.org  for connecting to the data federation  Outbound UDP port 9930 for reporting to  xrd-report.osgstorage.org  and  xrd-mon.osgstorage.org  for monitoring.  Hardware requirements:  We recommend that an origin has at least 1Gbps connectivity and 8GB of RAM.\n  We suggest that several gigabytes of local disk space be available for log files,\n  although some logging verbosity can be reduced.   As with all OSG software installations, there are some one-time steps to prepare in advance:   Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/stashcache/install-origin/#installing-the-origin", 
            "text": "The origin service consists of one or more XRootD daemons and their dependencies for the authentication infrastructure.\nTo simplify installation, OSG provides convenience RPMs that install all required\nsoftware with a single command:  root@host #  yum install stash-origin  For this installation guide, we assume that the data to be exported to the federation is mounted at  /mnt/stash \nand owned by the  xrootd:xrootd  user.", 
            "title": "Installing the Origin"
        }, 
        {
            "location": "/data/stashcache/install-origin/#configuring-the-origin-server", 
            "text": "The  stash-origin  package provides a default configuration files in /etc/xrootd/xrootd-stash-origin.cfg  and  /etc/xrootd/config.d .\nAdministrators may provide additional configuration by placing files in  /etc/xrootd/config.d \nof the form  /etc/xrootd/config.d/1*.cfg  (for directives that need to be processed BEFORE the OSG configuration)\nor  /etc/xrootd/config.d/9*.cfg  (for directives that are processed AFTER the OSG configuration).  You  must  configure every variable in  /etc/xrootd/10-common-site-local.cfg  and  /etc/xrootd/10-origin-site-local.cfg .  The mandatory variables to configure are:     File  Config line  Description      10-common-site-local.cfg  set rootdir = /mnt/stash  The mounted filesystem path to export; this document calls it  /mnt/stash    10-common-site-local.cfg  set resourcename = YOUR_RESOURCE_NAME  The resource name registered with OSG    10-origin-site-local.cfg  set originexport = /VO  The directory relative to  rootdir  that is the top of the exported namespace for the origin services     For example, if the HCC VO would like to set up an origin server exporting from the mount point  /mnt/stash ,\nand HCC's registered namespace is  /hcc , then the following would be set in  10-common-site-local.cfg :  set   rootdir   =   / mnt / stash  set   resourcename   =   HCC_STASH_ORIGIN   And the following would be set in  10-origin-site-local.cfg :  set   originexport   =   / hcc   With this configuration, the data under  /mnt/stash/hcc/bio/datasets  would be available under the StashCache path /hcc/bio/datasets  and the data under  /mnt/stash/hcc/hep/generators  would be available under the StashCache path /hcc/hep/generators .   Warning  If you want to run origins for authenticated and unauthenticated data,\nyou  must  run them on separate hosts.\nThis requires registering a resource for each host.\nThis requirement will be removed in a future version of StashCache.    Warning  The StashCache namespace is  global  within a data federation.\nDirectories you export  must not  collide with directories provided by other origin servers; this is\nwhy the explicit registration is required.", 
            "title": "Configuring the Origin Server"
        }, 
        {
            "location": "/data/stashcache/install-origin/#managing-the-origin-service", 
            "text": "The origin service consists of the following SystemD units that you must directly manage:     Service name  Notes      xrootd@stash-origin.service  Performs data transfers (unauthenticated instance)    xrootd@stash-origin-auth.service  Performs data transfers (authenticated instance)     These services must be managed with  systemctl  and may start additional services as dependencies.\nAs a reminder, here are common service commands (all run as  root ):     To...  On EL7, run the command...      Start a service  systemctl start  SERVICE-NAME    Stop a service  systemctl stop  SERVICE-NAME    Enable a service to start on boot  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  systemctl disable  SERVICE-NAME     In addition, the origin service automatically uses the following SystemD units:     Service name  Notes      cmsd@stash-origin.service  Integrates the origin into the data federation (unauthenticated instance)    cmsd@stash-origin-auth.service  Integrates the origin into the data federation (authenticated instance)    stash-origin-authfile.timer  Updates the authorization files periodically    xcache-reporter.timer  Reports usage stats periodically    xrootd-renew-proxy.timer  Renews grid proxy periodically", 
            "title": "Managing the Origin Service"
        }, 
        {
            "location": "/data/stashcache/install-origin/#verifying-the-origin-server", 
            "text": "Once your server has been registered with the OSG and started,\nperform the following steps to verify that it is functional.", 
            "title": "Verifying the Origin Server"
        }, 
        {
            "location": "/data/stashcache/install-origin/#testing-availability", 
            "text": "To verify that your origin is correctly advertising its availability, run the following command from the origin server:  [user@server ~]$  xrdmapc -r --list s redirector.osgstorage.org:1094 0**** redirector.osgstorage.org:1094        Srv ceph-gridftp1.grid.uchicago.edu:1094        Srv stashcache.fnal.gov:1094        Srv stash.osgconnect.net:1094        Srv origin.ligo.caltech.edu:1094        Srv csiu.grid.iu.edu:1094   The output should list the hostname of your origin server.", 
            "title": "Testing availability"
        }, 
        {
            "location": "/data/stashcache/install-origin/#testing-directory-export", 
            "text": "To verify that the directories you are exporting are visible from the redirector,\nrun the following command from the origin server:  [user@server ~]$  xrdmapc -r --verify --list s redirector.osgstorage.org:1094  exported dir  0*rv* redirector.osgstorage.org:1094     +  Srv ceph-gridftp1.grid.uchicago.edu:1094     ?  Srv stashcache.fnal.gov:1094 [not authorized]     +  Srv stash.osgconnect.net:1094     -  Srv origin.ligo.caltech.edu:1094     ?  Srv csiu.grid.iu.edu:1094 [connect error]   Your server should be marked with a  +  to indicate that it contains the given path and the path was accessible.", 
            "title": "Testing directory export"
        }, 
        {
            "location": "/data/stashcache/install-origin/#testing-file-access", 
            "text": "To verify that you can download a file from the origin server, use the  stashcp  tool.\nPlace a test file in the exported dir. stashcp  is available in the  stashcache-client  RPM.\nRun the following command:  [user@host]$  stashcp  test\u00a0file  /tmp/testfile   If successful, there should be a file at  /tmp/testfile  with the contents of the test file on your origin server.\nIf unsuccessful, you can pass the  -d  flag to  stashcp  for debug info.  You can also test directly downloading from the origin via  xrdcp , which is available in the  xrootd-client  RPM.\nRun the following command:  [user@host]$  xrdcp xroot:// origin server :1094/ testfile  /tmp/testfile", 
            "title": "Testing file access"
        }, 
        {
            "location": "/data/stashcache/install-origin/#registering-the-origin", 
            "text": "To be part of the OSG StashCache Federation, your origin must be registered with the OSG .  The service type is  XRootD origin server .  The resource must also specify which VOs it will serve data from.\nTo do this, add an  AllowedVOs  list, with each line specifying a VO whose StashCache data the resource is willing to host.\nFor example:     MY_STASHCACHE_ORIGIN : \n     Service :   XRootD origin server \n       Description :   StashCache origin server \n     AllowedVOs : \n       -   GLOW \n       -   OSG   You can use the special value  ANY  to indicate that the origin will serve data from any VO that puts data on it.  In addition to the origin allowing a VOs via the  AllowedVOs  list,\nthat VO must also allow the origin in its  DataFederations/StashCache/AllowedOrigins  list.\nSee the page on  getting your VO's data into StashCache .", 
            "title": "Registering the Origin"
        }, 
        {
            "location": "/data/stashcache/install-origin/#getting-help", 
            "text": "To get assistance, please use the  this page  or contact   directly.", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/stashcache/vo-data/", 
            "text": "Getting VO Data into StashCache\n\n\nThis document describes the steps required to manage a VO's role\nin the StashCache Data Federation including selecting a namespace, registration,\nand selecting which resources are allowed to host or cache your data.\n\n\nFor general information about StashCache, see the \noverview document\n.\n\n\nSite admins should work together with VO managers in order to perform these steps.\n\n\nDefinitions\n\n\n\n\nNamespace:\n a directory tree in the federation that is used to find VO data.\n\n\nPublic data:\n data that can be read by anyone.\n\n\nProtected data:\n data that requires authentication to read.\n\n\n\n\nRequirements\n\n\nIn order for a Virtual Organization to join the StashCache Federation, the VO must already be registered in OSG Topology.\nSee the \nregistration document\n.\n\n\nChoosing Namespaces\n\n\nThe VO must pick one or more \"namespaces\" for their data.\nA namespace is a directory tree in the federation where VO data is found.\n\n\n\n\nNote\n\n\nNamespaces are global across the federation, so you must work with the StashCache Operations team\nto ensure that your VO's namespaces do not collide with those of another VO.\n\n\nSend an email to help@opensciencegrid.org with the following subject:\n\"Requesting StashCache namespaces for VO \n\"\nand put the desired namespaces in the body of the email.\n\n\n\n\nA namespace should be easy for your users to remember but not so generic that it collides with other VOs.\nWe recommend using the lowercase version of your VO as the top-level directory.\nIn addition, public data, if any, should be stored in a subdirectory named \nPUBLIC\n,\nand protected data, if any, should be stored in a subdirectory named \nPROTECTED\n.\n\n\nPutting this together, if your VO is named \nAstro\n, you should have:\n\n\n\n\n/astro/PUBLIC\n for public data\n\n\n/astro/PROTECTED\n for protected data\n\n\n\n\nSeparating the public and protected data in separate directory trees is preferred for technical reasons.\n\n\nRegistering Data Federation Information\n\n\nThe VO must allow one or more StashCache origins to host their data.\nAn origin will typically be hosted on a site owned by the VO.\nFor information about setting up an origin, see the \ninstallation document\n.\n\n\nIn order to declare your VO's role in the StashCache federation,\nyou must add StashCache information to your VO's YAML file in the OSG Topology repository.\n\n\nFor example, the full registration for the \nAstro\n VO may look something like the following:\n\n\nDataFederations\n:\n\n  \nStashCache\n:\n\n    \nNamespaces\n:\n\n      \n/astro/PUBLIC\n:\n\n        \n-\n \nPUBLIC\n\n    \nAllowedCaches\n:\n\n      \n-\n \nANY\n\n    \nAllowedOrigins\n:\n\n      \n-\n \nCHTC_STASHCACHE_ORIGIN\n\n\n\n\n\n\nThe sections are described below.\n\n\nNamespaces section\n\n\nIn the namespaces section, you will declare one or more namespaces and, for each namespace,\nlist who is allowed to access that namespace.\n\n\nThe list will contain one or more of these:\n\n\n\n\nFQAN:\nVOMS FQAN\n allows someone using a proxy with the specified VOMS FQAN\n\n\nDN:\nDN\n allows someone using a proxy with that specific DN\n\n\nPUBLIC\n allows anyone; this is used for public data\n\n\n\n\nA complete declaration looks like:\n\n\n    \nNamespaces\n:\n\n      \n/astro/PUBLIC\n:\n\n        \n-\n \nPUBLIC\n\n      \n/astro/PROTECTED\n:\n\n        \n-\n \nFQAN:/Astro\n\n        \n-\n \nDN:/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Matyas Selmeci\n\n\n\n\n\n\nThis declares two namespaces: \n/astro/PUBLIC\n for public data, and \n/astro/PROTECTED\n\nwhich can only be read by someone with the \n/Astro\n FQAN or by Matyas Selmeci.\n\n\nAllowedCaches list\n\n\nThe VO must allow one or more StashCache Caches to cache their data.\nThe more places a VO's data can be cached in, the bigger the data transfer benefit for the VO.\nThe majority of caches across OSG will automatically cache all \"public\" VO data.\nCaching \"protected\" VO data will often be done on a site owned by the VO.\nFor information about setting up a cache, see the \ninstallation document\n.\n\n\nAllowedCaches is a list of which caches are allowed to host copies of your data.\nThere are two cases:\n\n\n\n\nIf you only have public data, your AllowedCaches list can look like:\nAllowedCaches\n:\n\n    \n-\n \nANY\n\n\n\n\n\n\n\n\n\n\nThis allows any cache to host a copy of your data.\n\n\n\n\nIf you have some protected data, then AllowedCaches is a list of \nresources\n that are allowed to cache your data.\n   A resource is an entry in a \n/topology/\nFACILITY\n/\nSITE\n/\nRESOURCEGROUP\n.yaml\n file,\n   for example \"CHTC_STASHCACHE_CACHE\".\n\n\n\n\nThe following requirements must be met for the resource:\n\n\n\n\nIt must have an \"XRootD cache server\" service\n\n\nIt must have an AllowedVOs list that includes either your VO, \"ANY\", or \"ANY_PUBLIC\"\n\n\nIt must have a DN attribute with the DN of its host cert\n\n\n\n\nAllowedOrigins list\n\n\nAllowedOrigins is a list of which origins are allowed to host your data.\nThis is a list of \nresources\n.\nA resource is an entry in a \n/topology/\nFACILITY\n/\nSITE\n/\nRESOURCEGROUP\n.yaml\n file,\nfor example \"CHTC_STASHCACHE_ORIGIN\".\n\n\nThe following requirements must be met for the resource:\n\n\n\n\nIt must have an \"XRootD origin server\" service\n\n\nIt must have an AllowedVOs list that includes either your VO or \"ANY\"", 
            "title": "Getting VO data into StashCache"
        }, 
        {
            "location": "/data/stashcache/vo-data/#getting-vo-data-into-stashcache", 
            "text": "This document describes the steps required to manage a VO's role\nin the StashCache Data Federation including selecting a namespace, registration,\nand selecting which resources are allowed to host or cache your data.  For general information about StashCache, see the  overview document .  Site admins should work together with VO managers in order to perform these steps.", 
            "title": "Getting VO Data into StashCache"
        }, 
        {
            "location": "/data/stashcache/vo-data/#definitions", 
            "text": "Namespace:  a directory tree in the federation that is used to find VO data.  Public data:  data that can be read by anyone.  Protected data:  data that requires authentication to read.", 
            "title": "Definitions"
        }, 
        {
            "location": "/data/stashcache/vo-data/#requirements", 
            "text": "In order for a Virtual Organization to join the StashCache Federation, the VO must already be registered in OSG Topology.\nSee the  registration document .", 
            "title": "Requirements"
        }, 
        {
            "location": "/data/stashcache/vo-data/#choosing-namespaces", 
            "text": "The VO must pick one or more \"namespaces\" for their data.\nA namespace is a directory tree in the federation where VO data is found.   Note  Namespaces are global across the federation, so you must work with the StashCache Operations team\nto ensure that your VO's namespaces do not collide with those of another VO.  Send an email to help@opensciencegrid.org with the following subject:\n\"Requesting StashCache namespaces for VO  \"\nand put the desired namespaces in the body of the email.   A namespace should be easy for your users to remember but not so generic that it collides with other VOs.\nWe recommend using the lowercase version of your VO as the top-level directory.\nIn addition, public data, if any, should be stored in a subdirectory named  PUBLIC ,\nand protected data, if any, should be stored in a subdirectory named  PROTECTED .  Putting this together, if your VO is named  Astro , you should have:   /astro/PUBLIC  for public data  /astro/PROTECTED  for protected data   Separating the public and protected data in separate directory trees is preferred for technical reasons.", 
            "title": "Choosing Namespaces"
        }, 
        {
            "location": "/data/stashcache/vo-data/#registering-data-federation-information", 
            "text": "The VO must allow one or more StashCache origins to host their data.\nAn origin will typically be hosted on a site owned by the VO.\nFor information about setting up an origin, see the  installation document .  In order to declare your VO's role in the StashCache federation,\nyou must add StashCache information to your VO's YAML file in the OSG Topology repository.  For example, the full registration for the  Astro  VO may look something like the following:  DataFederations : \n   StashCache : \n     Namespaces : \n       /astro/PUBLIC : \n         -   PUBLIC \n     AllowedCaches : \n       -   ANY \n     AllowedOrigins : \n       -   CHTC_STASHCACHE_ORIGIN   The sections are described below.", 
            "title": "Registering Data Federation Information"
        }, 
        {
            "location": "/data/stashcache/vo-data/#namespaces-section", 
            "text": "In the namespaces section, you will declare one or more namespaces and, for each namespace,\nlist who is allowed to access that namespace.  The list will contain one or more of these:   FQAN: VOMS FQAN  allows someone using a proxy with the specified VOMS FQAN  DN: DN  allows someone using a proxy with that specific DN  PUBLIC  allows anyone; this is used for public data   A complete declaration looks like:       Namespaces : \n       /astro/PUBLIC : \n         -   PUBLIC \n       /astro/PROTECTED : \n         -   FQAN:/Astro \n         -   DN:/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=People/CN=Matyas Selmeci   This declares two namespaces:  /astro/PUBLIC  for public data, and  /astro/PROTECTED \nwhich can only be read by someone with the  /Astro  FQAN or by Matyas Selmeci.", 
            "title": "Namespaces section"
        }, 
        {
            "location": "/data/stashcache/vo-data/#allowedcaches-list", 
            "text": "The VO must allow one or more StashCache Caches to cache their data.\nThe more places a VO's data can be cached in, the bigger the data transfer benefit for the VO.\nThe majority of caches across OSG will automatically cache all \"public\" VO data.\nCaching \"protected\" VO data will often be done on a site owned by the VO.\nFor information about setting up a cache, see the  installation document .  AllowedCaches is a list of which caches are allowed to host copies of your data.\nThere are two cases:   If you only have public data, your AllowedCaches list can look like: AllowedCaches : \n     -   ANY     This allows any cache to host a copy of your data.   If you have some protected data, then AllowedCaches is a list of  resources  that are allowed to cache your data.\n   A resource is an entry in a  /topology/ FACILITY / SITE / RESOURCEGROUP .yaml  file,\n   for example \"CHTC_STASHCACHE_CACHE\".   The following requirements must be met for the resource:   It must have an \"XRootD cache server\" service  It must have an AllowedVOs list that includes either your VO, \"ANY\", or \"ANY_PUBLIC\"  It must have a DN attribute with the DN of its host cert", 
            "title": "AllowedCaches list"
        }, 
        {
            "location": "/data/stashcache/vo-data/#allowedorigins-list", 
            "text": "AllowedOrigins is a list of which origins are allowed to host your data.\nThis is a list of  resources .\nA resource is an entry in a  /topology/ FACILITY / SITE / RESOURCEGROUP .yaml  file,\nfor example \"CHTC_STASHCACHE_ORIGIN\".  The following requirements must be met for the resource:   It must have an \"XRootD origin server\" service  It must have an AllowedVOs list that includes either your VO or \"ANY\"", 
            "title": "AllowedOrigins list"
        }, 
        {
            "location": "/data/hadoop-overview/", 
            "text": "Hadoop Overview\n\n\nHadoop Introduction\n\n\nHadoop is a data processing framework.\nThe framework has two main parts - job scheduling and a distributed file system,\nthe Hadoop Distributed File System (HDFS).  \n\n\nWe currently utilize HDFS as a general-purpose file system. For this document,\nwe'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know\nthe distinction.\n\n\nThe HDFS file system has several features, some of which differ a bit from a typical\nfile system:\n\n\n\n\nEach file is broken up into 64 MB or 128 MB chunks (user configurable)\n\n\nThese chunks are stored on data nodes and served up from there; \n\n\nThe central namenode manages block locations, the namespace information, and block placement policies. \n\n\n\n\n\n\nHDFS provides a subset of POSIX semantics:\n\n\nRandom-access reads and non-random-access writes are fully supported. \n\n\nFsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs.\n\n\nRewriting closed files is not supported\n\n\n\n\n\n\n\n\nHadoop SE Components\n\n\nWe broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS\nauxiliary. \nThe components in each of these categories are outlined below:\n\n\n\n\nHDFS Core:\n\n\nNamenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer.\n\n\nDatanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible.\n\n\n\n\n\n\nGrid extensions\n\n\nGlobus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly.\n\n\nGratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP or XRootD server.  The probe parses the GridFTP or XRootD logs and generates transfer records.\n\n\nXRootD server plugin: XRootD is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for XRootD; integrating with XRootD provides a means to export HDFS securely outside the local cluster, as another XRootD plugin provides GSI-based authentication and authorization.\n\n\n\n\n\n\nHDFS auxiliary:\n\n\n\"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode.\n\n\nHadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster.\n\n\n\n\n\n\n\n\nIn addition to the server components, there are two client components:\n\n\n\n\nFUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls.\n\n\nHadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS.\n\n\n\n\n\n\n\n\nNamenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU.\n\n\nAs this is a single point of failure, the \nmost important\n requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives.\n\n\n\n\n\n\nSecondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability.\n\n\nDatanode: Each datanode should plan to dedicate about 1-1.5 GB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU.\n\n\n\n\nSizing Your Cluster\n\n\nThe minimal installation would involve 5 nodes:\n\n\n\n\nhadoop-name: The namenode for the Hadoop system.  \n\n\nhadoop-name2: This will run the HDFS secondary namenode. \n\n\nhadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds.\n\n\nhadoop-grid: Runs the Globus GridFTP server. \n\n\n\n\nIf desired, hadoop-name and hadoop-name2 may be virtualized. \nPrior to installation, DNS / host name resolution \nmust\n work. \nThat is, you should be able to resolve all the Hadoop servers either through DNS or /etc/hosts. \nBecause of the grid software, hadoop-grid \nmust\n have reverse DNS working.  \n\n\nLarger clusters have the same basic components but with  more HDFS datanodes and gridftp servers. \nAdding HDFS datanodes increases the capacity and  number of IOPS the cluster can provide.\nAdditional GridFTP servers will increase the data transfer rates to locations outside your data center.\nAs your cluster increases in size, virtualized namenodes may need to be moved to physical hardware.\n\n\nHadoop Security\n\n\nHDFS has Unix-like user/group authorization, but no strict authentication. \n\nHDFS should use a secure internal network which only non-malicious users are able to access\n. \nFor users with access to the local cluster, it is not difficult to bypass authentication.\n\n\nThe default ports are listed here\n.\n\n\nThere are some ways to improve security of your cluster:\n\n\n\n\nKeep the namenode behind a firewall. One possibility is to run Hadoop entirely on the private subnet of a cluster.\n\n\nUse firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000).\n\n\nFor clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop.\n\n\nThis is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch.\n\n\n\n\n\n\nThere exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones).\n\n\n\n\nThere are three options to export your data outside your cluster:\n\n\n\n\nGlobus GridFTP.\n\n\nXRootD.\n\n\nHTTP and HTTPS.  OSG utilizes the HTTP(S) protocol implementation built into the XRootD server.\n\n\n\n\nReferences\n\n\n\n\nHadoop Architecture", 
            "title": "HDFS Overview"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-overview", 
            "text": "", 
            "title": "Hadoop Overview"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-introduction", 
            "text": "Hadoop is a data processing framework.\nThe framework has two main parts - job scheduling and a distributed file system,\nthe Hadoop Distributed File System (HDFS).    We currently utilize HDFS as a general-purpose file system. For this document,\nwe'll use the words \"Hadoop\" and \"HDFS\" interchangeably, but it's nice to know\nthe distinction.  The HDFS file system has several features, some of which differ a bit from a typical\nfile system:   Each file is broken up into 64 MB or 128 MB chunks (user configurable)  These chunks are stored on data nodes and served up from there;   The central namenode manages block locations, the namespace information, and block placement policies.     HDFS provides a subset of POSIX semantics:  Random-access reads and non-random-access writes are fully supported.   Fsync and appends (after the file has been initially closed) are experimental and not available to OSG-based installs.  Rewriting closed files is not supported", 
            "title": "Hadoop Introduction"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-se-components", 
            "text": "We broadly break down the server components of the Hadoop SE into three categories: HDFS core, Grid extensions, and HDFS\nauxiliary. \nThe components in each of these categories are outlined below:   HDFS Core:  Namenode: The core metadata server of Hadoop. This is the most critical piece of the system, and there can only be one of these. This stores both the file system image and the file system journal. The namenode keeps all of the filesystem layout information (files, blocks, directories, permissions, etc) and the block locations. The filesystem layout is persisted on disk and the block locations are kept solely in memory. When a client opens a file, the namenode tells the client the locations of all the blocks in the file; the client then no longer needs to communicate with the namenode for data transfer.  Datanode: This node stores copies of the blocks in HDFS. They communicate with the namenode to perform \"housekeeping\" such as creating new replicas, transferring blocks between datanodes, and deleting excess blocks. They also communicate with the clients to transfer data. To reach the best scalability, there should be as many datanodes as possible.    Grid extensions  Globus GridFTP: The standard GridFTP from Globus. We use a plug-in module (using the Globus Direct Storage Interface) that allows the GridFTP process to use the HDFS C-bindings directly.  Gratia probe: Gratia is an accounting system that records batch system and transfer records to a database. The records are collected by a client program called a \"probe\" which runs on the GridFTP or XRootD server.  The probe parses the GridFTP or XRootD logs and generates transfer records.  XRootD server plugin: XRootD is an extremely flexible and powerful data server popular in the high energy physics community. There exists a HDFS plugin for XRootD; integrating with XRootD provides a means to export HDFS securely outside the local cluster, as another XRootD plugin provides GSI-based authentication and authorization.    HDFS auxiliary:  \"Secondary Namenode\": Perhaps more aptly called a \"checkpoint server\". This server downloads the file system image and journal from the namenode, merges the two together, and uploads the new file system image up to the namenode. This is done on a different server in order to reduce the memory footprint of the namenode.  Hadoop Balancer: This is a script (unlike the others, which are daemons) that runs on the namenode. It requests transfers of random blocks between the datanodes. This works until all datanodes have approximately the same percentage of free space. Well-balanced datanodes are necessary for having a healthy cluster.     In addition to the server components, there are two client components:   FUSE: This allows HDFS to be mounted as a filesystem on the worker nodes. FUSE is a Linux kernel module that allows kernel I/O calls to be translated into a call to a userspace program. In this case, a program called fuse_dfs translates the POSIX calls into HDFS C-binding calls.  Hadoop Command Line Client: This command line client exposes a lot of the Unix-like calls without mounting FUSE, plus access to the non-POSIX calls (such as setting quotas and file replication levels). For example, \"hadoop fs -ls /\" is equivalent to \"ls /mnt/hadoop\" if /mnt/hadoop is the mount point of HDFS.     Namenode: We recommend at least 8GB of RAM (minimum is 2GB RAM), preferably 16GB or more. A rough rule of thumb is 1GB per 100TB of raw disk space; the actual requirements is around 1GB per million objects (files, directories, and blocks). The CPU requirements are any modern multi-core server CPU. Typically, the namenode will only use 2-5% of your CPU.  As this is a single point of failure, the  most important  requirement is reliable hardware rather than high performance hardware. We suggest a node with redundant power supplies and at least 2 hard drives.    Secondary namenode: This node needs the same amount of RAM as the namenode for merging namespaces. It does not need to be high performance or high reliability.  Datanode: Each datanode should plan to dedicate about 1-1.5 GB of RAM to HDFS. A general rule of thumb is to dedicate 1 CPU to HDFS per 5TB of disk capacity under heavily load; clusters with moderate load (i.e., mostly sequential workflows) will need less. At idle, HDFS will consume almost no CPU.", 
            "title": "Hadoop SE Components"
        }, 
        {
            "location": "/data/hadoop-overview/#sizing-your-cluster", 
            "text": "The minimal installation would involve 5 nodes:   hadoop-name: The namenode for the Hadoop system.    hadoop-name2: This will run the HDFS secondary namenode.   hadoop-data1, hadoop-data2: Two HDFS datanodes. They will hold data for the system, so they should have sizable hard drives. As the Hadoop installation grows to many terabytes, this will be the only class of nodes one adds.  hadoop-grid: Runs the Globus GridFTP server.    If desired, hadoop-name and hadoop-name2 may be virtualized. \nPrior to installation, DNS / host name resolution  must  work. \nThat is, you should be able to resolve all the Hadoop servers either through DNS or /etc/hosts. \nBecause of the grid software, hadoop-grid  must  have reverse DNS working.    Larger clusters have the same basic components but with  more HDFS datanodes and gridftp servers. \nAdding HDFS datanodes increases the capacity and  number of IOPS the cluster can provide.\nAdditional GridFTP servers will increase the data transfer rates to locations outside your data center.\nAs your cluster increases in size, virtualized namenodes may need to be moved to physical hardware.", 
            "title": "Sizing Your Cluster"
        }, 
        {
            "location": "/data/hadoop-overview/#hadoop-security", 
            "text": "HDFS has Unix-like user/group authorization, but no strict authentication.  HDFS should use a secure internal network which only non-malicious users are able to access . \nFor users with access to the local cluster, it is not difficult to bypass authentication.  The default ports are listed here .  There are some ways to improve security of your cluster:   Keep the namenode behind a firewall. One possibility is to run Hadoop entirely on the private subnet of a cluster.  Use firewalls to protect the HDFS ports (default for the datanode is 50010 and 50075; for the namenode, 50070 and 9000).  For clusters utilizing FUSE, one can block outgoing connections to the HDFS ports except for user root. This means that only root-owned processes (such as FUSE-DFS) will be able to access Hadoop.  This is sufficient for grid environments, but does not protect one in the case where the attacker has physical access to the network switch.    There exists another option, currently untested. It is possible to limit all HDFS socket connections to SSL-based sockets. Using this to only allow known hosts to connect to HDFS and only allowing FUSE-DFS to connect on those known hosts, one might be able to satisfy even fairly stringent security folks (but not paranoid ones).   There are three options to export your data outside your cluster:   Globus GridFTP.  XRootD.  HTTP and HTTPS.  OSG utilizes the HTTP(S) protocol implementation built into the XRootD server.", 
            "title": "Hadoop Security"
        }, 
        {
            "location": "/data/hadoop-overview/#references", 
            "text": "Hadoop Architecture", 
            "title": "References"
        }, 
        {
            "location": "/data/install-hadoop/", 
            "text": "Installing and Maintaining HDFS\n\n\nHadoop Distributed File System\n (HDFS) is a scalable, reliable distributed file system developed in the Apache project. It is based on the map-reduce framework and design of the Google file system. The OSG distribution of Hadoop includes all components needed to operate a multi-terabyte storage site.\n\n\nThe purpose of this document is to provide Hadoop-based Storage Element administrators the information on how to prepare,\ninstall and validate OSG storage based on the Hadoop Distributed File System (HDFS).\nThe OSG supports a patched version HDFS from Cloudera's CDH5 distribution of HDFS\n(\nhttps://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html\n).\n\n\n\n\nNote\n\n\nThe OSG only supports HDFS on EL7 hosts\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \nhdfs\n and \nzookeeper\n on all nodes\n    as well as \nhadoop\n and \nmapred\n on the NameNodes\n\n\nFirewall:\n In the OSG, HDFS is intended to run as an internal service without any direct, external access to any of the nodes.\n    For more information on the ports used for communication between the various HDFS nodes, see the \n    \nCloudera documentation\n.\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nDesigning Your HDFS Cluster\n\n\nThere are several important components to an HDFS installation:\n\n\n\n\nNameNode\n: The NameNode functions as the directory server and coordinator of the HDFS cluster.\n    It houses all the meta-data for the hadoop cluster.\n\n\nSecondary NameNode (optional)\n: This is a secondary machine that periodically merges updates to the HDFS file\n    system back into the \nfsimage\n.\n    It must share a directory with the primary NameNode to exchange filesystem checkpoints.\n    An HDFS installation with a Secondary NameNode dramatically improves startup and restart times.\n\n\nDataNode\n: You will have many DataNodes. Each DataNode stores large blocks of files to for the hadoop cluster.\n\n\nClient\n: This is a documentation shorthand that refers to any machine with the hadoop client commands or\n    \nFUSE\n mount.\n\n\n\n\nInstalling HDFS\n\n\nAn OSG HDFS installation consists of HDFS and other support software (e.g., Gratia accounting).\nTo simplify installation, OSG provides convenience RPMs that install all required software.\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall the relevant packages based on the node you are installing:\n\n\n\n\n\n\n\n\nIf you are installing a(n)...\n\n\nThen run the following command...\n\n\n\n\n\n\n\n\n\n\nPrimary NameNode\n\n\nyum install osg-se-hadoop-namenode\n\n\n\n\n\n\nSecondary NameNode\n\n\nyum install osg-se-hadoop-secondarynamenode\n\n\n\n\n\n\nDataNode\n\n\nyum install osg-se-hadoop-datanode\n\n\n\n\n\n\n\n\n\n\n\n\nUpgrading HDFS\n\n\nThis section will guide you through the process to upgrade a HDFS 2.0.0 installation from OSG 3.3 to the HDFS 2.6.0\nfrom OSG 3.4.\n\n\n\n\nWarning\n\n\nThe upgrade process will involve downtime for your HDFS cluster. Please plan accordingly.\n\n\n\n\n\n\nNote\n\n\nThe OSG only offers HDFS 2.6.0 for EL7 hosts.\n\n\n\n\nThe upgrade process occurs in several steps:\n\n\n\n\nPreparing for the upgrade\n\n\nUpdating to OSG 3.4\n\n\nUpgrading the Primary NameNode\n\n\nUpgrading the DataNodes\n\n\nUpgrading the Secondary NameNode\n\n\nFinalizing the upgrade\n\n\n\n\nPreparing for the upgrade\n\n\nBefore upgrading, backup your configuration data and HDFS metadata.\n\n\n\n\n\n\nPut your Primary NameNode into safe mode:\n\n\nroot@primary-namenode #\n hdfs dfsadmin -safemode enter\n\nSafe mode is ON\n\n\n\n\n\n\n\n\n\n\nSave a clean copy of your HDFS namespace:\n\n\nroot@primary-namenode #\n hdfs dfsadmin -saveNamespace\n\nSave namespace successful\n\n\n\n\n\n\n\n\n\n\nShutdown the HDFS services on all of your HDFS nodes (see \nthis section\n for instructions).\n\n\n\n\n\n\nOn the Primary NameNode, verify that your NameNode service is off:\n\n\nroot@primary-namenode #\n /etc/init.d/hadoop-hdfs-namenode status\n\n\n\n\n\nThis command should indicate that your NameNode service is not running.\n\n\n\n\n\n\nFind the location of the directory with the HDFS metadata:\n\n\nroot@primary-namenode #\n grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml\n\n\n\n\n\nAnd look for the value of \ndfs.namenode.name.dir\n:\n\n\nproperty\n\n \nname\ndfs.namenode.name.dir\n/name\n\n  \nvalue\nfile:///var/lib/dfs/nn,file:///home/hadoop/dfs/nn\n/value\n\n\n\n\n\n\n\n\n\n\nBackup the directory that appears in the output using your backup method of choice.\n   If more than one directory appears in the list (as in the example above), choose the most convenient directory.\n   All of the directories in the list will have the same contents.\n\n\n\n\n\n\nUpdating to OSG 3.4\n\n\nOnce your HDFS services have been turned off and the HDFS metadata has been backed up, update each node to OSG 3.4 by\nfollowing the instructions in \nthis section\n.\n\n\nUpgrading the Primary NameNode\n\n\nTo upgrade your Primary NameNode, update all relevant packages then run the upgrade command.\n\n\n\n\n\n\nClear the yum cache:\n\n\nroot@primary-namenode #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate the HDFS RPMs:\n\n\nroot@primary-namenode #\n yum update osg-se-hadoop-namenode --enablerepo-osg-upcoming\n\n\n\n\n\n\n\n\n\nPerform the upgrade command:\n\n\nroot@primary-namenode #\n /etc/init.d/hadoop-hdfs-namenode upgrade\n\n\n\n\n\nThis will start the upgrade process for the HDFS metadata on your primary namenode.\nYou can follow the process by running\n\n\nroot@primary-namenode #\n tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode-\nhostname\n.log\n\n\n\n\n\n\n\n\n\nUpgrading the DataNodes\n\n\nOnce the Primary NameNode has completed its upgrade process, start the process of upgrading each of your DataNodes.\n\n\n\n\n\n\nClear the yum cache:\n\n\nroot@datanode #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate the HDFS RPMs:\n\n\nroot@datanode #\n yum update osg-se-hadoop-datanode --enablerepo-osg-upcoming\n\n\n\n\n\n\n\n\n\nStart the DataNode service:\n\n\nroot@datanode #\n /etc/init.d/hadoop-hdfs-datanode start\n\n\n\n\n\n\n\n\n\nAfter all the DataNodes have been brought back up, the Primary NameNode should exit safe mode automatically.\n   On the Primary NameNode, run the following command to verify is no longer in safe mode:\n\n\nroot@primary-namenode #\n hdfs dfsadmin -safemode get\n\nSafe mode is OFF\n\n\n\n\n\n\n\n\n\n\nUpgrading the Secondary NameNode\n\n\n\n\nNote\n\n\nThis section only applies to sites with a Secondary NameNode.\nIf you do not run a Secondary NameNode, skip to the \nnext section\n.\n\n\n\n\nOnce the Primary NameNode has exited safe mode, start the process of upgrading your Secondary NameNode.\n\n\n\n\n\n\nClear the yum cache:\n\n\nroot@secondary-namenode #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate the HDFS RPMs:\n\n\nroot@secondary-namenode #\n yum update osg-se-hadoop-secondarynamenode --enablerepo-osg-upcoming\n\n\n\n\n\n\n\n\n\nStart the Secondary NameNode service:\n\n\nroot@secondary-namenode #\n /etc/init.d/hadoop-hdfs-secondarynamenode start\n\n\n\n\n\n\n\n\n\nFinalizing the upgrade\n\n\n\n\n\n\nVerify that the HDFS cluster is running correctly by following the instructions in \nthis section\n.\n\n\n\n\n\n\nFinalize the upgrade from the Primary NameNode:\n\n\nroot@primary-namenode #\n hdfs dfsadmin -finalizeUpgrade\n\nFinalize upgrade successful\n\n\n\n\n\n\n\n\n\n\nConfiguring HDFS\n\n\n\n\nNote\n\n\nNeeded by: Hadoop NameNode, Hadoop DataNodes, Hadoop client, GridFTP\n\n\n\n\nHadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.\n\n\nHadoop configuration is stored in \n/etc/hadoop/conf\n. However, by default, these files are mostly blank. OSG provides a sample configuration in \n/etc/hadoop/conf.osg\n with most common values filled in. You will need to copy these into \n/etc/hadoop/conf\n before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify \nhdfs-site.xml\n and \ncore-site.xml\n. Review all the settings in these files, but listed below are common settings to modify:\n\n\n\n\n\n\n\n\nFile\n\n\nSetting\n\n\nExample\n\n\nComments\n\n\n\n\n\n\n\n\n\n\ncore-site.xml\n\n\nfs.default.name\n\n\nhdfs://namenode.domain.tld.:9000\n\n\nThis is the address of the NameNode\n\n\n\n\n\n\ncore-site.xml\n\n\nhadoop.tmp.dir\n\n\n/data/scratch\n\n\nScratch temp directory used by Hadoop\n\n\n\n\n\n\ncore-site.xml\n\n\nhadoop.log.dir\n\n\n/var/log/hadoop-hdfs\n\n\nLog directory used by Hadoop\n\n\n\n\n\n\ncore-site.xml\n\n\ndfs.umaskmode\n\n\n002\n\n\numask for permissions used by default\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.block.size\n\n\n134217728\n\n\nBlock size: 128MB by default\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.replication\n\n\n2\n\n\nDefault replication factor. Generally the same as dfs.replication.min/max\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.datanode.du.reserved\n\n\n100000000\n\n\nHow much free space hadoop will reserve for non-Hadoop usage\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.datanode.handler.count\n\n\n20\n\n\nNumber of server threads for DataNodes. Increase if you have many more client connections\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.namenode.handler.count\n\n\n40\n\n\nNumber of server threads for NameNodes. Increase if you need more connections\n\n\n\n\n\n\nhdfs-site.xml\n\n\ndfs.http.address\n\n\nnamenode.domain.tld.:50070\n\n\nWeb address for dfs health monitoring page\n\n\n\n\n\n\n\n\nSee \nhttp://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml\n for more parameters to configure.\n\n\n\n\nNote\n\n\nNameNodes must have a \n/etc/hosts_exclude\n present\n\n\n\n\nSpecial NameNode instructions for brand new installs\n\n\nIf this is a new installation (\nand only if this is a brand new installation\n), you should run the following command as the \nhdfs\n user. (Otherwise, be sure to \nchown\n your storage directory to hdfs after running):\n\n\nhadoop namenode -format\n\n\n\n\n\n\nThis will initialize the storage directory on your NameNode\n\n\n(optional) FUSE Client Configuration\n\n\nA FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"File system in User SpacE\") is a way to access HDFS using typical UNIX directory commands (i.e., POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.\n\n\nFUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with \nmodprobe fuse\n.\n\n\nYou can add the FUSE to be mounted at boot time by adding the following line to \n/etc/fstab\n:\n\n\nhadoop\n-\nfuse\n-\ndfs\n#\n \n%\nRED\n%/\nmnt\n/\nhadoop\n%\nENDCOLOR\n%\n \nfuse\n \nserver\n=%\nRED\n%\nnamenode\n.\nhost\n%\nENDCOLOR\n%\n,\nport\n=\n9000\n,\nrdbuffer\n=\n131072\n,\nallow_other\n \n0\n \n0\n\n\n\n\n\n\nBe sure to change the \n/mnt/hadoop\n mount point and \nnamenode.host\n to match your local configuration. To match the help documents, we recommend using \n/mnt/hadoop\n as your mountpoint.\n\n\nOnce your \n/etc/fstab\n is updated, to mount FUSE run:\n\n\nroot@host #\n mkdir /mnt/hadoop\n\nroot@host #\n mount /mnt/hadoop\n\n\n\n\n\nWhen mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:\n\n\n#\n mount /mnt/hadoop\n\nINFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop\n\n\nINFO fuse_options.c:110 Ignoring option allow_other\n\n\n\n\n\n\nIf you have troubles mounting FUSE refer to \nRunning FUSE in Debug Mode\n in the Troubleshooting section.\n\n\nCreating VO and User Areas\n\n\n\n\nNote\n\n\nGrid Users are needed by GridFTP nodes. VO areas are common to all nodes.\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.\n\n\nFor grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.\n\n\nNote that these users must be kept in sync with the authentication method.\n\n\nPrior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on \nall nodes\n. These are maintained in basic system files such as \n/etc/passwd\n and \n/etc/group\n.\n\n\n\n\nNote\n\n\nIn the examples below It is assumed a FUSE mount is set to \n/mnt/hadoop\n. As an alternative \nhadoop fs\n commands could have been used.\n\n\n\n\nFor clean HDFS operations and filesystem management:\n\n\n(a) Create top-level VO subdirectories under \n/mnt/hadoop\n.\n\n\nExample:\n\n\nroot@host #\n mkdir /mnt/hadoop/cms\n\nroot@host #\n mkdir /mnt/hadoop/dzero\n\nroot@host #\n mkdir /mnt/hadoop/sbgrid\n\nroot@host #\n mkdir /mnt/hadoop/fermigrid\n\nroot@host #\n mkdir /mnt/hadoop/cmstest\n\nroot@host #\n mkdir /mnt/hadoop/osg\n\n\n\n\n\n(b) Create individual top-level user areas, under each VO area, as needed.\n\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/michaelthomas\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/brianbockelman\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/douglasstrain\n\nroot@host #\n mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana\n\n\n\n\n\n(c) Adjust username:group ownership of each area.\n\n\nroot@host #\n chown -R cms:cms /mnt/hadoop/cms\n\nroot@host #\n chown -R sam:sam /mnt/hadoop/dzero\n\n\nroot@host #\n chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas\n\n\n\n\n\nGridFTP Configuration\n\n\ngridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop.\nBy now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as\ncreated the proper users/directories.\n\n\nThe default settings in \n/etc/gridftp.conf\n along with \n/etc/gridftp.d/gridftp-hdfs.conf\n are used by the init.d script\nand should be ok for most installations.\nThe file \n/etc/gridftp-hdfs/gridftp-debug.conf\n is used by \n/usr/bin/gridftp-hdfs-standalone\n for starting up the\nGridFTP server in a testing mode.\nAny additional config files under \n/etc/gridftp.d\n will be used for both the init.d and standalone GridFTP server.\n\n/etc/sysconfig/gridftp-hdfs\n contains additional site-specific environment variables that are used by the gridftp-hdfs\nDSI module in both the init.d and standalone GridFTP server.\nSome of the environment variables that can be used in \n/etc/sysconfig/gridftp-hdfs\n include:\n\n\n\n\n\n\n\n\nOption Name\n\n\nNeeds Editing?\n\n\nSuggested value\n\n\n\n\n\n\n\n\n\n\nGRIDFTP_HDFS_REPLICA_MAP\n\n\nNo\n\n\nFile containing a list of paths and replica values for setting the default # of replicas for specific file paths\n\n\n\n\n\n\nGRIDFTP_BUFFER_COUNT\n\n\nNo\n\n\nThe number of 1MB memory buffers used to reorder data streams before writing them to Hadoop\n\n\n\n\n\n\nGRIDFTP_FILE_BUFFER_COUNT\n\n\nNo\n\n\nThe number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop\n\n\n\n\n\n\nGRIDFTP_SYSLOG\n\n\nNo\n\n\nSet this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)\n\n\n\n\n\n\nGRIDFTP_HDFS_CHECKSUMS\n\n\nMaybe\n\n\nList of checksum calculations to perform on-the-fly (default: \n\"MD5,ADLER32,CRC32,CKSUM,CVMFS\"\n)\n\n\n\n\n\n\nGRIDFTP_HDFS_MOUNT_POINT\n\n\nMaybe\n\n\nThe location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. \nNote:\n this does not imply you need FUSE mounted on GridFTP nodes!\n\n\n\n\n\n\nGRIDFTP_LOAD_LIMIT\n\n\nNo\n\n\nGridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.\n\n\n\n\n\n\nTMPDIR\n\n\nMaybe\n\n\nThe temp directory where the file-based buffers are stored. Defaults to /tmp.\n\n\n\n\n\n\n\n\n/etc/sysconfig/gridftp-hdfs\n is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files (\nulimit -n\n).\n\n\nLastly, you will need to configure an authentication mechanism for GridFTP.\n\n\nConfiguring authentication\n\n\nFor information on how to configure authentication for your GridFTP installation, please refer to the \nconfiguring authentication section of the GridFTP guide\n.\n\n\nGridFTP Gratia Transfer Probe Configuration\n\n\n\n\nNote\n\n\nNeeded by GridFTP node only.\n\n\n\n\nSee the \nGridFTP documentation\n for configuration details.\n\n\nHadoop Storage Probe Configuration\n\n\n\n\nNote\n\n\nThis is only needed by the Hadoop NameNode\n\n\n\n\nHere are the most relevant file and directory locations:\n\n\n\n\n\n\n\n\nPurpose\n\n\nNeeds Editing?\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nProbe Configuration\n\n\nYes\n\n\n/etc/gratia/hadoop-storage/ProbeConfig\n\n\n\n\n\n\nProbe Executable\n\n\nNo\n\n\n/usr/share/gratia/hadoop-storage/hadoop_storage_probe\n\n\n\n\n\n\nLog files\n\n\nNo\n\n\n/var/log/gratia\n\n\n\n\n\n\nTemporary files\n\n\nNo\n\n\n/var/lib/gratia/tmp\n\n\n\n\n\n\n\n\nThe RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files\n\n\n/\netc\n/\ngratia\n/\nhadoop\n-\nstorage\n/\nProbeConfig\n\n\n/\netc\n/\ngratia\n/\nhadoop\n-\nstorage\n/\nstorage\n.\ncfg\n\n\n\n\n\n\nProbeConfig\n\n\nThis is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:\n\n\n\n\n\n\n\n\nAttribute\n\n\nNeeds Editing\n\n\nValue\n\n\n\n\n\n\n\n\n\n\nCollectorHost\n\n\nMaybe\n\n\nSet to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it.\n\n\n\n\n\n\nSiteName\n\n\nYes\n\n\nSet to the resource group name of your SE as registered in OIM.\n\n\n\n\n\n\nGrid\n\n\nMaybe\n\n\nSet to \"ITB\" if this is a test resource; otherwise, leave as OSG.\n\n\n\n\n\n\nEnableProbe\n\n\nYes\n\n\nSet to 1 to enable the probe.\n\n\n\n\n\n\n\n\nstorage.cfg\n\n\nThis file controls which paths in HDFS should be monitored. This is in the Windows INI format.\n\n\nNote: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location\n\n\nProbeConfig\n \n=\n \n/\netc\n/\ngratia\n/%\nRED\n%\nprobe\n/%\nENDCOLOR\n%\nhadoop\n-\nstorage\n/\nProbeConfig\n\n\n\n\n\n\nFor each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted.\n\n\nTo configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.\n\n\n[Area CMS /store]\n\n\nName\n \n=\n \nCMS /store\n\n\nPath\n \n=\n \n/user/cms/store/*\n\n\nTrim\n \n=\n \n/user/cms\n\n\n\n\n\n\nFor each such area, add a section to your configuration file.\n\n\nExample file\n\n\nBelow is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:\n\n\n[Gratia]\n\n\ngratia_location\n \n=\n \n/opt/vdt/gratia\n\n\nProbeConfig\n \n=\n \n%(gratia_location)s/probe/hadoop-storage/ProbeConfig\n\n\n\n[Area /store]\n\n\nName\n \n=\n \nCMS /store\n\n\nPath\n \n=\n \n/store/*\n\n\n\n[Area /store/user]\n\n\nName\n \n=\n \nCMS /store/user\n\n\nPath\n \n=\n \n/store/user/*\n\n\n\n[Area /user]\n\n\nName\n \n=\n \nHadoop /user\n\n\nPath\n \n=\n \n/user/*\n\n\n\n\n\n\n*\nNOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated:\n\n\ngratia_location\n \n=\n \n/\netc\n/\ngratia\n\n\nProbeConfig\n \n=\n \n%\n(\ngratia_location\n)\ns\n/\nhadoop\n-\nstorage\n/\nProbeConfig\n\n\n\n\n\n\nRunning Services\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nThe relevant service for each node is as follows:\n\n\n\n\n\n\n\n\nNode\n\n\nService\n\n\n\n\n\n\n\n\n\n\nPrimary NameNode\n\n\nhadoop-hdfs-namenode\n\n\n\n\n\n\nSecondary NameNode\n\n\nhadoop-hdfs-secondarynamenode\n\n\n\n\n\n\nDataNode\n\n\nhadoop-hdfs-datanode\n\n\n\n\n\n\nGridFTP\n\n\nglobus-gridftp-server\n\n\n\n\n\n\n\n\nValidation\n\n\nThe first thing you may want to do after installing and starting your primary NameNode is to verify that the web interface works. In your web browser go to:\n\n\nhttp\n:\n//%\nRED\n%\nnamenode\n.\nhostname\n%\nENDCOLOR\n%\n:\n50070\n/\ndfshealth\n.\njsp\n\n\n\n\n\n\nGet familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands.\n\n\n\n  \nShow detailed ouput\n\n   \n\n\nuser$ hadoop\n\n\nUsage: hadoop [--config confdir] COMMAND\n\n\nwhere COMMAND is one of:\n\n\n  namenode -format     format the DFS filesystem\n\n\n  secondarynamenode    run the DFS secondary namenode\n\n\n  namenode             run the DFS namenode\n\n\n  datanode             run a DFS datanode\n\n\n  dfsadmin             run a DFS admin client\n\n\n  mradmin              run a Map-Reduce admin client\n\n\n  fsck                 run a DFS filesystem checking utility\n\n\n  fs                   run a generic filesystem user client\n\n\n  balancer             run a cluster balancing utility\n\n\n  fetchdt              fetch a delegation token from the NameNode\n\n\n  jobtracker           run the MapReduce job Tracker node\n\n\n  pipes                run a Pipes job\n\n\n  tasktracker          run a MapReduce task Tracker node\n\n\n  job                  manipulate MapReduce jobs\n\n\n  queue                get information regarding JobQueues\n\n\n  version              print the version\n\n\n  jar \njar\n            run a jar file\n\n\n  distcp \nsrcurl\n \ndesturl\n copy file or directories recursively\n\n\n  archive -archiveName NAME -p \nparent path\n \nsrc\n* \ndest\n create a hadoop archive\n\n\n  oiv                  apply the offline fsimage viewer to an fsimage\n\n\n  classpath            prints the class path needed to get the\n\n\n                       Hadoop jar and the required libraries\n\n\n  daemonlog            get/set the log level for each daemon\n\n\n or\n\n\n  CLASSNAME            run the class named CLASSNAME\n\n\nMost commands print help when invoked w/o parameters.\n\n\n\n\n\n\n\n\n\n\nFor a list of supported filesystem commands:\n\n\n\n  \nShow 'hadoop fs' detailed ouput\n\n   \n\n\nuser$ hadoop fs\n\n\nUsage: java FsShell\n\n\n           [-ls \npath\n]\n\n\n           [-lsr \npath\n]\n\n\n           [-df [\npath\n]]\n\n\n           [-du \npath\n]\n\n\n           [-dus \npath\n]\n\n\n           [-count[-q] \npath\n]\n\n\n           [-mv \nsrc\n \ndst\n]\n\n\n           [-cp \nsrc\n \ndst\n]\n\n\n           [-rm [-skipTrash] \npath\n]\n\n\n           [-rmr [-skipTrash] \npath\n]\n\n\n           [-expunge]\n\n\n           [-put \nlocalsrc\n ... \ndst\n]\n\n\n           [-copyFromLocal \nlocalsrc\n ... \ndst\n]\n\n\n           [-moveFromLocal \nlocalsrc\n ... \ndst\n]\n\n\n           [-get [-ignoreCrc] [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-getmerge \nsrc\n \nlocaldst\n [addnl]]\n\n\n           [-cat \nsrc\n]\n\n\n           [-text \nsrc\n]\n\n\n           [-copyToLocal [-ignoreCrc] [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-moveToLocal [-crc] \nsrc\n \nlocaldst\n]\n\n\n           [-mkdir \npath\n]\n\n\n           [-setrep [-R] [-w] \nrep\n \npath/file\n]\n\n\n           [-touchz \npath\n]\n\n\n           [-test -[ezd] \npath\n]\n\n\n           [-stat [format] \npath\n]\n\n\n           [-tail [-f] \nfile\n]\n\n\n           [-chmod [-R] \nMODE[,MODE]... | OCTALMODE\n PATH...]\n\n\n           [-chown [-R] [OWNER][:[GROUP]] PATH...]\n\n\n           [-chgrp [-R] GROUP PATH...]\n\n\n           [-help [cmd]]\n\n\n\nGeneric options supported are\n\n\n-conf \nconfiguration file\n     specify an application configuration file\n\n\n-D \nproperty=value\n            use value for given property\n\n\n-fs \nlocal|namenode:port\n      specify a namenode\n\n\n-jt \nlocal|jobtracker:port\n    specify a job tracker\n\n\n-files \ncomma separated list of files\n    specify comma separated files to be copied to the map reduce cluster\n\n\n-libjars \ncomma separated list of jars\n    specify comma separated jar files to include in the classpath.\n\n\n-archives \ncomma separated list of archives\n    specify comma separated archives to be unarchived on the compute machines.\n\n\n\nThe general command line syntax is\n\n\nbin/hadoop command [genericOptions] [commandOptions]\n\n\n\n\n\n\n\n\n\n\nAn online guide is also available at \nApache Hadoop commands manual\n. You can use Hadoop commands to perform filesystem operations with more consistency.\n\n\nExample, to look into the internal hadoop namespace:\n\n\nuser$ hadoop fs -ls /\n\n\nFound 1 items\n\n\ndrwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage\n\n\n\n\n\n\nExample, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself \n/mnt/hadoop\n in Hadoop commands):\n\n\nroot@host #\n hadoop fs -chown -R engage:engage /engage\n\n\n\n\n\nExample, compare \nhadoop fs\n command vs. using FUSE mount:\n\n\nuser$ hadoop fs -ls /engage\n\n\nFound 3 items\n\n\n-rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso\n\n\n-rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz\n\n\n-rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz\n\n\n\nuser$ ls -l /mnt/hadoop/engage\n\n\ntotal 935855\n\n\n-rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso\n\n\n-rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz\n\n\n-rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz\n\n\n\n\n\n\nGridFTP Validation\n\n\n\n\nNote\n\n\nThe commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using \nvoms-proxy-init\n or \ngrid-proxy-init\n. Obtaining grid credentials is beyond the scope of this document.\n\n\n\n\nuser$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt\n\n\n\n\n\n\nIf you are having troubles running GridFTP refer to \nStarting GridFTP in Standalone Mode\n in the Troubleshooting section.\n\n\nTroubleshooting\n\n\nHadoop\n\n\nTo view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:\n\n\nhttp\n:\n//%\nRED\n%\nnamenode\n.\nhostname\n%\nENDCOLOR\n%\n:\n50070\n/\nconf\n\n\n\n\n\n\nYou will see the entire configuration in XML format, for example:\n\n\n\n  \nExpand XML configuration\n\n    \n\n\n?xml version=\n1.0\n encoding=\nUTF-8\n standalone=\nno\n?\nconfiguration\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3n.impl\n/name\nvalue\norg.apache.hadoop.fs.s3native.NativeS3FileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.cache.levels\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmap.sort.class\n/name\nvalue\norg.apache.hadoop.util.QuickSort\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nhadoop.tmp.dir\n/name\nvalue\n/data1/hadoop//scratch\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.native.lib\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.decommission.nodes.per.interval\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.need.client.auth\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.idlethreshold\n/name\nvalue\n4000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.system.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/system\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.data.dir.perm\n/name\nvalue\n755\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.hours\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.namenode.logging.level\n/name\nvalue\nall\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.address\n/name\nvalue\n0.0.0.0:50010\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.skip.checksum.errors\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.token.enable\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\nfs.default.name\n/name\nvalue\nhdfs://nagios.t2.ucsd.edu:9000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.child.tmp\n/name\nvalue\n./tmp\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.har.impl.disable.cache\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.reduce.max.skip.groups\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.threshold.pct\n/name\nvalue\n0.999f\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.heartbeats.in.second\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.namenode.handler.count\n/name\nvalue\n40\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.blockreport.initialDelay\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.instrumentation\n/name\nvalue\norg.apache.hadoop.mapred.JobTrackerMetricsInst\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.dns.nameserver\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.factor\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.timeout\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.tracker.failures\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.rpc.socket.factory.class.default\n/name\nvalue\norg.apache.hadoop.net.StandardSocketFactory\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.jobhistory.lru.cache.size\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hdfs.impl\n/name\nvalue\norg.apache.hadoop.hdfs.DistributedFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.map.auto.incr.proc.count\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.key.update.interval\n/name\nvalue\n600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.complete.cancel.delegation.tokens\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.mapfile.bloom.size\n/name\nvalue\n1048576\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.reduce.shuffle.connect.timeout\n/name\nvalue\n180000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.extension\n/name\nvalue\n30000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\ntasktracker.http.threads\n/name\nvalue\n50\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.shuffle.merge.percent\n/name\nvalue\n0.66\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.ftp.impl\n/name\nvalue\norg.apache.hadoop.fs.ftp.FTPFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compress\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nio.bytes.per.checksum\n/name\nvalue\n4096\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.healthChecker.script.timeout\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\ntopology.node.switch.mapping.impl\n/name\nvalue\norg.apache.hadoop.net.ScriptBasedMapping\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.server.keystore.resource\n/name\nvalue\nssl-server.xml\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.slowstart.completed.maps\n/name\nvalue\n0.05\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.max.attempts\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.ramfs.impl\n/name\nvalue\norg.apache.hadoop.fs.InMemoryFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.block.access.token.lifetime\n/name\nvalue\n600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.map.max.skip.records\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.name.edits.dir\n/name\nvalue\n${\ndfs\n.\nname\n.\ndir\n}\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.group.mapping\n/name\nvalue\norg.apache.hadoop.security.ShellBasedUnixGroupsMapping\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.dir\n/name\nvalue\n/jobtracker/jobsInfo\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\nhadoop.log.dir\n/name\nvalue\n/var/log/hadoop\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.buffer.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/s3\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.block.size\n/name\nvalue\n134217728\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njob.end.retry.attempts\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.file.impl\n/name\nvalue\norg.apache.hadoop.fs.LocalFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compression.type\n/name\nvalue\nRECORD\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir.minspacestart\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.ipc.address\n/name\nvalue\n0.0.0.0:50020\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.permissions\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\ntopology.script.number.args\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.mapfile.bloom.error.rate\n/name\nvalue\n0.005\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.tracker.blacklists\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile.maps\n/name\nvalue\n0-2\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.https.address\n/name\nvalue\n0.0.0.0:50475\n/value\n/property\n\n\nproperty\n!--Loaded from core-site.xml--\nname\ndfs.umaskmode\n/name\nvalue\n002\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.userlog.retain.hours\n/name\nvalue\n24\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.secondary.http.address\n/name\nvalue\ngratia-1:50090\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication.max\n/name\nvalue\n32\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.persist.jobstatus.active\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.authorization\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nlocal.cache.size\n/name\nvalue\n10737418240\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.min.split.size\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.token.renew-interval\n/name\nvalue\n86400000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.map.tasks\n/name\nvalue\n7919\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.child.java.opts\n/name\nvalue\n-Xmx200m\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.client.keystore.resource\n/name\nvalue\nssl-client.xml\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\ndfs.namenode.startup\n/name\nvalue\nREGULAR\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.queue.name\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.retiredjobs.cache.size\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.address\n/name\nvalue\n0.0.0.0:50470\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.balance.bandwidthPerSec\n/name\nvalue\n2000000000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.server.listen.queue.size\n/name\nvalue\n128\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njob.end.retry.interval\n/name\nvalue\n30000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.inmem.merge.threshold\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.attempts.to.start.skipping\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\nfs.checkpoint.dir\n/name\nvalue\n/var/hadoop/checkpoint-a\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.reduce.tasks\n/name\nvalue\n1543\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.merge.recordsBeforeProgress\n/name\nvalue\n10000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.userlog.limit.kb\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nwebinterface.private.actions\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.max.objects\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.shuffle.input.buffer.percent\n/name\nvalue\n0.70\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.spill.percent\n/name\nvalue\n0.80\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.tasks.speculative.execution\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.util.hash.type\n/name\nvalue\nmurmur\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.dns.nameserver\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.blockreport.intervalMsec\n/name\nvalue\n3600000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.max.attempts\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.acl-view-job\n/name\nvalue\n \n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.handler.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.client.block.write.retries\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.reduces.per.node\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.reduce.shuffle.read.timeout\n/name\nvalue\n180000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.expiry.interval\n/name\nvalue\n600000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.https.enable\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.maxtasks.per.job\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.job.history.block.size\n/name\nvalue\n3145728\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nkeep.failed.task.files\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.failed.volumes.tolerated\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile.reduces\n/name\nvalue\n0-2\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.tcpnodelay\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.output.compression.codec\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.map.index.skip\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.server.tcpnodelay\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.key.update-interval\n/name\nvalue\n86400000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.running.map.limit\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.progress.monitor.poll.interval\n/name\nvalue\n1000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.default.chunk.view.size\n/name\nvalue\n32768\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.logfile.size\n/name\nvalue\n10000000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.tasks.speculative.execution\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.tasktracker.outofband.heartbeat\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3n.block.size\n/name\nvalue\n67108864\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.datanode.du.reserved\n/name\nvalue\n10000000000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.security.authentication\n/name\nvalue\nsimple\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\nfs.checkpoint.period\n/name\nvalue\n3600\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.running.reduce.limit\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.reuse.jvm.num.tasks\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.web.ugi\n/name\nvalue\nwebuser,webgroup\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.completeuserjobs.maximum\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.df.interval\n/name\nvalue\n60000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.task-controller\n/name\nvalue\norg.apache.hadoop.mapred.DefaultTaskController\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.data.dir\n/name\nvalue\n/data1/hadoop//data\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.maxRetries\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.dns.interface\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.support.append\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.acl-modify-job\n/name\nvalue\n \n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/local\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hftp.impl\n/name\nvalue\norg.apache.hadoop.hdfs.HftpFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.permissions.supergroup\n/name\nvalue\nroot\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.trash.interval\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.sleepTimeSeconds\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.submit.replication\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication.min\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.har.impl\n/name\nvalue\norg.apache.hadoop.fs.HarFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.map.output.compression.codec\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.dns.interface\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.decommission.interval\n/name\nvalue\n30\n/value\n/property\n\n\nproperty\n!--Loaded from Unknown--\nname\ndfs.http.address\n/name\nvalue\nnagios:50070\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.job.tracker\n/name\nvalue\nnagios:9000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.heartbeat.interval\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.sorter.recordlimit\n/name\nvalue\n1000000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.name.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/dfs/name\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.line.input.format.linespermap\n/name\nvalue\n1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.taskScheduler\n/name\nvalue\norg.apache.hadoop.mapred.JobQueueTaskScheduler\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.instrumentation\n/name\nvalue\norg.apache.hadoop.mapred.TaskTrackerMetricsInst\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.datanode.http.address\n/name\nvalue\n0.0.0.0:50075\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.completion.poll.interval\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.max.maps.per.node\n/name\nvalue\n-1\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.local.dir.minspacekill\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.replication.interval\n/name\nvalue\n3\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.record.percent\n/name\nvalue\n0.05\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.kfs.impl\n/name\nvalue\norg.apache.hadoop.fs.kfs.KosmosFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.temp.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/temp\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.tasktracker.reduce.tasks.maximum\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.replication\n/name\nvalue\n2\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.checkpoint.edits.dir\n/name\nvalue\n${\nfs\n.\ncheckpoint\n.\ndir\n}\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.tasks.sleeptime-before-sigkill\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.reduce.input.buffer.percent\n/name\nvalue\n0.0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.indexcache.mb\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.job.split.metainfo.maxsize\n/name\nvalue\n10000000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.skip.reduce.auto.incr.proc.count\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nhadoop.logfile.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.automatic.close\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.compress.blocksize\n/name\nvalue\n1000000\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.hosts.exclude\n/name\nvalue\n/etc/hadoop-0.20/conf/hosts_exclude\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.block.size\n/name\nvalue\n67108864\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.tasktracker.taskmemorymanager.monitoring-interval\n/name\nvalue\n5000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.acls.enabled\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapreduce.jobtracker.staging.root.dir\n/name\nvalue\n${\nhadoop\n.\ntmp\n.\ndir\n}\n/mapred/staging\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.queue.names\n/name\nvalue\ndefault\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.access.time.precision\n/name\nvalue\n3600000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.hsftp.impl\n/name\nvalue\norg.apache.hadoop.hdfs.HsftpFileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.http.address\n/name\nvalue\n0.0.0.0:50060\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.parallel.copies\n/name\nvalue\n5\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.seqfile.lazydecompress\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.safemode.min.datanodes\n/name\nvalue\n0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nio.sort.mb\n/name\nvalue\n100\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.connection.maxidletime\n/name\nvalue\n10000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.compress.map.output\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.tracker.report.address\n/name\nvalue\n127.0.0.1:0\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.healthChecker.interval\n/name\nvalue\n60000\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.kill.max\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nipc.client.connect.max.retries\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.s3.impl\n/name\nvalue\norg.apache.hadoop.fs.s3.S3FileSystem\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.job.tracker.http.address\n/name\nvalue\n0.0.0.0:50030\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.file.buffer.size\n/name\nvalue\n4096\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.jobtracker.restart.recover\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.serializations\n/name\nvalue\norg.apache.hadoop.io.serializer.WritableSerialization\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.task.profile\n/name\nvalue\nfalse\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-site.xml--\nname\ndfs.datanode.handler.count\n/name\nvalue\n10\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\nmapred.reduce.copy.backoff\n/name\nvalue\n300\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.replication.considerLoad\n/name\nvalue\ntrue\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-default.xml--\nname\njobclient.output.filter\n/name\nvalue\nFAILED\n/value\n/property\n\n\nproperty\n!--Loaded from hdfs-default.xml--\nname\ndfs.namenode.delegation.token.max-lifetime\n/name\nvalue\n604800000\n/value\n/property\n\n\nproperty\n!--Loaded from mapred-site.xml--\nname\nmapred.tasktracker.map.tasks.maximum\n/name\nvalue\n4\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nio.compression.codecs\n/name\nvalue\norg.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec\n/value\n/property\n\n\nproperty\n!--Loaded from core-default.xml--\nname\nfs.checkpoint.size\n/name\nvalue\n67108864\n/value\n/property\n\n\n/configuration\n\n\n\n\n\n\n\n\n\n\nPlease refer to the \nApache Hadoop FAQ webpage\n for answers to common questions/concerns\n\n\nFUSE\n\n\nNotes on Building a FUSE Module\n\n\nIf you are running a custom kernel, then be sure to enable the \nfuse\n module with \nCONFIG_FUSE_FS=m\n in your kernel config. Building and installing a \nfuse\n kernel module for your custom kernel is beyond the scope of this document.\n\n\nRunning FUSE in Debug Mode\n\n\nTo start the FUSE mount in debug mode, you can run the FUSE mount command by hand:\n\n\nroot@host #\n  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server\n=\nnamenode.host\n,port\n=\n9000\n,rdbuffer\n=\n131072\n,allow_other -d\n\n\n\n\n\nDebug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.\n\n\nGridFTP\n\n\nStarting GridFTP in Standalone Mode\n\n\nIf you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:\n\n\nroot@host #\n gridftp-hdfs-standalone\n\n\n\n\n\nThe standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.\n\n\nFile Locations\n\n\n\n\n\n\n\n\nComponent\n\n\nFile Type\n\n\nLocation\n\n\nNeeds editing?\n\n\n\n\n\n\n\n\n\n\nHadoop\n\n\nLog files\n\n\n/var/log/hadoop/*\n\n\nNo\n\n\n\n\n\n\n\n\nPID files\n\n\n/var/run/hadoop/*.pid\n\n\nNo\n\n\n\n\n\n\n\n\ninit scripts\n\n\n/etc/init.d/hadoop\n\n\nNo\n\n\n\n\n\n\n\n\ninit script config file\n\n\n/etc/sysconfig/hadoop\n\n\nYes\n\n\n\n\n\n\n\n\nruntime config files\n\n\n/etc/hadoop/conf/*\n\n\nMaybe\n\n\n\n\n\n\n\n\nSystem binaries\n\n\n/usr/bin/hadoop\n\n\nNo\n\n\n\n\n\n\n\n\nJARs\n\n\n/usr/lib/hadoop/*\n\n\nNo\n\n\n\n\n\n\n\n\nruntime config files\n\n\n/etc/hosts_exclude\n\n\nYes, must be present on NameNodes\n\n\n\n\n\n\n\n\nLog files\n\n\n/var/log/gridftp-auth.log\n, \n/var/log/gridftp.log\n\n\nNo\n\n\n\n\n\n\nGridFTP\n\n\nTransfer log\n\n\n/var/log/gridftp.log\n\n\nNo\n\n\n\n\n\n\n\n\nAuthentication log\n\n\n/var/log/gridftp-auth.log\n\n\nNo\n\n\n\n\n\n\n\n\nLCMAPS auth error log\n\n\n/var/log/messages\n\n\nNo\n\n\n\n\n\n\n\n\ninit.d script\n\n\n/etc/init.d/globus-gridftp-server\n\n\nNo\n\n\n\n\n\n\n\n\nruntime config files\n\n\n/etc/gridftp-hdfs/*\n, \n/etc/sysconfig/gridftp-hdfs\n\n\nMaybe\n\n\n\n\n\n\n\n\nSystem binaries\n\n\n/usr/bin/gridftp-hdfs-standalone\n, \n/usr/sbin/globus-gridftp-server\n\n\nNo\n\n\n\n\n\n\n\n\nSystem libraries\n\n\n/usr/lib64/libglobus_gridftp_server_hdfs.so*\n\n\nNo\n\n\n\n\n\n\n\n\nLCMAPS VOMS configuration\n\n\n/etc/lcmaps.db\n\n\nYes\n\n\n\n\n\n\n\n\nCA certificates\n\n\n/etc/grid-security/certificates/*\n\n\nNo\n\n\n\n\n\n\n\n\nKnown Issues\n\n\nReplicas\n\n\nYou may need to change the following line in \n/usr/share/gridftp-hdfs/gridftp-hdfs-environment\n:\n\n\nexport\n \nGRIDFTP_HDFS_REPLICAS\n=\n2\n\n\n\n\n\n\ncopyFromLocal java IOException\n\n\nWhen trying to copy a local file into Hadoop you may come across the following java exception:\n\n\n\n  \nShow detailed java exception\n\n    \n\n\n11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]\n\n\nnodes == null\n\n\n11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file\n\n\n/osg/ddd\n - Aborting...\n\n\ncopyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0\n\n\nnodes, instead of 1\n\n\n11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :\n\n\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only\n\n\nbe replicated to 0 nodes, instead of 1\n\n\n        at\n\n\norg.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)\n\n\n        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)\n\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\n\n        at\n\n\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\n\n        at java.lang.reflect.Method.invoke(Method.java:597)\n\n\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)\n\n\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)\n\n\n        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)\n\n\n        at java.security.AccessController.doPrivileged(Native Method)\n\n\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n\n\n        at\n\n\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n\n\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)\n\n\n\n\n\n\n\n\n\n\nThis can occur if you try to install a DataNode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in \n/usr/lib/hadoop-0.20/conf/hdfs-site.xml\n:\n\n\nproperty\n\n  \nname\ndfs.datanode.du.reserved\n/name\n\n  \nvalue\n10000000000\n/value\n\n\n/property\n\n\n\n\n\n\nHadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReferences\n\n\n\n\nUsing Hadoop as a Grid Storage Element\n, \nJournal of Physics Conference Series, 2009\n.\n\n\nHadoop Distributed File System for the Grid\n, \nIEEE Nuclear Science Symposium, 2009\n.\n\n\n\n\nUsers\n\n\nThis installation will create following users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nhadoop\n\n\nRuns the NameNode services\n\n\n\n\n\n\nhdfs\n\n\nUsed by Hadoop to store data blocks and meta-data\n\n\n\n\n\n\nmapred\n\n\n\n\n\n\n\n\nzookeeper\n\n\n\n\n\n\n\n\n\n\nFor this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.", 
            "title": "Install HDFS"
        }, 
        {
            "location": "/data/install-hadoop/#installing-and-maintaining-hdfs", 
            "text": "Hadoop Distributed File System  (HDFS) is a scalable, reliable distributed file system developed in the Apache project. It is based on the map-reduce framework and design of the Google file system. The OSG distribution of Hadoop includes all components needed to operate a multi-terabyte storage site.  The purpose of this document is to provide Hadoop-based Storage Element administrators the information on how to prepare,\ninstall and validate OSG storage based on the Hadoop Distributed File System (HDFS).\nThe OSG supports a patched version HDFS from Cloudera's CDH5 distribution of HDFS\n( https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html ).   Note  The OSG only supports HDFS on EL7 hosts", 
            "title": "Installing and Maintaining HDFS"
        }, 
        {
            "location": "/data/install-hadoop/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  hdfs  and  zookeeper  on all nodes\n    as well as  hadoop  and  mapred  on the NameNodes  Firewall:  In the OSG, HDFS is intended to run as an internal service without any direct, external access to any of the nodes.\n    For more information on the ports used for communication between the various HDFS nodes, see the \n     Cloudera documentation .   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/data/install-hadoop/#designing-your-hdfs-cluster", 
            "text": "There are several important components to an HDFS installation:   NameNode : The NameNode functions as the directory server and coordinator of the HDFS cluster.\n    It houses all the meta-data for the hadoop cluster.  Secondary NameNode (optional) : This is a secondary machine that periodically merges updates to the HDFS file\n    system back into the  fsimage .\n    It must share a directory with the primary NameNode to exchange filesystem checkpoints.\n    An HDFS installation with a Secondary NameNode dramatically improves startup and restart times.  DataNode : You will have many DataNodes. Each DataNode stores large blocks of files to for the hadoop cluster.  Client : This is a documentation shorthand that refers to any machine with the hadoop client commands or\n     FUSE  mount.", 
            "title": "Designing Your HDFS Cluster"
        }, 
        {
            "location": "/data/install-hadoop/#installing-hdfs", 
            "text": "An OSG HDFS installation consists of HDFS and other support software (e.g., Gratia accounting).\nTo simplify installation, OSG provides convenience RPMs that install all required software.    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install the relevant packages based on the node you are installing:     If you are installing a(n)...  Then run the following command...      Primary NameNode  yum install osg-se-hadoop-namenode    Secondary NameNode  yum install osg-se-hadoop-secondarynamenode    DataNode  yum install osg-se-hadoop-datanode", 
            "title": "Installing HDFS"
        }, 
        {
            "location": "/data/install-hadoop/#upgrading-hdfs", 
            "text": "This section will guide you through the process to upgrade a HDFS 2.0.0 installation from OSG 3.3 to the HDFS 2.6.0\nfrom OSG 3.4.   Warning  The upgrade process will involve downtime for your HDFS cluster. Please plan accordingly.    Note  The OSG only offers HDFS 2.6.0 for EL7 hosts.   The upgrade process occurs in several steps:   Preparing for the upgrade  Updating to OSG 3.4  Upgrading the Primary NameNode  Upgrading the DataNodes  Upgrading the Secondary NameNode  Finalizing the upgrade", 
            "title": "Upgrading HDFS"
        }, 
        {
            "location": "/data/install-hadoop/#preparing-for-the-upgrade", 
            "text": "Before upgrading, backup your configuration data and HDFS metadata.    Put your Primary NameNode into safe mode:  root@primary-namenode #  hdfs dfsadmin -safemode enter Safe mode is ON     Save a clean copy of your HDFS namespace:  root@primary-namenode #  hdfs dfsadmin -saveNamespace Save namespace successful     Shutdown the HDFS services on all of your HDFS nodes (see  this section  for instructions).    On the Primary NameNode, verify that your NameNode service is off:  root@primary-namenode #  /etc/init.d/hadoop-hdfs-namenode status  This command should indicate that your NameNode service is not running.    Find the location of the directory with the HDFS metadata:  root@primary-namenode #  grep -C1 dfs.namenode.name.dir /etc/hadoop/conf/hdfs-site.xml  And look for the value of  dfs.namenode.name.dir :  property \n  name dfs.namenode.name.dir /name \n   value file:///var/lib/dfs/nn,file:///home/hadoop/dfs/nn /value     Backup the directory that appears in the output using your backup method of choice.\n   If more than one directory appears in the list (as in the example above), choose the most convenient directory.\n   All of the directories in the list will have the same contents.", 
            "title": "Preparing for the upgrade"
        }, 
        {
            "location": "/data/install-hadoop/#updating-to-osg-34", 
            "text": "Once your HDFS services have been turned off and the HDFS metadata has been backed up, update each node to OSG 3.4 by\nfollowing the instructions in  this section .", 
            "title": "Updating to OSG 3.4"
        }, 
        {
            "location": "/data/install-hadoop/#upgrading-the-primary-namenode", 
            "text": "To upgrade your Primary NameNode, update all relevant packages then run the upgrade command.    Clear the yum cache:  root@primary-namenode #  yum clean all --enablerepo = *    Update the HDFS RPMs:  root@primary-namenode #  yum update osg-se-hadoop-namenode --enablerepo-osg-upcoming    Perform the upgrade command:  root@primary-namenode #  /etc/init.d/hadoop-hdfs-namenode upgrade  This will start the upgrade process for the HDFS metadata on your primary namenode.\nYou can follow the process by running  root@primary-namenode #  tail -f /var/log/hadoop-hdfs/hadoop-hdfs-namenode- hostname .log", 
            "title": "Upgrading the Primary NameNode"
        }, 
        {
            "location": "/data/install-hadoop/#upgrading-the-datanodes", 
            "text": "Once the Primary NameNode has completed its upgrade process, start the process of upgrading each of your DataNodes.    Clear the yum cache:  root@datanode #  yum clean all --enablerepo = *    Update the HDFS RPMs:  root@datanode #  yum update osg-se-hadoop-datanode --enablerepo-osg-upcoming    Start the DataNode service:  root@datanode #  /etc/init.d/hadoop-hdfs-datanode start    After all the DataNodes have been brought back up, the Primary NameNode should exit safe mode automatically.\n   On the Primary NameNode, run the following command to verify is no longer in safe mode:  root@primary-namenode #  hdfs dfsadmin -safemode get Safe mode is OFF", 
            "title": "Upgrading the DataNodes"
        }, 
        {
            "location": "/data/install-hadoop/#upgrading-the-secondary-namenode", 
            "text": "Note  This section only applies to sites with a Secondary NameNode.\nIf you do not run a Secondary NameNode, skip to the  next section .   Once the Primary NameNode has exited safe mode, start the process of upgrading your Secondary NameNode.    Clear the yum cache:  root@secondary-namenode #  yum clean all --enablerepo = *    Update the HDFS RPMs:  root@secondary-namenode #  yum update osg-se-hadoop-secondarynamenode --enablerepo-osg-upcoming    Start the Secondary NameNode service:  root@secondary-namenode #  /etc/init.d/hadoop-hdfs-secondarynamenode start", 
            "title": "Upgrading the Secondary NameNode"
        }, 
        {
            "location": "/data/install-hadoop/#finalizing-the-upgrade", 
            "text": "Verify that the HDFS cluster is running correctly by following the instructions in  this section .    Finalize the upgrade from the Primary NameNode:  root@primary-namenode #  hdfs dfsadmin -finalizeUpgrade Finalize upgrade successful", 
            "title": "Finalizing the upgrade"
        }, 
        {
            "location": "/data/install-hadoop/#configuring-hdfs", 
            "text": "Note  Needed by: Hadoop NameNode, Hadoop DataNodes, Hadoop client, GridFTP   Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.  Hadoop configuration is stored in  /etc/hadoop/conf . However, by default, these files are mostly blank. OSG provides a sample configuration in  /etc/hadoop/conf.osg  with most common values filled in. You will need to copy these into  /etc/hadoop/conf  before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify  hdfs-site.xml  and  core-site.xml . Review all the settings in these files, but listed below are common settings to modify:     File  Setting  Example  Comments      core-site.xml  fs.default.name  hdfs://namenode.domain.tld.:9000  This is the address of the NameNode    core-site.xml  hadoop.tmp.dir  /data/scratch  Scratch temp directory used by Hadoop    core-site.xml  hadoop.log.dir  /var/log/hadoop-hdfs  Log directory used by Hadoop    core-site.xml  dfs.umaskmode  002  umask for permissions used by default    hdfs-site.xml  dfs.block.size  134217728  Block size: 128MB by default    hdfs-site.xml  dfs.replication  2  Default replication factor. Generally the same as dfs.replication.min/max    hdfs-site.xml  dfs.datanode.du.reserved  100000000  How much free space hadoop will reserve for non-Hadoop usage    hdfs-site.xml  dfs.datanode.handler.count  20  Number of server threads for DataNodes. Increase if you have many more client connections    hdfs-site.xml  dfs.namenode.handler.count  40  Number of server threads for NameNodes. Increase if you need more connections    hdfs-site.xml  dfs.http.address  namenode.domain.tld.:50070  Web address for dfs health monitoring page     See  http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml  for more parameters to configure.   Note  NameNodes must have a  /etc/hosts_exclude  present", 
            "title": "Configuring HDFS"
        }, 
        {
            "location": "/data/install-hadoop/#special-namenode-instructions-for-brand-new-installs", 
            "text": "If this is a new installation ( and only if this is a brand new installation ), you should run the following command as the  hdfs  user. (Otherwise, be sure to  chown  your storage directory to hdfs after running):  hadoop namenode -format   This will initialize the storage directory on your NameNode", 
            "title": "Special NameNode instructions for brand new installs"
        }, 
        {
            "location": "/data/install-hadoop/#optional-fuse-client-configuration", 
            "text": "A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or \"File system in User SpacE\") is a way to access HDFS using typical UNIX directory commands (i.e., POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.  FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with  modprobe fuse .  You can add the FUSE to be mounted at boot time by adding the following line to  /etc/fstab :  hadoop - fuse - dfs #   % RED %/ mnt / hadoop % ENDCOLOR %   fuse   server =% RED % namenode . host % ENDCOLOR % , port = 9000 , rdbuffer = 131072 , allow_other   0   0   Be sure to change the  /mnt/hadoop  mount point and  namenode.host  to match your local configuration. To match the help documents, we recommend using  /mnt/hadoop  as your mountpoint.  Once your  /etc/fstab  is updated, to mount FUSE run:  root@host #  mkdir /mnt/hadoop root@host #  mount /mnt/hadoop  When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:  #  mount /mnt/hadoop INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop  INFO fuse_options.c:110 Ignoring option allow_other   If you have troubles mounting FUSE refer to  Running FUSE in Debug Mode  in the Troubleshooting section.", 
            "title": "(optional) FUSE Client Configuration"
        }, 
        {
            "location": "/data/install-hadoop/#creating-vo-and-user-areas", 
            "text": "Note  Grid Users are needed by GridFTP nodes. VO areas are common to all nodes.   For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.  For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.  Note that these users must be kept in sync with the authentication method.  Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on  all nodes . These are maintained in basic system files such as  /etc/passwd  and  /etc/group .   Note  In the examples below It is assumed a FUSE mount is set to  /mnt/hadoop . As an alternative  hadoop fs  commands could have been used.   For clean HDFS operations and filesystem management:  (a) Create top-level VO subdirectories under  /mnt/hadoop .  Example:  root@host #  mkdir /mnt/hadoop/cms root@host #  mkdir /mnt/hadoop/dzero root@host #  mkdir /mnt/hadoop/sbgrid root@host #  mkdir /mnt/hadoop/fermigrid root@host #  mkdir /mnt/hadoop/cmstest root@host #  mkdir /mnt/hadoop/osg  (b) Create individual top-level user areas, under each VO area, as needed.  root@host #  mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina root@host #  mkdir -p /mnt/hadoop/cms/store/user/michaelthomas root@host #  mkdir -p /mnt/hadoop/cms/store/user/brianbockelman root@host #  mkdir -p /mnt/hadoop/cms/store/user/douglasstrain root@host #  mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana  (c) Adjust username:group ownership of each area.  root@host #  chown -R cms:cms /mnt/hadoop/cms root@host #  chown -R sam:sam /mnt/hadoop/dzero root@host #  chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas", 
            "title": "Creating VO and User Areas"
        }, 
        {
            "location": "/data/install-hadoop/#gridftp-configuration", 
            "text": "gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop.\nBy now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as\ncreated the proper users/directories.  The default settings in  /etc/gridftp.conf  along with  /etc/gridftp.d/gridftp-hdfs.conf  are used by the init.d script\nand should be ok for most installations.\nThe file  /etc/gridftp-hdfs/gridftp-debug.conf  is used by  /usr/bin/gridftp-hdfs-standalone  for starting up the\nGridFTP server in a testing mode.\nAny additional config files under  /etc/gridftp.d  will be used for both the init.d and standalone GridFTP server. /etc/sysconfig/gridftp-hdfs  contains additional site-specific environment variables that are used by the gridftp-hdfs\nDSI module in both the init.d and standalone GridFTP server.\nSome of the environment variables that can be used in  /etc/sysconfig/gridftp-hdfs  include:     Option Name  Needs Editing?  Suggested value      GRIDFTP_HDFS_REPLICA_MAP  No  File containing a list of paths and replica values for setting the default # of replicas for specific file paths    GRIDFTP_BUFFER_COUNT  No  The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop    GRIDFTP_FILE_BUFFER_COUNT  No  The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop    GRIDFTP_SYSLOG  No  Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)    GRIDFTP_HDFS_CHECKSUMS  Maybe  List of checksum calculations to perform on-the-fly (default:  \"MD5,ADLER32,CRC32,CKSUM,CVMFS\" )    GRIDFTP_HDFS_MOUNT_POINT  Maybe  The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths.  Note:  this does not imply you need FUSE mounted on GridFTP nodes!    GRIDFTP_LOAD_LIMIT  No  GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.    TMPDIR  Maybe  The temp directory where the file-based buffers are stored. Defaults to /tmp.     /etc/sysconfig/gridftp-hdfs  is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files ( ulimit -n ).  Lastly, you will need to configure an authentication mechanism for GridFTP.", 
            "title": "GridFTP Configuration"
        }, 
        {
            "location": "/data/install-hadoop/#configuring-authentication", 
            "text": "For information on how to configure authentication for your GridFTP installation, please refer to the  configuring authentication section of the GridFTP guide .", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/data/install-hadoop/#gridftp-gratia-transfer-probe-configuration", 
            "text": "Note  Needed by GridFTP node only.   See the  GridFTP documentation  for configuration details.", 
            "title": "GridFTP Gratia Transfer Probe Configuration"
        }, 
        {
            "location": "/data/install-hadoop/#hadoop-storage-probe-configuration", 
            "text": "Note  This is only needed by the Hadoop NameNode   Here are the most relevant file and directory locations:     Purpose  Needs Editing?  Location      Probe Configuration  Yes  /etc/gratia/hadoop-storage/ProbeConfig    Probe Executable  No  /usr/share/gratia/hadoop-storage/hadoop_storage_probe    Log files  No  /var/log/gratia    Temporary files  No  /var/lib/gratia/tmp     The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files  / etc / gratia / hadoop - storage / ProbeConfig  / etc / gratia / hadoop - storage / storage . cfg", 
            "title": "Hadoop Storage Probe Configuration"
        }, 
        {
            "location": "/data/install-hadoop/#probeconfig", 
            "text": "This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:     Attribute  Needs Editing  Value      CollectorHost  Maybe  Set to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it.    SiteName  Yes  Set to the resource group name of your SE as registered in OIM.    Grid  Maybe  Set to \"ITB\" if this is a test resource; otherwise, leave as OSG.    EnableProbe  Yes  Set to 1 to enable the probe.", 
            "title": "ProbeConfig"
        }, 
        {
            "location": "/data/install-hadoop/#storagecfg", 
            "text": "This file controls which paths in HDFS should be monitored. This is in the Windows INI format.  Note: for the current version of the storage.cfg, there is an error, and you may need to delete the \"probe/\" subdirectory for the ProbeConfig location  ProbeConfig   =   / etc / gratia /% RED % probe /% ENDCOLOR % hadoop - storage / ProbeConfig   For each logical \"area\" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted.  To configure an area named \"CMS /store\" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.  [Area CMS /store]  Name   =   CMS /store  Path   =   /user/cms/store/*  Trim   =   /user/cms   For each such area, add a section to your configuration file.", 
            "title": "storage.cfg"
        }, 
        {
            "location": "/data/install-hadoop/#example-file", 
            "text": "Below is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:  [Gratia]  gratia_location   =   /opt/vdt/gratia  ProbeConfig   =   %(gratia_location)s/probe/hadoop-storage/ProbeConfig  [Area /store]  Name   =   CMS /store  Path   =   /store/*  [Area /store/user]  Name   =   CMS /store/user  Path   =   /store/user/*  [Area /user]  Name   =   Hadoop /user  Path   =   /user/*   * NOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated:  gratia_location   =   / etc / gratia  ProbeConfig   =   % ( gratia_location ) s / hadoop - storage / ProbeConfig", 
            "title": "Example file"
        }, 
        {
            "location": "/data/install-hadoop/#running-services", 
            "text": "Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME     The relevant service for each node is as follows:     Node  Service      Primary NameNode  hadoop-hdfs-namenode    Secondary NameNode  hadoop-hdfs-secondarynamenode    DataNode  hadoop-hdfs-datanode    GridFTP  globus-gridftp-server", 
            "title": "Running Services"
        }, 
        {
            "location": "/data/install-hadoop/#validation", 
            "text": "The first thing you may want to do after installing and starting your primary NameNode is to verify that the web interface works. In your web browser go to:  http : //% RED % namenode . hostname % ENDCOLOR % : 50070 / dfshealth . jsp   Get familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands.  \n   Show detailed ouput \n     user$ hadoop  Usage: hadoop [--config confdir] COMMAND  where COMMAND is one of:    namenode -format     format the DFS filesystem    secondarynamenode    run the DFS secondary namenode    namenode             run the DFS namenode    datanode             run a DFS datanode    dfsadmin             run a DFS admin client    mradmin              run a Map-Reduce admin client    fsck                 run a DFS filesystem checking utility    fs                   run a generic filesystem user client    balancer             run a cluster balancing utility    fetchdt              fetch a delegation token from the NameNode    jobtracker           run the MapReduce job Tracker node    pipes                run a Pipes job    tasktracker          run a MapReduce task Tracker node    job                  manipulate MapReduce jobs    queue                get information regarding JobQueues    version              print the version    jar  jar             run a jar file    distcp  srcurl   desturl  copy file or directories recursively    archive -archiveName NAME -p  parent path   src *  dest  create a hadoop archive    oiv                  apply the offline fsimage viewer to an fsimage    classpath            prints the class path needed to get the                         Hadoop jar and the required libraries    daemonlog            get/set the log level for each daemon   or    CLASSNAME            run the class named CLASSNAME  Most commands print help when invoked w/o parameters.     For a list of supported filesystem commands:  \n   Show 'hadoop fs' detailed ouput \n     user$ hadoop fs  Usage: java FsShell             [-ls  path ]             [-lsr  path ]             [-df [ path ]]             [-du  path ]             [-dus  path ]             [-count[-q]  path ]             [-mv  src   dst ]             [-cp  src   dst ]             [-rm [-skipTrash]  path ]             [-rmr [-skipTrash]  path ]             [-expunge]             [-put  localsrc  ...  dst ]             [-copyFromLocal  localsrc  ...  dst ]             [-moveFromLocal  localsrc  ...  dst ]             [-get [-ignoreCrc] [-crc]  src   localdst ]             [-getmerge  src   localdst  [addnl]]             [-cat  src ]             [-text  src ]             [-copyToLocal [-ignoreCrc] [-crc]  src   localdst ]             [-moveToLocal [-crc]  src   localdst ]             [-mkdir  path ]             [-setrep [-R] [-w]  rep   path/file ]             [-touchz  path ]             [-test -[ezd]  path ]             [-stat [format]  path ]             [-tail [-f]  file ]             [-chmod [-R]  MODE[,MODE]... | OCTALMODE  PATH...]             [-chown [-R] [OWNER][:[GROUP]] PATH...]             [-chgrp [-R] GROUP PATH...]             [-help [cmd]]  Generic options supported are  -conf  configuration file      specify an application configuration file  -D  property=value             use value for given property  -fs  local|namenode:port       specify a namenode  -jt  local|jobtracker:port     specify a job tracker  -files  comma separated list of files     specify comma separated files to be copied to the map reduce cluster  -libjars  comma separated list of jars     specify comma separated jar files to include in the classpath.  -archives  comma separated list of archives     specify comma separated archives to be unarchived on the compute machines.  The general command line syntax is  bin/hadoop command [genericOptions] [commandOptions]     An online guide is also available at  Apache Hadoop commands manual . You can use Hadoop commands to perform filesystem operations with more consistency.  Example, to look into the internal hadoop namespace:  user$ hadoop fs -ls /  Found 1 items  drwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage   Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself  /mnt/hadoop  in Hadoop commands):  root@host #  hadoop fs -chown -R engage:engage /engage  Example, compare  hadoop fs  command vs. using FUSE mount:  user$ hadoop fs -ls /engage  Found 3 items  -rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso  -rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz  -rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz  user$ ls -l /mnt/hadoop/engage  total 935855  -rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso  -rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz  -rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz", 
            "title": "Validation"
        }, 
        {
            "location": "/data/install-hadoop/#gridftp-validation", 
            "text": "Note  The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using  voms-proxy-init  or  grid-proxy-init . Obtaining grid credentials is beyond the scope of this document.   user$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt   If you are having troubles running GridFTP refer to  Starting GridFTP in Standalone Mode  in the Troubleshooting section.", 
            "title": "GridFTP Validation"
        }, 
        {
            "location": "/data/install-hadoop/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/data/install-hadoop/#hadoop", 
            "text": "To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:  http : //% RED % namenode . hostname % ENDCOLOR % : 50070 / conf   You will see the entire configuration in XML format, for example:  \n   Expand XML configuration \n      ?xml version= 1.0  encoding= UTF-8  standalone= no ? configuration  property !--Loaded from core-default.xml-- name fs.s3n.impl /name value org.apache.hadoop.fs.s3native.NativeS3FileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.cache.levels /name value 2 /value /property  property !--Loaded from mapred-default.xml-- name map.sort.class /name value org.apache.hadoop.util.QuickSort /value /property  property !--Loaded from core-site.xml-- name hadoop.tmp.dir /name value /data1/hadoop//scratch /value /property  property !--Loaded from core-default.xml-- name hadoop.native.lib /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.decommission.nodes.per.interval /name value 5 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.need.client.auth /name value false /value /property  property !--Loaded from core-default.xml-- name ipc.client.idlethreshold /name value 4000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.system.dir /name value ${ hadoop . tmp . dir } /mapred/system /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.data.dir.perm /name value 755 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.hours /name value 0 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.namenode.logging.level /name value all /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.address /name value 0.0.0.0:50010 /value /property  property !--Loaded from core-default.xml-- name io.skip.checksum.errors /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.token.enable /name value false /value /property  property !--Loaded from Unknown-- name fs.default.name /name value hdfs://nagios.t2.ucsd.edu:9000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.child.tmp /name value ./tmp /value /property  property !--Loaded from core-default.xml-- name fs.har.impl.disable.cache /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.reduce.max.skip.groups /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.threshold.pct /name value 0.999f /value /property  property !--Loaded from mapred-default.xml-- name mapred.heartbeats.in.second /name value 100 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.namenode.handler.count /name value 40 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.blockreport.initialDelay /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.instrumentation /name value org.apache.hadoop.mapred.JobTrackerMetricsInst /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.dns.nameserver /name value default /value /property  property !--Loaded from mapred-default.xml-- name io.sort.factor /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.timeout /name value 600000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.tracker.failures /name value 4 /value /property  property !--Loaded from core-default.xml-- name hadoop.rpc.socket.factory.class.default /name value org.apache.hadoop.net.StandardSocketFactory /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.jobhistory.lru.cache.size /name value 5 /value /property  property !--Loaded from core-default.xml-- name fs.hdfs.impl /name value org.apache.hadoop.hdfs.DistributedFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.map.auto.incr.proc.count /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.key.update.interval /name value 600 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.complete.cancel.delegation.tokens /name value true /value /property  property !--Loaded from core-default.xml-- name io.mapfile.bloom.size /name value 1048576 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.reduce.shuffle.connect.timeout /name value 180000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.extension /name value 30000 /value /property  property !--Loaded from mapred-site.xml-- name tasktracker.http.threads /name value 50 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.shuffle.merge.percent /name value 0.66 /value /property  property !--Loaded from core-default.xml-- name fs.ftp.impl /name value org.apache.hadoop.fs.ftp.FTPFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compress /name value false /value /property  property !--Loaded from core-site.xml-- name io.bytes.per.checksum /name value 4096 /value /property  property !--Loaded from mapred-default.xml-- name mapred.healthChecker.script.timeout /name value 600000 /value /property  property !--Loaded from core-default.xml-- name topology.node.switch.mapping.impl /name value org.apache.hadoop.net.ScriptBasedMapping /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.server.keystore.resource /name value ssl-server.xml /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.slowstart.completed.maps /name value 0.05 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.max.attempts /name value 4 /value /property  property !--Loaded from core-default.xml-- name fs.ramfs.impl /name value org.apache.hadoop.fs.InMemoryFileSystem /value /property  property !--Loaded from hdfs-default.xml-- name dfs.block.access.token.lifetime /name value 600 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.map.max.skip.records /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.name.edits.dir /name value ${ dfs . name . dir } /value /property  property !--Loaded from core-default.xml-- name hadoop.security.group.mapping /name value org.apache.hadoop.security.ShellBasedUnixGroupsMapping /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.dir /name value /jobtracker/jobsInfo /value /property  property !--Loaded from core-site.xml-- name hadoop.log.dir /name value /var/log/hadoop /value /property  property !--Loaded from core-default.xml-- name fs.s3.buffer.dir /name value ${ hadoop . tmp . dir } /s3 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.block.size /name value 134217728 /value /property  property !--Loaded from mapred-default.xml-- name job.end.retry.attempts /name value 0 /value /property  property !--Loaded from core-default.xml-- name fs.file.impl /name value org.apache.hadoop.fs.LocalFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compression.type /name value RECORD /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir.minspacestart /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.ipc.address /name value 0.0.0.0:50020 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.permissions /name value true /value /property  property !--Loaded from core-default.xml-- name topology.script.number.args /name value 100 /value /property  property !--Loaded from core-default.xml-- name io.mapfile.bloom.error.rate /name value 0.005 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.tracker.blacklists /name value 4 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile.maps /name value 0-2 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.https.address /name value 0.0.0.0:50475 /value /property  property !--Loaded from core-site.xml-- name dfs.umaskmode /name value 002 /value /property  property !--Loaded from mapred-default.xml-- name mapred.userlog.retain.hours /name value 24 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.secondary.http.address /name value gratia-1:50090 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication.max /name value 32 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.persist.jobstatus.active /name value false /value /property  property !--Loaded from core-default.xml-- name hadoop.security.authorization /name value false /value /property  property !--Loaded from core-default.xml-- name local.cache.size /name value 10737418240 /value /property  property !--Loaded from mapred-default.xml-- name mapred.min.split.size /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.token.renew-interval /name value 86400000 /value /property  property !--Loaded from mapred-site.xml-- name mapred.map.tasks /name value 7919 /value /property  property !--Loaded from mapred-default.xml-- name mapred.child.java.opts /name value -Xmx200m /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.client.keystore.resource /name value ssl-client.xml /value /property  property !--Loaded from Unknown-- name dfs.namenode.startup /name value REGULAR /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.queue.name /name value default /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.retiredjobs.cache.size /name value 1000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.address /name value 0.0.0.0:50470 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.balance.bandwidthPerSec /name value 2000000000 /value /property  property !--Loaded from core-default.xml-- name ipc.server.listen.queue.size /name value 128 /value /property  property !--Loaded from mapred-default.xml-- name job.end.retry.interval /name value 30000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.inmem.merge.threshold /name value 1000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.attempts.to.start.skipping /name value 2 /value /property  property !--Loaded from hdfs-site.xml-- name fs.checkpoint.dir /name value /var/hadoop/checkpoint-a /value /property  property !--Loaded from mapred-site.xml-- name mapred.reduce.tasks /name value 1543 /value /property  property !--Loaded from mapred-default.xml-- name mapred.merge.recordsBeforeProgress /name value 10000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.userlog.limit.kb /name value 0 /value /property  property !--Loaded from core-default.xml-- name webinterface.private.actions /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.max.objects /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.shuffle.input.buffer.percent /name value 0.70 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.spill.percent /name value 0.80 /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.tasks.speculative.execution /name value true /value /property  property !--Loaded from core-default.xml-- name hadoop.util.hash.type /name value murmur /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.dns.nameserver /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.blockreport.intervalMsec /name value 3600000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.max.attempts /name value 4 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.acl-view-job /name value   /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.handler.count /name value 10 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.client.block.write.retries /name value 3 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.reduces.per.node /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.reduce.shuffle.read.timeout /name value 180000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.expiry.interval /name value 600000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.https.enable /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.maxtasks.per.job /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.job.history.block.size /name value 3145728 /value /property  property !--Loaded from mapred-default.xml-- name keep.failed.task.files /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.failed.volumes.tolerated /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile.reduces /name value 0-2 /value /property  property !--Loaded from core-default.xml-- name ipc.client.tcpnodelay /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.output.compression.codec /name value org.apache.hadoop.io.compress.DefaultCodec /value /property  property !--Loaded from mapred-default.xml-- name io.map.index.skip /name value 0 /value /property  property !--Loaded from core-default.xml-- name ipc.server.tcpnodelay /name value false /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.key.update-interval /name value 86400000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.running.map.limit /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name jobclient.progress.monitor.poll.interval /name value 1000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.default.chunk.view.size /name value 32768 /value /property  property !--Loaded from core-default.xml-- name hadoop.logfile.size /name value 10000000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.tasks.speculative.execution /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.tasktracker.outofband.heartbeat /name value false /value /property  property !--Loaded from core-default.xml-- name fs.s3n.block.size /name value 67108864 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.datanode.du.reserved /name value 10000000000 /value /property  property !--Loaded from core-default.xml-- name hadoop.security.authentication /name value simple /value /property  property !--Loaded from hdfs-site.xml-- name fs.checkpoint.period /name value 3600 /value /property  property !--Loaded from mapred-default.xml-- name mapred.running.reduce.limit /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.reuse.jvm.num.tasks /name value 1 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.web.ugi /name value webuser,webgroup /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.completeuserjobs.maximum /name value 100 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.df.interval /name value 60000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.task-controller /name value org.apache.hadoop.mapred.DefaultTaskController /value /property  property !--Loaded from hdfs-site.xml-- name dfs.data.dir /name value /data1/hadoop//data /value /property  property !--Loaded from core-default.xml-- name fs.s3.maxRetries /name value 4 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.dns.interface /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.support.append /name value true /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.acl-modify-job /name value   /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir /name value ${ hadoop . tmp . dir } /mapred/local /value /property  property !--Loaded from core-default.xml-- name fs.hftp.impl /name value org.apache.hadoop.hdfs.HftpFileSystem /value /property  property !--Loaded from hdfs-site.xml-- name dfs.permissions.supergroup /name value root /value /property  property !--Loaded from core-default.xml-- name fs.trash.interval /name value 0 /value /property  property !--Loaded from core-default.xml-- name fs.s3.sleepTimeSeconds /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.submit.replication /name value 10 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication.min /name value 1 /value /property  property !--Loaded from core-default.xml-- name fs.har.impl /name value org.apache.hadoop.fs.HarFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.map.output.compression.codec /name value org.apache.hadoop.io.compress.DefaultCodec /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.dns.interface /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.decommission.interval /name value 30 /value /property  property !--Loaded from Unknown-- name dfs.http.address /name value nagios:50070 /value /property  property !--Loaded from mapred-site.xml-- name mapred.job.tracker /name value nagios:9000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.heartbeat.interval /name value 3 /value /property  property !--Loaded from core-default.xml-- name io.seqfile.sorter.recordlimit /name value 1000000 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.name.dir /name value ${ hadoop . tmp . dir } /dfs/name /value /property  property !--Loaded from mapred-default.xml-- name mapred.line.input.format.linespermap /name value 1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.taskScheduler /name value org.apache.hadoop.mapred.JobQueueTaskScheduler /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.instrumentation /name value org.apache.hadoop.mapred.TaskTrackerMetricsInst /value /property  property !--Loaded from hdfs-default.xml-- name dfs.datanode.http.address /name value 0.0.0.0:50075 /value /property  property !--Loaded from mapred-default.xml-- name jobclient.completion.poll.interval /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.max.maps.per.node /name value -1 /value /property  property !--Loaded from mapred-default.xml-- name mapred.local.dir.minspacekill /name value 0 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.replication.interval /name value 3 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.record.percent /name value 0.05 /value /property  property !--Loaded from core-default.xml-- name fs.kfs.impl /name value org.apache.hadoop.fs.kfs.KosmosFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.temp.dir /name value ${ hadoop . tmp . dir } /mapred/temp /value /property  property !--Loaded from mapred-site.xml-- name mapred.tasktracker.reduce.tasks.maximum /name value 4 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.replication /name value 2 /value /property  property !--Loaded from core-default.xml-- name fs.checkpoint.edits.dir /name value ${ fs . checkpoint . dir } /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.tasks.sleeptime-before-sigkill /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.reduce.input.buffer.percent /name value 0.0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.indexcache.mb /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.job.split.metainfo.maxsize /name value 10000000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.skip.reduce.auto.incr.proc.count /name value true /value /property  property !--Loaded from core-default.xml-- name hadoop.logfile.count /name value 10 /value /property  property !--Loaded from core-default.xml-- name fs.automatic.close /name value true /value /property  property !--Loaded from core-default.xml-- name io.seqfile.compress.blocksize /name value 1000000 /value /property  property !--Loaded from hdfs-site.xml-- name dfs.hosts.exclude /name value /etc/hadoop-0.20/conf/hosts_exclude /value /property  property !--Loaded from core-default.xml-- name fs.s3.block.size /name value 67108864 /value /property  property !--Loaded from mapred-default.xml-- name mapred.tasktracker.taskmemorymanager.monitoring-interval /name value 5000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.acls.enabled /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapreduce.jobtracker.staging.root.dir /name value ${ hadoop . tmp . dir } /mapred/staging /value /property  property !--Loaded from mapred-default.xml-- name mapred.queue.names /name value default /value /property  property !--Loaded from hdfs-default.xml-- name dfs.access.time.precision /name value 3600000 /value /property  property !--Loaded from core-default.xml-- name fs.hsftp.impl /name value org.apache.hadoop.hdfs.HsftpFileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.http.address /name value 0.0.0.0:50060 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.parallel.copies /name value 5 /value /property  property !--Loaded from core-default.xml-- name io.seqfile.lazydecompress /name value true /value /property  property !--Loaded from hdfs-default.xml-- name dfs.safemode.min.datanodes /name value 0 /value /property  property !--Loaded from mapred-default.xml-- name io.sort.mb /name value 100 /value /property  property !--Loaded from core-default.xml-- name ipc.client.connection.maxidletime /name value 10000 /value /property  property !--Loaded from mapred-default.xml-- name mapred.compress.map.output /name value false /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.tracker.report.address /name value 127.0.0.1:0 /value /property  property !--Loaded from mapred-default.xml-- name mapred.healthChecker.interval /name value 60000 /value /property  property !--Loaded from core-default.xml-- name ipc.client.kill.max /name value 10 /value /property  property !--Loaded from core-default.xml-- name ipc.client.connect.max.retries /name value 10 /value /property  property !--Loaded from core-default.xml-- name fs.s3.impl /name value org.apache.hadoop.fs.s3.S3FileSystem /value /property  property !--Loaded from mapred-default.xml-- name mapred.job.tracker.http.address /name value 0.0.0.0:50030 /value /property  property !--Loaded from core-default.xml-- name io.file.buffer.size /name value 4096 /value /property  property !--Loaded from mapred-default.xml-- name mapred.jobtracker.restart.recover /name value false /value /property  property !--Loaded from core-default.xml-- name io.serializations /name value org.apache.hadoop.io.serializer.WritableSerialization /value /property  property !--Loaded from mapred-default.xml-- name mapred.task.profile /name value false /value /property  property !--Loaded from hdfs-site.xml-- name dfs.datanode.handler.count /name value 10 /value /property  property !--Loaded from mapred-default.xml-- name mapred.reduce.copy.backoff /name value 300 /value /property  property !--Loaded from hdfs-default.xml-- name dfs.replication.considerLoad /name value true /value /property  property !--Loaded from mapred-default.xml-- name jobclient.output.filter /name value FAILED /value /property  property !--Loaded from hdfs-default.xml-- name dfs.namenode.delegation.token.max-lifetime /name value 604800000 /value /property  property !--Loaded from mapred-site.xml-- name mapred.tasktracker.map.tasks.maximum /name value 4 /value /property  property !--Loaded from core-default.xml-- name io.compression.codecs /name value org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec /value /property  property !--Loaded from core-default.xml-- name fs.checkpoint.size /name value 67108864 /value /property  /configuration     Please refer to the  Apache Hadoop FAQ webpage  for answers to common questions/concerns", 
            "title": "Hadoop"
        }, 
        {
            "location": "/data/install-hadoop/#fuse", 
            "text": "", 
            "title": "FUSE"
        }, 
        {
            "location": "/data/install-hadoop/#notes-on-building-a-fuse-module", 
            "text": "If you are running a custom kernel, then be sure to enable the  fuse  module with  CONFIG_FUSE_FS=m  in your kernel config. Building and installing a  fuse  kernel module for your custom kernel is beyond the scope of this document.", 
            "title": "Notes on Building a FUSE Module"
        }, 
        {
            "location": "/data/install-hadoop/#running-fuse-in-debug-mode", 
            "text": "To start the FUSE mount in debug mode, you can run the FUSE mount command by hand:  root@host #   /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server = namenode.host ,port = 9000 ,rdbuffer = 131072 ,allow_other -d  Debug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.", 
            "title": "Running FUSE in Debug Mode"
        }, 
        {
            "location": "/data/install-hadoop/#gridftp", 
            "text": "", 
            "title": "GridFTP"
        }, 
        {
            "location": "/data/install-hadoop/#starting-gridftp-in-standalone-mode", 
            "text": "If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:  root@host #  gridftp-hdfs-standalone  The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.", 
            "title": "Starting GridFTP in Standalone Mode"
        }, 
        {
            "location": "/data/install-hadoop/#file-locations", 
            "text": "Component  File Type  Location  Needs editing?      Hadoop  Log files  /var/log/hadoop/*  No     PID files  /var/run/hadoop/*.pid  No     init scripts  /etc/init.d/hadoop  No     init script config file  /etc/sysconfig/hadoop  Yes     runtime config files  /etc/hadoop/conf/*  Maybe     System binaries  /usr/bin/hadoop  No     JARs  /usr/lib/hadoop/*  No     runtime config files  /etc/hosts_exclude  Yes, must be present on NameNodes     Log files  /var/log/gridftp-auth.log ,  /var/log/gridftp.log  No    GridFTP  Transfer log  /var/log/gridftp.log  No     Authentication log  /var/log/gridftp-auth.log  No     LCMAPS auth error log  /var/log/messages  No     init.d script  /etc/init.d/globus-gridftp-server  No     runtime config files  /etc/gridftp-hdfs/* ,  /etc/sysconfig/gridftp-hdfs  Maybe     System binaries  /usr/bin/gridftp-hdfs-standalone ,  /usr/sbin/globus-gridftp-server  No     System libraries  /usr/lib64/libglobus_gridftp_server_hdfs.so*  No     LCMAPS VOMS configuration  /etc/lcmaps.db  Yes     CA certificates  /etc/grid-security/certificates/*  No", 
            "title": "File Locations"
        }, 
        {
            "location": "/data/install-hadoop/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/data/install-hadoop/#replicas", 
            "text": "You may need to change the following line in  /usr/share/gridftp-hdfs/gridftp-hdfs-environment :  export   GRIDFTP_HDFS_REPLICAS = 2", 
            "title": "Replicas"
        }, 
        {
            "location": "/data/install-hadoop/#copyfromlocal-java-ioexception", 
            "text": "When trying to copy a local file into Hadoop you may come across the following java exception:  \n   Show detailed java exception \n      11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]  nodes == null  11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file  /osg/ddd  - Aborting...  copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0  nodes, instead of 1  11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :  org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only  be replicated to 0 nodes, instead of 1          at  org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)          at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)          at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)          at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)          at  sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)          at java.lang.reflect.Method.invoke(Method.java:597)          at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)          at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)          at java.security.AccessController.doPrivileged(Native Method)          at javax.security.auth.Subject.doAs(Subject.java:396)          at  org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)          at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)     This can occur if you try to install a DataNode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in  /usr/lib/hadoop-0.20/conf/hdfs-site.xml :  property \n   name dfs.datanode.du.reserved /name \n   value 10000000000 /value  /property   Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.", 
            "title": "copyFromLocal java IOException"
        }, 
        {
            "location": "/data/install-hadoop/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/data/install-hadoop/#references", 
            "text": "Using Hadoop as a Grid Storage Element ,  Journal of Physics Conference Series, 2009 .  Hadoop Distributed File System for the Grid ,  IEEE Nuclear Science Symposium, 2009 .", 
            "title": "References"
        }, 
        {
            "location": "/data/install-hadoop/#users", 
            "text": "This installation will create following users unless they are already created.     User  Comment      hadoop  Runs the NameNode services    hdfs  Used by Hadoop to store data blocks and meta-data    mapred     zookeeper      For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.", 
            "title": "Users"
        }, 
        {
            "location": "/submit/osg-flock/", 
            "text": "Configuring a submit host to flock to OSG\n\n\nIf you have a HTCondor submit node on your campus, it can be configured\nto spill over onto available resources on the Open Science Grid. In\nHTCondor terms this is called \nflocking\n\n\nIf you are interested in this solution, please open a\n\nnew ticket\n with the hostname.\n\n\nRequirements\n\n\nThe requirements are:\n\n\n\n\nA public IP address, forward and reverse DNS.\n\n\nAbility to open a few firewall incoming ports to the WAN.\n\n\nHTCondor has to authenticate via pool GSI (preffered) or password. For GSI, the submit host\n   has to have a host certificate.\n\n\nReporting to the OSG accounting system has to be enabled. This can\n   be accomplished by installing and configuring the \ngratia-probe-condor\n and \ngratia-probe-glideinwms\n RPMs.\n\n\nSubmitted jobs should have the \n+ProjectName\n attribute specified with\n   a valid registered project name.\n\n\n\n\nRequired Packages\n\n\nEnable the \nOSG Yum repository\n.\n\n\nInstall the packages required:\n\n\nroot@host #\n yum install osg-flock\n\n\n\n\n\nGratia Probe Configuration\n\n\n\n\n\n\nCopy over the recommended probe configuration:\n\n\nroot@host #\n cp /etc/gratia/condor/Probeconfig-flocking /etc/gratia/condor/Probeconfig\n\n\n\n\n\n\n\n\n\nFill in the values for \nProbeName\n and \nSiteName\n with the hostname and Topology resource name, respectively. For example:\n\n\nProbeName=\ncondor:foo.example.edu\n\nSiteName=\nOSG_US_RESOURCE_SUBMIT\n\n\n\n\n\n\n\n\n\n\nPlease remember to enable and start the probe:\n\n\nroot@host #\n systemctl \nenable\n gratia-probes-cron\n\nroot@host #\n systemctl start gratia-probes-cron\n\n\n\n\n\nGSI: Requesting a Host Certificate\n\n\nA host certificate is used for authenticating your submit host to the OSG\ninfrastructure. If you do not already have a certificate, you can request one\nusing \nthese instructions\n\n\nOptional\n Pool Password: HTCondor Configuration\n\n\nThis section is optional and should be only considered if the local admin cannot obtain\na host certificate for their submit host. To enable pool password authentication place a file here:\n\n/etc/condor/config.d/90-flock-pool-password.conf\n with the following contents.\n\n\n   \n#\n-- Flock to the OSG using pool password\n\n   \nSEC_PASSWORD_FILE\n \n=\n \n/\netc\n/\ncondor\n/\npool_password\n\n   \nSEC_DEFAULT_AUTHENTICATION_METHODS\n \n=\n \nFS\n,\nPASSWORD\n\n\n\n\n\n\nProject Names\n\n\nOSG will only run jobs tagged with a valid \nProjectName\n - this is the main attribute\nused for accounting. Please open at ticket to register a new project.\nJobs should specify which project to be accounted against by adding\nthe \n+ProjectName\n attribute. Note that the value is a string and hence\nthe double quotes are required. For example:\n\n\n+\nProjectName\n \n=\n \nSome_Name_Here\n\n\n\n\n\n\nGet Help\n\n\nIf you need help with setup or troubleshooting, see our \nhelp procedure\n.", 
            "title": "Flock to OSG"
        }, 
        {
            "location": "/submit/osg-flock/#configuring-a-submit-host-to-flock-to-osg", 
            "text": "If you have a HTCondor submit node on your campus, it can be configured\nto spill over onto available resources on the Open Science Grid. In\nHTCondor terms this is called  flocking  If you are interested in this solution, please open a new ticket  with the hostname.", 
            "title": "Configuring a submit host to flock to OSG"
        }, 
        {
            "location": "/submit/osg-flock/#requirements", 
            "text": "The requirements are:   A public IP address, forward and reverse DNS.  Ability to open a few firewall incoming ports to the WAN.  HTCondor has to authenticate via pool GSI (preffered) or password. For GSI, the submit host\n   has to have a host certificate.  Reporting to the OSG accounting system has to be enabled. This can\n   be accomplished by installing and configuring the  gratia-probe-condor  and  gratia-probe-glideinwms  RPMs.  Submitted jobs should have the  +ProjectName  attribute specified with\n   a valid registered project name.", 
            "title": "Requirements"
        }, 
        {
            "location": "/submit/osg-flock/#required-packages", 
            "text": "Enable the  OSG Yum repository .  Install the packages required:  root@host #  yum install osg-flock", 
            "title": "Required Packages"
        }, 
        {
            "location": "/submit/osg-flock/#gratia-probe-configuration", 
            "text": "Copy over the recommended probe configuration:  root@host #  cp /etc/gratia/condor/Probeconfig-flocking /etc/gratia/condor/Probeconfig    Fill in the values for  ProbeName  and  SiteName  with the hostname and Topology resource name, respectively. For example:  ProbeName= condor:foo.example.edu \nSiteName= OSG_US_RESOURCE_SUBMIT     Please remember to enable and start the probe:  root@host #  systemctl  enable  gratia-probes-cron root@host #  systemctl start gratia-probes-cron", 
            "title": "Gratia Probe Configuration"
        }, 
        {
            "location": "/submit/osg-flock/#gsi-requesting-a-host-certificate", 
            "text": "A host certificate is used for authenticating your submit host to the OSG\ninfrastructure. If you do not already have a certificate, you can request one\nusing  these instructions", 
            "title": "GSI: Requesting a Host Certificate"
        }, 
        {
            "location": "/submit/osg-flock/#optional-pool-password-htcondor-configuration", 
            "text": "This section is optional and should be only considered if the local admin cannot obtain\na host certificate for their submit host. To enable pool password authentication place a file here: /etc/condor/config.d/90-flock-pool-password.conf  with the following contents.      # -- Flock to the OSG using pool password \n    SEC_PASSWORD_FILE   =   / etc / condor / pool_password \n    SEC_DEFAULT_AUTHENTICATION_METHODS   =   FS , PASSWORD", 
            "title": "Optional Pool Password: HTCondor Configuration"
        }, 
        {
            "location": "/submit/osg-flock/#project-names", 
            "text": "OSG will only run jobs tagged with a valid  ProjectName  - this is the main attribute\nused for accounting. Please open at ticket to register a new project.\nJobs should specify which project to be accounted against by adding\nthe  +ProjectName  attribute. Note that the value is a string and hence\nthe double quotes are required. For example:  + ProjectName   =   Some_Name_Here", 
            "title": "Project Names"
        }, 
        {
            "location": "/submit/osg-flock/#get-help", 
            "text": "If you need help with setup or troubleshooting, see our  help procedure .", 
            "title": "Get Help"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/", 
            "text": "Installing and Maintaining the LCMAPS VOMS Plugin\n\n\nLCMAPS is a software library used on \nHTCondor-CE\n, \nGridFTP\n, and\n\nXRootD\n hosts for mapping grid certificates of incoming connections to specific\nUnix accounts.\nThe LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g.\n\n/cms/Role=production/Capability=NULL\n.\nIn OSG 3.4, the LCMAPS VOMS plugin replaced GUMS and edg-mkgridmap as the authentication method at OSG sites.\n\n\nThe OSG provides a default set of mappings from VOMS attributes to Unix accounts.\nBy configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to;\nadding custom mappings for specific users and VOMS attributes; and/or banning specific users and VOMS attributes.\n\n\nUse this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your\nresources on a per-VO basis.\n\n\nInstalling the LCMAPS VOMS Plugin\n\n\nTo install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages\n\n\n\n\n\n\nInstall \nlcmaps\n, the default mapfile, and the configuration tools:\n\n\nroot@host #\n yum install lcmaps vo-client-lcmaps-voms osg-configure-misc\n\n\n\n\n\n\n\n\n\nConfiguring the LCMAPS VOMS Plugin\n\n\nThe following section describes the steps required to configure the LCMAPS VOMS plugin for authentication.\nAdditionally, there are \noptional configuration\n instructions if you need to make changes to\nthe default mappings, or migrate from edg-mkgridmap or GUMS.\n\n\nSupporting mapped VOs and users\n\n\nEnsure Unix accounts exist for each VO, VO role, VO group, or user you choose to support in the \nmapfiles\n:\n\n\n\n\n\n\nConsult the default VO mappings in \n/usr/share/osg/voms-mapfile-default\n to determine the mapped Unix account names.\n    Each of the mapfiles has the following format:\n\n\nVO, VO role, VO group or user\n \n%\nRED\n%\nUnix\n \naccount\n%\nENDCOLOR\n%\n\n\n\n\n\n\n\n\n\n\nCreate Unix accounts for each VO, VO role, VO group, and user that you wish to support.\n    The full list of VOs is located in the \nOSG topology\n.\n    You are not expected to support all the VOs.\n    If you would like to support opportunistic usage, we recommend creating the following Unix accounts:\n\n\n\n\n\n\n\n\nVO name\n\n\nUnix account(s)\n\n\n\n\n\n\n\n\n\n\nGLOW\n\n\nglow\n\n\n\n\n\n\nOSG\n\n\nosg\n\n\n\n\n\n\nATLAS\n\n\nusatlas3\n\n\n\n\n\n\nCMS\n\n\ncmsuser\n\n\n\n\n\n\nFermilab\n\n\nfnalgrid\n\n\n\n\n\n\nHCC\n\n\nhcc\n\n\n\n\n\n\nGluex\n\n\ngluex\n\n\n\n\n\n\n\n\n\n\n\n\nEdit \n/etc/osg/config.d/30-gip.ini\n and specify the supported VOs per \nSubcluster or ResourceEntry section\n:\n\n\nallowed_vos\n=\nVO1,VO2...\n\n\n\n\n\n\n\n\n\n\nApplying configuration settings\n\n\nMaking changes to the OSG configuration files in the \n/etc/osg/config.d\n directory does not apply those settings to\nsoftware automatically.\nFor the OSG settings, use the \nosg-configure\n tool to validate (to a limited\nextent) and apply the settings to the relevant software components.\nIf instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the\n\nmanual configuration section\n.\n\n\n\n\n\n\nMake all changes to \n.ini\n files in the \n/etc/osg/config.d\n directory.\n\n\n\n\nNote\n\n\nThis document only describes the critical settings for the LCMAPS VOMS plugin and related software.\nYou may need to configure other software that is installed on your host, too.\n\n\n\n\n\n\n\n\nValidate the configuration settings:\n\n\nroot@host #\n osg-configure -v\n\n\n\n\n\n\n\n\n\nOnce the validation command succeeds without errors, apply the configuration settings:\n\n\nroot@host #\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following subsections contain information on migration from \nedg-mkgridmap\n, mapping or banning users by their\ncertificates' Distinguished Names (DNs) or by their proxies' VOMS attributes.\nAny optional configuration is to be performed after the installation and configuration sections above.\n\n\nFor a table of the configuration files and their order of evaluation, consult the \nreference section\n.\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nMigrating from GUMS\n\n\nMapping VOs\n\n\nMapping users\n\n\nBanning VOs\n\n\nBanning users\n\n\nMapping using all FQANs\n\n\n\n\nMigrating from edg-mkgridmap\n\n\nThe program edg-mkgridmap (found in the package \nedg-mkgridmap\n), used for authentication on HTCondor-CE, GridFTP, and\nXRootD hosts, is no longer supported by the OSG.\nThe LCMAPS VOMS plugin (package \nlcmaps-plugins-voms\n) now provides the same functionality.\nTo migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:\n\n\n\n\n\n\nConfigure user DN mappings:\n\n\n\n\nRemove \n/etc/grid-security/grid-mapfile\n\n\n\n\nCheck if you have a local grid mapfile:\n\n\nroot@host #\n grep gmf_local /etc/edg-mkgridmap.conf\n\n\n\n\n\n\n\n\n\nIf the above command returns a file that exists and has contents, move it to \n/etc/grid-security/grid-mapfile\n.\n\n\n\n\n\n\n\n\n\n\nIf you are converting an HTCondor-CE host, remove the HTCondor-CE \nGRIDMAP\n configuration. Otherwise, skip to the\n    next step.\n\n\n\n\n\n\nFind where \nGRIDMAP\n is set:\n\n\nroot@host #\n condor_ce_config_val -v GRIDMAP\n\n\n\n\n\n\n\n\n\nIf the above command returns a file, remove the \nGRIDMAP\n configuration from that file.\n   Repeat this until the command returns \nNot defined: GRIDMAP\n.\n\n\n\n\nReconfigure HTCondor-CE:\nroot@host #\n condor_ce_reconfig\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemove edg-mkgridmap and related packages:\n\n\nroot@host #\n yum erase edg-mkgridmap\n\n\n\n\n\n\n\nWarning\n\n\nIn the output from this command, yum should \nnot\n list other packages than the one.\nIf it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest\nOSG 3.4 versions (they should have \".osg34\" in their versions), and try again.\n\n\n\n\n\n\n\n\nMigrating from GUMS\n\n\nGUMS is no longer supported by the OSG and has been replaced by the LCMAPS VOMS plugin.\nNote that unlike GUMS, which runs on a central host, the LCMAPS VOMS plugin will run on your GUMS clients (e.g.\nHTCondor-CE, GridFTP, and XRootD).\nTo migrate any custom authentication configuration from GUMS to the LCMAPS VOMS plugin, perform the following procedure:\n\n\n\n\n\n\nOn your GUMS host, retrieve the conversion helper script and run it:\n\n\nroot@gums-host #\n wget https://raw.githubusercontent.com/opensciencegrid/osg-vo-config/mapfile-generator-0.2/bin/manual-mapfile-from-gumsdb.py\n\nroot@gums-host #\n python manual-mapfile-from-gumsdb.py\n\n\n\n\n\n\n\n\n\nVerify that the contents of \nban-mapfile.additions\n, \ngrid-mapfile.additions\n, and \nvoms-mapfile.additions\n include\n   any custom banned users, user mappings, and VO mappings, respectively.\n\n\n\n\nNote\n\n\nThe above files will not include all VO mappings; the OSG provides default VO mappings in\n\n/usr/share/osg/voms-mapfile-default\n\n\n\n\n\n\n\n\nOn each of your client hosts (e.g. HTCondor-CE, GridFTP, XRootD), perform the following:\n\n\n\n\nIf you have not done so already, \ninstall\n and\n   \nconfigure\n the LCMAPS VOMS plugin\n\n\nAppend each \n.additions\n file to its corresponding file in \n/etc/grid-security/\n (creating those files if they do\n   not exist)\n\n\n\n\n\n\n\n\nMapping VOs\n\n\nTo map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes, create \n/etc/grid-security/voms-mapfile\n.\nAn example of the format of a \nvoms-mapfile\n follows:\n\n\n#\n \nmap\n \nGLOW\n \njobs\n \nin\n \nthe\n \nchtc\n \ngroup\n \nto\n \nthe\n \nglow1\n \nUnix\n \naccount\n.\n\n\n/GLOW/chtc/*\n \nglow1\n\n\n#\n \nmap\n \nGLOW\n \njobs\n \nwith\n \nthe\n \nhtpc\n \nrole\n \nto\n \nthe\n \nglow2\n \nUnix\n \naccount\n.\n\n\n/GLOW/Role=htpc/*\n \nglow2\n\n\n#\n \nmap\n \nother\n \nGLOW\n \njobs\n \nto\n \nthe\n \nglow\n \nUnix\n \naccount\n.\n\n\n/GLOW/*\n \nglow\n\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix\naccount that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in\nthe file.\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped.\nIf a VO's Unix account is missing, that VO will not be able to access your resources.\n\n\nAdditionally, if you map VOMS attributes to a non-existent user in \n/etc/grid-security/voms-mapfile\n,\n\n/usr/share/osg/voms-mapfile-default\n will be considered next to find a mapping.\nThe best way to ban a VO is edit \n/etc/grid-security/ban-voms-mapfile\n as described in \nBanning VOs\n\nbelow.\nDo not edit \nvoms-mapfile-default\n as your changes will be overwritten upon updates.\n\n\n\n\nMapping users\n\n\nTo map specific users to Unix accounts based on their certificates' DNs, create \n/etc/grid-security/grid-mapfile\n.\nAn example of the format of a \ngrid-mapfile\n follows:\n\n\n#\n \nmap\n \nMatyas\ns FNAL DN to the \nmatyas\n \nUnix\n \naccount\n\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n \nmatyas\n\n\n\n\n\n\n\n\nNote\n\n\nThe Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be\nable to access your resources.\n\n\n\n\nBanning VOs\n\n\n/etc/grid-security/ban-voms-mapfile\n is used to ban an entire VO or a role withing a VO from accessing resources on\nyour machine.\nAn example of the format of a \nban-voms-mapfile\n follows:\n\n\n#\n \nban\n \nCMS\n \nproduction\n \njobs\n\n\n/cms/Role=production/*\n\n\n\n\n\n\nEach non-commented line is a shell-style pattern which is compared against a user's VOMS attributes.\nIf the pattern matches, that user will be unable to access your resources.\n\n\n\n\nDanger\n\n\nWhen banning VOs, you must restart the services using LCMAPS VOMS authentication (e.g. \ncondor-ce\n,\n\nglobus-gridftp-server\n, etc.) to clear any authentication caches.\n\n\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-voms-mapfile\n \nmust\n exist, even if you are not banning any VOs.\nIn that case, the file should not contain any entries. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nBanning users\n\n\n/etc/grid-security/ban-mapfile\n is used to ban specific users from accessing your resources based on their\ncertificates' DNs. An example of the format of a \nban-mapfile\n follows:\n\n\n#\n \nban\n \nMatyas\ns\n \nFNAL\n \nDN\n\n\n/DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas\n\n\n\n\n\n\n\n\nDanger\n\n\nWhen banning users, you must restart the services using LCMAPS VOMS authentication (e.g. \ncondor-ce\n,\n\nglobus-gridftp-server\n, etc.) to clear any authentication caches.\n\n\n\n\n\n\nWarning\n\n\n/etc/grid-security/ban-mapfile\n \nmust\n exist, even if you are not banning any users.\nIn that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.\n\n\n\n\nMapping using all FQANs\n\n\nBy default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping.\nIf you want to consider all FQANs, you must set the appropriate option.\n\n\n\n\n\n\nIf you are using osg-configure, set \nall_fqans = True\n in \n10-misc.ini\n, then run \nosg-configure -c\n\n\n\n\n\n\nIf you are configuring \nlcmaps.db\n manually (see \nmanual configuration\n below),\n    add \n\"-all-fqans\"\n to the module definitions for \nvomsmapfile\n and \ndefaultmapfile\n\n\n\n\n\n\nUsing the LCMAPS VOMS Plugin\n\n\nLCMAPS is a software library that is called for authentication;\ntherefore, there are no running services and it does not have to be invoked manually.\n\n\nValidating the LCMAPS VOMS Plugin VO Mappings\n\n\nTo validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:\n\n\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/grid-mapfile\n, or else it will generate a false positive\n\n\nVerify your DN is \nnot\n in \n/etc/grid-security/ban-mapfile\n, or else it will generate a false negative\n\n\n\n\nInstall the \nllrun\n and \nvoms-clients\n packages:\n\n\nroot@host #\n yum install llrun voms-clients\n\n\n\n\n\n\n\n\n\nAs an unprivileged user, create a VOMS proxy (filling in \nYOUR_VO\n with a VO you are a member of):\n\n\nuser@host $\n voms-proxy-init -voms \nYOUR_VO\n\n\n\n\n\n\n\n\n\n\nVerify that your credentials are mapped as expected:\n\n\nuser@host $\n llrun -s -l \nmode\n=\npem,policy\n=\nauthorize_only,db\n=\n/etc/lcmaps.db \n\\\n\n    -p/tmp/x509up_u\n`\nid -u\n`\n\n\n\n\n\n\n\n\n\n\nIf you did not get correctly mapped, check your proxy's FQAN by running:\n\n\nuser@host $\n voms-proxy-info -fqan\n\n\n\n\n\nand make sure it matches one of the patterns in \n/etc/grid-security/voms-mapfile\n or\n\n/usr/share/osg/voms-mapfile-default\n, and does not match any patterns in \n/etc/grid-security/ban-voms-mapfile\n.\n\n\nTroubleshooting the LCMAPS VOMS Plugin\n\n\nLCMAPS logs to \njournalctl\n (EL7) or \n/var/log/messages\n (EL6) and the verbosity of the logging can be increased by\nmodifying the appropriate configuration and restarting the relevant service.\nThis section outlines the configuration necessary to raise the debug level for the different hosts that can use LCMAPS\nVOMS authentication as well as common LCMAPS VOMS authentication issues.\n\n\nHTCondor-CE hosts\n\n\nIf you are troubleshooting an HTCondor-CE host, follow these instructions to raise the LCMAPS debug level:\n\n\n\n\n\n\nAdd the following text to \n/etc/sysconfig/condor-ce\n:\n\n\nexport\n \nLCMAPS_DEBUG_LEVEL\n=\n5\n\n\n# optional (uncomment the following line to output log messages to a file):\n\n\n# export LCMAPS_LOG_FILE=/tmp/lcmaps.log\n\n\n\n\n\n\n\n\n\n\nDisable HTCondor-CE authentication caches by creating \n/etc/condor-ce/config.d/99-disablegsicache.conf\n with the\n   following contents:\n\n\nGSS_ASSIST_GRIDMAP_CACHE_EXPIRATION\n \n=\n \n0\n\n\n\n\n\n\n\n\n\n\nRestart the \ncondor-ce\n service\n\n\n\n\n\n\n\n\nTip\n\n\nAfter you've completed troubleshooting, remember to revert the changes above and restart services!\n\n\n\n\nXRootD hosts\n\n\nIf you are troubleshooting an XRootD host, follow these instructions to raise the LCMAPS debug level:\n\n\n\n\n\n\nChoose the configuration file to edit based on the following table:\n\n\n\n\n\n\n\n\nIf you are running XRootD in...\n\n\nThen modify the following file...\n\n\n\n\n\n\n\n\n\n\nStandalone mode\n\n\n/etc/xrootd/xrootd-standalone.cfg\n\n\n\n\n\n\nClustered mode\n\n\n/etc/xrootd/xrootd-clustered.cfg\n\n\n\n\n\n\n\n\n\n\n\n\nSet \n--loglevel,5\n under the \n-authzfunparms\n of the \nsec.protocol /usr/lib64 gsi\n line. For example:\n\n\nsec\n.\nprotocol\n \n/\nusr\n/\nlib64\n \ngsi\n \n-\ncertdir\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\ncertificates\n \n\\\n\n            \n-\ncert\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrootd\n/\nxrootdcert\n.\npem\n \n\\\n\n            \n-\nkey\n:\n/\netc\n/\ngrid\n-\nsecurity\n/\nxrootd\n/\nxrootdkey\n.\npem\n \n-\ncrl\n:\n1\n \n\\\n\n            \n-\nauthzfun\n:\nlibXrdLcmaps\n.\nso\n \n-\nauthzfunparms\n:\n%\nRED\n%\n--loglevel,5\n \\\n\n            \n-\ngmapopt\n:\n10\n \n-\ngmapto\n:\n0\n\n\n\n\n\n\n\n\n\n\nRestart the \nxrootd\n service\n\n\n\n\n\n\n\n\nTip\n\n\nAfter you've completed troubleshooting, remember to revert the changes above and restart services!\n\n\n\n\nGridFTP hosts\n\n\nIf you are troubleshooting a GridFTP host, follow these instructions to raise the LCMAPS debug level:\n\n\n\n\n\n\nAdd the following text to \n/etc/sysconfig/globus-gridftp-server\n:\n\n\nexport\n \nLCMAPS_DEBUG_LEVEL\n=\n5\n\n\n# optional (uncomment the following line to output log messages to a file):\n\n\n# export LCMAPS_LOG_FILE=/tmp/lcmaps.log\n\n\n\n\n\n\n\n\n\n\nRestart the \nglobus-gridftp-server\n service.\n\n\n\n\n\n\n\n\nTip\n\n\nAfter you've completed troubleshooting, remember to revert the changes above and restart services!\n\n\n\n\nCommon issues\n\n\nWrong version of GridFTP\n\n\nIf you have the EPEL version of the GridFTP server, you may see error messages in \njournalctl\n (EL7),\n\nvar/log/messages\n (EL6), or the location specified by \nLCMAPS_LOG_FILE\n.\n\n\nSymptoms\n\n\nApr\n \n11\n \n13\n:\n51\n:\n41\n \natlas\n-\nhub\n \nglobus\n-\ngridftp\n-\nserver\n: \nYou\n \nare\n \nstill\n \nroot\n \nafter\n \nthe\n \nLCMAPS\n \nexecution\n. \nThe\n \nimplicit\n \nroot\n-\nmapping\n \nsafety\n \nis\n \nenabled\n. \nSee\n \ndocumentation\n \nfor\n \ndetails\n\n\n\n\n\n\nNext actions\n\n\n\n\n\n\nIf the versions of the \nglobus-gridftp-server-*\n packages do not end in \nosgXX.elY\n, \n   continue with these instructions.\n   To check the version of your \nglobus-gridftp-server-*\n, run the following command:\n\n\nuser@host $\n rpm -qa \nglobus-gridftp*\n\n\n\n\n\n\n\n\n\n\nVerify that the \npriority\n of the OSG repositories are set\n   properly\n\n\n\n\n\n\nClean your yum cache\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nReinstall \nglobus-gridftp-server\n:\n\n\nroot@host #\n yum update globus-gridftp-server\n\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nConfiguration Files\n\n\nThe files are evaluated in the following order, with earlier files taking precedence over later ones:\n\n\n\n\n\n\n\n\nFile\n\n\nProvider\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/ban-mapfile\n\n\nAdmin\n\n\nBan DNs\n\n\n\n\n\n\n/etc/grid-security/ban-voms-mapfile\n\n\nAdmin\n\n\nBan VOs\n\n\n\n\n\n\n/etc/grid-security/grid-mapfile\n\n\nAdmin\n\n\nMap DNs\n\n\n\n\n\n\n/etc/grid-security/voms-mapfile\n\n\nAdmin\n\n\nMap VOs\n\n\n\n\n\n\n/usr/share/osg/voms-mapfile-default\n\n\nOSG\n\n\nMap VOs (default)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n/usr/share/osg/voms-mapfile-default\n is not meant to be edited and will be overwritten on upgrades.\nAll VO mappings can be overridden by editing the above files in \n/etc/grid-security\n.\n\n\n\n\nManual Configuration\n\n\nThis section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via\nosg-configure (i.e., if you prefer a configuration management system like \nAnsible\n or\n\nPuppet\n).\nTherefore, the following instructions serve as a replacement for \nthis section\n above.\n\n\nLCMAPS is configured in \n/etc/lcmaps.db\n and since the VOMS plugin is a newer component, configuration for it may not\nbe present in your existing \n/etc/lcmaps.db\n file.\n\n\n\n\n\n\nEnsure the following lines are present in the \"Module definitions\" section (the top section, before\n    \nauthorize_only\n) of \n/etc/lcmaps.db\n:\n\n\ngridmapfile\n \n=\n \nlcmaps_localaccount.mod\n\n              \n-gridmap /etc/grid-security/grid-mapfile\n\n\nbanfile\n \n=\n \nlcmaps_ban_dn.mod\n\n          \n-banmapfile /etc/grid-security/ban-mapfile\n\n\nbanvomsfile\n \n=\n \nlcmaps_ban_fqan.mod\n\n              \n-banmapfile /etc/grid-security/ban-voms-mapfile\n\n\nvomsmapfile\n \n=\n \nlcmaps_voms_localaccount.mod\n\n              \n-gridmap /etc/grid-security/voms-mapfile\n\n\ndefaultmapfile\n \n=\n \nlcmaps_voms_localaccount2.mod\n\n                 \n-gridmap /usr/share/osg/voms-mapfile-default\n\n\n\nverifyproxynokey\n \n=\n \nlcmaps_verify_proxy2.mod\n\n          \n--allow-limited-proxy\n\n          \n--discard_private_key_absence\n\n          \n -certdir /etc/grid-security/certificates\n\n\n\n\n\n\n\n\n\n\nEdit the \nauthorize_only\n section so that it contains only the following uncommented lines:\n\n\nverifyproxynokey\n \n-\n \nbanfile\n\n\nbanfile\n \n-\n \nbanvomsfile\n \n|\n \nbad\n\n\nbanvomsfile\n \n-\n \ngridmapfile\n \n|\n \nbad\n\n\ngridmapfile\n \n-\n \ngood\n \n|\n \nvomsmapfile\n\n\nvomsmapfile\n \n-\n \ngood\n \n|\n \ndefaultmapfile\n\n\ndefaultmapfile\n \n-\n \ngood\n \n|\n \nbad\n\n\n\n\n\n\n\n\n\n\nEdit \n/etc/grid-security/gsi-authz.conf\n and ensure that it contains the following line with a newline at the end:\n\n\nglobus_mapping\n \nliblcas_lcmaps_gt4_mapping\n.\nso\n \nlcmaps_callout", 
            "title": "LCMAPS VOMS authentication"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-and-maintaining-the-lcmaps-voms-plugin", 
            "text": "LCMAPS is a software library used on  HTCondor-CE ,  GridFTP , and XRootD  hosts for mapping grid certificates of incoming connections to specific\nUnix accounts.\nThe LCMAPS VOMS plugin enables LCMAPS to make mapping decisions based on the VOMS attributes of grid certificates, e.g. /cms/Role=production/Capability=NULL .\nIn OSG 3.4, the LCMAPS VOMS plugin replaced GUMS and edg-mkgridmap as the authentication method at OSG sites.  The OSG provides a default set of mappings from VOMS attributes to Unix accounts.\nBy configuring LCMAPS, you can override these mappings, including changing the Unix account that a VO is mapped to;\nadding custom mappings for specific users and VOMS attributes; and/or banning specific users and VOMS attributes.  Use this page to learn how to install and configure the LCMAPS VOMS plugin to authenticate users to access your\nresources on a per-VO basis.", 
            "title": "Installing and Maintaining the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#installing-the-lcmaps-voms-plugin", 
            "text": "To install the LCMAPS VOMS plugin, make sure that your host is up to date before installing the required packages:    Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages    Install  lcmaps , the default mapfile, and the configuration tools:  root@host #  yum install lcmaps vo-client-lcmaps-voms osg-configure-misc", 
            "title": "Installing the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuring-the-lcmaps-voms-plugin", 
            "text": "The following section describes the steps required to configure the LCMAPS VOMS plugin for authentication.\nAdditionally, there are  optional configuration  instructions if you need to make changes to\nthe default mappings, or migrate from edg-mkgridmap or GUMS.", 
            "title": "Configuring the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#supporting-mapped-vos-and-users", 
            "text": "Ensure Unix accounts exist for each VO, VO role, VO group, or user you choose to support in the  mapfiles :    Consult the default VO mappings in  /usr/share/osg/voms-mapfile-default  to determine the mapped Unix account names.\n    Each of the mapfiles has the following format:  VO, VO role, VO group or user   % RED % Unix   account % ENDCOLOR %     Create Unix accounts for each VO, VO role, VO group, and user that you wish to support.\n    The full list of VOs is located in the  OSG topology .\n    You are not expected to support all the VOs.\n    If you would like to support opportunistic usage, we recommend creating the following Unix accounts:     VO name  Unix account(s)      GLOW  glow    OSG  osg    ATLAS  usatlas3    CMS  cmsuser    Fermilab  fnalgrid    HCC  hcc    Gluex  gluex       Edit  /etc/osg/config.d/30-gip.ini  and specify the supported VOs per  Subcluster or ResourceEntry section :  allowed_vos = VO1,VO2...", 
            "title": "Supporting mapped VOs and users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#applying-configuration-settings", 
            "text": "Making changes to the OSG configuration files in the  /etc/osg/config.d  directory does not apply those settings to\nsoftware automatically.\nFor the OSG settings, use the  osg-configure  tool to validate (to a limited\nextent) and apply the settings to the relevant software components.\nIf instead you wish to manage the LCMAPS VOMS plugin configuration yourself, skip to the manual configuration section .    Make all changes to  .ini  files in the  /etc/osg/config.d  directory.   Note  This document only describes the critical settings for the LCMAPS VOMS plugin and related software.\nYou may need to configure other software that is installed on your host, too.     Validate the configuration settings:  root@host #  osg-configure -v    Once the validation command succeeds without errors, apply the configuration settings:  root@host #  osg-configure -c", 
            "title": "Applying configuration settings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#optional-configuration", 
            "text": "The following subsections contain information on migration from  edg-mkgridmap , mapping or banning users by their\ncertificates' Distinguished Names (DNs) or by their proxies' VOMS attributes.\nAny optional configuration is to be performed after the installation and configuration sections above.  For a table of the configuration files and their order of evaluation, consult the  reference section .   Migrating from edg-mkgridmap  Migrating from GUMS  Mapping VOs  Mapping users  Banning VOs  Banning users  Mapping using all FQANs", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#migrating-from-edg-mkgridmap", 
            "text": "The program edg-mkgridmap (found in the package  edg-mkgridmap ), used for authentication on HTCondor-CE, GridFTP, and\nXRootD hosts, is no longer supported by the OSG.\nThe LCMAPS VOMS plugin (package  lcmaps-plugins-voms ) now provides the same functionality.\nTo migrate from edg-mkgridmap to the LCMAPS VOMS plugin, perform the following procedure:    Configure user DN mappings:   Remove  /etc/grid-security/grid-mapfile   Check if you have a local grid mapfile:  root@host #  grep gmf_local /etc/edg-mkgridmap.conf    If the above command returns a file that exists and has contents, move it to  /etc/grid-security/grid-mapfile .      If you are converting an HTCondor-CE host, remove the HTCondor-CE  GRIDMAP  configuration. Otherwise, skip to the\n    next step.    Find where  GRIDMAP  is set:  root@host #  condor_ce_config_val -v GRIDMAP    If the above command returns a file, remove the  GRIDMAP  configuration from that file.\n   Repeat this until the command returns  Not defined: GRIDMAP .   Reconfigure HTCondor-CE: root@host #  condor_ce_reconfig      Remove edg-mkgridmap and related packages:  root@host #  yum erase edg-mkgridmap   Warning  In the output from this command, yum should  not  list other packages than the one.\nIf it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest\nOSG 3.4 versions (they should have \".osg34\" in their versions), and try again.", 
            "title": "Migrating from edg-mkgridmap"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#migrating-from-gums", 
            "text": "GUMS is no longer supported by the OSG and has been replaced by the LCMAPS VOMS plugin.\nNote that unlike GUMS, which runs on a central host, the LCMAPS VOMS plugin will run on your GUMS clients (e.g.\nHTCondor-CE, GridFTP, and XRootD).\nTo migrate any custom authentication configuration from GUMS to the LCMAPS VOMS plugin, perform the following procedure:    On your GUMS host, retrieve the conversion helper script and run it:  root@gums-host #  wget https://raw.githubusercontent.com/opensciencegrid/osg-vo-config/mapfile-generator-0.2/bin/manual-mapfile-from-gumsdb.py root@gums-host #  python manual-mapfile-from-gumsdb.py    Verify that the contents of  ban-mapfile.additions ,  grid-mapfile.additions , and  voms-mapfile.additions  include\n   any custom banned users, user mappings, and VO mappings, respectively.   Note  The above files will not include all VO mappings; the OSG provides default VO mappings in /usr/share/osg/voms-mapfile-default     On each of your client hosts (e.g. HTCondor-CE, GridFTP, XRootD), perform the following:   If you have not done so already,  install  and\n    configure  the LCMAPS VOMS plugin  Append each  .additions  file to its corresponding file in  /etc/grid-security/  (creating those files if they do\n   not exist)", 
            "title": "Migrating from GUMS"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-vos", 
            "text": "To map VOs, VO roles, or VO groups to Unix accounts based on their VOMS attributes, create  /etc/grid-security/voms-mapfile .\nAn example of the format of a  voms-mapfile  follows:  #   map   GLOW   jobs   in   the   chtc   group   to   the   glow1   Unix   account .  /GLOW/chtc/*   glow1  #   map   GLOW   jobs   with   the   htpc   role   to   the   glow2   Unix   account .  /GLOW/Role=htpc/*   glow2  #   map   other   GLOW   jobs   to   the   glow   Unix   account .  /GLOW/*   glow   Each non-commented line is a shell-style pattern which is compared against the user's VOMS attributes, and a Unix\naccount that the user will be mapped to if the pattern matches.\nThe patterns are compared in the order they are listed in. Therefore, more general patterns should be placed later in\nthe file.   Note  The Unix account must exist for the user to be mapped.\nIf a VO's Unix account is missing, that VO will not be able to access your resources.  Additionally, if you map VOMS attributes to a non-existent user in  /etc/grid-security/voms-mapfile , /usr/share/osg/voms-mapfile-default  will be considered next to find a mapping.\nThe best way to ban a VO is edit  /etc/grid-security/ban-voms-mapfile  as described in  Banning VOs \nbelow.\nDo not edit  voms-mapfile-default  as your changes will be overwritten upon updates.", 
            "title": "Mapping VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-users", 
            "text": "To map specific users to Unix accounts based on their certificates' DNs, create  /etc/grid-security/grid-mapfile .\nAn example of the format of a  grid-mapfile  follows:  #   map   Matyas s FNAL DN to the  matyas   Unix   account  /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas   matyas    Note  The Unix account must exist for the user to be mapped. If a user's Unix account is missing, that user will not be\nable to access your resources.", 
            "title": "Mapping users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-vos", 
            "text": "/etc/grid-security/ban-voms-mapfile  is used to ban an entire VO or a role withing a VO from accessing resources on\nyour machine.\nAn example of the format of a  ban-voms-mapfile  follows:  #   ban   CMS   production   jobs  /cms/Role=production/*   Each non-commented line is a shell-style pattern which is compared against a user's VOMS attributes.\nIf the pattern matches, that user will be unable to access your resources.   Danger  When banning VOs, you must restart the services using LCMAPS VOMS authentication (e.g.  condor-ce , globus-gridftp-server , etc.) to clear any authentication caches.    Warning  /etc/grid-security/ban-voms-mapfile   must  exist, even if you are not banning any VOs.\nIn that case, the file should not contain any entries. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning VOs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#banning-users", 
            "text": "/etc/grid-security/ban-mapfile  is used to ban specific users from accessing your resources based on their\ncertificates' DNs. An example of the format of a  ban-mapfile  follows:  #   ban   Matyas s   FNAL   DN  /DC=gov/DC=fnal/O=Fermilab/OU=People/CN=Matyas Selmeci/CN=UID:matyas    Danger  When banning users, you must restart the services using LCMAPS VOMS authentication (e.g.  condor-ce , globus-gridftp-server , etc.) to clear any authentication caches.    Warning  /etc/grid-security/ban-mapfile   must  exist, even if you are not banning any users.\nIn that case, the file should be blank. If the file does not exist, LCMAPS will ban every user.", 
            "title": "Banning users"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#mapping-using-all-fqans", 
            "text": "By default, the LCMAPS VOMS plugin only considers the first FQAN of a VOMS proxy for mapping.\nIf you want to consider all FQANs, you must set the appropriate option.    If you are using osg-configure, set  all_fqans = True  in  10-misc.ini , then run  osg-configure -c    If you are configuring  lcmaps.db  manually (see  manual configuration  below),\n    add  \"-all-fqans\"  to the module definitions for  vomsmapfile  and  defaultmapfile", 
            "title": "Mapping using all FQANs"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#using-the-lcmaps-voms-plugin", 
            "text": "LCMAPS is a software library that is called for authentication;\ntherefore, there are no running services and it does not have to be invoked manually.", 
            "title": "Using the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#validating-the-lcmaps-voms-plugin-vo-mappings", 
            "text": "To validate the LCMAPS VOMS plugin by itself, use the following procedure to test mapping your own cert to a user:   Verify your DN is  not  in  /etc/grid-security/grid-mapfile , or else it will generate a false positive  Verify your DN is  not  in  /etc/grid-security/ban-mapfile , or else it will generate a false negative   Install the  llrun  and  voms-clients  packages:  root@host #  yum install llrun voms-clients    As an unprivileged user, create a VOMS proxy (filling in  YOUR_VO  with a VO you are a member of):  user@host $  voms-proxy-init -voms  YOUR_VO     Verify that your credentials are mapped as expected:  user@host $  llrun -s -l  mode = pem,policy = authorize_only,db = /etc/lcmaps.db  \\ \n    -p/tmp/x509up_u ` id -u `     If you did not get correctly mapped, check your proxy's FQAN by running:  user@host $  voms-proxy-info -fqan  and make sure it matches one of the patterns in  /etc/grid-security/voms-mapfile  or /usr/share/osg/voms-mapfile-default , and does not match any patterns in  /etc/grid-security/ban-voms-mapfile .", 
            "title": "Validating the LCMAPS VOMS Plugin VO Mappings"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#troubleshooting-the-lcmaps-voms-plugin", 
            "text": "LCMAPS logs to  journalctl  (EL7) or  /var/log/messages  (EL6) and the verbosity of the logging can be increased by\nmodifying the appropriate configuration and restarting the relevant service.\nThis section outlines the configuration necessary to raise the debug level for the different hosts that can use LCMAPS\nVOMS authentication as well as common LCMAPS VOMS authentication issues.", 
            "title": "Troubleshooting the LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#htcondor-ce-hosts", 
            "text": "If you are troubleshooting an HTCondor-CE host, follow these instructions to raise the LCMAPS debug level:    Add the following text to  /etc/sysconfig/condor-ce :  export   LCMAPS_DEBUG_LEVEL = 5  # optional (uncomment the following line to output log messages to a file):  # export LCMAPS_LOG_FILE=/tmp/lcmaps.log     Disable HTCondor-CE authentication caches by creating  /etc/condor-ce/config.d/99-disablegsicache.conf  with the\n   following contents:  GSS_ASSIST_GRIDMAP_CACHE_EXPIRATION   =   0     Restart the  condor-ce  service     Tip  After you've completed troubleshooting, remember to revert the changes above and restart services!", 
            "title": "HTCondor-CE hosts"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#xrootd-hosts", 
            "text": "If you are troubleshooting an XRootD host, follow these instructions to raise the LCMAPS debug level:    Choose the configuration file to edit based on the following table:     If you are running XRootD in...  Then modify the following file...      Standalone mode  /etc/xrootd/xrootd-standalone.cfg    Clustered mode  /etc/xrootd/xrootd-clustered.cfg       Set  --loglevel,5  under the  -authzfunparms  of the  sec.protocol /usr/lib64 gsi  line. For example:  sec . protocol   / usr / lib64   gsi   - certdir : / etc / grid - security / certificates   \\ \n             - cert : / etc / grid - security / xrootd / xrootdcert . pem   \\ \n             - key : / etc / grid - security / xrootd / xrootdkey . pem   - crl : 1   \\ \n             - authzfun : libXrdLcmaps . so   - authzfunparms : % RED % --loglevel,5  \\ \n             - gmapopt : 10   - gmapto : 0     Restart the  xrootd  service     Tip  After you've completed troubleshooting, remember to revert the changes above and restart services!", 
            "title": "XRootD hosts"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#gridftp-hosts", 
            "text": "If you are troubleshooting a GridFTP host, follow these instructions to raise the LCMAPS debug level:    Add the following text to  /etc/sysconfig/globus-gridftp-server :  export   LCMAPS_DEBUG_LEVEL = 5  # optional (uncomment the following line to output log messages to a file):  # export LCMAPS_LOG_FILE=/tmp/lcmaps.log     Restart the  globus-gridftp-server  service.     Tip  After you've completed troubleshooting, remember to revert the changes above and restart services!", 
            "title": "GridFTP hosts"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#common-issues", 
            "text": "", 
            "title": "Common issues"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#wrong-version-of-gridftp", 
            "text": "If you have the EPEL version of the GridFTP server, you may see error messages in  journalctl  (EL7), var/log/messages  (EL6), or the location specified by  LCMAPS_LOG_FILE .  Symptoms  Apr   11   13 : 51 : 41   atlas - hub   globus - gridftp - server :  You   are   still   root   after   the   LCMAPS   execution .  The   implicit   root - mapping   safety   is   enabled .  See   documentation   for   details   Next actions    If the versions of the  globus-gridftp-server-*  packages do not end in  osgXX.elY , \n   continue with these instructions.\n   To check the version of your  globus-gridftp-server-* , run the following command:  user@host $  rpm -qa  globus-gridftp*     Verify that the  priority  of the OSG repositories are set\n   properly    Clean your yum cache  root@host #  yum clean all --enablerepo = *    Reinstall  globus-gridftp-server :  root@host #  yum update globus-gridftp-server", 
            "title": "Wrong version of GridFTP"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#configuration-files", 
            "text": "The files are evaluated in the following order, with earlier files taking precedence over later ones:     File  Provider  Purpose      /etc/grid-security/ban-mapfile  Admin  Ban DNs    /etc/grid-security/ban-voms-mapfile  Admin  Ban VOs    /etc/grid-security/grid-mapfile  Admin  Map DNs    /etc/grid-security/voms-mapfile  Admin  Map VOs    /usr/share/osg/voms-mapfile-default  OSG  Map VOs (default)      Warning  /usr/share/osg/voms-mapfile-default  is not meant to be edited and will be overwritten on upgrades.\nAll VO mappings can be overridden by editing the above files in  /etc/grid-security .", 
            "title": "Configuration Files"
        }, 
        {
            "location": "/security/lcmaps-voms-authentication/#manual-configuration", 
            "text": "This section is intended for use as reference if you choose to forego configuring the LCMAPS VOMS plugin via\nosg-configure (i.e., if you prefer a configuration management system like  Ansible  or Puppet ).\nTherefore, the following instructions serve as a replacement for  this section  above.  LCMAPS is configured in  /etc/lcmaps.db  and since the VOMS plugin is a newer component, configuration for it may not\nbe present in your existing  /etc/lcmaps.db  file.    Ensure the following lines are present in the \"Module definitions\" section (the top section, before\n     authorize_only ) of  /etc/lcmaps.db :  gridmapfile   =   lcmaps_localaccount.mod \n               -gridmap /etc/grid-security/grid-mapfile  banfile   =   lcmaps_ban_dn.mod \n           -banmapfile /etc/grid-security/ban-mapfile  banvomsfile   =   lcmaps_ban_fqan.mod \n               -banmapfile /etc/grid-security/ban-voms-mapfile  vomsmapfile   =   lcmaps_voms_localaccount.mod \n               -gridmap /etc/grid-security/voms-mapfile  defaultmapfile   =   lcmaps_voms_localaccount2.mod \n                  -gridmap /usr/share/osg/voms-mapfile-default  verifyproxynokey   =   lcmaps_verify_proxy2.mod \n           --allow-limited-proxy \n           --discard_private_key_absence \n            -certdir /etc/grid-security/certificates     Edit the  authorize_only  section so that it contains only the following uncommented lines:  verifyproxynokey   -   banfile  banfile   -   banvomsfile   |   bad  banvomsfile   -   gridmapfile   |   bad  gridmapfile   -   good   |   vomsmapfile  vomsmapfile   -   good   |   defaultmapfile  defaultmapfile   -   good   |   bad     Edit  /etc/grid-security/gsi-authz.conf  and ensure that it contains the following line with a newline at the end:  globus_mapping   liblcas_lcmaps_gt4_mapping . so   lcmaps_callout", 
            "title": "Manual Configuration"
        }, 
        {
            "location": "/security/host-certs/", 
            "text": "Host Certificates\n\n\n\n\nNote\n\n\nThis document describes how to get \nhost\n certificates.\nFor instructions on how to get \nuser\n certificates, see the \nUser Certificates document\n.\n\n\n\n\nHost certificates are \nX.509 certificates\n that are used to securely identify\nservers and to establish encrypted connections between services and clients.\nIn the OSG, some grid resources (e.g., HTCondor-CE, XRootD, GridFTP) require host certificates.\nIf you are unsure if your host needs a host certificate, please consult the installation instructions for the software\nyou are interested in installing.\n\n\nTo acquire a host certificate, you must submit a request to a Certificate Authority (CA).\nWe recommend requesting host certificates from one of the following CAs:\n\n\n\n\n\n\nInCommon IGTF\n:\n  an IGTF-accredited CA for services that interact with the WLCG;\n  requires a subscription, generally held by an institution\n\n\n\n\nImportant\n\n\nFor integration with the OSG, InCommon host certificates must be issued by the\n\nIGTF CA\n\nand not the InCommon RSA CA.\n\n\n\n\n\n\n\n\nLet's Encrypt\n:\n  a free, automated, and open CA frequently used for web services;\n  see the \nsecurity team's position on Let's Encrypt\n\n  for more details.\n  Let's Encrypt is not IGTF-accredited so their certificates are not suitable for WLCG services.\n\n\n\n\n\n\nIf neither of the above options work for your site, the OSG also accepts all\n\nIGTF-accredited CAs\n.\n\n\nBefore Starting\n\n\nBefore requesting a new host certificate, use \nopenssl\n to check if your host already has a valid certificate, i.e. the\npresent is between \nnotBefore\n and \nnotAfter\n dates and times.\nIf so, you may safely skip this document:\n\n\nuser@host $\n openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n\nsubject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org\n\n\nissuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1\n\n\nnotBefore=Jan  4 21:08:09 2010 GMT\n\n\nnotAfter=Jan  4 21:08:09 2011 GMT\n\n\n\n\n\n\nIf you are using OpenSSL 1.1, you may notice minor formatting differences.\n\n\nRequesting InCommon IGTF Host Certificates\n\n\nMany institutions in the United States already subscribe to InCommon and offer IGTF certificate services.\nIf your institution is in the list of \nInCommon subscribers\n,\ncontinue with the instructions below.\nIf your institution is not in the list, Let's Encrypt certificates do not meet your needs, and you do not have access to\nanother IGTF CA subscription, please \ncontact us\n.\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nFrom a host that meets the above requirements, there are two options to get InCommon IGTF-accredited host certificates:\n\n\n\n\nRequesting certificates from a Registration Authority (RA)\n:\n   This requires a Certificate Signing Request (CSR), which can be generated with the \nosg-cert-request\n tool.\n\n\nRequesting certificates as an RA\n:\n   As an RA, you can request, approve, and retrieve certificates yourself through the InCommon REST API using the\n   \nosg-incommon-cert-request\n tool .\n\n\n\n\nInstall the \nosg-pki-tools\n where both command line tools are available:\n\n\nroot@host #\n yum install osg-pki-tools\n\n\n\n\n\nRequesting certificates from a registration authority\n\n\n\n\n\n\nGenerate a Certificate Signing Request (CSR) and private key using the \nosg-cert-request\n tool:\n\n\nuser@host $\n osg-cert-request --hostname \nHOSTNAME\n \n\\\n\n             --country \nCOUNTRY\n \n\\\n\n             --state \nSTATE\n \n\\\n\n             --locality \nLOCALITY\n \n\\\n\n             --organization \nORGANIZATION\n\n\n\n\n\n\nYou may also add \nDNS Subject Alternative Names\n (SAN) to\nthe request by specifying any number of \n--altname \nSAN\n.\nFor example, the following generates a CSR for \ntest.opensciencegrid.org\n with \nfoo.opensciencegrid.org\n and\n\nbar.opensciencegrid.org\n as SANs:\n\n\nuser@host $\n osg-cert-request --hostname test.opensciencegrid.org \n\\\n\n             --country US \n\\\n\n             --state Wisconsin \n\\\n\n             --locality Madison \n\\\n\n             --organization \nUniversity of Wisconsin\n \n\\\n\n             --altname foo.opensciencegrid.org \n\\\n\n             --altname bar.opensciencegrid.org\n\n\n\n\n\nIf successful, the CSR will be named \nHOSTNAME\n.req\n and the private key will be named \nHOSTNAME\n-key.pem\n.\nAdditional options and descriptions can be found \nhere\n.\n\n\n\n\n\n\nFind your institution-specific InCommon contact\n   (e.g. \nUW-Madison InCommon contact\n\n   submit the CSR that you generated above, and request a 1-year \nIGTF Server Certificate\n for \nOTHER\n server software.\n\n\n\n\nAfter the certificate has been issued by your institution, download the host certificate only (not the full chain) to\n   its intended host and copy over the key you generated above.\n\n\n\n\nVerify that the issuer \nCN\n field is \nInCommon IGTF Server CA\n:\n\n\n$\n openssl x509 -in \nPATH TO CERTIFICATE\n -noout -issuer\n\nissuer= /C=US/O=Internet2/OU=InCommon/CN=InCommon IGTF Server CA\n\n\n\n\n\n\n\n\n\n\nInstall the host certificate and key:\n\n\nroot@host #\n cp \nPATH TO CERTIFICATE\n /etc/grid-security/hostcert.pem\n\nroot@host #\n chmod \n444\n /etc/grid-security/hostcert.pem\n\nroot@host #\n cp \nPATH TO KEY\n /etc/grid-security/hostkey.pem\n\nroot@host #\n chmod \n400\n /etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\n\nRequesting certificates as a registration authority\n\n\nIf you are a Registration Authority for your institution, skip ahead to \nthis section\n.\nIf you are not already a Registration Authority (RA) for your institution, you must request to be made one:\n\n\n\n\nFind your institution-specific InCommon contact\n   (e.g. \nUW-Madison InCommon contact\n),\n\n\n\n\nRequest a Department Registration Authority user with SSL auto-approve enabled and a client certificate:\n\n\n\n\nIf they do not grant your request, you will not be able to request, approve, and retrieve certificates yourself.\n  Instead, you must \nrequest certificates from your RA\n.\n\n\nIf they grant your request, you will receive an email with instructions for requesting your client certificate.\n  Download the \n.p12\n file and extract the certificate and key:\nuser@host $\n openssl pkcs12 -in incommon_file.p12 -nocerts -out ~/path_to_dir/incommon_user_key.pem\n\nuser@host $\n openssl pkcs12 -in incommon_file.p12 -nokeys -out ~/path_to_dir/incommon_user_cert.pem\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind your institution-specific organization and department codes at the InCommon Cert Manager (https://cert-manager.com/customer/InCommon).\n   These are numeric codes that should be specified through the command line using the -O/--orgcode ORG,DEPT option:\n\n\n\n\nOrganization code is shown as OrgID under Settings \n Organizations \n Edit\n\n\nDepartment code is shown as OrgID under Settings \n Organizations \n Departments \n Edit\n\n\n\n\n\n\n\n\nOnce you have RA privileges, you may request, approve, and retrieve host certificates using \nosg-incommon-cert-request\n:\n\n\n\n\n\n\n\n\nRequesting a certificate with a single hostname \nHOSTNAME\n:\n\n\nuser@host $\n osg-incommon-cert-request --username \nINCOMMON_LOGIN\n \n\\\n\n            --cert ~/path_to_dir/incommon_user_cert.pem \n\\\n\n            --pkey ~/path_to_dir/incommon_user_key.pem \n\\ \n\n\n            --hostname \nHOSTNAME\n\n\n            [--orgcode \nORG,DEPT\n]\n\n\n\n\n\n\n\n\n\n\nRequesting a certificate with Subject Alternative Names (SANs):\n\n\nuser@host $\n osg-incommon-cert-request --username \nINCOMMON_LOGIN\n \n\\\n\n            --cert ~/path_to_dir/incommon_user_cert.pem \n\\\n\n            --pkey ~/path_to_dir/incommon_user_key.pem \n\\\n\n            --hostname \nHOSTNAME\n \n\\\n\n            --altname \nALTNAME\n \n\\\n\n            --altname \nALTNAME2\n\n\n            [--orgcode \nORG,DEPT\n]\n\n\n\n\n\n\n\n\n\n\nRequesting certificates in bulk using a hostfile name:\n\n\nuser@host $\n osg-incommon-cert-request --username \nINCOMMON_LOGIN\n \n\\\n\n            --cert ~/path_to_dir/incommon_user_cert.pem \n\\\n\n            --pkey ~/path_to_dir/incommon_user_key.pem \n\\\n\n            --hostfile ~/path_to_file/hostfile.txt \n\\\n\n            \n[\n--orgcode \nORG,DEPT\n]\n\n\n\n\n\n\nWhere the contents of \nhostfile.txt\n contain one hostname and any number of SANs per line:\n\n\nhostname01.yourdomain\n\n\nhostname02.yourdomain hostnamealias.yourdomain hostname03.yourdomain\n\n\nhostname04.yourdomain hostname05.yourdomain\n\n\n\n\n\n\n\n\n\n\nRequesting Host Certificates Using \nLet's Encrypt\n\n\nLet's Encrypt\n is a free, automated, and open CA frequently used for web services;\nsee the \nsecurity team's position on Let's Encrypt\n\nfor more details.\nLet's Encrypt can be used to obtain host certificates as an alternative to InCommon if your institution does not have\nan InCommon subscription.\n\n\n\n\n\n\nInstall the \ncertbot\n package (available from the EPEL 7 repository):\n\n\nroot@host #\n yum install certbot\n\n\n\n\n\n\n\n\n\nIf you have any service running on port 80, you will have to disable it temporarily to obtain certificates, as Let's\n   Encrypt needs to bind on it temporarily in order to verify the host.\n   For instance, if you already have an HTCondor-CE set up with the\n   \nHTCondor-CE View service\n\n   running, stop the HTCondor-CE View service, as it listens on port 80.\n\n\n\n\n\n\nRun the following command to obtain the host certificate with Let's Encrypt:\n\n\nroot@host #\n certbot certonly --standalone --email \nADMIN_EMAIL\n -d \nHOST\n\n\n\n\n\n\n\n\n\n\nSet up hostcert/hostkey links:\n\n\nroot@host #\n ln -s /etc/letsencrypt/live/*/cert.pem /etc/grid-security/hostcert.pem\n\nroot@host #\n ln -s /etc/letsencrypt/live/*/privkey.pem /etc/grid-security/hostkey.pem\n\nroot@host #\n chmod \n0600\n /etc/letsencrypt/archive/*/privkey*.pem\n\n\n\n\n\n\n\n\n\nRenewing Let's Encrypt host certificates\n\n\nBefore the host certificate expires, you can renew it with the following command:\n\n\nroot@host #\n certbot renew\n\n\n\n\n\n\n\nNote\n\n\n\n\nRenewing a certificate requires you to temporarily disable services running on port 80 so that\n   certbot can bind to it to verify the host.\n\n\nTo automate the renewal process, you need to choose between using a cron job (SL6 and SL7 hosts) and a systemd timer\n(SL7 hosts only).\nThe two sections below outline both methods for automatically renewing your certificate.\n\n\nAutomating renewals using cron\n\n\nTo automate a monthly renewal with a cron job; you can create \n/etc/cron.d/certbot-renew\n with the following\ncontents:\n\n\n* * 1 * * root certbot renew\n\n\n\n\n\n\nAutomating renewals using systemd timers\n\n\nTo automate a monthly  renewal using systemd, you'll need to create two files.\nThe first is a service file that tells systemd how to invoke certbot.\nThe second is to generate a timer file that tells systemd how often to run the service.\nThe steps to setup the timer are as follows:\n\n\n\n\n\n\nCreate a service file called \n/etc/systemd/system/certbot.service\n with the following contents\n\n\n[Unit]\n\n\nDescription\n=\nLet\ns Encrypt renewal\n\n\n\n[Service]\n\n\nType\n=\noneshot\n\n\nExecStart\n=\n/usr/bin/certbot renew --quiet --agree-tos\n\n\n\n\n\n\n\n\n\n\nOnce the certbot service is working correctly, you will need to create the timer file.\n   Create the timer file at \n/etc/systemd/system/certbot.timer\n) with the following contents:\n\n\n[Unit]\n\n\nDescription\n=\nTwice daily renewal of Let\ns Encrypt\ns certificates\n\n\n\n[Timer]\n\n\nOnCalendar\n=\n0/12:00:00\n\n\nRandomizedDelaySec\n=\n1h\n\n\nPersistent\n=\ntrue\n\n\n\n[Install]\n\n\nWantedBy\n=\ntimers.target\n\n\n\n\n\n\n\n\n\n\nUpdate the systemd manager configuration:\n\n\nroot@host #\n systemctl daemon-reload\n\n\n\n\n\n\n\n\n\nStart and enable the certbot service and timer:\n\n\nroot@host #\n systemctl start certbot.service\n\nroot@host #\n systemctl \nenable\n certbot.service\n\nroot@host #\n systemctl start certbot.timer\n\nroot@host #\n systemctl \nenable\n certbot.timer\n\n\n\n\n\n\n\n\n\nYou can verify that the timer is active by running \nsystemctl list-timers\n.\n\n\n\n\nNote\n\n\nVerify that the service has started correctly by running \nsystemctl status certbot.service\n. The timer may fail \nwithout warnings if the service does not run correctly.\n\n\n\n\nRequesting Service Certificates\n\n\nPreviously, the OSG recommended using separate X.509 certificates, called \"service certificates\", for each grid service\non a host.\nThis practice has become less popular as sites have separated SSL-requiring services to their own hosts.\n\n\nIn the case where your host is only running a single service that requires a service certificate, we recommend using\nyour \nhost certificate\n as your service certificate.\nEnsure that the ownership of the host certificate and key are appropriate for the service you are running.\n\n\nIf you are running multiple services that require host certificates, we recommend requesting a certificate whose\nCommonName is \nservice\n-hostname\n and has the hostname in the list of subject alternative names.\n\n\nFrequently Asked Questions\n\n\nCan I use any host to request a certificate for a different host?\n\n\nYES, you can use any host to create a certificate signing request as long as the hostname for the certificate is a fully\nqualified domain name.\n\n\nHow do I renew a host certificate?\n\n\nFor Let's Encrypt certificates, see \nthis section\n\n\nFor other certificates, there is no separate renewal procedure.\nInstead, request a new certificate using one of the methods above.\n\n\nHow can I check if I have a host certificate installed already?\n\n\nBy default the host certificate key pair will be installed in \n/etc/grid-security/hostcert.pem\n and\n\n/etc/grid-security/hostkey.pem\n.\nYou can use \nopenssl\n to access basic information about the certificate:\n\n\nroot@host #\n openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n\nsubject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org\n\n\nissuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1\n\n\nnotBefore=Apr  8 00:00:00 2013 GMT\n\n\nnotAfter=May 17 12:00:00 2014 GMT\n\n\n\n\n\n\nHow can I check the expiration time of my installed host certificate?\n\n\nUse the following \nopenssl\n command to find the dates that your host certificate is valid:\n\n\nroot@host #\n openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout\n\nnotBefore=Jan  4 21:08:41 2010 GMT\n\n\nnotAfter=Jan  4 21:08:41 2011 GMT\n\n\n\n\n\n\nReferences\n\n\n\n\nCILogon documentation for requesting InCommon certificates\n\n\n\n\nUseful OpenSSL commands (from NCSA)\n - e.g. how to convert the format of your certificate.\n\n\n\n\n\n\nOfficial Let's Encrypt setup guide\n\n\n\n\n\n\nAnother \nLet's Encrypt setup reference\n\n    Under Getting your host certificate, we follow the first \"Setting up\" section.", 
            "title": "Host Certificates"
        }, 
        {
            "location": "/security/host-certs/#host-certificates", 
            "text": "Note  This document describes how to get  host  certificates.\nFor instructions on how to get  user  certificates, see the  User Certificates document .   Host certificates are  X.509 certificates  that are used to securely identify\nservers and to establish encrypted connections between services and clients.\nIn the OSG, some grid resources (e.g., HTCondor-CE, XRootD, GridFTP) require host certificates.\nIf you are unsure if your host needs a host certificate, please consult the installation instructions for the software\nyou are interested in installing.  To acquire a host certificate, you must submit a request to a Certificate Authority (CA).\nWe recommend requesting host certificates from one of the following CAs:    InCommon IGTF :\n  an IGTF-accredited CA for services that interact with the WLCG;\n  requires a subscription, generally held by an institution   Important  For integration with the OSG, InCommon host certificates must be issued by the IGTF CA \nand not the InCommon RSA CA.     Let's Encrypt :\n  a free, automated, and open CA frequently used for web services;\n  see the  security team's position on Let's Encrypt \n  for more details.\n  Let's Encrypt is not IGTF-accredited so their certificates are not suitable for WLCG services.    If neither of the above options work for your site, the OSG also accepts all IGTF-accredited CAs .", 
            "title": "Host Certificates"
        }, 
        {
            "location": "/security/host-certs/#before-starting", 
            "text": "Before requesting a new host certificate, use  openssl  to check if your host already has a valid certificate, i.e. the\npresent is between  notBefore  and  notAfter  dates and times.\nIf so, you may safely skip this document:  user@host $  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org  issuer=/DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1  notBefore=Jan  4 21:08:09 2010 GMT  notAfter=Jan  4 21:08:09 2011 GMT   If you are using OpenSSL 1.1, you may notice minor formatting differences.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/security/host-certs/#requesting-incommon-igtf-host-certificates", 
            "text": "Many institutions in the United States already subscribe to InCommon and offer IGTF certificate services.\nIf your institution is in the list of  InCommon subscribers ,\ncontinue with the instructions below.\nIf your institution is not in the list, Let's Encrypt certificates do not meet your needs, and you do not have access to\nanother IGTF CA subscription, please  contact us .  As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories   From a host that meets the above requirements, there are two options to get InCommon IGTF-accredited host certificates:   Requesting certificates from a Registration Authority (RA) :\n   This requires a Certificate Signing Request (CSR), which can be generated with the  osg-cert-request  tool.  Requesting certificates as an RA :\n   As an RA, you can request, approve, and retrieve certificates yourself through the InCommon REST API using the\n    osg-incommon-cert-request  tool .   Install the  osg-pki-tools  where both command line tools are available:  root@host #  yum install osg-pki-tools", 
            "title": "Requesting InCommon IGTF Host Certificates"
        }, 
        {
            "location": "/security/host-certs/#requesting-certificates-from-a-registration-authority", 
            "text": "Generate a Certificate Signing Request (CSR) and private key using the  osg-cert-request  tool:  user@host $  osg-cert-request --hostname  HOSTNAME   \\ \n             --country  COUNTRY   \\ \n             --state  STATE   \\ \n             --locality  LOCALITY   \\ \n             --organization  ORGANIZATION   You may also add  DNS Subject Alternative Names  (SAN) to\nthe request by specifying any number of  --altname  SAN .\nFor example, the following generates a CSR for  test.opensciencegrid.org  with  foo.opensciencegrid.org  and bar.opensciencegrid.org  as SANs:  user@host $  osg-cert-request --hostname test.opensciencegrid.org  \\ \n             --country US  \\ \n             --state Wisconsin  \\ \n             --locality Madison  \\ \n             --organization  University of Wisconsin   \\ \n             --altname foo.opensciencegrid.org  \\ \n             --altname bar.opensciencegrid.org  If successful, the CSR will be named  HOSTNAME .req  and the private key will be named  HOSTNAME -key.pem .\nAdditional options and descriptions can be found  here .    Find your institution-specific InCommon contact\n   (e.g.  UW-Madison InCommon contact \n   submit the CSR that you generated above, and request a 1-year  IGTF Server Certificate  for  OTHER  server software.   After the certificate has been issued by your institution, download the host certificate only (not the full chain) to\n   its intended host and copy over the key you generated above.   Verify that the issuer  CN  field is  InCommon IGTF Server CA :  $  openssl x509 -in  PATH TO CERTIFICATE  -noout -issuer issuer= /C=US/O=Internet2/OU=InCommon/CN=InCommon IGTF Server CA     Install the host certificate and key:  root@host #  cp  PATH TO CERTIFICATE  /etc/grid-security/hostcert.pem root@host #  chmod  444  /etc/grid-security/hostcert.pem root@host #  cp  PATH TO KEY  /etc/grid-security/hostkey.pem root@host #  chmod  400  /etc/grid-security/hostkey.pem", 
            "title": "Requesting certificates from a registration authority"
        }, 
        {
            "location": "/security/host-certs/#requesting-certificates-as-a-registration-authority", 
            "text": "If you are a Registration Authority for your institution, skip ahead to  this section .\nIf you are not already a Registration Authority (RA) for your institution, you must request to be made one:   Find your institution-specific InCommon contact\n   (e.g.  UW-Madison InCommon contact ),   Request a Department Registration Authority user with SSL auto-approve enabled and a client certificate:   If they do not grant your request, you will not be able to request, approve, and retrieve certificates yourself.\n  Instead, you must  request certificates from your RA .  If they grant your request, you will receive an email with instructions for requesting your client certificate.\n  Download the  .p12  file and extract the certificate and key: user@host $  openssl pkcs12 -in incommon_file.p12 -nocerts -out ~/path_to_dir/incommon_user_key.pem user@host $  openssl pkcs12 -in incommon_file.p12 -nokeys -out ~/path_to_dir/incommon_user_cert.pem      Find your institution-specific organization and department codes at the InCommon Cert Manager (https://cert-manager.com/customer/InCommon).\n   These are numeric codes that should be specified through the command line using the -O/--orgcode ORG,DEPT option:   Organization code is shown as OrgID under Settings   Organizations   Edit  Department code is shown as OrgID under Settings   Organizations   Departments   Edit     Once you have RA privileges, you may request, approve, and retrieve host certificates using  osg-incommon-cert-request :     Requesting a certificate with a single hostname  HOSTNAME :  user@host $  osg-incommon-cert-request --username  INCOMMON_LOGIN   \\ \n            --cert ~/path_to_dir/incommon_user_cert.pem  \\ \n            --pkey ~/path_to_dir/incommon_user_key.pem  \\               --hostname  HOSTNAME              [--orgcode  ORG,DEPT ]     Requesting a certificate with Subject Alternative Names (SANs):  user@host $  osg-incommon-cert-request --username  INCOMMON_LOGIN   \\ \n            --cert ~/path_to_dir/incommon_user_cert.pem  \\ \n            --pkey ~/path_to_dir/incommon_user_key.pem  \\ \n            --hostname  HOSTNAME   \\ \n            --altname  ALTNAME   \\ \n            --altname  ALTNAME2              [--orgcode  ORG,DEPT ]     Requesting certificates in bulk using a hostfile name:  user@host $  osg-incommon-cert-request --username  INCOMMON_LOGIN   \\ \n            --cert ~/path_to_dir/incommon_user_cert.pem  \\ \n            --pkey ~/path_to_dir/incommon_user_key.pem  \\ \n            --hostfile ~/path_to_file/hostfile.txt  \\ \n             [ --orgcode  ORG,DEPT ]   Where the contents of  hostfile.txt  contain one hostname and any number of SANs per line:  hostname01.yourdomain  hostname02.yourdomain hostnamealias.yourdomain hostname03.yourdomain  hostname04.yourdomain hostname05.yourdomain", 
            "title": "Requesting certificates as a registration authority"
        }, 
        {
            "location": "/security/host-certs/#requesting-host-certificates-using-lets-encrypt", 
            "text": "Let's Encrypt  is a free, automated, and open CA frequently used for web services;\nsee the  security team's position on Let's Encrypt \nfor more details.\nLet's Encrypt can be used to obtain host certificates as an alternative to InCommon if your institution does not have\nan InCommon subscription.    Install the  certbot  package (available from the EPEL 7 repository):  root@host #  yum install certbot    If you have any service running on port 80, you will have to disable it temporarily to obtain certificates, as Let's\n   Encrypt needs to bind on it temporarily in order to verify the host.\n   For instance, if you already have an HTCondor-CE set up with the\n    HTCondor-CE View service \n   running, stop the HTCondor-CE View service, as it listens on port 80.    Run the following command to obtain the host certificate with Let's Encrypt:  root@host #  certbot certonly --standalone --email  ADMIN_EMAIL  -d  HOST     Set up hostcert/hostkey links:  root@host #  ln -s /etc/letsencrypt/live/*/cert.pem /etc/grid-security/hostcert.pem root@host #  ln -s /etc/letsencrypt/live/*/privkey.pem /etc/grid-security/hostkey.pem root@host #  chmod  0600  /etc/letsencrypt/archive/*/privkey*.pem", 
            "title": "Requesting Host Certificates Using Let's Encrypt"
        }, 
        {
            "location": "/security/host-certs/#renewing-lets-encrypt-host-certificates", 
            "text": "Before the host certificate expires, you can renew it with the following command:  root@host #  certbot renew   Note   Renewing a certificate requires you to temporarily disable services running on port 80 so that\n   certbot can bind to it to verify the host.  To automate the renewal process, you need to choose between using a cron job (SL6 and SL7 hosts) and a systemd timer\n(SL7 hosts only).\nThe two sections below outline both methods for automatically renewing your certificate.", 
            "title": "Renewing Let's Encrypt host certificates"
        }, 
        {
            "location": "/security/host-certs/#automating-renewals-using-cron", 
            "text": "To automate a monthly renewal with a cron job; you can create  /etc/cron.d/certbot-renew  with the following\ncontents:  * * 1 * * root certbot renew", 
            "title": "Automating renewals using cron"
        }, 
        {
            "location": "/security/host-certs/#automating-renewals-using-systemd-timers", 
            "text": "To automate a monthly  renewal using systemd, you'll need to create two files.\nThe first is a service file that tells systemd how to invoke certbot.\nThe second is to generate a timer file that tells systemd how often to run the service.\nThe steps to setup the timer are as follows:    Create a service file called  /etc/systemd/system/certbot.service  with the following contents  [Unit]  Description = Let s Encrypt renewal  [Service]  Type = oneshot  ExecStart = /usr/bin/certbot renew --quiet --agree-tos     Once the certbot service is working correctly, you will need to create the timer file.\n   Create the timer file at  /etc/systemd/system/certbot.timer ) with the following contents:  [Unit]  Description = Twice daily renewal of Let s Encrypt s certificates  [Timer]  OnCalendar = 0/12:00:00  RandomizedDelaySec = 1h  Persistent = true  [Install]  WantedBy = timers.target     Update the systemd manager configuration:  root@host #  systemctl daemon-reload    Start and enable the certbot service and timer:  root@host #  systemctl start certbot.service root@host #  systemctl  enable  certbot.service root@host #  systemctl start certbot.timer root@host #  systemctl  enable  certbot.timer    You can verify that the timer is active by running  systemctl list-timers .   Note  Verify that the service has started correctly by running  systemctl status certbot.service . The timer may fail \nwithout warnings if the service does not run correctly.", 
            "title": "Automating renewals using systemd timers"
        }, 
        {
            "location": "/security/host-certs/#requesting-service-certificates", 
            "text": "Previously, the OSG recommended using separate X.509 certificates, called \"service certificates\", for each grid service\non a host.\nThis practice has become less popular as sites have separated SSL-requiring services to their own hosts.  In the case where your host is only running a single service that requires a service certificate, we recommend using\nyour  host certificate  as your service certificate.\nEnsure that the ownership of the host certificate and key are appropriate for the service you are running.  If you are running multiple services that require host certificates, we recommend requesting a certificate whose\nCommonName is  service -hostname  and has the hostname in the list of subject alternative names.", 
            "title": "Requesting Service Certificates"
        }, 
        {
            "location": "/security/host-certs/#frequently-asked-questions", 
            "text": "", 
            "title": "Frequently Asked Questions"
        }, 
        {
            "location": "/security/host-certs/#can-i-use-any-host-to-request-a-certificate-for-a-different-host", 
            "text": "YES, you can use any host to create a certificate signing request as long as the hostname for the certificate is a fully\nqualified domain name.", 
            "title": "Can I use any host to request a certificate for a different host?"
        }, 
        {
            "location": "/security/host-certs/#how-do-i-renew-a-host-certificate", 
            "text": "For Let's Encrypt certificates, see  this section  For other certificates, there is no separate renewal procedure.\nInstead, request a new certificate using one of the methods above.", 
            "title": "How do I renew a host certificate?"
        }, 
        {
            "location": "/security/host-certs/#how-can-i-check-if-i-have-a-host-certificate-installed-already", 
            "text": "By default the host certificate key pair will be installed in  /etc/grid-security/hostcert.pem  and /etc/grid-security/hostkey.pem .\nYou can use  openssl  to access basic information about the certificate:  root@host #  openssl x509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout subject= /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=host.opensciencegrid.org  issuer= /DC=org/DC=cilogon/C=US/O=CILogon/CN=CILogon OSG CA 1  notBefore=Apr  8 00:00:00 2013 GMT  notAfter=May 17 12:00:00 2014 GMT", 
            "title": "How can I check if I have a host certificate installed already?"
        }, 
        {
            "location": "/security/host-certs/#how-can-i-check-the-expiration-time-of-my-installed-host-certificate", 
            "text": "Use the following  openssl  command to find the dates that your host certificate is valid:  root@host #  openssl x509 -in /etc/grid-security/hostcert.pem -dates -noout notBefore=Jan  4 21:08:41 2010 GMT  notAfter=Jan  4 21:08:41 2011 GMT", 
            "title": "How can I check the expiration time of my installed host certificate?"
        }, 
        {
            "location": "/security/host-certs/#references", 
            "text": "CILogon documentation for requesting InCommon certificates   Useful OpenSSL commands (from NCSA)  - e.g. how to convert the format of your certificate.    Official Let's Encrypt setup guide    Another  Let's Encrypt setup reference \n    Under Getting your host certificate, we follow the first \"Setting up\" section.", 
            "title": "References"
        }, 
        {
            "location": "/security/user-certs/", 
            "text": "User Certificates\n\n\n\n\nNote\n\n\nThis document describes how to get and set up a \npersonal\n certificate (also called a grid user certificate).\nFor instructions on how to get \nhost\n certificates, see the \nHost Certificates document\n.\n\n\n\n\nGetting a User Certificate\n\n\nThis section describes how to get and set up a personal certificate to use on OSG.\nYou need a user certificate if you are going to interact directly with OSG resources or infrastructure. \nExamples of this would be tasks like managing OASIS, directly running jobs on OSG resources, interacting directly with \nOSG storage elements, or to get private contact information from OSG systems.\nCurrently, you can get a user certificate from CILogon.\nYou may also be able to user other CAs to get a certificate; if your virtual organization (VO) requires that you get a\ncertificate from a different CA, \ncontact your VO Support Center\n for\ninstructions.\n\n\nKnow your responsibilities\n\n\nIf your account or user certificate is compromised, you \nmust\n notify the issuer of your certificate. \nIn addition, you should update your certificate and revoke the old certificate if any of the information in the\ncertificate (such as name or email address) change.\nFor the CILogon RA send email to \nca@cilogon.org\n.\nAdditional responsibilities required by the CILogon CA are given on \ntheir page\n.  \n\n\nGetting a certificate from CILogon\n\n\nYou will have to obtain your user certificate using the \nCILogon web UI\n.\nFollow the steps below to get an user certificate:\n\n\n\n\nOpen the CILogon page, \nhttps://cilogon.org\n, in your browser of choice\n\n\n\n\nFirst, either search for your institution and select it or scroll through list and do the same.\n\n\nIf your institution is not on the list, please contact your institution's IT support to see if they can support\nCILogon.\n\n\n.\n\n\n\n\nNote\n\n\nMake sure to select your educational institution from CILogon's list.\nFor instance, do not use OpenID (e.g. Google, Github, etc.) as a provider since not all OSG resources support\ncertificates using an OpenID provider.\n\n\n\n\n\n\n\n\nClick the \nLog On\n button and enter your instutional credentials if prompted.\n\n\n\n\nUpon successfully entering your credentials, you'll get a page asking for you to enter a password.\n   Enter a password that is at least 12 characters long and then click on the \nGet New Certificate\n button.\n\n\n\n\n\n\n\n\nThe web page will generate a \nusercred.p12\n file and prompt you to download it.\n   The certificate will be protected using the password you entered in the prior step.\n\n\n\n\nCertificate formats\n\n\nYour user certficate can be stored in a few different formats.\nThe two most common formats used in OSG are the \nPKCS12\n and\n\nPEM\n formats.\nIn the PEM format, your user certificate is stored in two separate files: one for the certificate and another for the\nprivate key.\nThe PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain.\nMost OSG user tools will work with both but will try to use PEM files first.   \n\n\nTo convert a PKCS12 file to  PEM files, do the following.  \n\n\n\n\n\n\nFirst, extract your user certificate from your PKCS12 file by running the following command.  You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called \nusercred.p12\n.  After running, the PEM certificate will be written to \nusercert.pem\n. \n\n\nuser@host $\n openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys\n\nEnter Import Password:\n\n\nMAC verified OK\n\n\n\n\n\n\n\n\n\n\nSecond, extract the private key by running the following command. You'll be prompted for two different passwords.  The first prompt will be for the password that you used to create the certficate.  The second prompt will be for the password that will encrypt the PEM certificate that will be created.  As before, the invocation assumes that your PKCS12 certificate is located in \nusercred.p12\n. After running, the PEM certificate with your private key will be written to \nuserkey.pem\n.\n\n\nuser@host $\n openssl pkcs12 -in usercred.p12 -out userkey.pem  -nocerts\n\nEnter Import Password:\n\n\nMAC verified OK\n\n\nEnter PEM pass phrase:\n\n\nVerifying - Enter PEM pass phrase:\n\n\n\n\n\n\n\n\n\n\nRevoking your user certificate\n\n\nIf the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate.\nIn addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct\ninformation.\n\n\nIf you have a CILogon issued certificate, contact \nca@cilogon.org\n in order revoke your certificate.\nIf you received a certificate from another CA, please contact the CA to intiate a certificate revocation.\n\n\nGetting a Certificate from a Service Provider with cigetcert\n\n\nYou may also get a user certificate from a SAML 2.0 Service Provider such as your home institution or XSEDE.\nThis kind of certificate is short-lived, typically valid only for a week.\nTherefore it is not suitable for using in your browser.\nHowever, it is useful for command-line access to site services such as compute or storage.\n\n\nYou will need to use the \ncigetcert\n tool to get a certificate this way.\nUse yum to install the \ncigetcert\n package from the OSG repositories.\n\n\nThis is a new way of getting a certificate and does not work with all institutions.\nTo get a list of institutions supported by \ncigetcert\n, run:\n\n\nuser@host $\n cigetcert --listinstitutions\n\nClemson University\n\n\nFermi National Accelerator Laboratory\n\n\nLIGO Scientific Collaboration\n\n\nLTER Network\n\n\n...\n\n\n\n\n\n\nTo get a certificate, run\n\n\nuser@host $\n cigetcert -i \nINSTITUTION\n\n\nAuthorizing ...... authorized\n\n\nFetching certificate ..... fetched\n\n\nStoring certificate in /tmp/x509up_u46142\n\n\nYour certificate is valid until: Fri Apr 13 17:03:13 2018\n\n\n\n\n\n\nAuthentication is controlled by the institution;\ndepending on the institution, you may need a valid Kerberos token, or will be prompted for a password.\n\n\nIf all goes well, you should see output similar to what's above.\nThe certificate is created in \n/tmp/x509up_u\nYOUR UID\n, which is the same place proxies are created by \ngrid-proxy-init\n.\n\n\nYou may specify default arguments in the \nCIGETCERTOPTS\n environment variable.\nThis can save you from having to type in the entire institution name every time you want a cert.\nFor example, to always use FNAL as the institution, put this in your \n.bashrc\n:\n\n\nexport\n \nCIGETCERTOPTS\n=\n-i \nFermi National Accelerator Laboratory\n\n\n\n\n\n\nYour VO may also provide specific instructions for how to best use this tool.\nContact your VO support center for details.\n\n\nFinally, \ncigetcert\n has advanced features, such as the ability to load configuration from a server, or store the cert on a MyProxy server.\nSee the \nmanual page for cigetcert\n for more information.\n\n\nUsing cigetcert with XSEDE credentials\n\n\ncigetcert\n also works with XSEDE as the service provider.\nTo use XSEDE credentials, you will first need an account at \nhttps://portal.xsede.org\n.\nIn addition, you \nneed\n to set up two-factor authentication with XSEDE; see their \nMFA documentation\n for details.\nPush notifications using the Duo Mobile app are required.\n\n\nOnce you have set all those up, run \ncigetcert\n as follows:\n\n\nuser@host $\n cigetcert -u \nUSERNAME\n -i XSEDE\n\n\n\n\n\nUSERNAME\n is your username at portal.xsede.org.\nYou will get prompted to \"Enter XSEDE Kerberos Password.\"\nEnter the password for your account at portal.xsede.org.\nYou should then get a 2FA authentication request with Duo Mobile; once you accept this, \ncigetcert\n will issue the certificate.\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReferences\n\n\n\n\nUseful Documentation.OpenSSL commands (from NCSA)\n - e.g. how to convert the format of your certificate.\n\n\nManual page for cigetcert", 
            "title": "User Certificates"
        }, 
        {
            "location": "/security/user-certs/#user-certificates", 
            "text": "Note  This document describes how to get and set up a  personal  certificate (also called a grid user certificate).\nFor instructions on how to get  host  certificates, see the  Host Certificates document .", 
            "title": "User Certificates"
        }, 
        {
            "location": "/security/user-certs/#getting-a-user-certificate", 
            "text": "This section describes how to get and set up a personal certificate to use on OSG.\nYou need a user certificate if you are going to interact directly with OSG resources or infrastructure. \nExamples of this would be tasks like managing OASIS, directly running jobs on OSG resources, interacting directly with \nOSG storage elements, or to get private contact information from OSG systems.\nCurrently, you can get a user certificate from CILogon.\nYou may also be able to user other CAs to get a certificate; if your virtual organization (VO) requires that you get a\ncertificate from a different CA,  contact your VO Support Center  for\ninstructions.", 
            "title": "Getting a User Certificate"
        }, 
        {
            "location": "/security/user-certs/#know-your-responsibilities", 
            "text": "If your account or user certificate is compromised, you  must  notify the issuer of your certificate. \nIn addition, you should update your certificate and revoke the old certificate if any of the information in the\ncertificate (such as name or email address) change.\nFor the CILogon RA send email to  ca@cilogon.org .\nAdditional responsibilities required by the CILogon CA are given on  their page .", 
            "title": "Know your responsibilities"
        }, 
        {
            "location": "/security/user-certs/#getting-a-certificate-from-cilogon", 
            "text": "You will have to obtain your user certificate using the  CILogon web UI .\nFollow the steps below to get an user certificate:   Open the CILogon page,  https://cilogon.org , in your browser of choice   First, either search for your institution and select it or scroll through list and do the same.  If your institution is not on the list, please contact your institution's IT support to see if they can support\nCILogon.  .   Note  Make sure to select your educational institution from CILogon's list.\nFor instance, do not use OpenID (e.g. Google, Github, etc.) as a provider since not all OSG resources support\ncertificates using an OpenID provider.     Click the  Log On  button and enter your instutional credentials if prompted.   Upon successfully entering your credentials, you'll get a page asking for you to enter a password.\n   Enter a password that is at least 12 characters long and then click on the  Get New Certificate  button.     The web page will generate a  usercred.p12  file and prompt you to download it.\n   The certificate will be protected using the password you entered in the prior step.", 
            "title": "Getting a certificate from CILogon"
        }, 
        {
            "location": "/security/user-certs/#certificate-formats", 
            "text": "Your user certficate can be stored in a few different formats.\nThe two most common formats used in OSG are the  PKCS12  and PEM  formats.\nIn the PEM format, your user certificate is stored in two separate files: one for the certificate and another for the\nprivate key.\nThe PKCS12 format stores the certificate and private key in a single file along with an optional certificate chain.\nMost OSG user tools will work with both but will try to use PEM files first.     To convert a PKCS12 file to  PEM files, do the following.      First, extract your user certificate from your PKCS12 file by running the following command.  You'll be prompted for the password you used to create the certificate. The invocation assumes that the PKCS12 file is called  usercred.p12 .  After running, the PEM certificate will be written to  usercert.pem .   user@host $  openssl pkcs12 -in usercred.p12 -out usercert.pem -nodes -clcerts -nokeys Enter Import Password:  MAC verified OK     Second, extract the private key by running the following command. You'll be prompted for two different passwords.  The first prompt will be for the password that you used to create the certficate.  The second prompt will be for the password that will encrypt the PEM certificate that will be created.  As before, the invocation assumes that your PKCS12 certificate is located in  usercred.p12 . After running, the PEM certificate with your private key will be written to  userkey.pem .  user@host $  openssl pkcs12 -in usercred.p12 -out userkey.pem  -nocerts Enter Import Password:  MAC verified OK  Enter PEM pass phrase:  Verifying - Enter PEM pass phrase:", 
            "title": "Certificate formats"
        }, 
        {
            "location": "/security/user-certs/#revoking-your-user-certificate", 
            "text": "If the security of your certificate or private key has been compromised, you have a responsibility to revoke the certificate.\nIn addition, if your name or email address changes, you must revoke your certificate and get a new one with the correct\ninformation.  If you have a CILogon issued certificate, contact  ca@cilogon.org  in order revoke your certificate.\nIf you received a certificate from another CA, please contact the CA to intiate a certificate revocation.", 
            "title": "Revoking your user certificate"
        }, 
        {
            "location": "/security/user-certs/#getting-a-certificate-from-a-service-provider-with-cigetcert", 
            "text": "You may also get a user certificate from a SAML 2.0 Service Provider such as your home institution or XSEDE.\nThis kind of certificate is short-lived, typically valid only for a week.\nTherefore it is not suitable for using in your browser.\nHowever, it is useful for command-line access to site services such as compute or storage.  You will need to use the  cigetcert  tool to get a certificate this way.\nUse yum to install the  cigetcert  package from the OSG repositories.  This is a new way of getting a certificate and does not work with all institutions.\nTo get a list of institutions supported by  cigetcert , run:  user@host $  cigetcert --listinstitutions Clemson University  Fermi National Accelerator Laboratory  LIGO Scientific Collaboration  LTER Network  ...   To get a certificate, run  user@host $  cigetcert -i  INSTITUTION  Authorizing ...... authorized  Fetching certificate ..... fetched  Storing certificate in /tmp/x509up_u46142  Your certificate is valid until: Fri Apr 13 17:03:13 2018   Authentication is controlled by the institution;\ndepending on the institution, you may need a valid Kerberos token, or will be prompted for a password.  If all goes well, you should see output similar to what's above.\nThe certificate is created in  /tmp/x509up_u YOUR UID , which is the same place proxies are created by  grid-proxy-init .  You may specify default arguments in the  CIGETCERTOPTS  environment variable.\nThis can save you from having to type in the entire institution name every time you want a cert.\nFor example, to always use FNAL as the institution, put this in your  .bashrc :  export   CIGETCERTOPTS = -i  Fermi National Accelerator Laboratory   Your VO may also provide specific instructions for how to best use this tool.\nContact your VO support center for details.  Finally,  cigetcert  has advanced features, such as the ability to load configuration from a server, or store the cert on a MyProxy server.\nSee the  manual page for cigetcert  for more information.", 
            "title": "Getting a Certificate from a Service Provider with cigetcert"
        }, 
        {
            "location": "/security/user-certs/#using-cigetcert-with-xsede-credentials", 
            "text": "cigetcert  also works with XSEDE as the service provider.\nTo use XSEDE credentials, you will first need an account at  https://portal.xsede.org .\nIn addition, you  need  to set up two-factor authentication with XSEDE; see their  MFA documentation  for details.\nPush notifications using the Duo Mobile app are required.  Once you have set all those up, run  cigetcert  as follows:  user@host $  cigetcert -u  USERNAME  -i XSEDE  USERNAME  is your username at portal.xsede.org.\nYou will get prompted to \"Enter XSEDE Kerberos Password.\"\nEnter the password for your account at portal.xsede.org.\nYou should then get a 2FA authentication request with Duo Mobile; once you accept this,  cigetcert  will issue the certificate.", 
            "title": "Using cigetcert with XSEDE credentials"
        }, 
        {
            "location": "/security/user-certs/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/security/user-certs/#references", 
            "text": "Useful Documentation.OpenSSL commands (from NCSA)  - e.g. how to convert the format of your certificate.  Manual page for cigetcert", 
            "title": "References"
        }, 
        {
            "location": "/security/certificate-management/", 
            "text": "Managing Certificates\n\n\nThe OSG provides several tools to assist in the management of host and CA certificates.  This page serves as a\nreference guide for several of these tools:\n\n\n\n\nosg-pki-tools\n: command line tools for requesting and managing user and host certificates.\n\n\nosg-ca-certs-updater\n: A package for auto-updating CAs on a server host.\n\n\nosg-ca-manage\n: A tool for detailed management of CA directories outside RPMs.\n\n\n\n\n\n\nNote\n\n\nThis is a reference document and not introduction on how to install CA certificates or request\nhost / user certificates.  Most users will want the \nCA overview\n,\n\nhost certificate overview\n, or \nuser certificate overview\n documents.\n\n\n\n\nOSG PKI Command Line Clients\n\n\nOverview\n\n\nThe OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the \nOIM Web Interface\n.\n\n\nPrerequisites\n\n\nIf you have not already done so, you need to \nconfigure the OSG software repositories\n.\n\n\nInstallation\n\n\nThe command-line scripts have been packaged as an RPM and are available from the OSG repositories.\n\n\nTo install the RPM, run:\n\n\nroot@host #\n yum install osg-pki-tools\n\n\n\n\n\nUsage\n\n\nDocumentation for usage of the osg-pki-tools can be found \nhere\n\n\nOSG CA Certificates Updater\n\n\nThis section explains the installation and use of \nosg-ca-certs-updater\n, a package that provides automatic updates of\nCA certificates.\n\n\nRequirements\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstall instructions\n\n\nRun the following command to install the latest version of the updater.\n\n\nroot@host#\n yum install osg-ca-certs-updater\n\n\n\n\n\nServices\n\n\nStarting and Enabling Services\n\n\nRun the following to enable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron start\n\n\n\n\n\nRun the following to enable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron on\n\n\n\n\n\nRun both commands if you wish for the service to activate immediately and remain active throughout reboots.\n\n\nStopping and Disabling Services\n\n\nEnter the following to disable the updater. This will persist until the machine is rebooted.\n\n\nroot@host#\n service osg-ca-certs-updater-cron stop\n\n\n\n\n\nEnter the following to disable the updater when the machine is rebooted.\n\n\nroot@host#\n chkconfig osg-ca-certs-updater-cron off\n\n\n\n\n\nRun both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.\n\n\nConfiguration\n\n\nWhile there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the \ncron\n entry of the service. This entry is located in the file \n/etc/cron.d/osg-ca-certs-updater\n. Please see the Unix manual page for \ncrontab\n in section 5 for an explanation of the format. The manual page can be accessed by the command \nman 5 crontab\n. The valid command-line arguments can be listed by running \nosg-ca-certs-updater --help\n. Reasonable defaults have been provided, namely:\n\n\n\n\nAttempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.\n\n\nRun the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.\n\n\nDelay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.\n\n\nDo not warn the administrator about update failures that have happened less than 72 hours since the last successful update.\n\n\nLog errors only.\n\n\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nConfiguration file\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nosg-ca-certs-updater\n\n\nCron entry for periodically launching the updater\n\n\n/etc/cron.d/osg-ca-certs-updater\n\n\nCommand-line arguments to the updater can be specified here\n\n\n\n\n\n\nosg-release\n\n\nRepo definition files for production OSG repositories\n\n\n/etc/yum.repos.d/osg.repo\n or \n/etc/yum.repos.d/osg-el6.repo\n\n\nMake sure these repositories are enabled and reachable from the host you are trying to update\n\n\n\n\n\n\n\n\nLog files\n\n\nLogging is performed to the console by default. Please see the manual for your \ncron\n daemon to find out how it handles console output.\n\n\nA logfile can be specified via the \n-l\n / \n--logfile\n command-line option.\n\n\nIf logging to syslog via the \n-s\n / \n--log-to-syslog\n option, the updater will write to the \nuser\n section of the syslog. The file \n/etc/syslog.conf\n determines where syslog messages are saved.\n\n\nReferences\n\n\nSome guides on X.509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nSome examples about verifying the certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nManaging CAs\n\n\nThe osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation.\n\n\nUsage examples\n\n\nDocumentation for usage of the osg-ca-manage tool can be found \nhere\n\n\n\n\nNote\n\n\nThese commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed.\n\n\n\n\nInstall a certificate authority package\n\n\nBefore you proceed to install a Certificate Authority Package you should decide which of the available packages to install.\n\n\n\n\nosg\n, the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the \nSecurity Team\n.\n\n\nigtf\n, the package is a redistribution of the unchanged CA distribution from the IGTF\n\n\nurl\n a package provided at a given URL\n\n\n\n\n\n\nNote\n\n\nIf in doubt, please consult the policies of your home institution and get in contact with the \nSecurity Team\n.\n\n\n\n\nNext decide at what location to install the Certificate Authority Package:\n\n\n\n\non the \nroot\n file system in a system directory \n/etc/grid-security/certificates\n\n\nin a \ncustom\n directory that can also be shared\n\n\n\n\nSetup the CA certificates\n\n\nThe Certificate Authority Package is preferably be used by grid users without root privileges \nor\n if the CA certificates will not be shared by other installations on the same host.\n\n\nroot@host #\n osg-ca-manage setupca --location \nroot\n --url osg\n\nSetting CA Certificates for at \n/etc/grid-security/certificates\n\n\n\nSetup completed successfully.\n\n\n\n\n\n\nAfter a successful installation the certificates will be installed in (\n/etc/grid-security/certificates\n in this example). Typically to write into this default location you will need root privileges.\n\n\nIf you need to need to install it with out root privileges use\n\n\nuser@host $\n osg-ca-manage setupca --location \n$HOME\n/certificates\n --url osg\n\nSetting CA Certificates for at \n$HOME/certificates\n\n\n\nSetup completed successfully.\n\n\n\n\n\n\nAdding a directory of local CAs\n\n\nroot@host #\nosg-ca-manage add --cadir /etc/grid-security/localca\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nHere is the resulting file after add\n\n\n##\ncat\n \n/\netc\n/\nosg\n/\nosg\n-\nupdate\n-\ncerts\n.\nconf\n\n# \nConfiguration\n \nfile\n \nfor\n \nosg\n-\nupdate\n-\ncerts\n\n\n# \nThis\n \nfile\n \nhas\n \nbeen\n \nregenerated\n \nby\n \nosg\n-\nca\n-\nmanage\n, \nwhich\n \nremoves\n \nmost\n\n# \ncomments\n.  \nYou\n \ncan\n \nstill\n \nmanually\n \nmodify\n \nit\n, \nany\n \nmanual\n \nchange\n \nwill\n\n# \nbe\n \npreserved\n \nif\n \nosg\n-\nca\n-\nmanage\n \nis\n \nused\n \nagain\n.\n\n## \nThe\n \nparent\n \nlocation\n \ncertificates\n \nwill\n \nbe\n \ninstalled\n \nat\n.\n\ninstall_dir\n \n=\n \n/\netc\n/\ngrid\n-\nsecurity\n\n\n## \ncacerts_url\n \nis\n \nthe\n \nURL\n \nof\n \nyour\n \ncertificate\n \ndistribution\n\n\ncacerts_url\n \n=\n \nhttps\n:\n//\nrepo\n.\nopensciencegrid\n.\norg\n/\npacman\n/\ncadist\n/\nca\n-\ncerts\n-\nversion\n-\nigtf\n-\nnew\n\n\n## \nlog\n \nspecifies\n \nwhere\n \nlogging\n \noutput\n \nwill\n \ngo\n\n\nlog\n \n=\n \n/\nvar\n/\nlog\n/\nosg\n-\nupdate\n-\ncerts\n.\nlog\n\n\n## \ninclude\n \nspecifies\n \nfiles\n \n(\nfull\n \npathnames\n)\n \nthat\n \nshould\n \nbe\n \ncopied\n\n## \ninto\n \nthe\n \ncertificates\n \ninstallation\n \nafter\n \nan\n \nupdate\n \nhas\n \noccured\n.\n\ninclude\n=/\netc\n/\ngrid\n-\nsecurity\n/\nlocalca\n/*\n\n\n\n## exclude_ca specifies a CA (not full pathnames, but just the hash\n\n\n## of the CA you want to exclude) that should be removed from the\n\n\n## certificates installation after an update has occured.\n\n\n\ndebug = 0\n\n\n\n\n\n\nRemoving a directory of local CAs\n\n\nroot@host #\nosg-ca-manage remove --cadir /etc/grid-security/localca\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nRemoving a particular CA included in OSG CA package\n\n\nroot@host #\nosg-ca-manage remove --caname ce33db76\n\nSymlink detected for hash: We have determided that the hash value you entered belong to the CA \nIRAN-GRID.pem\n. If you wish to add this CA back you will have to use this name is the parameter.\n\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nThe resulting config file after the remove is as follows\n\n\n##\ncat\n \n/\netc\n/\nosg\n/\nosg\n-\nupdate\n-\ncerts\n.\nconf\n\n# \nConfiguration\n \nfile\n \nfor\n \nosg\n-\nupdate\n-\ncerts\n\n\n# \nThis\n \nfile\n \nhas\n \nbeen\n \nregenerated\n \nby\n \nosg\n-\nca\n-\nmanage\n, \nwhich\n \nremoves\n \nmost\n\n# \ncomments\n.  \nYou\n \ncan\n \nstill\n \nmanually\n \nmodify\n \nit\n, \nany\n \nmanual\n \nchange\n \nwill\n\n# \nbe\n \npreserved\n \nif\n \nosg\n-\nca\n-\nmanage\n \nis\n \nused\n \nagain\n.\n\n## \nThe\n \nparent\n \nlocation\n \ncertificates\n \nwill\n \nbe\n \ninstalled\n \nat\n.\n\ninstall_dir\n \n=\n \n/\netc\n/\ngrid\n-\nsecurity\n\n\n## \ncacerts_url\n \nis\n \nthe\n \nURL\n \nof\n \nyour\n \ncertificate\n \ndistribution\n\n\ncacerts_url\n \n=\n \nhttps\n:\n//\nrepo\n.\nopensciencegrid\n.\norg\n/\npacman\n/\ncadist\n/\nca\n-\ncerts\n-\nversion\n-\nigtf\n-\nnew\n\n\n## \nlog\n \nspecifies\n \nwhere\n \nlogging\n \noutput\n \nwill\n \ngo\n\n\nlog\n \n=\n \n/\nvar\n/\nlog\n/\nosg\n-\nupdate\n-\ncerts\n.\nlog\n\n\n## \ninclude\n \nspecifies\n \nfiles\n \n(\nfull\n \npathnames\n)\n \nthat\n \nshould\n \nbe\n \ncopied\n\n## \ninto\n \nthe\n \ncertificates\n \ninstallation\n \nafter\n \nan\n \nupdate\n \nhas\n \noccured\n.\n\n## \nexclude_ca\n \nspecifies\n \na\n \nCA\n \n(\nnot\n \nfull\n \npathnames\n, \nbut\n \njust\n \nthe\n \nhash\n\n## \nof\n \nthe\n \nCA\n \nyou\n \nwant\n \nto\n \nexclude\n)\n \nthat\n \nshould\n \nbe\n \nremoved\n \nfrom\n \nthe\n\n## \ncertificates\n \ninstallation\n \nafter\n \nan\n \nupdate\n \nhas\n \noccured\n.\n\nexclude_ca\n \n=\n \nIRAN\n-\nGRID\n\n\n\ndebug\n \n=\n \n0\n\n\n\n\n\n\nAdding a CA from the OSG CA package\n\n\nroot@host #\nosg-ca-manage add --caname IRAN-GRID\n\nNOTE:\n\n\n    You did not specify the --auto-refresh flag.\n\n\n    So the changes made to the configuration will not be reflected till the next time\n\n\n    when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.\n\n\n    Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.\n\n\n\n\n\n\nInspect installed CA certificates\n\n\nYou can inspect the list of CA Certificates that have been installed:\n\n\nuser@host $\n osg-ca-manage listCA\n\nHash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new\n\n\nHash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new\n\n\n[...]\n\n\n\n\n\n\nAny certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the \nOSG Security Team\n and review the policies of your home institution.\n\n\nTroubleshooting\n\n\nUseful configuration and log files\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out\n\n\n\n\n\n\n\n\n\n\nReferences\n\n\n\n\nInstalling the Certificate Authorities Certificates and the related RPMs", 
            "title": "Certificate Management Reference"
        }, 
        {
            "location": "/security/certificate-management/#managing-certificates", 
            "text": "The OSG provides several tools to assist in the management of host and CA certificates.  This page serves as a\nreference guide for several of these tools:   osg-pki-tools : command line tools for requesting and managing user and host certificates.  osg-ca-certs-updater : A package for auto-updating CAs on a server host.  osg-ca-manage : A tool for detailed management of CA directories outside RPMs.    Note  This is a reference document and not introduction on how to install CA certificates or request\nhost / user certificates.  Most users will want the  CA overview , host certificate overview , or  user certificate overview  documents.", 
            "title": "Managing Certificates"
        }, 
        {
            "location": "/security/certificate-management/#osg-pki-command-line-clients", 
            "text": "", 
            "title": "OSG PKI Command Line Clients"
        }, 
        {
            "location": "/security/certificate-management/#overview", 
            "text": "The OSG PKI Command Line Clients provide a command-line interface for requesting and issuing host certificates from the OSG PKI. They complement the  OIM Web Interface .", 
            "title": "Overview"
        }, 
        {
            "location": "/security/certificate-management/#prerequisites", 
            "text": "If you have not already done so, you need to  configure the OSG software repositories .", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/security/certificate-management/#installation", 
            "text": "The command-line scripts have been packaged as an RPM and are available from the OSG repositories.  To install the RPM, run:  root@host #  yum install osg-pki-tools", 
            "title": "Installation"
        }, 
        {
            "location": "/security/certificate-management/#usage", 
            "text": "Documentation for usage of the osg-pki-tools can be found  here", 
            "title": "Usage"
        }, 
        {
            "location": "/security/certificate-management/#osg-ca-certificates-updater", 
            "text": "This section explains the installation and use of  osg-ca-certs-updater , a package that provides automatic updates of\nCA certificates.", 
            "title": "OSG CA Certificates Updater"
        }, 
        {
            "location": "/security/certificate-management/#requirements", 
            "text": "As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Requirements"
        }, 
        {
            "location": "/security/certificate-management/#install-instructions", 
            "text": "Run the following command to install the latest version of the updater.  root@host#  yum install osg-ca-certs-updater", 
            "title": "Install instructions"
        }, 
        {
            "location": "/security/certificate-management/#services", 
            "text": "", 
            "title": "Services"
        }, 
        {
            "location": "/security/certificate-management/#starting-and-enabling-services", 
            "text": "Run the following to enable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron start  Run the following to enable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron on  Run both commands if you wish for the service to activate immediately and remain active throughout reboots.", 
            "title": "Starting and Enabling Services"
        }, 
        {
            "location": "/security/certificate-management/#stopping-and-disabling-services", 
            "text": "Enter the following to disable the updater. This will persist until the machine is rebooted.  root@host#  service osg-ca-certs-updater-cron stop  Enter the following to disable the updater when the machine is rebooted.  root@host#  chkconfig osg-ca-certs-updater-cron off  Run both commands if you wish for the service to deactivate immediately and not get reactivated during reboots.", 
            "title": "Stopping and Disabling Services"
        }, 
        {
            "location": "/security/certificate-management/#configuration", 
            "text": "While there is no configuration file, the behavior of the updater can be adjusted by command-line arguments that are specified in the  cron  entry of the service. This entry is located in the file  /etc/cron.d/osg-ca-certs-updater . Please see the Unix manual page for  crontab  in section 5 for an explanation of the format. The manual page can be accessed by the command  man 5 crontab . The valid command-line arguments can be listed by running  osg-ca-certs-updater --help . Reasonable defaults have been provided, namely:   Attempt an update no more often than every 23 hours. Due to the random wait (see below), having a 24-hour minimum time between updates would cause the update time to slowly slide back every day.  Run the script every 6 hours. We run the script more often than we update so that downtime at the wrong moment does not cause the update to be delayed for a full day.  Delay for a random amount of time up to 30 minutes before updating, to reduce load spikes on OSG repositories.  Do not warn the administrator about update failures that have happened less than 72 hours since the last successful update.  Log errors only.", 
            "title": "Configuration"
        }, 
        {
            "location": "/security/certificate-management/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/certificate-management/#useful-configuration-and-log-files", 
            "text": "", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/security/certificate-management/#configuration-file", 
            "text": "Package  File Description  Location  Comment      osg-ca-certs-updater  Cron entry for periodically launching the updater  /etc/cron.d/osg-ca-certs-updater  Command-line arguments to the updater can be specified here    osg-release  Repo definition files for production OSG repositories  /etc/yum.repos.d/osg.repo  or  /etc/yum.repos.d/osg-el6.repo  Make sure these repositories are enabled and reachable from the host you are trying to update", 
            "title": "Configuration file"
        }, 
        {
            "location": "/security/certificate-management/#log-files", 
            "text": "Logging is performed to the console by default. Please see the manual for your  cron  daemon to find out how it handles console output.  A logfile can be specified via the  -l  /  --logfile  command-line option.  If logging to syslog via the  -s  /  --log-to-syslog  option, the updater will write to the  user  section of the syslog. The file  /etc/syslog.conf  determines where syslog messages are saved.", 
            "title": "Log files"
        }, 
        {
            "location": "/security/certificate-management/#references", 
            "text": "Some guides on X.509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Some examples about verifying the certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html", 
            "title": "References"
        }, 
        {
            "location": "/security/certificate-management/#managing-cas", 
            "text": "The osg-ca-manage tool provides a unified interface to manage the CA Certificate installations. This page provides the instructions on using this command. It provides status commands that allows you to list the CAs and the validity of the CAs and CRLs included in the installation. The manage commands allow you to fetch CAs and CRLs, change the distribution URL, as well as add and remove CAs from your local installation.", 
            "title": "Managing CAs"
        }, 
        {
            "location": "/security/certificate-management/#usage-examples", 
            "text": "Documentation for usage of the osg-ca-manage tool can be found  here   Note  These commands will not work if of the osg-ca-certs (or igtf-ca-certs) RPM packages are installed.", 
            "title": "Usage examples"
        }, 
        {
            "location": "/security/certificate-management/#install-a-certificate-authority-package", 
            "text": "Before you proceed to install a Certificate Authority Package you should decide which of the available packages to install.   osg , the package recommended to be used by production resources on the OSG. It is based on the CA distribution from the IGTF, but it may differ slightly as decided by the  Security Team .  igtf , the package is a redistribution of the unchanged CA distribution from the IGTF  url  a package provided at a given URL    Note  If in doubt, please consult the policies of your home institution and get in contact with the  Security Team .   Next decide at what location to install the Certificate Authority Package:   on the  root  file system in a system directory  /etc/grid-security/certificates  in a  custom  directory that can also be shared", 
            "title": "Install a certificate authority package"
        }, 
        {
            "location": "/security/certificate-management/#setup-the-ca-certificates", 
            "text": "The Certificate Authority Package is preferably be used by grid users without root privileges  or  if the CA certificates will not be shared by other installations on the same host.  root@host #  osg-ca-manage setupca --location  root  --url osg Setting CA Certificates for at  /etc/grid-security/certificates  Setup completed successfully.   After a successful installation the certificates will be installed in ( /etc/grid-security/certificates  in this example). Typically to write into this default location you will need root privileges.  If you need to need to install it with out root privileges use  user@host $  osg-ca-manage setupca --location  $HOME /certificates  --url osg Setting CA Certificates for at  $HOME/certificates  Setup completed successfully.", 
            "title": "Setup the CA certificates"
        }, 
        {
            "location": "/security/certificate-management/#adding-a-directory-of-local-cas", 
            "text": "root@host # osg-ca-manage add --cadir /etc/grid-security/localca NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.   Here is the resulting file after add  ## cat   / etc / osg / osg - update - certs . conf \n#  Configuration   file   for   osg - update - certs \n\n#  This   file   has   been   regenerated   by   osg - ca - manage ,  which   removes   most \n#  comments .   You   can   still   manually   modify   it ,  any   manual   change   will \n#  be   preserved   if   osg - ca - manage   is   used   again .\n\n##  The   parent   location   certificates   will   be   installed   at . install_dir   =   / etc / grid - security \n\n##  cacerts_url   is   the   URL   of   your   certificate   distribution  cacerts_url   =   https : // repo . opensciencegrid . org / pacman / cadist / ca - certs - version - igtf - new \n\n##  log   specifies   where   logging   output   will   go  log   =   / var / log / osg - update - certs . log \n\n##  include   specifies   files   ( full   pathnames )   that   should   be   copied \n##  into   the   certificates   installation   after   an   update   has   occured . include =/ etc / grid - security / localca /*  ## exclude_ca specifies a CA (not full pathnames, but just the hash  ## of the CA you want to exclude) that should be removed from the  ## certificates installation after an update has occured.  debug = 0", 
            "title": "Adding a directory of local CAs"
        }, 
        {
            "location": "/security/certificate-management/#removing-a-directory-of-local-cas", 
            "text": "root@host # osg-ca-manage remove --cadir /etc/grid-security/localca NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.", 
            "title": "Removing a directory of local CAs"
        }, 
        {
            "location": "/security/certificate-management/#removing-a-particular-ca-included-in-osg-ca-package", 
            "text": "root@host # osg-ca-manage remove --caname ce33db76 Symlink detected for hash: We have determided that the hash value you entered belong to the CA  IRAN-GRID.pem . If you wish to add this CA back you will have to use this name is the parameter.  NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.   The resulting config file after the remove is as follows  ## cat   / etc / osg / osg - update - certs . conf \n#  Configuration   file   for   osg - update - certs \n\n#  This   file   has   been   regenerated   by   osg - ca - manage ,  which   removes   most \n#  comments .   You   can   still   manually   modify   it ,  any   manual   change   will \n#  be   preserved   if   osg - ca - manage   is   used   again .\n\n##  The   parent   location   certificates   will   be   installed   at . install_dir   =   / etc / grid - security \n\n##  cacerts_url   is   the   URL   of   your   certificate   distribution  cacerts_url   =   https : // repo . opensciencegrid . org / pacman / cadist / ca - certs - version - igtf - new \n\n##  log   specifies   where   logging   output   will   go  log   =   / var / log / osg - update - certs . log \n\n##  include   specifies   files   ( full   pathnames )   that   should   be   copied \n##  into   the   certificates   installation   after   an   update   has   occured .\n\n##  exclude_ca   specifies   a   CA   ( not   full   pathnames ,  but   just   the   hash \n##  of   the   CA   you   want   to   exclude )   that   should   be   removed   from   the \n##  certificates   installation   after   an   update   has   occured . exclude_ca   =   IRAN - GRID  debug   =   0", 
            "title": "Removing a particular CA included in OSG CA package"
        }, 
        {
            "location": "/security/certificate-management/#adding-a-ca-from-the-osg-ca-package", 
            "text": "root@host # osg-ca-manage add --caname IRAN-GRID NOTE:      You did not specify the --auto-refresh flag.      So the changes made to the configuration will not be reflected till the next time      when CAs and CRLs are updated respectively by osg-update-certs and fetch-crl running from cron.      Run `osg-ca-manage refreshCA` and `osg-ca-manage fetchCRL` to commit your changes immediately.", 
            "title": "Adding a CA from the OSG CA package"
        }, 
        {
            "location": "/security/certificate-management/#inspect-installed-ca-certificates", 
            "text": "You can inspect the list of CA Certificates that have been installed:  user@host $  osg-ca-manage listCA Hash=09ff08b7; Subject= /C=FR/O=CNRS/CN=CNRS2-Projets; Issuer= /C=FR/O=CNRS/CN=CNRS2; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new  Hash=0a12b607; Subject= /DC=org/DC=ugrid/CN=UGRID CA; Issuer= /DC=org/DC=ugrid/CN=UGRID CA; Accreditation=Unknown; Status=https://repo.opensciencegrid.org/pacman/cadist/ca-certs-version-new  [...]   Any certificate issued by any of the Certificate Authorities listed will be trusted. If in doubt please contact the  OSG Security Team  and review the policies of your home institution.", 
            "title": "Inspect installed CA certificates"
        }, 
        {
            "location": "/security/certificate-management/#troubleshooting_1", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/security/certificate-management/#useful-configuration-and-log-files_1", 
            "text": "Logs and configuration:     File Description  Location  Comment      Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    Log file of osg-update-certs  /var/log/osg-update-certs.log     Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out     Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out     Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/security/certificate-management/#references_1", 
            "text": "Installing the Certificate Authorities Certificates and the related RPMs", 
            "title": "References"
        }, 
        {
            "location": "/common/ca/", 
            "text": "Installing Certificate Authorities (CAs)\n\n\nThe \ncertificate authorities\n (CAs) provide the trust roots for the\n\npublic key infrastructure\n OSG uses to maintain integrity of its sites and services.\nThis document provides details of various options to install the Certificate Authority (CA) certificates and have up-to-date\n\ncertificate revocation lists\n (CRLs) on your OSG hosts.\n\n\nWe provide three options for installing CA certificates that offer varying levels of control:\n\n\n\n\nInstall an RPM for a specific set of CA certificates (\ndefault\n)\n\n\nInstall \nosg-ca-scripts\n, a set of scripts that provide fine-grained CA management\n\n\nInstall an RPM that doesn't install \nany\n CAs.\n    This is useful if you'd like to manage CAs yourself while satisfying RPM dependencies.\n\n\n\n\nPrior to following the instructions on this page, you must enable our \nyum repositories\n\n\nInstalling CA Certificates\n\n\nPlease choose one of the three options to install CA certificates.\n\n\nOption 1: Install an RPM for a specific set of CA certificates\n\n\n\n\nNote\n\n\nThis option is the default if you install OSG software without pre-installing CAs.\nFor example, \nyum install osg-ce\n will bring in \nosg-ca-certs\n by default.\n\n\n\n\nIn the OSG repositories, you will find two different sets of predefined CA certificates:\n\n\n\n\n(\ndefault\n) The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions\n\n\nThe \nIGTF\n CA certificates\n\n\n\n\nSee \nthis page\n for details of the contents of the OSG CA package.\n\n\n\n\n\n\n\n\nIf you chose...\n\n\nThen run the following command...\n\n\n\n\n\n\n\n\n\n\nOSG CA certificates\n\n\nyum install osg-ca-certs\n\n\n\n\n\n\nIGTF CA certificates\n\n\nyum install igtf-ca-certs\n\n\n\n\n\n\n\n\nTo automatically keep your RPM installation of CAs up to date, we recommend the \nOSG CA certificates updater\n service.\n\n\nOption 2: Install osg-ca-scripts\n\n\nThe \nosg-ca-scripts\n package provides scripts to install and update predefined sets of CAs with the ability to add or remove specific CAs. \n\n\n\n\nThe OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions\n\n\nThe \nIGTF\n CA certificates\n\n\n\n\nSee \nthis page\n for details of the contents of the OSG CA package.\n\n\n\n\n\n\nInstall the \nosg-ca-scripts\n package:\n\n\nroot@host #\n yum install osg-ca-scripts\n\n\n\n\n\n\n\n\n\nChoose and install the CA certificate set:\n\n\n\n\n\n\n\n\nIf you choose...\n\n\nThen run the following command...\n\n\n\n\n\n\n\n\n\n\nOSG CA certificates\n\n\nosg-ca-manage setupCA --location root --url osg\n\n\n\n\n\n\nIGTF CA certificates\n\n\nosg-ca-manage setupCA --location root --url igtf\n\n\n\n\n\n\n\n\n\n\n\n\nEnable the \nosg-update-certs-cron\n service to enable periodic CA updates. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\n\n\n\n\n(Optional) To add a new CA:\n\n\nosg\n-\nca\n-\nmanage\n \nadd\n \n[\n--dir \nlocal_dir\n] --hash \nCA-HASH\n\n\n\n\n\n\n\n\n\n\n(Optional) To remove a CA\n\n\nosg\n-\nca\n-\nmanage\n \nremove\n \n--hash \nCA-HASH\n\n\n\n\n\n\n\n\n\n\nA complete set of options available though \nosg-ca-manage\n command, can be found in the \nosg-ca-manage documentation\n\n\nOption 3: Site-managed CAs\n\n\nIf you want to handle the list of CAs completely internally to your site, you can utilize the \nempty-ca-certs\n RPM to satisfy\nRPM dependencies while not actually installing any CAs. To install this RPM, run the following command:\n\n\nroot@host #\n yum install empty-ca-certs \u2013-enablerepo\n=\nosg-empty\n\n\n\n\n\n\n\nWarning\n\n\nIf you choose this option, you are responsible for installing and maintaining the CA certificates. They must be installed in \n/etc/grid-security/certificates\n, or a symlink must be made from that location to the directory that contains the CA certificates.\n\n\n\n\nInstalling other CAs\n\n\nIn addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is, \nosg-ca-certs\n and the like, but not \nosg-ca-scripts\n.) They are in addition to the above RPMs, so do not only install these extra CAs.\n\n\n\n\n\n\n\n\nSet of CAs\n\n\nRPM name\n\n\nInstallation command (as root)\n\n\n\n\n\n\n\n\n\n\ncilogon-openid\n\n\ncilogon-openid-ca-cert\n\n\nyum install cilogon-openid-ca-cert\n\n\n\n\n\n\n\n\nVerifying CA Certificates\n\n\nAfter installing the CA certificates, they can be verified with the following command:\n\n\nroot@host #\n curl --cacert \nCA FILE\n \n\\\n\n              --capath \nCA DIRECTORY\n \n\\\n\n              -o /dev/null \n\\\n\n              https://gracc.opensciencegrid.org \n\\\n\n              \n \necho\n \nCA certificate installation verified\n\n\n\n\n\n\nWhere \nCA FILE\n is the path to a valid X.509 CA certificate and \nCA DIRECTORY\n is the path to the directory\ncontaining the installed CA certificates.\nFor example, the following command can be used to verify a default OSG CA certificate installation:\n\n\nroot@host #\n curl --cacert /etc/grid-security/certificates/cilogon-osg.pem \n\\\n\n              --capath /etc/grid-security/certificates/ \n\\\n\n              -o /dev/null \n\\\n\n              https://gracc.opensciencegrid.org \n\\\n\n              \n \necho\n \nCA certificate installation verified\n\n\n  %\n Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\n                                 Dload  Upload   Total   Spent    Left  Speed\n\n\n100 22005    0 22005    0     0  86633      0 --:--:-- --:--:-- --:--:--  499k\n\n\nCA certificate installation verified\n\n\n\n\n\n\nIf you do not see \nCA certificate installation verified\n this means that your CA certificate installation is broken.\nFirst, ensure that your CA installation is up-to-date and if you continue to see issues please \ncontact us\n.\n\n\nManaging Certificate Revocation Lists\n\n\nIn addition to CA certificates, you must have updated Certificate Revocation Lists (CRLs). CRLs contain certificate blacklists that OSG software uses to ensure that your hosts are only talking to valid clients or servers. To maintain up to date CAs, you will need to run the \nfetch-crl\n services. \n\n\n\n\nNote\n\n\nNormally \nfetch-crl\n is installed when you install the rest of the software and you do not need to explicitly install it.\nIf you do wish to install it manually, run the following command:\n\n\nroot@host #\n yum install fetch-crl\n\n\n\n\n\n\n\nIf you do not wish to change the frequency of \nfetch-crl\n updates (default: every 6 hours) or use syslog for \nfetch-crl\n output, \n\nskip to the service management section\n\n\nOptional: configuring \nfetch-crl\n\n\nThe following sub-sections contain optional configuration instructions.\n\n\n\n\nNote\n\n\nNote that the \nnosymlinks\n option in the configuration files refers to ignoring links within the certificates directory (e.g. two different names for the same file). It is perfectly fine if the path of the CA certificates directory itself (\ninfodir\n) is a link to a directory.\n\n\n\n\nChanging the frequency of \nfetch-crl-cron\n\n\nTo modify the times that \nfetch-crl-cron\n runs, edit \n/etc/cron.d/fetch-crl\n.\n\n\nLogging with syslog\n\n\nfetch-crl\n can produce quite a bit of output when run in verbose mode. To send \nfetch-crl\n output to syslog, use the following instructions:\n\n\n\n\n\n\nChange the configuration file to enable syslog:\n\n\nlogmode\n \n=\n \nsyslog\n\n\nsyslogfacility\n \n=\n \ndaemon\n\n\n\n\n\n\n\n\n\n\nMake sure the file \n/var/log/daemon\n exists, e.g. touching the file\n\n\n\n\nChange \n/etc/logrotate.d\n files to rotate it\n\n\n\n\nManaging \nfetch-crl\n services\n\n\nfetch-crl\n is installed as two different system services. The fetch-crl-boot service runs \nfetch-crl\n and is intended to only be enabled or disabled. The \nfetch-crl-cron\n service runs \nfetch-crl\n every 6 hours (with a random sleep time included). Both services are disabled by default. At the very minimum, the \nfetch-crl-cron\n service needs to be enabled and started, otherwise services will begin to fail as existing CRLs expire.\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-cron\n\n\nRuns \nfetch-crl\n every 6 hours\n\n\n\n\n\n\n\n\nfetch-crl-boot\n\n\nRuns \nfetch-crl\n immediately\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReferences\n\n\nx509 certificates:\n\n\n\n\nUseful commands: \nhttp://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html\n\n\nInstall GSI authentication on a server: \nhttp://security.ncsa.illinois.edu/research/wssec/gsihttps/\n\n\nCertificates how-to: \nhttp://www.nordugrid.org/documents/certificate_howto.html\n\n\n\n\nVerifying certificates:\n\n\n\n\nhttp://gagravarr.org/writing/openssl-certs/others.shtml\n\n\nhttp://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/\n\n\nhttp://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html\n\n\n\n\nRelated software:\n\n\n\n\nosg-ca-manage\n\n\nosg-ca-certs-updater\n\n\n\n\nConfiguration files\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nCA File Location\n\n\n/etc/grid-security/certificates\n\n\n\n\n\n\n\n\nAll CA Packages\n\n\nIndex files\n\n\n/etc/grid-security/certificates/INDEX.html\n or \n/etc/grid-security/certificates/INDEX.txt\n\n\nLatest version also available at \nhttp://repo.opensciencegrid.org/pacman/cadist/\n\n\n\n\n\n\nAll CA Packages\n\n\nChange Log\n\n\n/etc/grid-security/certificates/CHANGES\n\n\nLatest version also available at \nhttp://repo.opensciencegrid.org/pacman/cadist/CHANGES\n\n\n\n\n\n\nosg-ca-certs or igtf-ca-certs\n\n\ncontain only CA files\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nConfiguration File for osg-update-certs\n\n\n/etc/osg/osg-update-certs.conf\n\n\nThis file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.\n\n\n\n\n\n\nfetch-crl-3.x\n\n\nConfiguration file\n\n\n/etc/fetch-crl.conf\n\n\n\n\n\n\n\n\n\n\nThe index and change log files contain a summary of all the CA distributed and their version.\n\n\nLogs files\n\n\n\n\n\n\n\n\nPackage\n\n\nFile Description\n\n\nLocation\n\n\n\n\n\n\n\n\n\n\nosg-ca-scripts\n\n\nLog file of osg-update-certs\n\n\n/var/log/osg-update-certs.log\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-update-certs\n\n\n/var/log/osg-ca-certs-status.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of osg-ca-manage\n\n\n/var/log/osg-ca-manage.system.out\n\n\n\n\n\n\nosg-ca-scripts\n\n\nStdout of initial CA setup\n\n\n/var/log/osg-setup-ca-certificates.system.out", 
            "title": "CA Certificates"
        }, 
        {
            "location": "/common/ca/#installing-certificate-authorities-cas", 
            "text": "The  certificate authorities  (CAs) provide the trust roots for the public key infrastructure  OSG uses to maintain integrity of its sites and services.\nThis document provides details of various options to install the Certificate Authority (CA) certificates and have up-to-date certificate revocation lists  (CRLs) on your OSG hosts.  We provide three options for installing CA certificates that offer varying levels of control:   Install an RPM for a specific set of CA certificates ( default )  Install  osg-ca-scripts , a set of scripts that provide fine-grained CA management  Install an RPM that doesn't install  any  CAs.\n    This is useful if you'd like to manage CAs yourself while satisfying RPM dependencies.   Prior to following the instructions on this page, you must enable our  yum repositories", 
            "title": "Installing Certificate Authorities (CAs)"
        }, 
        {
            "location": "/common/ca/#installing-ca-certificates", 
            "text": "Please choose one of the three options to install CA certificates.", 
            "title": "Installing CA Certificates"
        }, 
        {
            "location": "/common/ca/#option-1-install-an-rpm-for-a-specific-set-of-ca-certificates", 
            "text": "Note  This option is the default if you install OSG software without pre-installing CAs.\nFor example,  yum install osg-ce  will bring in  osg-ca-certs  by default.   In the OSG repositories, you will find two different sets of predefined CA certificates:   ( default ) The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions  The  IGTF  CA certificates   See  this page  for details of the contents of the OSG CA package.     If you chose...  Then run the following command...      OSG CA certificates  yum install osg-ca-certs    IGTF CA certificates  yum install igtf-ca-certs     To automatically keep your RPM installation of CAs up to date, we recommend the  OSG CA certificates updater  service.", 
            "title": "Option 1: Install an RPM for a specific set of CA certificates"
        }, 
        {
            "location": "/common/ca/#option-2-install-osg-ca-scripts", 
            "text": "The  osg-ca-scripts  package provides scripts to install and update predefined sets of CAs with the ability to add or remove specific CAs.    The OSG CA certificates. This is similar to the IGTF set but may have a small number of additions or deletions  The  IGTF  CA certificates   See  this page  for details of the contents of the OSG CA package.    Install the  osg-ca-scripts  package:  root@host #  yum install osg-ca-scripts    Choose and install the CA certificate set:     If you choose...  Then run the following command...      OSG CA certificates  osg-ca-manage setupCA --location root --url osg    IGTF CA certificates  osg-ca-manage setupCA --location root --url igtf       Enable the  osg-update-certs-cron  service to enable periodic CA updates. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME       (Optional) To add a new CA:  osg - ca - manage   add   [ --dir  local_dir ] --hash  CA-HASH     (Optional) To remove a CA  osg - ca - manage   remove   --hash  CA-HASH     A complete set of options available though  osg-ca-manage  command, can be found in the  osg-ca-manage documentation", 
            "title": "Option 2: Install osg-ca-scripts"
        }, 
        {
            "location": "/common/ca/#option-3-site-managed-cas", 
            "text": "If you want to handle the list of CAs completely internally to your site, you can utilize the  empty-ca-certs  RPM to satisfy\nRPM dependencies while not actually installing any CAs. To install this RPM, run the following command:  root@host #  yum install empty-ca-certs \u2013-enablerepo = osg-empty   Warning  If you choose this option, you are responsible for installing and maintaining the CA certificates. They must be installed in  /etc/grid-security/certificates , or a symlink must be made from that location to the directory that contains the CA certificates.", 
            "title": "Option 3: Site-managed CAs"
        }, 
        {
            "location": "/common/ca/#installing-other-cas", 
            "text": "In addition to the above CAs, you can install other CAs via RPM. These only work with the RPMs that provide CAs (that is,  osg-ca-certs  and the like, but not  osg-ca-scripts .) They are in addition to the above RPMs, so do not only install these extra CAs.     Set of CAs  RPM name  Installation command (as root)      cilogon-openid  cilogon-openid-ca-cert  yum install cilogon-openid-ca-cert", 
            "title": "Installing other CAs"
        }, 
        {
            "location": "/common/ca/#verifying-ca-certificates", 
            "text": "After installing the CA certificates, they can be verified with the following command:  root@host #  curl --cacert  CA FILE   \\ \n              --capath  CA DIRECTORY   \\ \n              -o /dev/null  \\ \n              https://gracc.opensciencegrid.org  \\ \n                 echo   CA certificate installation verified   Where  CA FILE  is the path to a valid X.509 CA certificate and  CA DIRECTORY  is the path to the directory\ncontaining the installed CA certificates.\nFor example, the following command can be used to verify a default OSG CA certificate installation:  root@host #  curl --cacert /etc/grid-security/certificates/cilogon-osg.pem  \\ \n              --capath /etc/grid-security/certificates/  \\ \n              -o /dev/null  \\ \n              https://gracc.opensciencegrid.org  \\ \n                 echo   CA certificate installation verified    %  Total    % Received % Xferd  Average Speed   Time    Time     Time  Current                                  Dload  Upload   Total   Spent    Left  Speed  100 22005    0 22005    0     0  86633      0 --:--:-- --:--:-- --:--:--  499k  CA certificate installation verified   If you do not see  CA certificate installation verified  this means that your CA certificate installation is broken.\nFirst, ensure that your CA installation is up-to-date and if you continue to see issues please  contact us .", 
            "title": "Verifying CA Certificates"
        }, 
        {
            "location": "/common/ca/#managing-certificate-revocation-lists", 
            "text": "In addition to CA certificates, you must have updated Certificate Revocation Lists (CRLs). CRLs contain certificate blacklists that OSG software uses to ensure that your hosts are only talking to valid clients or servers. To maintain up to date CAs, you will need to run the  fetch-crl  services.    Note  Normally  fetch-crl  is installed when you install the rest of the software and you do not need to explicitly install it.\nIf you do wish to install it manually, run the following command:  root@host #  yum install fetch-crl   If you do not wish to change the frequency of  fetch-crl  updates (default: every 6 hours) or use syslog for  fetch-crl  output,  skip to the service management section", 
            "title": "Managing Certificate Revocation Lists"
        }, 
        {
            "location": "/common/ca/#optional-configuring-fetch-crl", 
            "text": "The following sub-sections contain optional configuration instructions.   Note  Note that the  nosymlinks  option in the configuration files refers to ignoring links within the certificates directory (e.g. two different names for the same file). It is perfectly fine if the path of the CA certificates directory itself ( infodir ) is a link to a directory.", 
            "title": "Optional: configuring fetch-crl"
        }, 
        {
            "location": "/common/ca/#changing-the-frequency-of-fetch-crl-cron", 
            "text": "To modify the times that  fetch-crl-cron  runs, edit  /etc/cron.d/fetch-crl .", 
            "title": "Changing the frequency of fetch-crl-cron"
        }, 
        {
            "location": "/common/ca/#logging-with-syslog", 
            "text": "fetch-crl  can produce quite a bit of output when run in verbose mode. To send  fetch-crl  output to syslog, use the following instructions:    Change the configuration file to enable syslog:  logmode   =   syslog  syslogfacility   =   daemon     Make sure the file  /var/log/daemon  exists, e.g. touching the file   Change  /etc/logrotate.d  files to rotate it", 
            "title": "Logging with syslog"
        }, 
        {
            "location": "/common/ca/#managing-fetch-crl-services", 
            "text": "fetch-crl  is installed as two different system services. The fetch-crl-boot service runs  fetch-crl  and is intended to only be enabled or disabled. The  fetch-crl-cron  service runs  fetch-crl  every 6 hours (with a random sleep time included). Both services are disabled by default. At the very minimum, the  fetch-crl-cron  service needs to be enabled and started, otherwise services will begin to fail as existing CRLs expire.     Software  Service name  Notes      Fetch CRL  fetch-crl-cron  Runs  fetch-crl  every 6 hours     fetch-crl-boot  Runs  fetch-crl  immediately     Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Managing fetch-crl services"
        }, 
        {
            "location": "/common/ca/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/common/ca/#references", 
            "text": "x509 certificates:   Useful commands:  http://security.ncsa.illinois.edu/research/grid-howtos/usefulopenssl.html  Install GSI authentication on a server:  http://security.ncsa.illinois.edu/research/wssec/gsihttps/  Certificates how-to:  http://www.nordugrid.org/documents/certificate_howto.html   Verifying certificates:   http://gagravarr.org/writing/openssl-certs/others.shtml  http://www.cyberciti.biz/faq/test-ssl-certificates-diagnosis-ssl-certificate/  http://www.cyberciti.biz/tips/debugging-ssl-communications-from-unix-shell-prompt.html   Related software:   osg-ca-manage  osg-ca-certs-updater", 
            "title": "References"
        }, 
        {
            "location": "/common/ca/#configuration-files", 
            "text": "Package  File Description  Location  Comment      All CA Packages  CA File Location  /etc/grid-security/certificates     All CA Packages  Index files  /etc/grid-security/certificates/INDEX.html  or  /etc/grid-security/certificates/INDEX.txt  Latest version also available at  http://repo.opensciencegrid.org/pacman/cadist/    All CA Packages  Change Log  /etc/grid-security/certificates/CHANGES  Latest version also available at  http://repo.opensciencegrid.org/pacman/cadist/CHANGES    osg-ca-certs or igtf-ca-certs  contain only CA files      osg-ca-scripts  Configuration File for osg-update-certs  /etc/osg/osg-update-certs.conf  This file may be edited by hand, though it is recommended to use osg-ca-manage to set configuration parameters.    fetch-crl-3.x  Configuration file  /etc/fetch-crl.conf      The index and change log files contain a summary of all the CA distributed and their version.", 
            "title": "Configuration files"
        }, 
        {
            "location": "/common/ca/#logs-files", 
            "text": "Package  File Description  Location      osg-ca-scripts  Log file of osg-update-certs  /var/log/osg-update-certs.log    osg-ca-scripts  Stdout of osg-update-certs  /var/log/osg-ca-certs-status.system.out    osg-ca-scripts  Stdout of osg-ca-manage  /var/log/osg-ca-manage.system.out    osg-ca-scripts  Stdout of initial CA setup  /var/log/osg-setup-ca-certificates.system.out", 
            "title": "Logs files"
        }, 
        {
            "location": "/common/yum/", 
            "text": "OSG Yum Repositories\n\n\nThis document introduces Yum repositories and how they are used in the OSG.\nIf you are unfamiliar with Yum, see the \ndocumentation on using Yum and RPM\n.\n\n\nRepositories\n\n\nThe OSG hosts multiple repositories at \nrepo.opensciencegrid.org\n that are\nintended for public use:\n\n\n\n\n\n\n\n\nThe OSG Yum repository...\n\n\nContains RPMs that...\n\n\n\n\n\n\n\n\n\n\nosg\n\n\nare considered production-ready (default).\n\n\n\n\n\n\nosg-rolling\n\n\nare considered production-ready but are released at faster pace than the \nosg\n repository.\n\n\n\n\n\n\nosg-testing\n\n\nhave passed developer or integration testing but not acceptance testing\n\n\n\n\n\n\nosg-development\n\n\nhave not passed developer, integration or acceptance testing. Do not use without instruction from the OSG Software and Release Team.\n\n\n\n\n\n\nosg-upcoming\n, \nosg-upcoming-rolling\n, \nosg-upcoming-testing\n, \nosg-upcoming-development\n\n\nhave newer versions that may require manual action after an update. See \nthis section\n for details.\n\n\n\n\n\n\nosg-contrib\n\n\nhave been contributed from outside of the OSG Software and Release Team. See \nthis section\n for details.\n\n\n\n\n\n\n\n\nOSG's RPM packages also rely on external packages provided by supported OSes and EPEL.\nYou must have the following repositories available and enabled:\n\n\n\n\nOS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories, including \"extras\" repositories)\n\n\nEPEL repositories\n\n\nOSG repositories\n\n\n\n\nIf any of these repositories are missing, you may end up with installation issues or missing dependencies.\n\n\n\n\nDanger\n\n\nOther repositories, such as \njpackage\n, \ndag\n, or \nrpmforge\n, are not supported and you may encounter problems if\nyou use them.\n\n\n\n\nUpcoming Software\n\n\nCertain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\":\nupgrading to them would likely require manual intervention after their installation.\nWe do not want sites to unwittingly upgrade to these versions.\nFor the benefit of the sites that are interested in upgrading to these versions, we want to provide the same assurance\nof quality and production-readiness that we guarantee for the \nosg\n release repository.\n\n\nDue to the relatively small number of such packages, a full fork of the OSG 3 distribution is not warranted.\nInstead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.\n\n\nThese repositories have the same structure as our standard repositories.\nFor example, there are \nosg-upcoming-testing\n and \nosg-upcoming\n repositories, which are analagous to the \nosg-testing\n\nand \nosg\n repositories, respectively.\n\n\nA full installation of our software stack is \nnot\n possible using only the \nosg-upcoming\n repositories, since they\ncontain a small subset of the software we ship.\nBoth the main \nosg\n and the \nosg-upcoming\n repositories will need to be enabled for the installation to work.\nBecause of this, interoperability will be maintained between the main \nosg\n and \nosg-upcoming\n.\n\n\nDepending on test results from sites, some packages in \nosg-upcoming\n may eventually end up in the main \nosg\n branch.\nThe rest of the packages will eventually form the basis of the next \nOSG release series\n\n(e.g. \"OSG 3.5\").\n\n\nContrib Software\n\n\nIn addition to our regular software repositories, we also have a \ncontrib\n (short for \"contributed\") software repository.\nThis is software that is does not go through the same software testing and release processes as the official OSG\nSoftware release, but may be useful to you.\nParticularly, contrib software is not guaranteed to be compatible with the rest of the OSG Software stack nor is it\nsupported by the OSG.\n\n\nThe definitive list of software in the contrib repository can be found here:\n\n\n\n\nOSG 3.4 EL6 contrib software repository\n\n\nOSG 3.4 EL7 contrib software repository\n\n\n\n\nIf you would like to distribute your software in the OSG \ncontrib\n repository, please \ncontact us\n with a\ndescription of your software, what users it serves, and relevant RPM packaging.\n\n\nInstalling Yum Repositories\n\n\nInstall the Yum priorities plugin\n\n\nThe Yum priorities plugin is used to tell Yum to prefer OSG packages over EPEL or OS packages.\nIt is important to install and enable the Yum priorities plugin before installing grid software to ensure that you are\ngetting the OSG-supported versions.\n\n\n\n\n\n\nInstall the Yum priorities package:\n\n\nroot@host #\n yum install yum-plugin-priorities\n\n\n\n\n\n\n\n\n\nEnsure that \n/etc/yum.conf\n has the following line in the \n[main]\n section:\n\n\nplugins\n=\n1\n\n\n\n\n\n\n\n\n\n\nEnable the \"extras\" OS repositories\n\n\nSome packages depend on packages in the \"extras\" repositories of your OS,\nso you must ensure that those repositories are enabled.\n\n\nThe instructions for this vary based on your OS:\n\n\n\n\n\n\nOn Scientific Linux, install the \nyum-conf-extras\n RPM package,\n  and ensure that the \nsl-extras\n repo in \n/etc/yum.repos.d/sl-extras.repo\n is enabled.\n\n\n\n\n\n\nOn CentOS, ensure that the \nextras\n repo in \n/etc/yum.repos.d/CentOS-Base.repo\n is enabled.\n\n\n\n\n\n\nOn Red Hat Enterprise Linux, ensure that the \nServer-Extras\n channel is enabled.\n\n\n\n\n\n\n\n\nNote\n\n\nA repository is enabled if it has \nenabled=1\n in its definition,\nor if the \nenabled\n line is missing\n(i.e. it is enabled unless specified otherwise.)\n\n\n\n\nInstall the EPEL repositories\n\n\nOSG software depends on packages distributed via the \nEPEL\n repositories.\nYou must install and enable these first.\n\n\n\n\n\n\nInstall the EPEL repository, if not already present.  Choose the right version to match your OS version.\n\n\n#\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n\n\nroot@host #\n yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm\n\n#\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n\n\nroot@host #\n yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n\n\n\n\n\n\n\n\n\nVerify that \n/etc/yum.repos.d/epel.repo\n exists; the \n[epel]\n section should contain:\n\n\n\n\n\n\nThe line \nenabled=1\n\n\n\n\n\n\nEither no \npriority\n setting, or a \npriority\n setting that is 99 or higher\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nIf you have your own mirror or configuration of the EPEL repository, you \nMUST\n verify that the priority of the\nEPEL repository is either missing, or 99 or a higher number.\nThe OSG repositories must have a better (numerically lower) priority than the EPEL repositories;\notherwise, you might have dependency resolution (\"depsolving\") issues.\n\n\n\n\nInstall the OSG Repositories\n\n\nThis document assumes a fresh install.\nFor instructions on upgrading from one OSG series to another, see the\n\nrelease series document\n.\n\n\nInstall the OSG repositories:\n\n\n#\n# EPEL 6 (For RHEL 6, CentOS 6, and SL 6)\n\n\nroot@host #\n yum install https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n#\n# EPEL 7 (For RHEL 7, CentOS 7, and SL 7)\n\n\nroot@host #\n yum install https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\nThe only OSG repository enabled by default is the release one.\nIf you want to \nenable another one\n (e.g. \nosg-testing\n), then edit its file\n(e.g. \n/etc/yum.repos.d/osg-testing.repo\n) and change the enabled option from 0 to 1:\n\n\n[osg-testing]\n\n\nname\n=\nOSG Software for Enterprise Linux 7 - Testing - $basearch\n\n\n#baseurl=https://repo.opensciencegrid.org/osg/3.4/el7/testing/$basearch\n\n\nmirrorlist\n=\nhttps://repo.opensciencegrid.org/mirror/osg/3.4/el7/testing/$basearch\n\n\nfailovermethod\n=\npriority\n\n\npriority\n=\n98\n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1\n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nOptional Configuration\n\n\nEnable automatic security updates\n\n\nFor production services, we suggest only changing software versions during controlled downtime.\nTherefore we recommend security-only automatic updates or disabling automatic updates entirely.\n\n\nTo enable only security related automatic updates:\n\n\n\n\n\n\nOn Enterprise Linux 6, edit \n/etc/sysconfig/yum-autoupdate\n and set \nUSE_YUMSEC=\"true\"\n\n\n\n\n\n\nOn Enterprise Linux 7, edit \n/etc/yum/yum-cron.conf\n and set \nupdate_cmd = security\n\n\n\n\n\n\nTo disable automatic updates entirely:\n\n\n\n\n\n\nOn Enterprise Linux 6, edit \n/etc/sysconfig/yum-autoupdate\n and set \nENABLED=\"false\"\n\n\n\n\n\n\nOn Enterprise Linux 7, run:\n\n\nroot@host #\n service yum-cron stop\n\n\n\n\n\n\n\n\n\nConfiguring Spacewalk priorities\n\n\nSites using \nSpacewalk\n to manage RPM packages will need to configure OSG Yum\nrepository priorities using their Spacewalk ID. For example, if the OSG 3.4 repository's Spacewalk ID is\n\ncentos_7_osg34_dev\n, modify \n/etc/yum/pluginconf.d/90-osg.conf\n to include the following:\n\n\n```\n[centos_7_osg_34_dev]\npriority = 98\n\n\nRepository Mirrors\n\n\nIf you run a large site (\n20 nodes), you should consider setting up a local mirror for the OSG repositories.\nA local Yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.\n\n\nAdd the following to a file in \n/etc/cron.d\n:\n\n\n%\nRED\n%\nRANDOM\n%\nENDCOLOR\n%\n \n*\n \n*\n \n*\n \n*\n \nroot\n \nrsync\n \n-\naH\n \nrsync\n:\n//\nrepo\n.\nopensciencegrid\n.\norg\n/\nosg\n/\n \n/\nvar\n/\nwww\n/\nhtml\n/\nosg\n/\n\n\n\n\n\n\nOr, to mirror only a single repository:\n\n\n%\nRED\n%\nRANDOM\n%\nENDCOLOR\n%\n \n*\n \n*\n \n*\n \n*\n \nroot\n \nrsync\n \n-\naH\n \nrsync\n:\n//\nrepo\n.\nopensciencegrid\n.\norg\n/\nosg\n/%\nRED\n%\nOSG_RELEASE\n%\nENDCOLOR\n%/\nel6\n/\ndevelopment\n \n/\nvar\n/\nwww\n/\nhtml\n/\nosg\n/%\nRED\n%\nOSG_RELEASE\n%\nENDCOLOR\n%/\nel6\n\n\n\n\n\n\nReplace \nRANDOM\n with a number between 0 and 59.\n\n\nReplace \nOSG_RELEASE\n with the OSG release you would like to use (e.g. '3.4').\n\n\nOn your worker node, you can replace the \nbaseurl\n line of \n/etc/yum.repos.d/osg.repo\n with the appropriate URL for your\nmirror.\n\n\nIf you are interested in having your mirror be part of the OSG's default set of mirrors,\n\nplease file a support ticket\n.\n\n\nReference\n\n\n\n\nBasic use of Yum", 
            "title": "OSG Yum Repos"
        }, 
        {
            "location": "/common/yum/#osg-yum-repositories", 
            "text": "This document introduces Yum repositories and how they are used in the OSG.\nIf you are unfamiliar with Yum, see the  documentation on using Yum and RPM .", 
            "title": "OSG Yum Repositories"
        }, 
        {
            "location": "/common/yum/#repositories", 
            "text": "The OSG hosts multiple repositories at  repo.opensciencegrid.org  that are\nintended for public use:     The OSG Yum repository...  Contains RPMs that...      osg  are considered production-ready (default).    osg-rolling  are considered production-ready but are released at faster pace than the  osg  repository.    osg-testing  have passed developer or integration testing but not acceptance testing    osg-development  have not passed developer, integration or acceptance testing. Do not use without instruction from the OSG Software and Release Team.    osg-upcoming ,  osg-upcoming-rolling ,  osg-upcoming-testing ,  osg-upcoming-development  have newer versions that may require manual action after an update. See  this section  for details.    osg-contrib  have been contributed from outside of the OSG Software and Release Team. See  this section  for details.     OSG's RPM packages also rely on external packages provided by supported OSes and EPEL.\nYou must have the following repositories available and enabled:   OS repositories (SL 6/7, CentOS 6/7, or RHEL 6/7 repositories, including \"extras\" repositories)  EPEL repositories  OSG repositories   If any of these repositories are missing, you may end up with installation issues or missing dependencies.   Danger  Other repositories, such as  jpackage ,  dag , or  rpmforge , are not supported and you may encounter problems if\nyou use them.", 
            "title": "Repositories"
        }, 
        {
            "location": "/common/yum/#upcoming-software", 
            "text": "Certain sites have requested new versions of software that would be considered \"disruptive\" or \"experimental\":\nupgrading to them would likely require manual intervention after their installation.\nWe do not want sites to unwittingly upgrade to these versions.\nFor the benefit of the sites that are interested in upgrading to these versions, we want to provide the same assurance\nof quality and production-readiness that we guarantee for the  osg  release repository.  Due to the relatively small number of such packages, a full fork of the OSG 3 distribution is not warranted.\nInstead, we have created a separate set of repositories that contain only the \"disruptive\" versions of the software.  These repositories have the same structure as our standard repositories.\nFor example, there are  osg-upcoming-testing  and  osg-upcoming  repositories, which are analagous to the  osg-testing \nand  osg  repositories, respectively.  A full installation of our software stack is  not  possible using only the  osg-upcoming  repositories, since they\ncontain a small subset of the software we ship.\nBoth the main  osg  and the  osg-upcoming  repositories will need to be enabled for the installation to work.\nBecause of this, interoperability will be maintained between the main  osg  and  osg-upcoming .  Depending on test results from sites, some packages in  osg-upcoming  may eventually end up in the main  osg  branch.\nThe rest of the packages will eventually form the basis of the next  OSG release series \n(e.g. \"OSG 3.5\").", 
            "title": "Upcoming Software"
        }, 
        {
            "location": "/common/yum/#contrib-software", 
            "text": "In addition to our regular software repositories, we also have a  contrib  (short for \"contributed\") software repository.\nThis is software that is does not go through the same software testing and release processes as the official OSG\nSoftware release, but may be useful to you.\nParticularly, contrib software is not guaranteed to be compatible with the rest of the OSG Software stack nor is it\nsupported by the OSG.  The definitive list of software in the contrib repository can be found here:   OSG 3.4 EL6 contrib software repository  OSG 3.4 EL7 contrib software repository   If you would like to distribute your software in the OSG  contrib  repository, please  contact us  with a\ndescription of your software, what users it serves, and relevant RPM packaging.", 
            "title": "Contrib Software"
        }, 
        {
            "location": "/common/yum/#installing-yum-repositories", 
            "text": "", 
            "title": "Installing Yum Repositories"
        }, 
        {
            "location": "/common/yum/#install-the-yum-priorities-plugin", 
            "text": "The Yum priorities plugin is used to tell Yum to prefer OSG packages over EPEL or OS packages.\nIt is important to install and enable the Yum priorities plugin before installing grid software to ensure that you are\ngetting the OSG-supported versions.    Install the Yum priorities package:  root@host #  yum install yum-plugin-priorities    Ensure that  /etc/yum.conf  has the following line in the  [main]  section:  plugins = 1", 
            "title": "Install the Yum priorities plugin"
        }, 
        {
            "location": "/common/yum/#enable-the-extras-os-repositories", 
            "text": "Some packages depend on packages in the \"extras\" repositories of your OS,\nso you must ensure that those repositories are enabled.  The instructions for this vary based on your OS:    On Scientific Linux, install the  yum-conf-extras  RPM package,\n  and ensure that the  sl-extras  repo in  /etc/yum.repos.d/sl-extras.repo  is enabled.    On CentOS, ensure that the  extras  repo in  /etc/yum.repos.d/CentOS-Base.repo  is enabled.    On Red Hat Enterprise Linux, ensure that the  Server-Extras  channel is enabled.     Note  A repository is enabled if it has  enabled=1  in its definition,\nor if the  enabled  line is missing\n(i.e. it is enabled unless specified otherwise.)", 
            "title": "Enable the \"extras\" OS repositories"
        }, 
        {
            "location": "/common/yum/#install-the-epel-repositories", 
            "text": "OSG software depends on packages distributed via the  EPEL  repositories.\nYou must install and enable these first.    Install the EPEL repository, if not already present.  Choose the right version to match your OS version.  # # EPEL 6 (For RHEL 6, CentOS 6, and SL 6)  root@host #  yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-6.noarch.rpm # # EPEL 7 (For RHEL 7, CentOS 7, and SL 7)  root@host #  yum install https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm    Verify that  /etc/yum.repos.d/epel.repo  exists; the  [epel]  section should contain:    The line  enabled=1    Either no  priority  setting, or a  priority  setting that is 99 or higher       Warning  If you have your own mirror or configuration of the EPEL repository, you  MUST  verify that the priority of the\nEPEL repository is either missing, or 99 or a higher number.\nThe OSG repositories must have a better (numerically lower) priority than the EPEL repositories;\notherwise, you might have dependency resolution (\"depsolving\") issues.", 
            "title": "Install the EPEL repositories"
        }, 
        {
            "location": "/common/yum/#install-the-osg-repositories", 
            "text": "This document assumes a fresh install.\nFor instructions on upgrading from one OSG series to another, see the release series document .  Install the OSG repositories:  # # EPEL 6 (For RHEL 6, CentOS 6, and SL 6)  root@host #  yum install https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm # # EPEL 7 (For RHEL 7, CentOS 7, and SL 7)  root@host #  yum install https://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm  The only OSG repository enabled by default is the release one.\nIf you want to  enable another one  (e.g.  osg-testing ), then edit its file\n(e.g.  /etc/yum.repos.d/osg-testing.repo ) and change the enabled option from 0 to 1:  [osg-testing]  name = OSG Software for Enterprise Linux 7 - Testing - $basearch  #baseurl=https://repo.opensciencegrid.org/osg/3.4/el7/testing/$basearch  mirrorlist = https://repo.opensciencegrid.org/mirror/osg/3.4/el7/testing/$basearch  failovermethod = priority  priority = 98  enabled = 1  gpgcheck = 1  gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG", 
            "title": "Install the OSG Repositories"
        }, 
        {
            "location": "/common/yum/#optional-configuration", 
            "text": "", 
            "title": "Optional Configuration"
        }, 
        {
            "location": "/common/yum/#enable-automatic-security-updates", 
            "text": "For production services, we suggest only changing software versions during controlled downtime.\nTherefore we recommend security-only automatic updates or disabling automatic updates entirely.  To enable only security related automatic updates:    On Enterprise Linux 6, edit  /etc/sysconfig/yum-autoupdate  and set  USE_YUMSEC=\"true\"    On Enterprise Linux 7, edit  /etc/yum/yum-cron.conf  and set  update_cmd = security    To disable automatic updates entirely:    On Enterprise Linux 6, edit  /etc/sysconfig/yum-autoupdate  and set  ENABLED=\"false\"    On Enterprise Linux 7, run:  root@host #  service yum-cron stop", 
            "title": "Enable automatic security updates"
        }, 
        {
            "location": "/common/yum/#configuring-spacewalk-priorities", 
            "text": "Sites using  Spacewalk  to manage RPM packages will need to configure OSG Yum\nrepository priorities using their Spacewalk ID. For example, if the OSG 3.4 repository's Spacewalk ID is centos_7_osg34_dev , modify  /etc/yum/pluginconf.d/90-osg.conf  to include the following:  ```\n[centos_7_osg_34_dev]\npriority = 98", 
            "title": "Configuring Spacewalk priorities"
        }, 
        {
            "location": "/common/yum/#repository-mirrors", 
            "text": "If you run a large site ( 20 nodes), you should consider setting up a local mirror for the OSG repositories.\nA local Yum mirror allows you to reduce the amount of external bandwidth used when updating or installing packages.  Add the following to a file in  /etc/cron.d :  % RED % RANDOM % ENDCOLOR %   *   *   *   *   root   rsync   - aH   rsync : // repo . opensciencegrid . org / osg /   / var / www / html / osg /   Or, to mirror only a single repository:  % RED % RANDOM % ENDCOLOR %   *   *   *   *   root   rsync   - aH   rsync : // repo . opensciencegrid . org / osg /% RED % OSG_RELEASE % ENDCOLOR %/ el6 / development   / var / www / html / osg /% RED % OSG_RELEASE % ENDCOLOR %/ el6   Replace  RANDOM  with a number between 0 and 59.  Replace  OSG_RELEASE  with the OSG release you would like to use (e.g. '3.4').  On your worker node, you can replace the  baseurl  line of  /etc/yum.repos.d/osg.repo  with the appropriate URL for your\nmirror.  If you are interested in having your mirror be part of the OSG's default set of mirrors, please file a support ticket .", 
            "title": "Repository Mirrors"
        }, 
        {
            "location": "/common/yum/#reference", 
            "text": "Basic use of Yum", 
            "title": "Reference"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/", 
            "text": "Configuration with OSG-Configure\n\n\nOSG-Configure and the INI files in \n/etc/osg/config.d\n allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.\n\n\nThis page gives an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nInvocation and script usage\n\n\nThe \nosg-configure\n script is used to process the INI files and apply changes to the system.\n\nosg-configure\n must be run as root.\n\n\nThe typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.\n\n\nTo verify the config files, run:\n\n\n[root@server] osg-configure -v\n\n\n\n\n\n\nOSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:\n\n\n\n\nRequired option not filled in\n\n\nInvalid value\n\n\nSyntax error\n\n\nInconsistencies between options\n\n\n\n\nTo apply changes, run:\n\n\n[root@server] osg-configure -c\n\n\n\n\n\n\nIf your INI files do not change, then re-running \nosg-configure -c\n will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.\n\n\nOSG-Configure is split up into modules. Normally, all modules are run when calling \nosg-configure\n.\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:\n\n\n[root@server] osg-configure -l\n\n\n\n\n\n\nIf the module can be run separately, specify it with the \n-m \nMODULE\n option:\n\n\n[root@server] osg-configure -c -m \nMODULE\n\n\n\n\n\n\nOptions may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:\n\n\n[root@server] osg-configure -o \nOPTION\n\n\n[root@server] osg-configure -o \nSECTION\n.\nOPTION\n\n\n\n\n\n\nLogs are written to \n/var/log/osg/osg-configure.log\n.\nIf something goes wrong, specify the \n-d\n flag to add more verbose output to \nosg-configure.log\n.\n\n\nThe rest of this document will detail what to specify in the INI files.\n\n\nConventions\n\n\nIn the tables below:\n\n\n\n\nMandatory options for a section are given in \nbold\n type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.\n\n\nOptions that are not found in the default ini file are in \nitalics\n.\n\n\n\n\nSyntax and layout\n\n\nThe configuration files used by \nosg-configure\n are the one supported by Python's \nSafeConfigParser\n, similar in format to the \nINI configuration file\n used by MS Windows:\n\n\n\n\nConfig files are separated into sections, specified by a section name in square brackets (e.g. \n[Section 1]\n)\n\n\nOptions should be set using \nname = value\n pairs\n\n\nLines that begin with \n;\n or \n#\n are comments\n\n\nLong lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in \nemail RFC 822\n)\n\n\nVariable substitutions are supported -- \nsee below\n\n\n\n\nosg-configure\n reads and uses all of the files in \n/etc/osg/config.d\n that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called \n[PBS]\n can be given in \n20-pbs.ini\n as well as \n99-local-site-settings.ini\n.\n\n\nEach of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g. \n99-local-site-settings.ini\n) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.\n\n\nVariable substitution\n\n\nThe osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name \nmyoption\n in the section, you can substitute the value of that option elsewhere in the section by referring to it as \n%(myoption)s\n.\n\n\n\n\nNote\n\n\nThe trailing \ns\n is required. Also, option names cannot have a variable subsitution in them.\n\n\n\n\nSpecial Settings\n\n\nIf a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.\n\n\nIgnore setting\n\n\nThe \nenabled\n option, specifying whether a service is enabled or not, is a boolean but also accepts \nIgnore\n as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using \nFalse\n (or the \n%(disabled)s\n variable), because using \nFalse\n results in the service associated with the section being disabled. \nosg-configure\n will not change the configuration of the service if the \nenabled\n is set to \nIgnore\n.\n\n\nThis is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the \nIgnore\n setting so that \nosg-configure\n does not alter the service's configuration and status.\n\n\nConfiguration sections\n\n\nThe OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g. \n[Section 1]\n). The configuration is split in multiple files and options form one section can be in more than one files.\n\n\nThe following sections give an overview of the options for each of the sections of the configuration files that \nosg-configure\n uses.\n\n\nBosco\n\n\nThis section is contained in \n/etc/osg/config.d/20-bosco.ini\n which is provided by the \nosg-configure-bosco\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Bosco jobmanager is being used or not.\n\n\n\n\n\n\nusers\n\n\nString\n\n\nA comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.\n\n\n\n\n\n\nendpoint\n\n\nString\n\n\nThe remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of \n, exactly as you would use to ssh into the remote cluster.\n\n\n\n\n\n\nbatch\n\n\nString\n\n\nThe type of scheduler installed on the remote cluster.\n\n\n\n\n\n\nssh_key\n\n\nString\n\n\nThe location of the ssh key, as created above.\n\n\n\n\n\n\n\n\nCondor\n\n\nThis section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-condor.ini\n which is provided by the \nosg-configure-condor\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Condor jobmanager is being used or not.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nThis should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use \n/usr\n which works for the RPM installation.\n\n\n\n\n\n\ncondor_config\n\n\nString\n\n\nThis should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the \nosg-configure\n script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use \n/etc/condor/condor_config\n, the default for the RPM installation.\n\n\n\n\n\n\n\n\nLSF\n\n\nThis section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-lsf.ini\n which is provided by the \nosg-configure-lsf\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the LSF jobmanager is being used or not.\n\n\n\n\n\n\nlsf_location\n\n\nString\n\n\nThis should be set to be directory where lsf is installed\n\n\n\n\n\n\n\n\nPBS\n\n\nThis section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-pbs.ini\n which is provided by the \nosg-configure-pbs\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the PBS jobmanager is being used or not.\n\n\n\n\n\n\npbs_location\n\n\nString\n\n\nThis should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.\n\n\n\n\n\n\naccounting_log_directory\n\n\nString\n\n\nThis setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.\n\n\n\n\n\n\npbs_server\n\n\nString\n\n\nThis setting is optional and should point to your PBS server node if it is different from your OSG CE\n\n\n\n\n\n\n\n\nSGE\n\n\nThis section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-sge.ini\n which is provided by the \nosg-configure-sge\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the SGE jobmanager is being used or not.\n\n\n\n\n\n\nsge_root\n\n\nString\n\n\nThis should be set to be directory where sge is installed (e.g. same as \n$SGE_ROOT\n variable).\n\n\n\n\n\n\nsge_cell\n\n\nString\n\n\nThe sge_cell setting should be set to the value of $SGE_CELL for your SGE install.\n\n\n\n\n\n\ndefault_queue\n\n\nString\n\n\nThis setting determines queue that jobs should be placed in if the job description does not specify a queue.\n\n\n\n\n\n\navailable_queues\n\n\nString\n\n\nThis setting indicates which queues are available on the cluster and should be used for validation when \nvalidate_queues\n is set.\n\n\n\n\n\n\nvalidate_queues\n\n\nString\n\n\nThis setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nvalidate_queues\n:\n\nIf \navailable_queues\n is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.\n\n\n\n\nSlurm\n\n\nThis section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the \nenabled\n setting should be set to \nFalse\n.\n\n\nThis section is contained in \n/etc/osg/config.d/20-slurm.ini\n which is provided by the \nosg-configure-slurm\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the Slurm jobmanager is being used or not.\n\n\n\n\n\n\nslurm_location\n\n\nString\n\n\nThis should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.\n\n\n\n\n\n\ndb_host\n\n\nString\n\n\nHostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_port\n\n\nString\n\n\nPort of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_user\n\n\nString\n\n\nUsername used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_pass\n\n\nString\n\n\nThe location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\ndb_name\n\n\nString\n\n\nName of the SLURM database. This information is needed to configure the SLURM gratia probe.\n\n\n\n\n\n\nslurm_cluster\n\n\nString\n\n\nThe name of the Slurm cluster\n\n\n\n\n\n\n\n\nGratia\n\n\nThis section configures Gratia. If \nprobes\n is set to \nUNAVAILABLE\n, then \nosg-configure\n will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes, \n%(osg-jobmanager-gratia)s\n, \n%(osg-gridftp-gratia)s\n, \n%(osg-metric-gratia)s\n, \n%(itb-jobmanager-gratia)s\n, \n%(itb-gridftp-gratia)s\n, \n%(itb-metric-gratia)s\n are defined in the default configuration files to make it easier to specify the standard osg reporting.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gratia.ini\n which is provided by the \nosg-configure-gratia\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n , \nFalse\n, \nIgnore\n\n\nThis should be set to True if gratia should be configured and enabled on the installation being configured.\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThis should be set to the resource name as given in the OIM registration\n\n\n\n\n\n\nprobes\n\n\nString\n\n\nThis should be set to the gratia probes that should be enabled. A probe is specified by using as \n[probe_type]:server:port\n. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nprobes\n:\n\nLegal values for \nprobe_type\n are:\n\n\n\n\nmetric\n (for RSV)\n\n\njobmanager\n (for the appropriate jobmanager probe)\n\n\ngridftp\n (for the GridFTP transfer probe)\n\n\n\n\n\n\nInfo Services\n\n\nReporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.\n\n\nThis section is contained in \n/etc/osg/config.d/30-infoservices.ini\n, which is provided by the \nosg-configure-infoservices\n RPM. (This is for historical reasons.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nTrue if reporting should be configured and enabled\n\n\n\n\n\n\nce_collectors\n\n\nString\n\n\nThe server(s) HTCondor-CE information should be sent to. See note\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nce_collectors\n:\n\n\n\n\nSet this to \nDEFAULT\n to report to the OSG Production or ITB servers (depending on your \nSite Information\n configuration).\n\n\nSet this to \nPRODUCTION\n to report to the OSG Production servers\n\n\nSet this to \nITB\n to report to the OSG ITB servers\n\n\nOtherwise, set this to the \nhostname:port\n of a host running a \ncondor-ce-collector\n daemon\n\n\n\n\n\n\nRSV\n\n\nThis section handles the configuration and setup of the RSV services.\n\n\nThis section is contained in \n/etc/osg/config.d/30-rsv.ini\n which is provided by the \nosg-configure-rsv\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the rsv  service is being used or not.\n\n\n\n\n\n\nrsv_user\n\n\nString\n\n\nThis gives username that rsv will run under.  If this is blank or set to \nUNAVAILABLE\n, it will default to rsv.\n\n\n\n\n\n\ngratia_probes\n\n\nString\n\n\nThis settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer\n\n\n\n\n\n\nce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\nhtcondor_ce_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:2812\n).\n\n\n\n\n\n\ngums_hosts\n\n\nString\n\n\nThis option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of \nCE\n or \nSE\n FQDNs (and \nnot a GUMS server FQDN\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the GridFTP servers that the RSV GridFTP probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host.iu.edu:2812,my.host2,my.host3\n).\n\n\n\n\n\n\ngridftp_dir\n\n\nString\n\n\nThis should be the directory that the GridFTP probes should use during testing.  This defaults to \n/tmp\n if left blank or set to \nUNAVAILABLE\n.\n\n\n\n\n\n\nsrm_hosts\n\n\nString\n\n\nThis option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs (\nhostname[:port/service]\n) separated by a comma (e.g. \nmy.host,my.host2,my.host3:8444\n).\n\n\n\n\n\n\nsrm_dir\n\n\nString\n\n\nThis should be the directory that the srm probes should use during testing.\n\n\n\n\n\n\nsrm_webservice_path\n\n\nString\n\n\nThis option gives the webservice path that SRM probes need to use along with the host:port. See note.\n\n\n\n\n\n\nservice_cert\n\n\nString\n\n\nThis option should point to the public key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nuser_proxy\n setting is set, it will default to \n/etc/grid-security/rsvcert.pem\n\n\n\n\n\n\nservice_key\n\n\nString\n\n\nThis option should point to the private key file (pem) for your service  certificate. If this is left blank or set to \nUNAVAILABLE\n and the \nservice_cert\n setting is enabled, it will default to \n/etc/grid-security/rsvkey.pem\n .\n\n\n\n\n\n\nservice_proxy\n\n\nString\n\n\nThis should point to the location of the rsv proxy file. If this is left blank or set to \nUNAVAILABLE\n and the use_service_cert  setting is enabled, it will default to \n/tmp/rsvproxy\n.\n\n\n\n\n\n\nuser_proxy\n\n\nString\n\n\nIf you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to \nUNAVAILABE\n or \nDEFAULT\n.\n\n\n\n\n\n\nsetup_rsv_nagios\n\n\nTrue\n, \nFalse\n\n\nThis option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.\n This plugin is provided as an experimental component, and admins are recommend \nnot to enable\n it on production resources.\n\n\n\n\n\n\nrsv_nagios_conf_file\n\n\nString\n\n\nThis option indicates the location of the rsv nagios  file to use for configuration details. This file \nneeds to be configured locally for RSV-Nagios forwarding to work\n -- see inline comments in file for more information.\n\n\n\n\n\n\ncondor_location\n\n\nString\n\n\nIf you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsrm_webservice_path\n:\n\nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use \nsrm/v2/server\n as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example: \nsrm/v2/server\n) for the SRM probes to pass on their\nSE.\n\n\n\n\nSubcluster / Resource Entry\n\n\nSubcluster and Resource Entry configuration is for reporting about the worker resources on your site. A \nsubcluster\n is a homogeneous set of worker node hardware; a \nresource\n is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.\n\n\nAt least one Subcluster or Resource Entry section\n is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.\n\n\nThis section is contained in \n/etc/osg/config.d/30-gip.ini\n which is provided by the \nosg-configure-gip\n RPM. (This is for historical reasons.)\n\n\nThis configuration uses multiple sections of the OSG configuration files:\n\n\n\n\nSubcluster*\n: options about homogeneous subclusters\n\n\nResource Entry*\n: options for specifying ATLAS queues for AGIS\n\n\n\n\nNotes for multi-CE sites.\n\n\nIf you would like to properly advertise multiple CEs per cluster, make sure that you:\n\n\n\n\nSet the value of site_name in the \"Site Information\" section to be the same for each CE.\n\n\nHave the \nexact\n same configuration values for the Subcluster* and Resource Entry* sections in each CE.\n\n\n\n\nSubcluster Configuration\n\n\nEach homogeneous set of worker node hardware is called a \nsubcluster\n. For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format: \n[Subcluster CHANGEME]\n, where CHANGEME is the globally unique subcluster name (yes, it must be a \nglobally\n unique name for the whole grid, not just unique to your site. Get creative.)\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the Section label; it should be \nglobally unique\n\n\n\n\n\n\nram_mb\n\n\nPositive Integer\n\n\nMegabytes of RAM per node\n\n\n\n\n\n\ncores_per_node\n\n\nPositive Integer\n\n\nNumber of cores per node\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this subcluster (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this subcluster. The default is 1440, or the equivalent of one day.\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted in order to run on this subcluster\n\n\n\n\n\n\nextra_transforms\n\n\nClassad\n\n\nTransformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nResource Entry Configuration (ATLAS only)\n\n\nIf you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new \nResource Entry\n section with a unique name in the following format: \n[Resource Entry RESOURCE]\n where RESOURCE is a globally unique resource name (it must be a \nglobally\n unique name for the whole grid, not just unique to your site). The following options are required for the \nResource Entry\n section and are used to generate the data required by AGIS:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nname\n\n\nString\n\n\nThe same name that is in the \nResource Entry\n label; it must be \nglobally unique\n\n\n\n\n\n\nmax_wall_time\n\n\nPositive Integer\n\n\nMaximum wall-clock time, in minutes, that a job is allowed to run on this resource\n\n\n\n\n\n\nqueue\n\n\nString\n\n\nThe queue to which jobs should be submitted to run on this resource\n\n\n\n\n\n\ncpucount\n (alias \ncores_per_node\n)\n\n\nPositive Integer\n\n\nNumber of cores that a job using this resource can get\n\n\n\n\n\n\nmaxmemory\n (alias \nram_mb\n)\n\n\nPositive Integer\n\n\nMaximum amount of memory (in MB) that a job using this resource can get\n\n\n\n\n\n\nallowed_vos\n\n\nComma-separated List or \n*\n\n\nThe VOs that are allowed to run jobs on this resource (autodetected if \n*\n). Optional on OSG 3.3\n\n\n\n\n\n\n\n\nThe following attributes are optional:\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nsubclusters\n\n\nComma-separated List\n\n\nThe physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file\n\n\n\n\n\n\nvo_tag\n\n\nString\n\n\nAn arbitrary label that is added to jobs routed through this resource\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nallowed_vos\n is mandatory\n\n\n\n\nGateway\n\n\nThis section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses \ncondor-ce\n as the gateway.\n\n\nThis section is contained in \n/etc/osg/config.d/10-gateway.ini\n which is provided by the \nosg-configure-gateway\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nhtcondor_gateway_enabled\n\n\nTrue\n, \nFalse\n\n\n(default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.\n\n\n\n\n\n\njob_envvar_path\n\n\nString\n\n\nThe value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.\n\n\n\n\n\n\n\n\nLocal Settings\n\n\nThis section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the \nosg-local-job-environment.conf\n verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The \nosg-local-job-environment.conf\n file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.\n\n\nAdding a line such as \nMy_Setting = my_Value\n would result in the an environment variable called \nMy_Setting\n set to \nmy_Value\n in the job's environment. \nmy_Value\n can also be defined in terms of an environment variable (i.e \nMy_Setting = $my_Value\n) that will be evaluated on the worker node. For example, to add a variable \nMY_PATH\n set to \n/usr/local/myapp\n, you'd have the following:\n\n\n[Local Settings]\n\n\n\nMY_PATH\n \n=\n \n/usr/local/myapp\n\n\n\n\n\n\nThis section is contained in \n/etc/osg/config.d/40-localsettings.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\nMisc Services\n\n\nThis section handles the configuration of services that do not have a dedicated section for their configuration.\n\n\nThis section is contained in \n/etc/osg/config.d/10-misc.ini\n which is provided by the \nosg-configure-misc\n RPM.\n\n\nThis section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the \nauthentication section of the HTCondor-CE install documents\n.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nglexec_location\n\n\nString\n\n\nThis gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g. \n$FOO\n) that will be evaluated on the worker node. If it is not installed, set this to \nUNAVAILABLE\n. glExec does not work with the \nvomsmap\n authorization method on OSG 3.3 and \nis entirely unsupported starting in OSG 3.4\n\n\n\n\n\n\ngums_host\n\n\nString\n\n\nThis setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to \nxacml\n. If GUMS is not used, this should be set to \nUNAVAILABLE\n. \nGUMS is deprecated in OSG 3.4\n\n\n\n\n\n\nauthorization_method\n\n\ngridmap\n, \nxacml\n, \nlocal-gridmap\n, \nvomsmap\n\n\nThis indicates which authorization method your site uses. \nxacml\n \nis deprecated in OSG 3.4\n\n\n\n\n\n\nedit_lcmaps_db\n\n\nTrue\n, \nFalse\n\n\n(Optional, default True) If true, osg-configure will overwrite \n/etc/lcmaps.db\n to set your authorization method. The previous version will be backed up to \n/etc/lcmaps.db.pre-configure\n\n\n\n\n\n\nall_fqans\n\n\nTrue\n, \nFalse\n\n\n(Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see \ndocumentation\n\n\n\n\n\n\ncopy_host_cert_for_service_certs\n\n\nTrue\n, \nFalse\n\n\n(Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS\n\n\n\n\n\n\n\n\nOSG 3.4 changes:\n\n\n\n\nglexec_location\n must be \nUNAVAILABLE\n or unset\n\n\nauthorization_method\n defaults to \nvomsmap\n\n\nauthorization_method\n will raise a warning if set to \nxacml\n\n\n\n\nSite Information\n\n\nThe settings found in the \nSite Information\n section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.\n\n\nThis section is contained in \n/etc/osg/config.d/40-siteinfo.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngroup\n\n\nOSG\n , \nOSG-ITB\n\n\nThis should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG\n\n\n\n\n\n\nhost_name\n\n\nString\n\n\nThis should be set to be hostname of the CE that is being configured\n\n\n\n\n\n\nresource\n\n\nString\n\n\nThe resource name of this CE endpoint as registered in OIM.\n\n\n\n\n\n\nresource_group\n\n\nString\n\n\nThe resource_group of this CE as registered in OIM.\n\n\n\n\n\n\nsponsor\n\n\nString\n\n\nThis should be set to the sponsor of the resource. See note.\n\n\n\n\n\n\nsite_policy\n\n\nUrl\n\n\nThis should be a url pointing to the resource's usage policy\n\n\n\n\n\n\ncontact\n\n\nString\n\n\nThis should be the name of the resource's admin contact\n\n\n\n\n\n\nemail\n\n\nEmail address\n\n\nThis should be the email address of the admin contact for the resource\n\n\n\n\n\n\ncity\n\n\nString\n\n\nThis should be the city that the resource is located in\n\n\n\n\n\n\ncountry\n\n\nString\n\n\nThis should be two letter country code for the country that the resource is located in.\n\n\n\n\n\n\nlongitude\n\n\nNumber\n\n\nThis should be the longitude of the resource. It should be a number between -180 and 180.\n\n\n\n\n\n\nlatitude\n\n\nNumber\n\n\nThis should be the latitude of the resource. It should be a number between -90 and 90.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nsponsor\n:\n\nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.\n\n\n\n\nSquid\n\n\nThis section handles the configuration and setup of the squid web caching and proxy service.\n\n\nThis section is contained in \n/etc/osg/config.d/01-squid.ini\n which is provided by the \nosg-configure-squid\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nTrue\n, \nFalse\n, \nIgnore\n\n\nThis indicates whether the squid service is being used or not.\n\n\n\n\n\n\nlocation\n\n\nString\n\n\nThis should be set to the \nhostname:port\n of the squid server.\n\n\n\n\n\n\n\n\nStorage\n\n\nThis section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the\n\nEnvironment Variables\n\ndescription and \nSite Planning\n discussions for explanations of the various storage models and the requirements for them.\n\n\nThis section is contained in \n/etc/osg/config.d/10-storage.ini\n which is provided by the \nosg-configure-ce\n RPM.\n\n\n\n\n\n\n\n\nOption\n\n\nValues Accepted\n\n\nExplanation\n\n\n\n\n\n\n\n\n\n\nse_available\n\n\nTrue\n, \nFalse\n\n\nThis indicates whether there is an associated SE available.\n\n\n\n\n\n\ndefault_se\n\n\nString\n\n\nIf an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.\n\n\n\n\n\n\ngrid_dir\n\n\nString\n\n\nThis setting should point to the directory which holds the files from the OSG worker node package. See note\n\n\n\n\n\n\napp_dir\n\n\nString\n\n\nThis setting should point to the directory which contains the VO specific applications. See note\n\n\n\n\n\n\ndata_dir\n\n\nString\n\n\nThis setting should point to a directory that can be used to store and stage data in and out of the cluster. See note\n\n\n\n\n\n\nworker_node_temp\n\n\nString\n\n\nThis directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note\n\n\n\n\n\n\nsite_read\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be read to stage in data via the variable \n$OSG_SITE_READ\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\nsite_write\n\n\nString\n\n\nThis setting should be the location or url to a directory that can be write to stage out data via the variable \n$OSG_SITE_WRITE\n. This is an url if you are using a SE. If not set, the default is UNAVAILABLE\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nAll of these can be defined in terms of an environment variable\n(e.g. \n$FOO\n) that will be evaluated on the worker node.\n\n\n\n\ngrid_dir\n:\n\nIf you have installed the worker node client via RPM (the normal case) it\nshould be \n/etc/osg/wn-client\n.  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.\n\n\nThis directory will be accessed via the \n$OSG_GRID\n environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.\n\n\napp_dir\n:\n\nThis directory will be accesed via the \n$OSG_APP\n environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory \netc/\n with 1777 permissions.\n\n\nThis directory may also be in OASIS, in which case set \napp_dir\n to\n\n/cvmfs/oasis.opensciencegrid.org\n. (The CE does not need write access in\nthat case.)\n\n\ndata_dir\n:\n\nThis directory can be accessed via the \n$OSG_DATA\n environment variable. It\nshould be readable and writable on both the CE and worker nodes.\n\n\nworker_node_temp\n:\n\nThis directory will be accessed via the \n$OSG_WN_TMP\n environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-with-osg-configure", 
            "text": "OSG-Configure and the INI files in  /etc/osg/config.d  allow a high level configuration of OSG services.\nThis document outlines the settings and options found in the INI files for system administers that are installing and configuring OSG software.  This page gives an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "Configuration with OSG-Configure"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#invocation-and-script-usage", 
            "text": "The  osg-configure  script is used to process the INI files and apply changes to the system. osg-configure  must be run as root.  The typical workflow of OSG-Configure is to first edit the INI files, then verify them, then apply the changes.  To verify the config files, run:  [root@server] osg-configure -v   OSG-Configure will list any errors in your configuration, usually including the section and option where the problem is.\nPotential problems are:   Required option not filled in  Invalid value  Syntax error  Inconsistencies between options   To apply changes, run:  [root@server] osg-configure -c   If your INI files do not change, then re-running  osg-configure -c  will result in the same configuration as when you ran it the last time.\nThis allows you to experiment with your settings without having to worry about messing up your system.  OSG-Configure is split up into modules. Normally, all modules are run when calling  osg-configure .\nHowever, it is possible to run specific modules separately.\nTo see a list of modules, including whether they can be run separately, run:  [root@server] osg-configure -l   If the module can be run separately, specify it with the  -m  MODULE  option:  [root@server] osg-configure -c -m  MODULE   Options may be specified in multiple INI files, which may make it hard to determine which value OSG-Configure uses.\nYou may query the final value of an option via one of these methods:  [root@server] osg-configure -o  OPTION  [root@server] osg-configure -o  SECTION . OPTION   Logs are written to  /var/log/osg/osg-configure.log .\nIf something goes wrong, specify the  -d  flag to add more verbose output to  osg-configure.log .  The rest of this document will detail what to specify in the INI files.", 
            "title": "Invocation and script usage"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#conventions", 
            "text": "In the tables below:   Mandatory options for a section are given in  bold  type. Sometime the default value may be OK and no edit required, but the variable has to be in the file.  Options that are not found in the default ini file are in  italics .", 
            "title": "Conventions"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#syntax-and-layout", 
            "text": "The configuration files used by  osg-configure  are the one supported by Python's  SafeConfigParser , similar in format to the  INI configuration file  used by MS Windows:   Config files are separated into sections, specified by a section name in square brackets (e.g.  [Section 1] )  Options should be set using  name = value  pairs  Lines that begin with  ;  or  #  are comments  Long lines can be split up using continutations: each white space character can be preceded by a newline to fold/continue the field on a new line (same syntax as specified in  email RFC 822 )  Variable substitutions are supported --  see below   osg-configure  reads and uses all of the files in  /etc/osg/config.d  that have a \".ini\" suffix. The files in this directory are ordered with a numeric prefix with higher numbers being applied later and thus having higher precedence (e.g. 00-foo.ini has a lower precedence than 99-local-site-settings.ini). Configuration sections and options can be specified multiple times in different files. E.g. a section called  [PBS]  can be given in  20-pbs.ini  as well as  99-local-site-settings.ini .  Each of the files are successively read and merged to create a final configuration that is then used to configure OSG software. Options and settings in files read later override the ones in previous files. This allows admins to create a file with local settings (e.g.  99-local-site-settings.ini ) that can be read last and which will be take precedence over the default settings in configuration files installed by various RPMs and which will not be overwritten if RPMs are updated.", 
            "title": "Syntax and layout"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#variable-substitution", 
            "text": "The osg-configure parser allows variables to be defined and used in the configuration file:\nany option set in a given section can be used as a variable in that section.  Assuming that you have set an option with the name  myoption  in the section, you can substitute the value of that option elsewhere in the section by referring to it as  %(myoption)s .   Note  The trailing  s  is required. Also, option names cannot have a variable subsitution in them.", 
            "title": "Variable substitution"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#special-settings", 
            "text": "If a setting is set to UNAVAILABLE or DEFAULT or left blank, osg-configure will try to use a sensible default for setting if possible.", 
            "title": "Special Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#ignore-setting", 
            "text": "The  enabled  option, specifying whether a service is enabled or not, is a boolean but also accepts  Ignore  as a possible value. Using Ignore, results in the service associated with the section being ignored entirely (and any configuration is skipped). This differs from using  False  (or the  %(disabled)s  variable), because using  False  results in the service associated with the section being disabled.  osg-configure  will not change the configuration of the service if the  enabled  is set to  Ignore .  This is useful, if you have a complex configuration for a given that can't be set up using the ini configuration files. You can manually configure that service by hand editing config files, manually start/stop the service and then use the  Ignore  setting so that  osg-configure  does not alter the service's configuration and status.", 
            "title": "Ignore setting"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#configuration-sections", 
            "text": "The OSG configuration is divided into sections with each section starting with a section name in square brackets (e.g.  [Section 1] ). The configuration is split in multiple files and options form one section can be in more than one files.  The following sections give an overview of the options for each of the sections of the configuration files that  osg-configure  uses.", 
            "title": "Configuration sections"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#bosco", 
            "text": "This section is contained in  /etc/osg/config.d/20-bosco.ini  which is provided by the  osg-configure-bosco  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Bosco jobmanager is being used or not.    users  String  A comma separated string. The existing usernames on the CE for which to install Bosco and allow submissions. In order to have separate usernames per VO, for example the CMS VO to have the cms username, each user must have Bosco installed. The osg-configure service will install Bosco on each of the users listed here.    endpoint  String  The remote cluster submission host for which Bosco will submit jobs to the scheduler. This is in the form of  , exactly as you would use to ssh into the remote cluster.    batch  String  The type of scheduler installed on the remote cluster.    ssh_key  String  The location of the ssh key, as created above.", 
            "title": "Bosco"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#condor", 
            "text": "This section describes the parameters for a Condor jobmanager if it's being used in the current CE installation. If Condor is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-condor.ini  which is provided by the  osg-configure-condor  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Condor jobmanager is being used or not.    condor_location  String  This should be set to be directory where condor is installed. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_LOCATION environment variable if available otherwise it will use  /usr  which works for the RPM installation.    condor_config  String  This should be set to be path where the condor_config file is located. If this is set to a blank variable, DEFAULT or UNAVAILABLE, the  osg-configure  script will try to get this from the CONDOR_CONFIG environment variable if available otherwise it will use  /etc/condor/condor_config , the default for the RPM installation.", 
            "title": "Condor"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#lsf", 
            "text": "This section describes the parameters for a LSF jobmanager if it's being used in the current CE installation. If LSF is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-lsf.ini  which is provided by the  osg-configure-lsf  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the LSF jobmanager is being used or not.    lsf_location  String  This should be set to be directory where lsf is installed", 
            "title": "LSF"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#pbs", 
            "text": "This section describes the parameters for a pbs jobmanager if it's being used in the current CE installation. If PBS is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-pbs.ini  which is provided by the  osg-configure-pbs  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the PBS jobmanager is being used or not.    pbs_location  String  This should be set to be directory where pbs is installed. osg-configure will try to loocation for the pbs binaries in pbs_location/bin.    accounting_log_directory  String  This setting is used to tell Gratia where to find your accounting log files, and it is required for proper accounting.    pbs_server  String  This setting is optional and should point to your PBS server node if it is different from your OSG CE", 
            "title": "PBS"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#sge", 
            "text": "This section describes the parameters for a SGE jobmanager if it's being used in the current CE installation. If SGE is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-sge.ini  which is provided by the  osg-configure-sge  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the SGE jobmanager is being used or not.    sge_root  String  This should be set to be directory where sge is installed (e.g. same as  $SGE_ROOT  variable).    sge_cell  String  The sge_cell setting should be set to the value of $SGE_CELL for your SGE install.    default_queue  String  This setting determines queue that jobs should be placed in if the job description does not specify a queue.    available_queues  String  This setting indicates which queues are available on the cluster and should be used for validation when  validate_queues  is set.    validate_queues  String  This setting determines whether the globus jobmanager should check the job RSL and verify that any queue specified matches a queue available on the cluster. See note.      Note  validate_queues : \nIf  available_queues  is set, that list of queues will be used for\nvalidation, otherwise SGE will be queried for available queues.", 
            "title": "SGE"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#slurm", 
            "text": "This section describes the parameters for a Slurm jobmanager if it's being used in the current CE installation. If Slurm is not being used, the  enabled  setting should be set to  False .  This section is contained in  /etc/osg/config.d/20-slurm.ini  which is provided by the  osg-configure-slurm  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the Slurm jobmanager is being used or not.    slurm_location  String  This should be set to be directory where slurm is installed. osg-configure will try to location for the slurm binaries in slurm_location/bin.    db_host  String  Hostname of the machine hosting the SLURM database. This information is needed to configure the SLURM gratia probe.    db_port  String  Port of where the SLURM database is listening. This information is needed to configure the SLURM gratia probe.    db_user  String  Username used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_pass  String  The location of a file containing the password used to access the SLURM database. This information is needed to configure the SLURM gratia probe.    db_name  String  Name of the SLURM database. This information is needed to configure the SLURM gratia probe.    slurm_cluster  String  The name of the Slurm cluster", 
            "title": "Slurm"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gratia", 
            "text": "This section configures Gratia. If  probes  is set to  UNAVAILABLE , then  osg-configure  will use appropriate default values. If you need to specify custom reporting (e.g. a local gratia collector) in addition to the default probes,  %(osg-jobmanager-gratia)s ,  %(osg-gridftp-gratia)s ,  %(osg-metric-gratia)s ,  %(itb-jobmanager-gratia)s ,  %(itb-gridftp-gratia)s ,  %(itb-metric-gratia)s  are defined in the default configuration files to make it easier to specify the standard osg reporting.  This section is contained in  /etc/osg/config.d/30-gratia.ini  which is provided by the  osg-configure-gratia  RPM.     Option  Values Accepted  Explanation      enabled  True  ,  False ,  Ignore  This should be set to True if gratia should be configured and enabled on the installation being configured.    resource  String  This should be set to the resource name as given in the OIM registration    probes  String  This should be set to the gratia probes that should be enabled. A probe is specified by using as  [probe_type]:server:port . See note      Note  probes : \nLegal values for  probe_type  are:   metric  (for RSV)  jobmanager  (for the appropriate jobmanager probe)  gridftp  (for the GridFTP transfer probe)", 
            "title": "Gratia"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#info-services", 
            "text": "Reporting to the central CE Collectors is configured in this section.  In the majority of cases, this file can be left untouched; you only need to configure this section if you wish to report to your own CE Collector instead of the ones run by OSG Operations.  This section is contained in  /etc/osg/config.d/30-infoservices.ini , which is provided by the  osg-configure-infoservices  RPM. (This is for historical reasons.)     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  True if reporting should be configured and enabled    ce_collectors  String  The server(s) HTCondor-CE information should be sent to. See note      Note  ce_collectors :   Set this to  DEFAULT  to report to the OSG Production or ITB servers (depending on your  Site Information  configuration).  Set this to  PRODUCTION  to report to the OSG Production servers  Set this to  ITB  to report to the OSG ITB servers  Otherwise, set this to the  hostname:port  of a host running a  condor-ce-collector  daemon", 
            "title": "Info Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#rsv", 
            "text": "This section handles the configuration and setup of the RSV services.  This section is contained in  /etc/osg/config.d/30-rsv.ini  which is provided by the  osg-configure-rsv  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the rsv  service is being used or not.    rsv_user  String  This gives username that rsv will run under.  If this is blank or set to  UNAVAILABLE , it will default to rsv.    gratia_probes  String  This settings indicates which rsv gratia probes should be used.  It is a list of probes separated by a comma.  Valid probes are metric, condor, pbs, lsf, sge, managedfork, hadoop-transfer, and gridftp-transfer    ce_hosts  String  This option lists the serviceURI of the CEs that generic RSV CE probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    htcondor_ce_hosts  String  This option lists the serviceURI of the HTCondor-CE-based CEs that the RSV HTCondor-CE probes should check. This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:2812 ).    gums_hosts  String  This option lists the serviceURI or FQDN of the CEs or SEs, using GUMS for authentication, that the RSV GUMS probes should check.  This should be a list of  CE  or  SE  FQDNs (and  not a GUMS server FQDN ) separated by a comma (e.g.  my.host,my.host2,my.host3 ).    gridftp_hosts  String  This option lists the serviceURI of the GridFTP servers that the RSV GridFTP probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host.iu.edu:2812,my.host2,my.host3 ).    gridftp_dir  String  This should be the directory that the GridFTP probes should use during testing.  This defaults to  /tmp  if left blank or set to  UNAVAILABLE .    srm_hosts  String  This option lists the serviceURI of the srm servers that the RSV srm probes should check.  This should be a list of serviceURIs ( hostname[:port/service] ) separated by a comma (e.g.  my.host,my.host2,my.host3:8444 ).    srm_dir  String  This should be the directory that the srm probes should use during testing.    srm_webservice_path  String  This option gives the webservice path that SRM probes need to use along with the host:port. See note.    service_cert  String  This option should point to the public key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  user_proxy  setting is set, it will default to  /etc/grid-security/rsvcert.pem    service_key  String  This option should point to the private key file (pem) for your service  certificate. If this is left blank or set to  UNAVAILABLE  and the  service_cert  setting is enabled, it will default to  /etc/grid-security/rsvkey.pem  .    service_proxy  String  This should point to the location of the rsv proxy file. If this is left blank or set to  UNAVAILABLE  and the use_service_cert  setting is enabled, it will default to  /tmp/rsvproxy .    user_proxy  String  If you don't use a service certificate for rsv, you will need to specify a  proxy file that RSV should use in the proxy_file setting.  If this is set, then  service_cert, service_key, and service_proxy should be left blank, or set to  UNAVAILABE  or  DEFAULT .    setup_rsv_nagios  True ,  False  This option indicates whether rsv should upload results to a local  nagios server instance. This should be set to True or False.  This plugin is provided as an experimental component, and admins are recommend  not to enable  it on production resources.    rsv_nagios_conf_file  String  This option indicates the location of the rsv nagios  file to use for configuration details. This file  needs to be configured locally for RSV-Nagios forwarding to work  -- see inline comments in file for more information.    condor_location  String  If you installed Condor in a non-standard location (somewhere other than /usr, which is where the RPM puts it)  you must specify the path to the install dir here.      Note  srm_webservice_path : \nFor dcache installations, this should work if left blank. However\nBestman-xrootd SEs normally use  srm/v2/server  as web service path, and so\nBestman-xrootd admins will have to pass this option with the appropriate\nvalue (for example:  srm/v2/server ) for the SRM probes to pass on their\nSE.", 
            "title": "RSV"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-resource-entry", 
            "text": "Subcluster and Resource Entry configuration is for reporting about the worker resources on your site. A  subcluster  is a homogeneous set of worker node hardware; a  resource  is a set of subcluster(s) with common capabilities that will be reported to the ATLAS AGIS system.  At least one Subcluster or Resource Entry section  is required on a CE; please populate the information for all your subclusters. This information will be reported to a central collector and will be used to send GlideIns / pilot jobs to your site; having accurate information is necessary for OSG jobs to effectively use your resources.  This section is contained in  /etc/osg/config.d/30-gip.ini  which is provided by the  osg-configure-gip  RPM. (This is for historical reasons.)  This configuration uses multiple sections of the OSG configuration files:   Subcluster* : options about homogeneous subclusters  Resource Entry* : options for specifying ATLAS queues for AGIS", 
            "title": "Subcluster / Resource Entry"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#notes-for-multi-ce-sites", 
            "text": "If you would like to properly advertise multiple CEs per cluster, make sure that you:   Set the value of site_name in the \"Site Information\" section to be the same for each CE.  Have the  exact  same configuration values for the Subcluster* and Resource Entry* sections in each CE.", 
            "title": "Notes for multi-CE sites."
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#subcluster-configuration", 
            "text": "Each homogeneous set of worker node hardware is called a  subcluster . For each subcluster in your cluster, fill in the information about the worker node hardware by creating a new Subcluster section with a unique name in the following format:  [Subcluster CHANGEME] , where CHANGEME is the globally unique subcluster name (yes, it must be a  globally  unique name for the whole grid, not just unique to your site. Get creative.)     Option  Values Accepted  Explanation      name  String  The same name that is in the Section label; it should be  globally unique    ram_mb  Positive Integer  Megabytes of RAM per node    cores_per_node  Positive Integer  Number of cores per node    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this subcluster (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this subcluster. The default is 1440, or the equivalent of one day.    queue  String  The queue to which jobs should be submitted in order to run on this subcluster    extra_transforms  Classad  Transformation attributes which the HTCondor Job Router should apply to incoming jobs so they can run on this subcluster     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Subcluster Configuration"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#resource-entry-configuration-atlas-only", 
            "text": "If you are configuring a CE for the ATLAS VO, you must provide hardware information to advertise the queues that are available to AGIS. For each queue, create a new  Resource Entry  section with a unique name in the following format:  [Resource Entry RESOURCE]  where RESOURCE is a globally unique resource name (it must be a  globally  unique name for the whole grid, not just unique to your site). The following options are required for the  Resource Entry  section and are used to generate the data required by AGIS:     Option  Values Accepted  Explanation      name  String  The same name that is in the  Resource Entry  label; it must be  globally unique    max_wall_time  Positive Integer  Maximum wall-clock time, in minutes, that a job is allowed to run on this resource    queue  String  The queue to which jobs should be submitted to run on this resource    cpucount  (alias  cores_per_node )  Positive Integer  Number of cores that a job using this resource can get    maxmemory  (alias  ram_mb )  Positive Integer  Maximum amount of memory (in MB) that a job using this resource can get    allowed_vos  Comma-separated List or  *  The VOs that are allowed to run jobs on this resource (autodetected if  * ). Optional on OSG 3.3     The following attributes are optional:     Option  Values Accepted  Explanation      subclusters  Comma-separated List  The physical subclusters the resource entry refers to; must be defined as Subcluster sections elsewhere in the file    vo_tag  String  An arbitrary label that is added to jobs routed through this resource     OSG 3.4 changes:   allowed_vos  is mandatory", 
            "title": "Resource Entry Configuration (ATLAS only)"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#gateway", 
            "text": "This section gives information about the options in the Gateway section of the configuration files. These options control the behavior of job gateways on the CE. CEs are based on HTCondor-CE, which uses  condor-ce  as the gateway.  This section is contained in  /etc/osg/config.d/10-gateway.ini  which is provided by the  osg-configure-gateway  RPM.     Option  Values Accepted  Explanation      htcondor_gateway_enabled  True ,  False  (default True). True if the CE is using HTCondor-CE, False otherwise. HTCondor-CE will be configured to support enabled batch systems. RSV will use HTCondor-CE to launch remote probes.    job_envvar_path  String  The value of the PATH environment variable to put into HTCondor jobs running with HTCondor-CE. This value is ignored if not using that batch system/gateway combination.", 
            "title": "Gateway"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#local-settings", 
            "text": "This section differs from other sections in that there are no set options in this section. Rather, the options set in this section will be placed in the  osg-local-job-environment.conf  verbatim. The options in this section are case sensitive and the case will be preserved when they are converted to environment variables. The  osg-local-job-environment.conf  file gets sourced by jobs run on your cluster so any variables set in this section will appear in the environment of jobs run on your system.  Adding a line such as  My_Setting = my_Value  would result in the an environment variable called  My_Setting  set to  my_Value  in the job's environment.  my_Value  can also be defined in terms of an environment variable (i.e  My_Setting = $my_Value ) that will be evaluated on the worker node. For example, to add a variable  MY_PATH  set to  /usr/local/myapp , you'd have the following:  [Local Settings]  MY_PATH   =   /usr/local/myapp   This section is contained in  /etc/osg/config.d/40-localsettings.ini  which is provided by the  osg-configure-ce  RPM.", 
            "title": "Local Settings"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#misc-services", 
            "text": "This section handles the configuration of services that do not have a dedicated section for their configuration.  This section is contained in  /etc/osg/config.d/10-misc.ini  which is provided by the  osg-configure-misc  RPM.  This section primarily deals with authentication/authorization. For information on suggested settings for your CE, see the  authentication section of the HTCondor-CE install documents .     Option  Values Accepted  Explanation      glexec_location  String  This gives the location of the glExec installation on the worker nodes, if it is present. Can be defined in terms of an environment variable (e.g.  $FOO ) that will be evaluated on the worker node. If it is not installed, set this to  UNAVAILABLE . glExec does not work with the  vomsmap  authorization method on OSG 3.3 and  is entirely unsupported starting in OSG 3.4    gums_host  String  This setting is used to indicate the hostname of the GUMS host that should be used for authentication, if the authorization method below is set to  xacml . If GUMS is not used, this should be set to  UNAVAILABLE .  GUMS is deprecated in OSG 3.4    authorization_method  gridmap ,  xacml ,  local-gridmap ,  vomsmap  This indicates which authorization method your site uses.  xacml   is deprecated in OSG 3.4    edit_lcmaps_db  True ,  False  (Optional, default True) If true, osg-configure will overwrite  /etc/lcmaps.db  to set your authorization method. The previous version will be backed up to  /etc/lcmaps.db.pre-configure    all_fqans  True ,  False  (Optional, default False) If true, vomsmap auth will use all VOMS FQANs of a proxy for mapping -- see  documentation    copy_host_cert_for_service_certs  True ,  False  (Optional, default False) If true, osg-configure will create a copy or copies of your host cert and key as service certs for RSV and (on OSG 3.3) GUMS     OSG 3.4 changes:   glexec_location  must be  UNAVAILABLE  or unset  authorization_method  defaults to  vomsmap  authorization_method  will raise a warning if set to  xacml", 
            "title": "Misc Services"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#site-information", 
            "text": "The settings found in the  Site Information  section are described below. This section is used to give information about a resource such as resource name, site sponsors, administrators, etc.  This section is contained in  /etc/osg/config.d/40-siteinfo.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Description      group  OSG  ,  OSG-ITB  This should be set to either OSG or OSG-ITB depending on whether your resource is in the OSG or OSG-ITB group. Most sites should specify OSG    host_name  String  This should be set to be hostname of the CE that is being configured    resource  String  The resource name of this CE endpoint as registered in OIM.    resource_group  String  The resource_group of this CE as registered in OIM.    sponsor  String  This should be set to the sponsor of the resource. See note.    site_policy  Url  This should be a url pointing to the resource's usage policy    contact  String  This should be the name of the resource's admin contact    email  Email address  This should be the email address of the admin contact for the resource    city  String  This should be the city that the resource is located in    country  String  This should be two letter country code for the country that the resource is located in.    longitude  Number  This should be the longitude of the resource. It should be a number between -180 and 180.    latitude  Number  This should be the latitude of the resource. It should be a number between -90 and 90.      Note  sponsor : \nIf your resource has multiple sponsors, you can separate them using commas\nor specify the percentage using the following format 'osg, atlas, cms' or\n'osg:10, atlas:45, cms:45'. The percentages must add up to 100 if multiple\nsponsors are used. If you have a sponsor that is not an OSG VO, you can\nindicate this by using 'local' as the VO.", 
            "title": "Site Information"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#squid", 
            "text": "This section handles the configuration and setup of the squid web caching and proxy service.  This section is contained in  /etc/osg/config.d/01-squid.ini  which is provided by the  osg-configure-squid  RPM.     Option  Values Accepted  Explanation      enabled  True ,  False ,  Ignore  This indicates whether the squid service is being used or not.    location  String  This should be set to the  hostname:port  of the squid server.", 
            "title": "Squid"
        }, 
        {
            "location": "/other/configuration-with-osg-configure/#storage", 
            "text": "This section gives information about the options in the Storage section of the configuration file.\nSeveral of these values are constrained and need to be set in a way that is consistent with one of the OSG storage models.\nPlease review the Storage Related Parameters section of the Environment Variables \ndescription and  Site Planning  discussions for explanations of the various storage models and the requirements for them.  This section is contained in  /etc/osg/config.d/10-storage.ini  which is provided by the  osg-configure-ce  RPM.     Option  Values Accepted  Explanation      se_available  True ,  False  This indicates whether there is an associated SE available.    default_se  String  If an SE is available at your cluster, set default_se to the hostname of this SE, otherwise set default_se to UNAVAILABLE.    grid_dir  String  This setting should point to the directory which holds the files from the OSG worker node package. See note    app_dir  String  This setting should point to the directory which contains the VO specific applications. See note    data_dir  String  This setting should point to a directory that can be used to store and stage data in and out of the cluster. See note    worker_node_temp  String  This directory should point to a directory that can be used as scratch space on compute nodes. If not set, the default is UNAVAILABLE. See note    site_read  String  This setting should be the location or url to a directory that can be read to stage in data via the variable  $OSG_SITE_READ . This is an url if you are using a SE. If not set, the default is UNAVAILABLE    site_write  String  This setting should be the location or url to a directory that can be write to stage out data via the variable  $OSG_SITE_WRITE . This is an url if you are using a SE. If not set, the default is UNAVAILABLE      Note  All of these can be defined in terms of an environment variable\n(e.g.  $FOO ) that will be evaluated on the worker node.   grid_dir : \nIf you have installed the worker node client via RPM (the normal case) it\nshould be  /etc/osg/wn-client .  If you have installed the worker node in a\nspecial location (perhaps via the worker node client tarball or via OASIS),\nit should be the location of that directory.  This directory will be accessed via the  $OSG_GRID  environment variable.\nIt should be visible on all of the compute nodes. Read access is required,\nthough worker nodes don't need write access.  app_dir : \nThis directory will be accesed via the  $OSG_APP  environment variable. It\nshould be visible on both the CE and worker nodes. Only the CE needs to\nhave write access to this directory. This directory must also contain a\nsub-directory  etc/  with 1777 permissions.  This directory may also be in OASIS, in which case set  app_dir  to /cvmfs/oasis.opensciencegrid.org . (The CE does not need write access in\nthat case.)  data_dir : \nThis directory can be accessed via the  $OSG_DATA  environment variable. It\nshould be readable and writable on both the CE and worker nodes.  worker_node_temp : \nThis directory will be accessed via the  $OSG_WN_TMP  environment variable.\nIt should allow read and write access on a worker node and can be visible\nto just that worker node.", 
            "title": "Storage"
        }, 
        {
            "location": "/common/registration/", 
            "text": "Registering in the OSG\n\n\nThe OSG keeps a registry containing active projects, virtual organizations (VOs), resources, and resource\ndowntimes stored as as \nYAML files\n\nin the \ntopology GitHub repository\n.\nThis registry is used for \naccounting data\n, contact information, and resource\navailability, particularly if your site is part of the \nWorld LHC Computing Grid\n (WLCG).\n\n\nUse this page to learn how to register information in the OSG.\n\n\nRegistration Requirements\n\n\nThe instructions in this document require the following:\n\n\n\n\nA \nGitHub\n account\n\n\nA working knowledge of \nGitHub collaboration\n\n\nOSG contact\n registration\n\n\n\n\nRegistering Contacts\n\n\nThe OSG keeps a database of contact information for resources, virtual organizations, and projects.\nBefore submitting any registrations in the topology repository, you must have an entry in the contact database with an\nassociated GitHub ID.\nTo register as an OSG contact or add your GitHub ID to your pre-existing contact entry, search for your name in the\n\ncontact database\n and choose one of the following:\n\n\n\n\nIf you find your name and see an associated \nGitHub\n account\n, you can start submitting registrations!\n\n\nIf you find your name but do not see an associated \nGitHub\n account\n, send an email to \n\n  with your GitHub ID.\n\n\n\n\nIf you cannot find your name\n, send an email to \n with the following information:\n\n\n\n\nFull name\n\n\nPrimary email address\n\n\nGitHub user name\n\n\nDescription of your OSG affiliation, e.g. FermiGrid site administrator, senior scientist for the DUNE experiment,\n  etc.\n\n\nContact information of site, virtual organization, or project sponsor to prove your affiliation\n\n\n\n\n\n\nPrivacy\n\n\nThe OSG treats any email addresses and phone numbers as confidential data but does not make any guarantees of\nprivacy.\n\n\n\n\n\n\n\n\nRegistering Resources\n\n\nAn OSG resource is a host that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts.\nSee the full list of services that should be registered in the OSG topology\n\nhere\n.\n\n\nOSG resources are stored under a hierarchy of facilities, sites, and resource groups, defined as follows:\n\n\n\n\n\n\n\n\nLevel\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nFacility\n\n\nThe institution or company where your resource is located, e.g. \nUniversity of Wisconsin\n\n\n\n\n\n\nSite\n\n\nSmaller than a facility; typically represents an academic department, research group, or a computing center, e.g. \nCHTC\n for the Center for High Throughput Computing.\n\n\n\n\n\n\nResource Group\n\n\nA logical grouping of resources at a site. Production and testing resources should be placed into separate Resource Groups.\n\n\n\n\n\n\nResource\n\n\nA host belonging to a resource group that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts. A resource may provide more than one service.\n\n\n\n\n\n\n\n\nOSG resources are stored in the GitHub repository as YAML files under a directory structure that reflects the above\nhierarchy, i.e. \ntopology/\nFACILITY\n/\nSITE\n/\nRESOURCE GROUP\n.yaml\n from the\n\nroot of the topology repository\n.\n\n\nSearching for resources\n\n\nWhether you are registering a new resource or modifying an existing resource, start by searching for the FQDN of your\nhost to avoid any duplicate registrations:\n\n\n\n\n\n\nOpen the \ntopology repository\n in your browser.\n\n\n\n\n\n\nSearch the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar\n   (e.g., \n\"glidein2.chtc.wisc.edu\"\n):\n\n\n\n\n\n\nIf the search doesn't return any results\n, skip to \nthese instructions\n for registering a new\n  resource.\n\n\nIf the search returns a single YAML file\n, open the link to the YAML file and skip to\n  \nthese instructions\n for modifying an existing resources.\n\n\nIf the search returns more than one YAML file\n, please \ncontact us\n.\n\n\n\n\n\n\n\n\nNew resources\n\n\nTo register a new resource, follow the instructions below:\n\n\n\n\n\n\nIf you haven't already, verify that the FQDN of your resource is not \nalready registered\n\n\n\n\n\n\nChoose the names of your facility, site, and resource group, ensuring that the names match any pre-existing\n   facilities, sites, or resource groups (including case and spaces).\n   Follow the instructions below, replacing instances of \nFACILITY\n, \nSITE\n, and \nRESOURCE GROUP\n with the\n   corresponding names that you chose above:\n\n\n\n\n\n\nIf your resource group already exists under your facility and site, open the following URL in your browser:\n\n\nhttps\n:\n//\ngithub\n.\ncom\n/\nopensciencegrid\n/\ntopology\n/\nedit\n/\nmaster\n/\ntopology\n/\nFACILITY\n/\nSITE\n/\nRESOURCE\n \nGROUP\n.\nyaml\n\n\n\n\n\n\nFor example, to add a resource to the \nCHTC\n resource group for the \nCHTC\n site at the \nUniversity of\nWisconsin\n, open the following URL:\n\n\nhttps\n:\n//\ngithub\n.\ncom\n/\nopensciencegrid\n/\ntopology\n/\nedit\n/\nmaster\n/\ntopology\n/\nUniversity\n \nof\n \nWisconsin\n/\nCHTC\n/\nCHTC\n.\nyaml\n\n\n\n\n\n\n\n\n\n\nIf any of your facility, site, or resource group do not exist, open the following URL in your browser:\n\n\nhttps\n:\n//\ngithub\n.\ncom\n/\nopensciencegrid\n/\ntopology\n/\nnew\n/\nmaster\n?\nfilename\n=\ntopology\n/\nFACILITY\n/\nSITE\n/\nRESOURCE\n \nGROUP\n.\nyaml\n\n\n\n\n\n\nFor example, to create a \nCHTC-Slurm-HPC\n resource group for the Center for High Throughput Computing (\nCHTC\n)\nat the \nUniversity of Wisconsin\n, open the following URL:\n\n\nhttps\n:\n//\ngithub\n.\ncom\n/\nopensciencegrid\n/\ntopology\n/\nnew\n/\nmaster\n?\nfilename\n=\ntopology\n/\nUniversity\n \nof\n \nWisconsin\n/\nCHTC\n/\nCHTC\n-\nSlurm\n-\nHPC\n.\nyaml\n\n\n\n\n\n\n\n\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nMake changes with the \nGitHub file editor\n using\n   the \nresource group template\n\n   as a guide.\n   You may leave any \nID\n or \nGroupID\n fields blank.\n   When adding new entries, make sure that the formatting and indentation of your entry matches that of the template.\n\n\n\n\n\n\nSubmit your changes as a pull request, providing a descriptive commit message. For example:\n\n\nAdding\n \na\n \nnew\n \ncompute\n \nelement\n \nto\n \nthe\n \nCHTC\n\n\n\n\n\n\n\n\n\n\nModifying existing resources\n\n\nTo modify an existing resource, follow these instructions:\n\n\n\n\n\n\nFind the resource that you would like to modify by \nsearching GitHub\n, and open the link to\n   the YAML file.\n\n\n\n\n\n\nClick the branch selector button next to the file path and select the \nmaster\n branch.\n\n\n\n\n\n\n\n\nMake changes with the \nGitHub file editor\n using\n   the \nresource group template\n\n   as a guide.\n   You may leave any \nID\n or \nGroupID\n fields blank.\n   Make sure that the formatting and indentation of the modified entry does not change.\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nSubmit your changes as a pull request, providing a descriptive commit message. For example:\n\n\nUpdating\n \nadministrative\n \ncontact\n \ninformation\n \nfor\n \nCHTC\n-\nglidein2\n\n\n\n\n\n\n\n\n\n\nRetiring resources\n\n\nTo retire an already registered resource, set \nActive: false\n. For example:\n\n\n...\n\n\nProduction\n:\n \ntrue\n\n\nResources\n:\n\n  \nGLOW\n:\n\n\n    \nActive\n:\n \nfalse\n\n\n    \n...\n\n    \nServices\n:\n\n      \nCE\n:\n\n        \nDescription\n:\n \nCompute\n \nElement\n\n        \nDetails\n:\n\n          \nhidden\n:\n \nfalse\n\n\n\n\n\n\nIf the \nActive\n attribute does not already exist within the resource definition, add it.\nIf your resource becomes available again, set \nActive: true\n.\n\n\nRegistering Resource Downtimes\n\n\nResource downtime is a finite period of time for which one or more of the grid services of a registered resource are\nunavailable.\n\n\n\n\nWarning\n\n\nIf you expect your resource to be indefinitely unavailable, \nretire the resource\n instead of\nregistering a downtime.\n\n\n\n\nDowntimes are stored in YAML files alongside the resource group YAML files as described \nhere\n.\n\n\nFor example, downtimes for resources in the \nCHTC-Slurm-HPC\n resource group of the \nCHTC\n site at the \nUniversity of\nWisconsin\n can be found and registered in the following file, relative to the\n\nroot of the topology repository\n:\n\n\ntopology\n/\nUniversity\n \nof\n \nWisconsin\n/\nCHTC\n/\nCHTC\n-\nSlurm\n-\nHPC_downtime\n.\nyaml\n\n\n\n\n\n\n\n\nNote\n\n\nDo not put downtime updates in the same pull request as other topology updates.\n\n\n\n\nRegistering new downtime\n\n\nTo register a new downtime for a registered resource,\nyou will use a webform to generate the contents of the downtime entry,\ncopy it into the downtime file corresponding to your resource,\nand submit it as a GitHub pull request.\nFollow the instructions below:\n\n\n\n\n\n\nOpen the \ndowntime generation webform\n in your browser.\n\n\n\n\n\n\nSelect your facility from the corresponding list.\n\n\n\n\n\n\nSelect the resource that will be down from the corresponding list.\n\n\n\n\n\n\nSelect all the services that will be down. To select multiple, use Control-Click on Windows and Linux,\n    or Command-Click on macOS.\n\n\n\n\n\n\nFill the other fields with information about the downtime.\n\n\n\n\n\n\nClick the \nGenerate\n button.\n\n\n\n\n\n\nIf the information is valid, a block of text will be displayed in the box labeled \nGenerated YAML\n.\n    Otherwise, check for error messages and fix your input.\n\n\n\n\n\n\nFollow the instructions shown below the generated block of text.\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nWait for OSG staff to approve and merge your new downtime.\n\n\n\n\n\n\nModifying existing downtime\n\n\nIn case an already registered downtime is incorrect or need to be updated to reflect new information, you can modify\nexisting downtime entries using the GitHub editor.\n\n\n\n\nFailure\n\n\nChanges to the \nID\n or \nCreatedTime\n fields will be rejected.\n\n\n\n\nTo modify an existing downtime entry for a registered resource,\nmanually make the changes in the matching downtime YAML file.\nFollow the instructions below:\n\n\n\n\n\n\nOpen the \ntopology repository\n in your browser.\n\n\n\n\n\n\nIf you do not know the facility, site, and resource group of the resource the downtime entry refers to,\n    search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar\n    (e.g., \n\"glidein2.chtc.wisc.edu\"\n):\n\n\n\n\n\n\nIf the search returns a single YAML file\n,\n  note the name of the facility, site, and resource group and continue to the next step.\n\n\nIf the search doesn't return any results or returns more than one YAML file\n,\n  please \ncontact us\n.\n\n\n\n\n\n\n\n\nOpen the following URL in your browser using the facility, site, and resource group names to replace\n    \nFACILITY\n, \nSITE\n, and \nRESOURCE GROUP\n, respectively:\n\n\n \nhttps\n:\n//\ngithub\n.\ncom\n/\nopensciencegrid\n/\ntopology\n/\nedit\n/\nmaster\n/\ntopology\n/\nFACILITY\n/\nSITE\n/\nRESOURCE\n \nGROUP\n_downtime\n.\nyaml\n\n\n\n\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nMake changes with the \nGitHub file editor\n using\n    the \ndowntime template\n\n    as a reference.\n    Make sure that the formatting and indentation of the modified entry does not change.\n\n\n\n\n\n\nSubmit your changes as a pull request, providing a descriptive commit message. For example:\n\n\nMove\n \nforward\n \nend\n \ndate\n \nfor\n \nCHTC\n-\nglidein2\n \nregular\n \nmaintenance\n\n\n\n\n\n\n\n\n\n\nWait for OSG staff to approve and merge your modified downtime.\n\n\n\n\n\n\nRegistering Virtual Organizations\n\n\nVirtual Organizations (VOs) are sets of groups or individuals defined by some common cyber-infrastructure need.\nThis can be a scientific experiment, a university campus or a distributed research effort.\nA VO represents all its members and their common needs in a grid environment.\nA VO also includes the group\u2019s computing/storage resources and services.\nFor more information about VOs, see \nthis page\n.\n\n\n\n\nInfo\n\n\nBefore submitting a registration for a new VO, please \ncontact us\n describing your organization's\ncomputing needs.\n\n\n\n\nVO information is stored as YAML files in the \nvirtual-organizations\n directory of the\n\ntopology repository\n.\nTo modify a VO's information or register a new VO, follow the instructions below:\n\n\n\n\n\n\nOpen the \ntopology repository\n in your\n   browser.\n\n\n\n\n\n\nIf you see your VO in the list, open the file and continue to the next step.\n   If you do not see your VO in the list, click \nCreate new file\n button:\n\n\n\n\nIn the new file dialog, enter \nVO\n.yaml\n, replacing \nVO\n with the name of your VO.\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nMake changes with the \nGitHub file editor\n using\n   the \nVO template\n\n   as a guide.\n   You may leave any \nID\n fields blank.\n   If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry.\n\n\n\n\n\n\nSubmit your changes as a pull request, providing a descriptive commit message. For example:\n\n\nUpdating\n \ncontact\n \ninformation\n \nfor\n \nthe\n \nGLOW\n \nVO\n\n\n\n\n\n\n\n\n\n\nRegistering Projects\n\n\n\n\nInfo\n\n\nBefore submitting a registration for a new project, please \ncontact us\n describing your organization's\ncomputing needs.\n\n\n\n\nProject information is stored as YAML files in the \nprojects\n directory of the\n\ntopology repository\n.\nTo modify a VO's information or register a new VO, follow the instructions below:\n\n\n\n\n\n\nOpen the \ntopology repository\n in your browser.\n\n\n\n\n\n\nIf you see your project in the list, open the file and continue to the next step.\n   If you do not see your project in the list, click \nCreate new file\n button:\n\n\n\n\nIn the new file dialog, enter \nPROJECT\n.yaml\n, replacing \nPROJECT\n with the name of your project.\n\n\n\n\n\"You're editing a file in a project you don't have write access to.\"\n\n\nIf you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.\n\n\n\n\n\n\n\n\nMake changes with the \nGitHub file editor\n using\n   the \nproject template\n as a guide.\n   You may leave any \nID\n fields blank.\n   If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry.\n\n\n\n\n\n\nSubmit your changes as a pull request, providing a descriptive commit message. For example:\n\n\nUpdating\n \ncontact\n \ninformation\n \nfor\n \nthe\n \nMu2e\n \nproject\n\n\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.", 
            "title": "Registration"
        }, 
        {
            "location": "/common/registration/#registering-in-the-osg", 
            "text": "The OSG keeps a registry containing active projects, virtual organizations (VOs), resources, and resource\ndowntimes stored as as  YAML files \nin the  topology GitHub repository .\nThis registry is used for  accounting data , contact information, and resource\navailability, particularly if your site is part of the  World LHC Computing Grid  (WLCG).  Use this page to learn how to register information in the OSG.", 
            "title": "Registering in the OSG"
        }, 
        {
            "location": "/common/registration/#registration-requirements", 
            "text": "The instructions in this document require the following:   A  GitHub  account  A working knowledge of  GitHub collaboration  OSG contact  registration", 
            "title": "Registration Requirements"
        }, 
        {
            "location": "/common/registration/#registering-contacts", 
            "text": "The OSG keeps a database of contact information for resources, virtual organizations, and projects.\nBefore submitting any registrations in the topology repository, you must have an entry in the contact database with an\nassociated GitHub ID.\nTo register as an OSG contact or add your GitHub ID to your pre-existing contact entry, search for your name in the contact database  and choose one of the following:   If you find your name and see an associated  GitHub  account , you can start submitting registrations!  If you find your name but do not see an associated  GitHub  account , send an email to  \n  with your GitHub ID.   If you cannot find your name , send an email to   with the following information:   Full name  Primary email address  GitHub user name  Description of your OSG affiliation, e.g. FermiGrid site administrator, senior scientist for the DUNE experiment,\n  etc.  Contact information of site, virtual organization, or project sponsor to prove your affiliation    Privacy  The OSG treats any email addresses and phone numbers as confidential data but does not make any guarantees of\nprivacy.", 
            "title": "Registering Contacts"
        }, 
        {
            "location": "/common/registration/#registering-resources", 
            "text": "An OSG resource is a host that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts.\nSee the full list of services that should be registered in the OSG topology here .  OSG resources are stored under a hierarchy of facilities, sites, and resource groups, defined as follows:     Level  Definition      Facility  The institution or company where your resource is located, e.g.  University of Wisconsin    Site  Smaller than a facility; typically represents an academic department, research group, or a computing center, e.g.  CHTC  for the Center for High Throughput Computing.    Resource Group  A logical grouping of resources at a site. Production and testing resources should be placed into separate Resource Groups.    Resource  A host belonging to a resource group that provides grid services, e.g. Compute Elements, storage endpoints, or perfSonar hosts. A resource may provide more than one service.     OSG resources are stored in the GitHub repository as YAML files under a directory structure that reflects the above\nhierarchy, i.e.  topology/ FACILITY / SITE / RESOURCE GROUP .yaml  from the root of the topology repository .", 
            "title": "Registering Resources"
        }, 
        {
            "location": "/common/registration/#searching-for-resources", 
            "text": "Whether you are registering a new resource or modifying an existing resource, start by searching for the FQDN of your\nhost to avoid any duplicate registrations:    Open the  topology repository  in your browser.    Search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar\n   (e.g.,  \"glidein2.chtc.wisc.edu\" ):    If the search doesn't return any results , skip to  these instructions  for registering a new\n  resource.  If the search returns a single YAML file , open the link to the YAML file and skip to\n   these instructions  for modifying an existing resources.  If the search returns more than one YAML file , please  contact us .", 
            "title": "Searching for resources"
        }, 
        {
            "location": "/common/registration/#new-resources", 
            "text": "To register a new resource, follow the instructions below:    If you haven't already, verify that the FQDN of your resource is not  already registered    Choose the names of your facility, site, and resource group, ensuring that the names match any pre-existing\n   facilities, sites, or resource groups (including case and spaces).\n   Follow the instructions below, replacing instances of  FACILITY ,  SITE , and  RESOURCE GROUP  with the\n   corresponding names that you chose above:    If your resource group already exists under your facility and site, open the following URL in your browser:  https : // github . com / opensciencegrid / topology / edit / master / topology / FACILITY / SITE / RESOURCE   GROUP . yaml   For example, to add a resource to the  CHTC  resource group for the  CHTC  site at the  University of\nWisconsin , open the following URL:  https : // github . com / opensciencegrid / topology / edit / master / topology / University   of   Wisconsin / CHTC / CHTC . yaml     If any of your facility, site, or resource group do not exist, open the following URL in your browser:  https : // github . com / opensciencegrid / topology / new / master ? filename = topology / FACILITY / SITE / RESOURCE   GROUP . yaml   For example, to create a  CHTC-Slurm-HPC  resource group for the Center for High Throughput Computing ( CHTC )\nat the  University of Wisconsin , open the following URL:  https : // github . com / opensciencegrid / topology / new / master ? filename = topology / University   of   Wisconsin / CHTC / CHTC - Slurm - HPC . yaml      \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Make changes with the  GitHub file editor  using\n   the  resource group template \n   as a guide.\n   You may leave any  ID  or  GroupID  fields blank.\n   When adding new entries, make sure that the formatting and indentation of your entry matches that of the template.    Submit your changes as a pull request, providing a descriptive commit message. For example:  Adding   a   new   compute   element   to   the   CHTC", 
            "title": "New resources"
        }, 
        {
            "location": "/common/registration/#modifying-existing-resources", 
            "text": "To modify an existing resource, follow these instructions:    Find the resource that you would like to modify by  searching GitHub , and open the link to\n   the YAML file.    Click the branch selector button next to the file path and select the  master  branch.     Make changes with the  GitHub file editor  using\n   the  resource group template \n   as a guide.\n   You may leave any  ID  or  GroupID  fields blank.\n   Make sure that the formatting and indentation of the modified entry does not change.   \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Submit your changes as a pull request, providing a descriptive commit message. For example:  Updating   administrative   contact   information   for   CHTC - glidein2", 
            "title": "Modifying existing resources"
        }, 
        {
            "location": "/common/registration/#retiring-resources", 
            "text": "To retire an already registered resource, set  Active: false . For example:  ...  Production :   true  Resources : \n   GLOW :       Active :   false       ... \n     Services : \n       CE : \n         Description :   Compute   Element \n         Details : \n           hidden :   false   If the  Active  attribute does not already exist within the resource definition, add it.\nIf your resource becomes available again, set  Active: true .", 
            "title": "Retiring resources"
        }, 
        {
            "location": "/common/registration/#registering-resource-downtimes", 
            "text": "Resource downtime is a finite period of time for which one or more of the grid services of a registered resource are\nunavailable.   Warning  If you expect your resource to be indefinitely unavailable,  retire the resource  instead of\nregistering a downtime.   Downtimes are stored in YAML files alongside the resource group YAML files as described  here .  For example, downtimes for resources in the  CHTC-Slurm-HPC  resource group of the  CHTC  site at the  University of\nWisconsin  can be found and registered in the following file, relative to the root of the topology repository :  topology / University   of   Wisconsin / CHTC / CHTC - Slurm - HPC_downtime . yaml    Note  Do not put downtime updates in the same pull request as other topology updates.", 
            "title": "Registering Resource Downtimes"
        }, 
        {
            "location": "/common/registration/#registering-new-downtime", 
            "text": "To register a new downtime for a registered resource,\nyou will use a webform to generate the contents of the downtime entry,\ncopy it into the downtime file corresponding to your resource,\nand submit it as a GitHub pull request.\nFollow the instructions below:    Open the  downtime generation webform  in your browser.    Select your facility from the corresponding list.    Select the resource that will be down from the corresponding list.    Select all the services that will be down. To select multiple, use Control-Click on Windows and Linux,\n    or Command-Click on macOS.    Fill the other fields with information about the downtime.    Click the  Generate  button.    If the information is valid, a block of text will be displayed in the box labeled  Generated YAML .\n    Otherwise, check for error messages and fix your input.    Follow the instructions shown below the generated block of text.   \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Wait for OSG staff to approve and merge your new downtime.", 
            "title": "Registering new downtime"
        }, 
        {
            "location": "/common/registration/#modifying-existing-downtime", 
            "text": "In case an already registered downtime is incorrect or need to be updated to reflect new information, you can modify\nexisting downtime entries using the GitHub editor.   Failure  Changes to the  ID  or  CreatedTime  fields will be rejected.   To modify an existing downtime entry for a registered resource,\nmanually make the changes in the matching downtime YAML file.\nFollow the instructions below:    Open the  topology repository  in your browser.    If you do not know the facility, site, and resource group of the resource the downtime entry refers to,\n    search the repository for the FQDN of your resource wrapped in double-quotes using the GitHub search bar\n    (e.g.,  \"glidein2.chtc.wisc.edu\" ):    If the search returns a single YAML file ,\n  note the name of the facility, site, and resource group and continue to the next step.  If the search doesn't return any results or returns more than one YAML file ,\n  please  contact us .     Open the following URL in your browser using the facility, site, and resource group names to replace\n     FACILITY ,  SITE , and  RESOURCE GROUP , respectively:    https : // github . com / opensciencegrid / topology / edit / master / topology / FACILITY / SITE / RESOURCE   GROUP _downtime . yaml    \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Make changes with the  GitHub file editor  using\n    the  downtime template \n    as a reference.\n    Make sure that the formatting and indentation of the modified entry does not change.    Submit your changes as a pull request, providing a descriptive commit message. For example:  Move   forward   end   date   for   CHTC - glidein2   regular   maintenance     Wait for OSG staff to approve and merge your modified downtime.", 
            "title": "Modifying existing downtime"
        }, 
        {
            "location": "/common/registration/#registering-virtual-organizations", 
            "text": "Virtual Organizations (VOs) are sets of groups or individuals defined by some common cyber-infrastructure need.\nThis can be a scientific experiment, a university campus or a distributed research effort.\nA VO represents all its members and their common needs in a grid environment.\nA VO also includes the group\u2019s computing/storage resources and services.\nFor more information about VOs, see  this page .   Info  Before submitting a registration for a new VO, please  contact us  describing your organization's\ncomputing needs.   VO information is stored as YAML files in the  virtual-organizations  directory of the topology repository .\nTo modify a VO's information or register a new VO, follow the instructions below:    Open the  topology repository  in your\n   browser.    If you see your VO in the list, open the file and continue to the next step.\n   If you do not see your VO in the list, click  Create new file  button:   In the new file dialog, enter  VO .yaml , replacing  VO  with the name of your VO.   \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Make changes with the  GitHub file editor  using\n   the  VO template \n   as a guide.\n   You may leave any  ID  fields blank.\n   If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry.    Submit your changes as a pull request, providing a descriptive commit message. For example:  Updating   contact   information   for   the   GLOW   VO", 
            "title": "Registering Virtual Organizations"
        }, 
        {
            "location": "/common/registration/#registering-projects", 
            "text": "Info  Before submitting a registration for a new project, please  contact us  describing your organization's\ncomputing needs.   Project information is stored as YAML files in the  projects  directory of the topology repository .\nTo modify a VO's information or register a new VO, follow the instructions below:    Open the  topology repository  in your browser.    If you see your project in the list, open the file and continue to the next step.\n   If you do not see your project in the list, click  Create new file  button:   In the new file dialog, enter  PROJECT .yaml , replacing  PROJECT  with the name of your project.   \"You're editing a file in a project you don't have write access to.\"  If you see this message in the GitHub file editor, this is normal and it is because you do not have direct write\naccess to the OSG copy of the topology data, which is why you are creating a pull request.     Make changes with the  GitHub file editor  using\n   the  project template  as a guide.\n   You may leave any  ID  fields blank.\n   If you are modifying existing entries, make sure you do not change formatting or indentation of the modified entry.    Submit your changes as a pull request, providing a descriptive commit message. For example:  Updating   contact   information   for   the   Mu2e   project", 
            "title": "Registering Projects"
        }, 
        {
            "location": "/common/registration/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/other/gsissh/", 
            "text": "Installing and Maintaining GSI OpenSSH\n\n\nThis document contains instructions to install and configure the GSI OpenSSH server available in the OSG repository for\nuse on your cluster.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section\nbelow\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \ngsisshd\n and \ngsisshd\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling GSI OpenSSH\n\n\nInstall the GSI OpenSSH rpms:\n\n\nroot\n@server\n \n#\n \nyum\n \ninstall\n \ngsi\n-\nopenssh\n-\nserver\n \ngsi\n-\nopenssh\n-\nclients\n\n\n\n\n\n\nConfiguring GSI OpenSSH\n\n\nIn order to get a running instance of the GSI OpenSSH server, you'll need to change the default configuration. \nHowever, before you go any further, you'll need to decide whether you want GSI OpenSSH to be your primary ssh service or\nnot (e.g. whether the GSI OpenSSH service will replace your existing SSH service). \nRegardless of your choice, you should probably have both services use the same host keys.\n\nThis can be done by running the following commands :\n\n\nroot@host #\n \ncd\n /etc/gsissh\n\nroot@host #\n ln -s /etc/ssh/ssh_host_rsa_key ssh_host_rsa_key\n\nroot@host #\n ln -s /etc/ssh/ssh_host_rsa_key.pub ssh_host_rsa_key.pub\n\nroot@host #\n ln -s /etc/ssh/ssh_host_dsa_key ssh_host_dsa_key\n\nroot@host #\n ln -s /etc/ssh/ssh_host_dsa_key.pub ssh_host_dsa_key.pub\n\nroot@host #\n ln -s /etc/ssh/ssh_host_ecdsa_key ssh_host_ecdsa_key\n\nroot@host #\n ln -s /etc/ssh/ssh_host_ecdsa_key.pub ssh_host_ecdsa_key.pub\n\nroot@host #\n ln -s /etc/ssh/ssh_host_ed25519_key ssh_host_ed25519_key\n\nroot@host #\n ln -s /etc/ssh/ssh_host_ed25519_key.pub ssh_host_ed25519_key.pub\n\n\n\n\n\n\n\nNote\n\n\nYour system may not have all of these host keys\n\n\n\n\nIf you choose not to replace your existing SSH service, you'll need to change the port setting in the GSI OpenSSH\nconfiguration to another port (e.g. 2222) so that you can run both SSH services at the same time.\nThis can be done by editing \n/etc/gsissh/sshd\n and setting \nPort 2222\n.\n\n\n\n\nNote\n\n\nRegardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (e.g., have \n!\n in the password field of \n/etc/shadow\n).\n\n\n\n\nConfiguring authentication\n\n\nTo configure authentication for GSI OpenSSH, follow the instructions in \nthe LCMAPS VOMS plugin document\n\nto prepare the LCMAPS VOMS plugin.\n\n\nUsing GSI OpenSSH\n\n\nThe following table gives the commands needed to start, stop, enable, and disable GSI OpenSSH.\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart  service\n\n\nservice gsisshd start\n\n\nsystemctl start gsisshd\n\n\n\n\n\n\nStop a  service\n\n\nservice gsisshd stop\n\n\nsystemctl stop gsisshd\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig gsisshd on\n\n\nsystemctl enable gsisshd\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig gsisshd off\n\n\nsystemctl disable gsisshd\n\n\n\n\n\n\n\n\nValidating GSI OpenSSH\n\n\nAfter starting the \ngsisshd\n service you can check if it is running correctly\n\n\nuser@client $\n grid-proxy-init\n\nYour identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name\n\n\nEnter GRID pass phrase for this identity:\n\n\nCreating proxy ............................................................................................... Done\n\n\nYour proxy is valid until: Sat Apr 23 08:18:27 2016\n\n\n$\n gsissh localhost -p \n2222\n\n\nLast login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu\n\n\n$\n\n\n\n\n\n\nTroubleshooting\n\n\nYou can get information on troubleshooting errors on the \nNCSA page\n.\n\n\nTo troubleshoot LCMAPS authorization, you can add the following to \n/etc/sysconfig/gsisshd\n and choose a higher debug\nlevel:\n\n\n# level 0: no messages, 1: errors, 2: also warnings, 3: also notices,\n\n\n#  4: also info, 5: maximum debug\n\n\nLCMAPS_DEBUG_LEVEL\n=\n2\n\n\n\n\n\n\nOutput goes to \n/var/log/messages\n or \njournalctl\n by default.\n\n\nHelp\n\n\nTo get assistance please use this \nHelp Procedure\n.\n\n\nReference\n\n\nUseful configuration and log files\n\n\nConfiguration Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nConfiguration File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/sshd_config\n\n\nConfiguration file\n\n\n\n\n\n\ngsisshd\n\n\n/etc/sysconfig/gsisshd\n\n\nEnvironment variables for gsisshd\n\n\n\n\n\n\ngsisshd\n\n\n/etc/lcmaps.db\n\n\nLCMAPS configuration\n\n\n\n\n\n\n\n\nLog Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nLog File\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/var/log/messages\n\n\nAll log messages\n\n\n\n\n\n\n\n\nOther Files\n\n\n\n\n\n\n\n\nService or Process\n\n\nFile\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostcert.pem\n\n\nHost certificate\n\n\n\n\n\n\ngsisshd\n\n\n/etc/grid-security/hostkey.pem\n\n\nX.509 host key\n\n\n\n\n\n\ngsisshd\n\n\n/etc/gsissh/ssh_host_rsa_key\n\n\nRSA Host key", 
            "title": "Install GSI-enabled SSH"
        }, 
        {
            "location": "/other/gsissh/#installing-and-maintaining-gsi-openssh", 
            "text": "This document contains instructions to install and configure the GSI OpenSSH server available in the OSG repository for\nuse on your cluster.", 
            "title": "Installing and Maintaining GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section\nbelow  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  gsisshd  and  gsisshd   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/other/gsissh/#installing-gsi-openssh", 
            "text": "Install the GSI OpenSSH rpms:  root @server   #   yum   install   gsi - openssh - server   gsi - openssh - clients", 
            "title": "Installing GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#configuring-gsi-openssh", 
            "text": "In order to get a running instance of the GSI OpenSSH server, you'll need to change the default configuration. \nHowever, before you go any further, you'll need to decide whether you want GSI OpenSSH to be your primary ssh service or\nnot (e.g. whether the GSI OpenSSH service will replace your existing SSH service). \nRegardless of your choice, you should probably have both services use the same host keys. \nThis can be done by running the following commands :  root@host #   cd  /etc/gsissh root@host #  ln -s /etc/ssh/ssh_host_rsa_key ssh_host_rsa_key root@host #  ln -s /etc/ssh/ssh_host_rsa_key.pub ssh_host_rsa_key.pub root@host #  ln -s /etc/ssh/ssh_host_dsa_key ssh_host_dsa_key root@host #  ln -s /etc/ssh/ssh_host_dsa_key.pub ssh_host_dsa_key.pub root@host #  ln -s /etc/ssh/ssh_host_ecdsa_key ssh_host_ecdsa_key root@host #  ln -s /etc/ssh/ssh_host_ecdsa_key.pub ssh_host_ecdsa_key.pub root@host #  ln -s /etc/ssh/ssh_host_ed25519_key ssh_host_ed25519_key root@host #  ln -s /etc/ssh/ssh_host_ed25519_key.pub ssh_host_ed25519_key.pub   Note  Your system may not have all of these host keys   If you choose not to replace your existing SSH service, you'll need to change the port setting in the GSI OpenSSH\nconfiguration to another port (e.g. 2222) so that you can run both SSH services at the same time.\nThis can be done by editing  /etc/gsissh/sshd  and setting  Port 2222 .   Note  Regardless of the authorization method used for the user, any \naccount that will be used with GSI OpenSSH must have a shell \nassigned to it and not be locked (e.g., have  !  in the password field of  /etc/shadow ).", 
            "title": "Configuring GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#configuring-authentication", 
            "text": "To configure authentication for GSI OpenSSH, follow the instructions in  the LCMAPS VOMS plugin document \nto prepare the LCMAPS VOMS plugin.", 
            "title": "Configuring authentication"
        }, 
        {
            "location": "/other/gsissh/#using-gsi-openssh", 
            "text": "The following table gives the commands needed to start, stop, enable, and disable GSI OpenSSH.     To...  On EL6, run the command...  On EL7, run the command...      Start  service  service gsisshd start  systemctl start gsisshd    Stop a  service  service gsisshd stop  systemctl stop gsisshd    Enable a service to start on boot  chkconfig gsisshd on  systemctl enable gsisshd    Disable a service from starting on boot  chkconfig gsisshd off  systemctl disable gsisshd", 
            "title": "Using GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#validating-gsi-openssh", 
            "text": "After starting the  gsisshd  service you can check if it is running correctly  user@client $  grid-proxy-init Your identity: /DC=ch/DC=cern/OU=Organic Units/OU=Users/CN=User Name  Enter GRID pass phrase for this identity:  Creating proxy ............................................................................................... Done  Your proxy is valid until: Sat Apr 23 08:18:27 2016  $  gsissh localhost -p  2222  Last login: Tue Sep 18 16:08:03 2012 from itb4.uchicago.edu  $", 
            "title": "Validating GSI OpenSSH"
        }, 
        {
            "location": "/other/gsissh/#troubleshooting", 
            "text": "You can get information on troubleshooting errors on the  NCSA page .  To troubleshoot LCMAPS authorization, you can add the following to  /etc/sysconfig/gsisshd  and choose a higher debug\nlevel:  # level 0: no messages, 1: errors, 2: also warnings, 3: also notices,  #  4: also info, 5: maximum debug  LCMAPS_DEBUG_LEVEL = 2   Output goes to  /var/log/messages  or  journalctl  by default.", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/other/gsissh/#help", 
            "text": "To get assistance please use this  Help Procedure .", 
            "title": "Help"
        }, 
        {
            "location": "/other/gsissh/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/other/gsissh/#useful-configuration-and-log-files", 
            "text": "Configuration Files     Service or Process  Configuration File  Description      gsisshd  /etc/gsissh/sshd_config  Configuration file    gsisshd  /etc/sysconfig/gsisshd  Environment variables for gsisshd    gsisshd  /etc/lcmaps.db  LCMAPS configuration     Log Files     Service or Process  Log File  Description      gsisshd  /var/log/messages  All log messages     Other Files     Service or Process  File  Description      gsisshd  /etc/grid-security/hostcert.pem  Host certificate    gsisshd  /etc/grid-security/hostkey.pem  X.509 host key    gsisshd  /etc/gsissh/ssh_host_rsa_key  RSA Host key", 
            "title": "Useful configuration and log files"
        }, 
        {
            "location": "/other/install-gwms-frontend/", 
            "text": "GlideinWMS VO Frontend Installation\n\n\nThis document describes how to install the Glidein Workflow Managment System\n(GlideinWMS) VO Frontend for use with the OSG Glidein factory. This software is\nthe minimum requirement for a VO to use GlideinWMS.\n\n\nThis document assumes expertise with HTCondor and familiarity with the GlideinWMS\nsoftware. It \ndoes not\n cover anything but the simplest possible install.\nPlease consult the \nGlideinWMS reference\ndocumentation\n\nfor advanced topics, including non-\nroot\n, non-RPM-based installation.\n\n\nThis document covers three components of the GlideinWMS a VO needs to install:\n\n\n\n\nUser Pool Collectors\n: A set of \ncondor_collector\n processes. Pilots submitted by the factory will join to one of these collectors to form a HTCondor pool.\n\n\nUser Pool Schedd\n: A \ncondor_schedd\n. Users may submit HTCondor vanilla universe jobs to this schedd; it will run jobs in the HTCondor pool formed by the \nUser Pool Collectors\n.\n\n\nGlidein Frontend\n: The frontend will periodically query the \nUser Pool Schedd\n to determine the desired number of running job slots. If necessary, it will request the Factory to launch additional pilots.\n\n\n\n\nThis guide covers installation of all three components on the same host: it is\ndesigned for small to medium VOs (see the Hardware Requirements below). Given a\nsignificant, large host, we have been able to scale the single-host install to\n20,000 running jobs.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux users \napache\n (UID 48), \ncondor\n, \nfrontend\n, and \ngratia\n\n\nNetwork:\n The VO frontend must have reliable network connectivity and be on the public internet (i.e. no NAT).\n    The latest version requires the following TCP ports to be open:\n\n\n80 (HTTP) for monitoring and serving configuration to workers\n\n\n9618 (HTCondor shared port) for HTCondor daemons including the Schedd and User Collector\n\n\n9620 to 9660 for secondary collectors (depending on configuration, see below)\n\n\n\n\n\n\nHost choice\n: The GlideinWMS VO Frontend has the following hardware requirements for a production host:\n\n\nCPU\n: Four cores, preferably no more than 2 years old.\n\n\nRAM\n: 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.\n\n\nDisk\n: 30GB will be sufficient for all the binaries, config and log files related to GlideinWMS. As this will be an interactive submit host, have enough disk space for your users' jobs. \n\n\n\n\n\n\n\n\n\n\nNote\n\n\nThe default configuration uses a port range (9620 to 9660) for the secondary collectors.\nYou can configure the secondary collectors to use the shared port 9618 instead;\nthis will become the default in the future.\n\n\n\n\n\n\nNote\n\n\nGlideinWMS versions prior to 3.4.1 also required port 9615 for the Schedd,\nand did not support using shared port for the secondary collectors.\nIf you are upgrading a standalone submit host from version 3.4 or earlier, the default open \nport has changed from 9615 to 9618, and you need to update your firewall rules to reflect this change.\nYou can figure out which port will be used by running the following command:\n\n\nconsole\n condor_config_val SHARED_PORT_ARGS\n\n\n\n\nFor more detailed information, see \nConfiguring GlideinWMS Frontend\n.\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has a \nsupported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nCredentials and Proxies\n\n\nThe VO Frontend will use two credentials in its interactions with the other GlideinWMS services. At this time, these will be proxy files.\n\n\n\n\nthe \nVO Frontend proxy\n (used to authenticate with the other GlideinWMS services).\n\n\none or more GlideinWMS \npilot proxies\n (used/delegated to the Factory services and submitted on the GlideinWMS pilot jobs).\n\n\n\n\nThe \nVO Frontend proxy\n and the \n pilot proxy\n can\nbe the same. By default, the VO Frontend will run as user \nfrontend\n (UID is\nmachine dependent) so these proxies must be owned by the user \nfrontend\n.\n\n\n\n\nNote\n\n\nBoth proxies need to be passwordless to allow \nautomatic proxy renewal\n.\n\n\n\n\nVO Frontend proxy\n\n\nThe use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the \n\nproxy configuration section\n.\n\n\nYou must give the Factory operations team the DN of this proxy when you\ninitially setup the Frontend and each time the DN changes\n.\n\n\nPilot proxies\n\n\nThese proxies are used by the Factory to submit the GlideinWMS pilot jobs.\nTherefore, they must be authorized to access to the CEs (Factory entry points)\nwhere jobs are submitted. There is no need to notify the Factory operation about\nthe DN of these proxies (neither at the initial registration nor for subsequent\nchanges). These additional proxies have no special requirements or controls added by the\nFactory but will probably require VO attributes because of the CEs: if you are\nable to use one of these proxies to submit jobs to the corresponding CEs where the Factory runs\nGlideinWMS pilots for you, then the proxies are OK. You can test each of your proxies using\n\nglobusrun\n or HTCondor-G.\n\n\nTo check the important information about a PEM certificate you can use: \nopenssl\nx509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout\n. You\nwill need that to find out information for the configuration files and the\nrequest to the GlideinWMS Factory.\n\n\nOSG Factory access\n\n\nBefore installing the GlideinWMS VO Frontend you need the information about a \nGlidein Factory\n that you can access:\n\n\n\n\n(recommended) OSG is managing a factory at UCSD\n\n\nYou have another Glidein Factory that you can access\n\n\n\n\nTo request access to the OSG Glidein Factory at UCSD you have to send an email to \n providing:\n\n\n\n\nYour Name\n\n\nThe VO that is utilizing the VO Frontend\n\n\nThe DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the \nproxy configuration section\n)\n\n\nYou can propose a security name that will have to be confirmed/changed by the Factory managers (see below)\n\n\n\n\nA list of sites where you want to run:\n\n\n\n\n\n\nYour VO must be supported on those sites\n\n\n\n\nYou can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers\n\n\nYou can start with one single site\n\n\n\n\nIn the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend\n\n\n\n\nThe exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".\n\n\nThe host of the Factory Collector: \ngfactory-1.t2.ucsd.edu\n\n\nThe DN os the factory, e.g. \n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n\nThe factory identity, e.g.: \ngfactory@gfactory-1.t2.ucsd.edu\n\n\nThe identity on the Factory you will be mapped to. Something like: \nusername@gfactory-1.t2.ucsd.edu\n\n\nYour security name. A unique name, usually containing your VO name: \nMy_SecName\n\n\nA string to add in the main Factory query_expr in the Frontend configuration, e.g. \nstringListMember(\"\nVO\n\",GLIDEIN_Supported_VOs)\n. This is used to select the entries you can use. From there you get the correct name of the VO (above in this list).\n\n\n\n\nInstalling GlideinWMS Frontend\n\n\nInstalling HTCondor\n\n\nIf you don't have HTCondor already installed, you can install the HTCondor RPM\nfrom the OSG repository:\n\n\nroot@host #\n yum install condor.x86_64\n\n\n\n\n\nIf you already have installed HTCondor using a tarball or a source other than\nthe OSG ROM, you will need to install the empty-condor RPM:\n\n\nroot@host #\n yum install empty-condor --enablerepo\n=\nosg-empty\n\n\n\n\n\nInstalling the VO Frontend RPM\n\n\nInstall the RPM and dependencies (be prepared for a lot of dependencies).\n\n\nroot@host #\n yum install glideinwms-vofrontend\n\n\n\n\n\nThis will install the current production release verified and tested by OSG with\ndefault HTCondor configuration. This command will install the GlideinWMS\nvofrontend, HTCondor, the OSG client, and all the required dependencies all on one\nnode.\n\n\nIf you wish to install a different version of GlideinWMS, add the \"--enablerepo\"\nargument to the command as follows:\n\n\n\n\nyum install --enablerepo=osg-testing glideinwms-vofrontend\n: The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.\n\n\nyum install --enablerepo=osg-upcoming glideinwms-vofrontend\n: The most recent development series release, i.e. version 3.3 release. This has newer features such as cloud submission support, but is less tested.\n\n\n\n\nNote that these commands will install default HTCondor configurations with all\nGlideinWMS services on one node.\n\n\nInstalling GlideinWMS Frontend on Multiple Nodes (Advanced)\n\n\nFor advanced users expecting heavy usage on their submit node, you may want to\nconsider splitting the user collector, user submit, and vo frontend\nservices. This can be doing using the following three commands (on different\nmachines):\n\n\nroot@host #\n yum install glideinwms-vofrontend-standalone\n\nroot@host #\n yum install glideinwms-usercollector\n\nroot@host #\n yum install glideinwms-userschedd\n\n\n\n\n\nIn addition, you will need to perform the following steps:\n\n\n\n\nOn the vofrontend and userschedd, modify \nCONDOR_HOST\n to point to your usercollector. This is in \n/etc/condor/config.d/00_gwms_general.config\n. You can also override this value by placing it in a new config file. (For instance, \n/etc/condor/config.d/99_local_custom.config\n to avoid rpmsave/rpmnew conflicts on upgrades).\n\n\nIn \n/etc/condor/certs/condor_mapfile\n, you will need to add the DNs of each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the \nglidecondor_addDN\n to add these values.\n\n\nIn the \n/etc/gwms-frontend/frontend.xml\n file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend.xml are in the following sections.\n\n\n\n\nConfiguring GlideinWMS Frontend\n\n\nAfter installing the RPM, you need to configure the components of the GlideinWMS VO Frontend:\n\n\n\n\nEdit Frontend configuration options\n\n\nEdit HTCondor configuration options\n\n\nCreate a HTCondor grid map file\n\n\nReconfigure and Start the Frontend\n\n\n\n\nConfiguring the Frontend\n\n\nThe VO Frontend configuration file is \n/etc/gwms-frontend/frontend.xml\n. The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.\n\n\n\n\n\n\nThe VO you are affiliated with. This will identify those CEs that the GlideinWMS pilot will be authorized to run on using the \npilot proxy\n described previously in \nthis section\n. Sometimes the whole \nquery_expr\n is provided to you by the Factory operators (see Factory access above):\n\n\nfactory\n \nquery_expr=\n((stringListMember(\nVO\n, GLIDEIN_Supported_VOs)))\n\n\n\n\n\n\n\n\n\n\nFactory collector information. The \nusername\n that you are assigned by the Factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production Factory, you will have to change also \nDN\n, \nfactory_identity\n and \nnode\n attributes. (refer to the information provided to you by the Factory operator):\n\n\ncollector\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu\n\n           \ncomment=\nDefine factory collector globally for simplicity\n\n           \nfactory_identity=\ngfactory@gfactory-1.t2.ucsd.edu\n\n           \nmy_identity=\nusername@gfactory-1.t2.ucsd.edu\n\n           \nnode=\ngfactory-1.t2.ucsd.edu\n/\n\n\n\n\n\n\n\n\n\n\nFrontend security information.\n\n\n\n\nThe \nclassad_proxy\n in the security entry is the location of the VO Frontend proxy described previously \nhere\n.\n\n\nThe \nproxy_DN\n is the DN of the \nclassad_proxy\n above.\n\n\nThe \nsecurity_name\n identifies this VO Frontend to the the Factory, It is provided by the Factory operator.\n\n\nThe \nabsfname\n in the credential entry is the location of the GlideinWMS \npilot\n proxy described in the requirements section \nhere\n. There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources). \nThe type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the Factory. If there is no match, between these two attributes in one of the credentials and the corresponding ones in some entry in one of the Factories, then this Frontend cannot trigger glideins.\n\nBoth the \nclassad_proxy\n and \nabsfname\n files should be owned by \nfrontend\n user.\nsecurity\n \nclassad_proxy=\n/tmp/vo_proxy\n \nproxy_DN=\nDN of vo_proxy\n\n      \nproxy_selection_plugin=\nProxyAll\n\n      \nsecurity_name=\nThe security name, this is used by factory\n\n      \nsym_key=\naes_256_cbc\n\n      \ncredentials\n\n        \ncredential\n \nabsfname=\n/tmp/pilot_proxy\n \nsecurity_class=\nfrontend\n\n        \ntrust_domain=\nOSG\n \ntype=\ngrid_proxy\n/\n\n      \n/credentials\n\n\n/security\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe schedd information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nfullname\n attribute is the fully qualified domain name of the host where you installed the VO Frontend (\nhostname --fqdn\n).\nA secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the Frontend to service requests from multiple submit hosts.\nschedds\n\n  \nschedd\n \nDN=\nCert DN used by the schedd at fullname:\n\n        \nfullname=\nHostname of the schedd\n/\n\n   \nschedd\n \nDN=\nCert DN used by the second Schedd at fullname:\n\n         \nfullname=\nschedd name@Hostname of second schedd\n/\n\n\n/schedds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe User Collector information.\n\n\n\n\nThe \nDN\n of the \nVO Frontend Proxy\n described previously \nhere\n.\n\n\nThe \nnode\n attribute is the full hostname of the collectors (\nhostname --fqdn\n) and port\n\n\nThe \nsecondary\n attribute indicates whether the element is for the primary or secondary collectors (True/False).\n The default HTCondor configuration of the VO Frontend starts multiple Collector processes on the host (\n/etc/condor/config.d/11_gwms_secondary_collectors.config\n). The \nDN\n and \nhostname\n on the first line are the hostname and the host certificate of the VO Frontend. The \nDN\n and \nhostname\n on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector connection can be defined as sinful string for the sock case , e.g., hostname.domain.tld:9618?sock=collector16.\n\n\n\n\n\n\n\n\n[Example 1]\n\n\n        \n:::\nxml\n\n        \ncollector\n \nDN\n=\nDN of main collector\n\n               \nnode\n=\nhostname.domain.tld:9618\n \nsecondary\n=\nFalse\n/\n\n        \ncollector\n \nDN\n=\nDN of secondary collectors (usually same as DN in line above)\n\n               \nnode\n=\nhostname.domain.tld:9620-9660\n \nsecondary\n=\nTrue\n/\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIn GlideinWMS v3.4.1, shared port only configuration is incompatible if talking to older Factories (v3.4 or older). We strongly recommend any user of GlideinWMS Frontend v3.4.1 or newer, to transition to the use of shared port for secondary collectors and CCBs.\nThe shared port configuration is incompatible if your Frontend is talking to Factories v3.4 or older and you'll get an error telling you to wait.\nTo transition to the use of shared port for secondary collectors, you have to change the collectors section in the Frontend configuration. If you are using the default port range for the secondary collectors as shown in [Example 2] below, then you should replace it with port \n9618\n and the sock-range as shown in [Example 1] above.\n\n\nIf you have a more complex configuration, please read the \ndetailed GlideinWMS configuration\n\n\n[Example 2]\n\n\n        \n:::\nxml\n\n        \ncollector\n \nDN\n=\nDN of main collector\n\n               \nnode\n=\nhostname.domain.tld:9618\n \nsecondary\n=\nFalse\n/\n\n        \ncollector\n \nDN\n=\nDN of secondary collectors (usually same as DN in line above)\n\n               \nnode\n=\n\u201c\nhostname\n.\ndomain\n.\ntld\n:\n9618\n?\nsock\n=\ncollector0\n-\n40\n secondary=\nTrue\n/\n\n\n\n\n\n\n\n\nThe CCBs information.\n    If you have a different configuration of the HTCondor Connection Brokering (CCB servers) from the default (usually the section is empty as the User Collectors acts as CCB if needed), you can set the connection in the CCB section the same way that User Collector information previously mentioned. Also, the same rules for transition to shared_port of the connections, apply to the CCBs.\n    \n:::\nxml\n\n    \nccb\n \nDN\n=\nDN of the CCB server\n\n           \nnode\n=\nhostname.domain.tld:9618\n/\n\n    \nccb\n \nDN\n=\nDN of the CCB server\n\n           \nnode\n=\n\u201c\nhostname\n.\ndomain\n.\ntld\n:\n9618\n?\nsock\n=\ncollector0\n-\n40\n secondary=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nThe Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):\n\n\n\n\nfrontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)\n\n\nfor RPM installs, work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)\n\n\n\n\n\n\nUsing a Different Factory\n\n\nThe configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:\n\n\n\n\nreplace \ngfactory@gfactory-1.t2.ucsd.edu\n and \ngfactory-1.t2.ucsd.edu\n with the correct values for your Factory. And control also that the name used for the Frontend () matches.\n\n\nmake sure that the Factory is advertising the attributes used in the Factory query expression (\nquery_expr\n).\n\n\n\n\nConfiguring HTCondor\n\n\nThe HTCondor configuration for the Frontend is placed in \n/etc/condor/config.d\n.\n\n\n\n\n00_gwms_general.config\n\n\n01_gwms_collectors.config\n\n\n02_gwms_schedds.config\n\n\n03_gwms_local.config\n\n\n11_gwms_secondary_collectors.config\n\n\n90_gwms_dns.config\n\n\n\n\nFor most installations create a new file named \n/etc/condor/config.d/92_local_condor.config\n\n\nUsing other HTCondor RPMs, e.g. UW Madison HTCondor RPM\n\n\nThe above procedure will work if you are using the OSG HTCondor RPMS. You can\nverify that you used the OSG HTCondor RPM by using \nyum list condor\n. The\nversion name should include \"osg\", e.g. \n8.6.4-3.osg.el7\n.\n\n\nIf you are using the UW Madison HTCondor RPMS, be aware of the following changes:\n\n\n\n\nThis HTCondor RPM uses a file \n/etc/condor/condor_config.local\n to add your local machine slot to the user pool.\n\n\nIf you want to disable this behavior (recommended), you should blank out that file or comment out the line in \n/etc/condor/condor_config\n for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to \n/etc/condor/config.d\n)\n\n\nNote that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the GlideinWMS RPMs, but please take note if you use this variable in your job submissions or other customizations.\n\n\n\n\nIn general if you are using a non OSG RPM or if you added custom configuration\nfiles for HTCondor please check the order of the configuration files:\n\n\nroot@host #\n condor_config_val -config\n\nConfiguration source:\n\n\n    /etc/condor/condor_config\n\n\nLocal configuration sources:\n\n\n    /etc/condor/config.d/00_gwms_general.config\n\n\n    /etc/condor/config.d/01_gwms_collectors.config\n\n\n    /etc/condor/config.d/02_gwms_schedds.config\n\n\n    /etc/condor/config.d/03_gwms_local.config\n\n\n    /etc/condor/config.d/11_gwms_secondary_collectors.config\n\n\n    /etc/condor/config.d/90_gwms_dns.config\n\n\n    /etc/condor/condor_config.local\n\n\n\n\n\n\nIf, like in the example above, the GlideinWMS configuration files are not the\nlast ones in the list please verify that important configuration options have\nnot been overridden by the other configuration files.\n\n\nVerifying your HTCondor configuration\n\n\n\n\n\n\nThe GlideinWMS configuration files in \n/etc/condor/config.d\n should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.\n\n\n\n\n\n\nVerify the alll the expected HTCondor daemons are running:\n\n\nroot@host #\n condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0\n\nCOLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,\n\n\nCOLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,\n\n\nCOLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,\n\n\nCOLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40\n\n\nDefined in \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n, line 193.\n\n\n\n\n\n\n\n\n\n\nIf you don't see all the collectors. shared port and the two schedd, then the\nconfiguration must be corrected. There should be \nno\n \nstartd\n daemons\nlisted.\n\n\nCreating a HTCondor grid mapfile.\n\n\nThe HTCondor mapfile (\n/etc/condor/certs/condor_mapfile\n) is used for\nauthentication between the GlideinWMS pilot running on a remote worker node, and\nthe local collector. HTCondor uses the mapfile to map certificates to pseudo-users\non the local machine. It is important that you map the DN's of:\n\n\n\n\n\n\nEach schedd proxy\n: The \nDN\n of each schedd that the frontend talks to. Specified in the frontend.xml schedd element \nDN\n attribute:\n\n\nschedds\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nYOUR_HOST\n/\n\n  \nschedd\n \nDN=\n/DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST\n \nfullname=\nschedd_jobs2@YOUR_HOST\n/\n\n\n/schedds\n\n\n\n\n\n\n\n\n\n\nFrontend proxy\n: The DN of the proxy that the Frontend uses to communicate with the other GlideinWMS services. Specified in the frontend.xml security element \nproxy_DN\n attribute:\n\n\nsecurity\n \nclassad_proxy=\n/tmp/vo_proxy\n \nproxy_DN=\nDN of vo_proxy\n \n....\n\n\n\n\n\n\n\n\n\n\nEach pilot proxy\n The DN of \neach\n proxy that the frontend forwards to the factory to use with the GlideinWMS pilots.  This allows the !GlideinWMS pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy \nabsfname\n attribute (you need to specify the \nDN\n of each of those proxies:\n\n\nsecurity\n \n....\n\n\nproxies\n\n   \n proxy\n \nabsfname=\n/tmp/vo_proxy\n \n....\n\n   \n:\n\n\n/proxies\n\n\n\n\n\n\n\n\n\n\nBelow is an example mapfile, by default found in\n\n/etc/condor/certs/condor_mapfile\n. In this example there are lines for each of\nservices mentioned above.\n\n\nGSI\n \nDN of schedd proxy\n \nschedd\n\n\nGSI\n \nDN of frontend proxy\n \nfrontend\n\n\nGSI\n \nDN of pilot proxy\n$\n \npilot_proxy\n\n\nGSI\n \n^/DC=org/DC=doegrids/OU=Services/CN=personal-submit-host2.mydomain.edu$\n \n%\nRED\nexample_of_format\n%\nENDCOLOR\n%\n\n\nGSI\n \n(.\n*\n)\n \nanonymous\n\n\nFS\n \n(.\n*\n)\n \n\\\n1\n\n\n\n\n\n\nRestarting HTCondor\n\n\nAfter configuring HTCondor, be sure to restart HTCondor:\n\n\nroot@host #\n service condor restart\n\n\n\n\n\nProxy Configuration\n\n\nGlideinWMS comes with the \ngwms-renew-proxies service\n that can automatically generate\nand renew the \npilot proxies\n and \nVO Frontend proxy\n. To configure this service, modify\n\n/etc/gwms-frontend/proxies.ini\n using the following instructions:\n\n\n\n\n\n\nFor each of your \npilot proxies\n, create a \n[PILOT \nNAME\n]\n section, where \nNAME\n is a descriptive\n   name for the proxy that is unique to your local configuration. In each section, set the \nproxy_cert\n, \nproxy_key\n,\n   \noutput\n, and \nvo\n corresponding to each pilot proxy:\n\n\n[PILOT \nNAME\n]\n\n\nproxy_cert\n \n=\n \nPATH TO THE PILOT CERTIFICATE\n\n\nproxy_key\n \n=\n \nPATH TO THE PILOT KEY\n\n\noutput\n \n=\n \nPATH TO CREATE THE PILOT PROXY\n\n\nvo\n \n=\n \nNAME OF VIRTUAL ORGANIZATION\n\n\n\n\n\n\nAdditionally, in each \n[PILOT \nNAME\n]\n section, you must specify how the proxy's VOMS attributes will be signed by\nsetting \nuse_voms_server\n. Choose one of the following options:\n\n\n\n\n\n\nTo directly sign the VOMS attributes (recommended), you must have access to the \nvo\n's certificate and key.\n  Specify the paths to the \nvo\n certificate and key, and optionally, the VOMS attribute (e.g. \n/osg/Role=NULL/Capability=NULL\n\n  for the OSG VO):\n\n\nuse_voms_server\n \n=\n \nfalse\n\n\nvo_cert\n \n=\n \nPATH\n \nTO\n \nTHE\n \nPILOT\n \nCERTIFICATE\n\n\nvo_key\n \n=\n \nPATH\n \nTO\n \nTHE\n \nPILOT\n \nKEY\n\n\nfqan\n \n=\n \nVOMS\n \nATTRIBUTE\n\n\n\n\n\n\n\n\nNote\n\n\nIf you do not have access to the \nvo\n's \nvoms_cert\n and \nvoms_key\n, contact the VO manager.\n\n\n\n\n\n\n\n\nTo have your proxy's VOMS attributes signed by the \nvo\n's VOMS server, set \nuse_voms_server = true\n\n  and the VOMS attribute (e.g. \n/osg/Role=NULL/Capability=NULL\n for the OSG VO):\n\n\nuse_voms_server\n \n=\n \ntrue\n\n\nfqan\n \n=\n \nVOMS\n \nATTRIBUTE\n\n\n\n\n\n\n\n\nWarning\n\n\nDue to the \nretirement of VOMS Admin server\n\nin the OSG, \nuse_voms_server = false\n is the preferred method for signing VOMS attributes. \n\n\n\n\n\n\n\n\nOptionally, the proxy renewal \nfrequency\n and \nlifetime\n (in hours) can be specified in each \n[PILOT \nNAME\n]\n section:\n\n\n#\n \nDefault\n:\n \n1\n\n\nfrequency\n \n=\n \nRENEWAL\n \nFREQUENCY\n\n\n#\n \nDefault\n:\n \n24\n\n\nlifetime\n \n=\n \nPROXY\n \nLIFETIME\n\n\n\n\n\n\n\n\n\n\nConfigure the location and output of the \nVO Frontend proxy\n under the \n[FRONTEND]\n section and set\n   the \nproxy_cert\n, \nproxy_key\n, and \noutput\n to paths corresponding to your VO Frontend:\n\n\n[FRONTEND]\n\n\nproxy_cert\n \n=\n \nPATH TO THE FRONTEND CERTIFICATE\n\n\nproxy_key\n \n=\n \nPATH TO THE FRONTEND KEY\n\n\noutput\n \n=\n \nPATH TO CREATE THE FRONTEND PROXY\n\n\n\n\n\n\n\n\nNote\n\n\noutput\n must be the same path as the \nclassad_proxy\n specified in \nthis section\n\n\n\n\n\n\n\n\n(OPTIONAL)\n If you are running the \ngwms-frontend\n service under a non-default user (default: \nfrontend\n),\n   specify the user as the owner of your proxies under the \n[COMMON]\n section:\n\n\n\n\nNote\n\n\nThe \n[COMMON]\n section is required but its contents are optional\n\n\n\n\n\n\n\n\nAdding Gratia Accounting and a Local Monitoring Page on a Production Server\n\n\nYou must report accounting information if you are running more than a few test jobs on the OSG .\n\n\n\n\n\n\nInstall the GlideinWMS Gratia Probe on each of your submit hosts in your GlideinWMS installation:\n\n\nroot@host #\n yum install gratia-probe-glideinwms\n\n\n\n\n\n\n\n\n\nEdit the ProbeConfig located in \n/etc/gratia/condor/ProbeConfig\n. First, edit the \nSiteName\n and \nProbeName\n to be a unique identifier for your GlideinWMS Submit host. There can be multiple probes (with different names) per site. If you haven't already, you should register your GlideinWMS submit host in \nOIM\n. Then you can use the name you used to register the resource.\n\n\nProbeName\n=\ncondor:\nhostname\n\n\nSiteName\n=\nHCC-GlideinWMW-Frontend\n\n\n\n\n\n\nNext, turn the probe on by setting \nEnableProbe\n:\n\n\nEnableProbe\n=\n1\n\n\n\n\n\n\n\n\n\n\nReconfigure HTCondor:\n\n\nroot@host #\n condor_reconfig\n\n\n\n\n\n\n\n\n\nOptional Accounting Configuration\n\n\nThe following sections contain additional configuration that may be required depending on the customizations you've made to your GlideinWMS frontend installation.\n\n\nUsers without Certificates\n\n\nIf you have users that submit jobs without a certificate explicitly declared in the submit file, you will need to add \nMapUnknownToGroup\n to the ProbeConfig. In the file \n/etc/gratia/condor/ProbeConfig\n, add the value after the \nEnableProbe\n.\n\n\n    \n...\n\n    \nSuppressGridLocalRecords\n=\n0\n\n    \nEnableProbe\n=\n1\n\n    \n%\nRED\n%\nMapUnknownToGroup\n=\n1\n%\nENDCOLOR\n%\n\n\n    \nTitle3\n=\nTuning parameter\n\n    \n...\n\n\n\n\n\n\nFurther, if you want to record all usage as coming from a single VO, you can configure the probe to override the 'guessed' VO. In the below example, replace the \nEngage\n with a registered VO that you would like to report as. If you don't have a VO that you are affiliated with, you may use Engage.\n\n\n...\n\n    \nMapUnknownToGroup\n=\n1\n\n    \nMapGroupToRole\n=\n1\n\n    \nVOOverride\n=\nEngage\n\n\n...\n\n\n\n\n\n\nNon-Standard HTCondor Install\n\n\nIf HTCondor is installed in a non-standard location (i.e., not RPMs, or relocated RPM outside \n/usr/bin\n), then you need to tell the probe where to find the HTCondor binaries. This can be done with a script with a special attribute in \n/etc/gratia/condor/ProbeConfig\n, \nCondorLocation\n. Point it to the location of the HTCondor install, such that \nCondorLocation/bin/condor_version\n exists.\n\n\nNew Data Directory\n\n\nIf your \nPER_JOB_HISTORY_DIR\n HTCondor configuration variable is different from the default value, you must update the value of \nDataFolder\n in \n/etc/gratia/condor/ProbeConfig\n. To check the value of \nPER_JOB_HISTORY_DIR\n run the following command:\n\n\nuser@host $\n condor_config_val PER_JOB_HISTORY_DIR\n\n\n\n\n\nDifferent collector and other customizations\n\n\nBy default the probe reports to the OSG GRACC. To change that you must edit the configuration file, \n/etc/gratia/condor/ProbeConfig\n, and replace the OSG production host with your desired one:\n\n\n...\n\n    \nCollectorHost\n=\ngratia-osg-prod.opensciencegrid.org:80\n\n    \nSSLHost\n=\ngratia-osg-prod.opensciencegrid.org:443\n\n    \nSSLRegistrationHost\n=\ngratia-osg-prod.opensciencegrid.org:80\n\n\n...\n\n\n\n\n\n\nOptional Configuration\n\n\nThe following configuration steps are optional and will likely not be required\nfor setting up a small site. If you do not need any of the following special\nconfigurations, skip to \nthe section on service\nactivation/deactivation\n.\n\n\n\n\nAllow users to specify where their jobs run\n\n\nCreating a group to test configuration changes\n\n\n\n\nAllowing users to specify where their jobs run\n\n\nIn order to allow users to specify the sites at which their jobs want to run (or\nto test a specific site), a Frontend can be configured to match on\n\nDESIRED_Sites\n or ignore it if not specified. Modify\n\n/etc/gwms-frontend/frontend.xml\n using the following instructions:\n\n\n\n\n\n\nIn the Frontend's global \nmatch\n stanza, set the \nmatch_expr\n:\n\n\n((job.get(\nDESIRED_Sites\n,\nnosite\n)==\nnosite\n) or (glidein[\nattrs\n][\nGLIDEIN_Site\n] in job.get(\nDESIRED_Sites\n,\nnosite\n).split(\n,\n)))\n\n\n\n\n\n\n\n\n\n\nIn the same \nmatch\n stanza, set the \nstart_expr\n:\n\n\n(\nDESIRED_Sites\n=?=\nundefined\n \n||\n \nstringListMember\n(\nGLIDEIN_Site\n,\nDESIRED_Sites\n,\n,\n))\n\n\n\n\n\n\n\n\n\n\nAdd the \nDESIRED_Sites\n attribute to the match attributes list:\n\n\nmatch_attrs\n\n   \nmatch_attr\n \nname=\nDESIRED_Sites\n \ntype=\nstring\n/\n\n\n/match_attrs\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend:\n\n\nroot@host #\n /etc/init.d/gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nCreating a group for testing configuration changes\n\n\nTo perform configuration changes without impacting production the recommended\nway is to create an ITB group in \n/etc/gwms-frontend/frontend.xml\n. This\ngroupwould only match jobs that have the \n+is_itb=True\n ClassAd.\n\n\n\n\n\n\nCreate a \ngroup\n named itb.\n\n\n\n\n\n\nSet the group's \nstart_expr\n so that the group's glideins will only match user jobs with \n+is_itb=True\n:\n\n\nmatch\n \nmatch_expr=\nTrue\n \nstart_expr=\n(is_itb)\n\n\n\n\n\n\n\n\n\n\nSet the \nfactory_query_expr\n so that this group only communicates with ITB factories:\n\n\nfactory\n \nquery_expr=\nFactoryType=?=\nitb\n\n\n\n\n\n\n\n\n\n\nSet the group's \ncollector\n stanza to reference the ITB factory, replacing \nusername@gfactory-1.t2.ucsd.edu\n with your factory identity:\n\n\ncollector\n \nDN=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu\n \n\\\n\n          \nfactory_identity=\ngfactory@glidein-itb.grid.iu.edu\n \n\\\n\n          \nmy_identity=\nusername@gfactory-1.t2.ucsd.edu\n \n\\\n\n          \nnode=\nglidein-itb.grid.iu.edu\n/\n\n\n\n\n\n\n\n\n\n\nSet the job \nquery_expr\n so that only ITB jobs appear in \ncondor_q\n:\n\n\njob\n \nquery_expr=\n(!isUndefined(is_itb) \n is_itb)\n\n\n\n\n\n\n\n\n\n\nReconfigure the Frontend (see the \nsection below\n):\n\n\n#\n on EL7 systems\n\nsystemctl reload gwms-frontend\n\n\n#\n on EL6 systems\n\nservice gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\n\nUsing GlideinWMS\n\n\nManaging GlideinWMS Services\n\n\nIn addition to the GlideinWMS service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n for more info\n\n\n\n\n\n\nGratia\n\n\ngratia-probes-cron\n\n\nAccounting software\n\n\n\n\n\n\nHTCondor\n\n\ncondor\n\n\n\n\n\n\n\n\nHTTPD\n\n\nhttpd\n\n\nGlideinWMS monitoring and staging\n\n\n\n\n\n\nGlideinWMS\n\n\ngwms-renew-proxies\n (EL6) or \ngwms-renew-proxies.timer\n (EL7)\n\n\nAutomatic proxy renewal\n\n\n\n\n\n\n\n\ngwms-frontend\n\n\nThe main GlideinWMS service\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo...\n\n\nOn EL6, run the command...\n\n\nOn EL7, run the command...\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\nsystemctl start \nSERVICE-NAME\n\n\n\n\n\n\nStop a  service\n\n\nservice \nSERVICE-NAME\n stop\n\n\nsystemctl stop \nSERVICE-NAME\n\n\n\n\n\n\nEnable a service to start on boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\nsystemctl enable \nSERVICE-NAME\n\n\n\n\n\n\nDisable a service from starting on boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\nsystemctl disable \nSERVICE-NAME\n\n\n\n\n\n\n\n\nReconfiguring GlideinWMS\n\n\nAfter changing the configuration of GlideinWMS, use the following table to find the appropriate command for your\noperating system (run as \nroot\n):\n\n\n\n\n\n\n\n\nIf your operating system is...\n\n\nRun the following command...\n\n\n\n\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nsystemctl reload gwms-frontend\n\n\n\n\n\n\nEnterprise Linux 6\n\n\nservice gwms-frontend reconfig\n\n\n\n\n\n\n\n\nUpgrading GlideinWMS FrontEnd\n\n\nAfter upgrading the GlideinWMS RPM, you must issue an upgrade command to GlideinWMS:\n\n\n\n\n\n\nIf you are using Enterprise Linux 7\n:\n\n\n\n\n\n\nStop the \ncondor\n and \ngwms-frontend\n services as specified in \nthis section\n\n\n\n\n\n\nIssue the upgrade command:\n\n\nroot@host #\n /usr/sbin/gwms-frontend upgrade\n\n\n\n\n\n\n\n\n\nStart the \ncondor\n and \ngwms-frontend\n services as specified in \nthis section\n\n\n\n\n\n\n\n\n\n\nIf you are using Enterprise Linux 6\n:\n\n\n\n\n\n\nUpgrade the GlideinWMS Frontend:\n\n\nroot@host #\n service gwms-frontend upgrade\n\n\n\n\n\n\n\n\n\nRestart the \ncondor\n service as specified in the \nmanaging GlideinWMS services section\n\n\n\n\n\n\n\n\n\n\nValidating GlideinWMS Frontend\n\n\nThe complete validation of the Frontend is the submission of actual jobs.\nHowever, there are a few things that can be checked prior to submitting user\njobs to HTCondor.\n\n\nVerifying Services Are Running\n\n\nThere are a few things that can be checked prior to submitting user jobs to HTCondor.\n\n\n\n\n\n\nVerify all HTCondor daemons are started.\n\n\nuser\n@host\n \n$\n \ncondor_config_val\n \n-\nverbose\n \nDAEMON_LIST\n\n\nDAEMON_LIST\n:\n \nMASTER\n,\n  \nCOLLECTOR\n,\n \nNEGOTIATOR\n,\n  \nSCHEDD\n,\n \nSHARED_PORT\n,\n \nSCHEDDJOBS2\n \nCOLLECTOR0\n \nCOLLECTOR1\n \nCOLLECTOR2\n\n\nCOLLECTOR3\n \nCOLLECTOR4\n \nCOLLECTOR5\n \nCOLLECTOR6\n \nCOLLECTOR7\n \nCOLLECTOR8\n \nCOLLECTOR9\n \nCOLLECTOR10\n \n,\n \nCOLLECTOR11\n,\n\n\nCOLLECTOR12\n,\n \nCOLLECTOR13\n,\n \nCOLLECTOR14\n,\n \nCOLLECTOR15\n,\n \nCOLLECTOR16\n,\n \nCOLLECTOR17\n,\n \nCOLLECTOR18\n,\n \nCOLLECTOR19\n,\n \nCOLLECTOR20\n,\n\n\nCOLLECTOR21\n,\n \nCOLLECTOR22\n,\n \nCOLLECTOR23\n,\n \nCOLLECTOR24\n,\n \nCOLLECTOR25\n,\n \nCOLLECTOR26\n,\n \nCOLLECTOR27\n,\n \nCOLLECTOR28\n,\n \nCOLLECTOR29\n,\n\n\nCOLLECTOR30\n,\n \nCOLLECTOR31\n,\n \nCOLLECTOR32\n,\n \nCOLLECTOR33\n,\n \nCOLLECTOR34\n,\n \nCOLLECTOR35\n,\n \nCOLLECTOR36\n,\n \nCOLLECTOR37\n,\n \nCOLLECTOR38\n,\n\n\nCOLLECTOR39\n,\n \nCOLLECTOR40\n\n\nDefined\n \nin\n \n/etc/condor/config.d/11_gwms_secondary_collectors.config\n,\n \nline\n \n193.\n\n\n\n\n\n\nIf you don't see all the \ncollectors and the two schedds\n, then the configuration must be corrected. There should be no startd daemons listed\n\n\n\n\n\n\nVerify all VO Frontend HTCondor services are communicating.\n\n\nuser@host $\n condor_status -any\n\nMyType               TargetType           Name\n\n\nglideresource        None                 MM_fermicloud026@gfactory_inst\n\n\nScheduler            None                 fermicloud020.fnal.gov\n\n\nDaemonMaster         None                 fermicloud020.fnal.gov\n\n\nNegotiator           None                 fermicloud020.fnal.gov\n\n\nCollector            None                 frontend_service@fermicloud020.fnal.gov\n\n\nScheduler            None                 schedd_jobs2@fermicloud020.fnal.gov\n\n\n\n\n\n\n\n\n\n\nTo see the details of the glidein resource use \ncondor_status -subsystem glideresource -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nVerify that the Factory is seeing correctly the Frontend using \ncondor_status -pool \nFACTORY_HOST\n -any -constraint 'FrontendName==\n\"\nFRONTEND_NAME_FROM_CONFIG\n\"\n' -l\n, including the GlideFactoryName.\n\n\n\n\n\n\nGlideinWMS Job submission\n\n\nHTCondor submit file \nglidein-job.sub\n. This is a simple job printing the hostname of the host where the job is running:\n\n\n#\nfile\n \nglidein\n-\njob\n.\nsub\n\n\nuniverse\n \n=\n \nvanilla\n\n\nexecutable\n \n=\n \n/\nbin\n/\nhostname\n\n\noutput\n \n=\n \nglidein\n/\ntest\n.\nout\n\n\nerror\n \n=\n \nglidein\n/\ntest\n.\nerr\n\n\nrequirements\n \n=\n \nIS_GLIDEIN\n \n==\n \nTrue\n\n\nlog\n \n=\n \nglidein\n/\ntest\n.\nlog\n\n\nShouldTransferFiles\n \n=\n \nYES\n\n\n\nwhen_to_transfer_output\n \n=\n \nON_EXIT\n\n\nqueue\n\n\n\n\n\n\nTo submit the job:\n\n\nroot@host #\n condor_submit glidein-job.sub\n\n\n\n\n\nThen you can control the job like a normal HTCondor job, e.g. to check the status of the job use \ncondor_q\n.\n\n\nMonitoring Web pages\n\n\nYou should be able to see the jobs also in the GlideinWMS monitoring pages that are\nmade available on the Web:\n\nhttp://gwms-frontend-host.domain/vofrontend/monitor/\n\n\nTroubleshooting GlideinWMS\n\n\nFile Locations\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nLogs\n\n\n/var/log/gwms-frontend/\n\n\n\n\n\n\nStartup script\n\n\n/etc/init.d/gwms-frontend (on EL6) - /usr/bin/gwms-frontend (on EL7)\n\n\n\n\n\n\nWeb Directory\n\n\n/var/lib/gwms-frontend/web-area\n\n\n\n\n\n\nWeb Base\n\n\n/var/lib/gwms-frontend/web-base\n\n\n\n\n\n\nWeb configuration\n\n\n/etc/httpd/conf.d/gwms-frontend.conf\n\n\n\n\n\n\nWorking Directory\n\n\n/var/lib/gwms-frontend/vofrontend/\n\n\n\n\n\n\nLock files\n\n\n/var/lib/gwms-frontend/vofrontend/lock/frontend.lock /var/lib/gwms-frontend/vofrontend/group_*/lock/frontend.lock\n\n\n\n\n\n\nStatus files\n\n\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n/var/lib/gwms-frontend\n is also the home directory of the \nfrontend\n user\n\n\n\n\nCertificates brief\n\n\nHere a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.\n\n\n\n\n\n\n\n\nFile Description\n\n\nFile Location\n\n\n\n\n\n\n\n\n\n\nConfiguration file\n\n\n/etc/gwms-frontend/frontend.xml\n\n\n\n\n\n\nHTCondor certificates map\n\n\n/etc/condor/certs/condor_mapfile (1)\n\n\n\n\n\n\nHost certificate and key (2)\n\n\n/etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem\n\n\n\n\n\n\nVO Frontend proxy (from host certificate)\n\n\n/tmp/vofe_proxy (3)\n\n\n\n\n\n\nPilot proxy\n\n\n/tmp/pilot_proxy (3)\n\n\n\n\n\n\n\n\n\n\n\n\nIf using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.\n\n\n\n\n\n\nUsed to create the VO Frontend proxy if following the \ninstructions above\n\n\n\n\n\n\nIf using the Frontend configuration and scripts described \nabove in this document\n. These paths are the ones specified in the configuration file.\n\n\n\n\n\n\nRemember also that when you change DN:\n\n\n\n\nThe VO Frontend certificate DN must be communicated to the GlideinWMS Factory (\nsee above\n)\n\n\nThe pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)\n\n\n\n\nIncrease the log level and change rotation policies\n\n\nYou can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the \nprocess_log\n section of \n/etc/gwms-frontend/frontend.xml\n:\n\n\nlog_retention\n\n   \nprocess_logs\n\n       \nprocess_log\n \nextension=\nall\n \nmax_days=\n7.0\n \nmax_mbytes=\n100.0\n \nmin_days=\n3.0\n \nmsg_types=\nDEBUG,EXCEPTION,INFO,ERROR,ERR\n/\n\n\n\n\n\n\nYou can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:\n\n\n\n\nmax_bytes is the max size of the log files\n\n\nmax_days it will be rotated.\n\n\ncompression specifies if rotated files are compressed\n\n\nbackup_count is the number of rotated log files kept\n\n\n\n\nFurther details are in the \nreference documentation\n.\n\n\nFrontend reconfig failing\n\n\nIf \nservice gwms-frontend reconfig\n fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that \n/etc/gwms-frontend/\n belongs to the \nfrontend\n user. It must be able to write to update the configuration file.\n\n\nFrontend failing to start\n\n\nIf the startup script of the frontend is failing, check the log file for errors (probably \n/var/log/gwms-frontend/frontend/frontend.\nTODAY\n.err.log\n and \n.debug.log\n).\n\n\nIf you find errors like \n\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"\n and \n\"IOError: [Errno 9] Bad file descriptor\"\n you may have an empty status file (\n/var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml\n) that causes GlideinWMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").\n\n\nRemove the status file. Then start the frontend. The Frontend will be fixed in future versions to handle this automatically.\n\n\nCertificates not there\n\n\nThe scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in \n/tmp\n or wherever you expect them):\n\n\n\n\nmake sure that the scripts are there and configured with the correct values\n\n\nmake sure that the scripts are executable\n\n\nmake sure that the scripts are in \nfrontend\n 's crontab\n\n\nmake sure that the certificates (or master proxy) used to generate the proxies is not expired\n\n\n\n\nFailed authentication\n\n\nIf you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:\n\n\n\n\ncheck that you have the right x509 certificates mentioned in the security section of \n/etc/gwms-frontend/frontend.xml\n\n\nthe owner must be \nfrontend\n (user running the frontend)\n\n\nthe permission must be 600\n\n\nthey must be valid for more than one hour (2/300 hours), at least the non VO part\n\n\n\n\n\n\ncheck that the clock is synchronized (see HostTimeSetup)\n\n\n\n\nFrontend doesn't trust Factory\n\n\nIf your frontend complains in the debug log:\n\n\ncode\n \n256\n:[\nError: communication error\n\\n\n, \nAUTHENTICATE:1003:Failed to authenticate with any method\n\\n\n, \nAUTHENTICATE:1004:Failed to authenticate using GSI\n\\n\n, \nGSI:5006:Failed to authenticate because the subject \n/DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu\n is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual).\n\\n\n, \nGSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.\n\n\n\n\n\n\nA possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file \n/var/lib/gwms-frontend/vofrontend/frontend.condor_config\n.\n\n\nNo security credentials match for factory pool ..., not advertising request\n\n\nYou may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the \ntrust_domain\n and \nauth_method\n of an entry in the Factory configuration is not matching any of the \ntrust_domain\n, \ntype\n couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.\n\n\nTo fix the problem make sure that those attributes match as desired.\n\n\nJobs not running\n\n\nIf your jobs remain Idle\n\n\n\n\nCheck the frontend log files (see above)\n\n\nCheck the HTCondor log files (\ncondor_config_val LOG\n will give you the correct log directory):\n\n\nSpecifically look the CollectorXXXLog files\n\n\n\n\n\n\n\n\nCommon causes of problems could be:\n\n\n\n\nx509 certificates\n\n\nmissing or expired or too short-lived proxy\n\n\nincorrect ownership or permission on the certificate/proxy file\n\n\nmissing certificates\n\n\n\n\n\n\nIf the Frontend http server is down in the glidein logs in the Factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from \nhttp://FRONTEND_HOST/vofrontend/stage\n.\"\n\n\ncheck that the http server is running and you can reach the URL (\nhttp://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg\n)\n\n\n\n\n\n\n\n\nAdvanced Configurations\n\n\n\n\nGlideinWMS Frontend on a Campus Grid\n\n\n\n\nGetting Help\n\n\nTo get assistance about the OSG software please use \nthis page\n.\n\n\nFor specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support \n\n\nTo request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to \n (see below).\n\n\nReferences\n\n\nDefinitions:\n\n\n\n\nWhat is a \nVirtual Organization\n\n\n\n\nDocuments about the Glidein-WMS system and the VO frontend:\n\n\n\n\nhttp://glideinwms.fnal.gov/\n\n\n\n\nUsers\n\n\nThe Glidein WMS Frontend installation will create the following users unless they are already created.\n\n\n\n\n\n\n\n\nUser\n\n\nDefault uid\n\n\nComment\n\n\n\n\n\n\n\n\n\n\napache\n\n\n48\n\n\nRuns httpd to provide the monitoring page (installed via dependencies).\n\n\n\n\n\n\ncondor\n\n\nnone\n\n\nHTCondor user (installed via dependencies).\n\n\n\n\n\n\nfrontend\n\n\nnone\n\n\nThis user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.\n\n\n\n\n\n\ngratia\n\n\nnone\n\n\nRuns the Gratia probes to collect accounting data (optional see \nthe Gratia section below\n)\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nUID 48 is reserved by RedHat for user \napache\n.  If it is already taken by a different username, you will experience errors.\n\n\n\n\nCertificates\n\n\nThis document has a \nproxy configuration section\n that\nuses the host certificate/key and a user certificate to generate the required\nproxies.\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nHere\n are instructions to request a host certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTCondor port range\n\n\ntcp\n\n\nLOWPORT, HIGHPORT\n\n\nYES\n\n\n\n\ncontiguous range of ports\n\n\n\n\n\n\nGlideinWMS Frontend\n\n\ntcp\n\n\n9618, 9620 to 9660\n\n\nYES\n\n\n\n\nHTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)\n\n\n\n\n\n\n\n\nThe VO frontend must have reliable network connectivity, be on the public\ninternet (no NAT), and preferably with no firewalls. Incoming TCP ports 9618 to 9660 must be open.", 
            "title": "Install GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glideinwms-vo-frontend-installation", 
            "text": "This document describes how to install the Glidein Workflow Managment System\n(GlideinWMS) VO Frontend for use with the OSG Glidein factory. This software is\nthe minimum requirement for a VO to use GlideinWMS.  This document assumes expertise with HTCondor and familiarity with the GlideinWMS\nsoftware. It  does not  cover anything but the simplest possible install.\nPlease consult the  GlideinWMS reference\ndocumentation \nfor advanced topics, including non- root , non-RPM-based installation.  This document covers three components of the GlideinWMS a VO needs to install:   User Pool Collectors : A set of  condor_collector  processes. Pilots submitted by the factory will join to one of these collectors to form a HTCondor pool.  User Pool Schedd : A  condor_schedd . Users may submit HTCondor vanilla universe jobs to this schedd; it will run jobs in the HTCondor pool formed by the  User Pool Collectors .  Glidein Frontend : The frontend will periodically query the  User Pool Schedd  to determine the desired number of running job slots. If necessary, it will request the Factory to launch additional pilots.   This guide covers installation of all three components on the same host: it is\ndesigned for small to medium VOs (see the Hardware Requirements below). Given a\nsignificant, large host, we have been able to scale the single-host install to\n20,000 running jobs.", 
            "title": "GlideinWMS VO Frontend Installation"
        }, 
        {
            "location": "/other/install-gwms-frontend/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux users  apache  (UID 48),  condor ,  frontend , and  gratia  Network:  The VO frontend must have reliable network connectivity and be on the public internet (i.e. no NAT).\n    The latest version requires the following TCP ports to be open:  80 (HTTP) for monitoring and serving configuration to workers  9618 (HTCondor shared port) for HTCondor daemons including the Schedd and User Collector  9620 to 9660 for secondary collectors (depending on configuration, see below)    Host choice : The GlideinWMS VO Frontend has the following hardware requirements for a production host:  CPU : Four cores, preferably no more than 2 years old.  RAM : 3GB plus 2MB per running job. For example, to sustain 2000 running jobs, a host with 5GB is needed.  Disk : 30GB will be sufficient for all the binaries, config and log files related to GlideinWMS. As this will be an interactive submit host, have enough disk space for your users' jobs.       Note  The default configuration uses a port range (9620 to 9660) for the secondary collectors.\nYou can configure the secondary collectors to use the shared port 9618 instead;\nthis will become the default in the future.    Note  GlideinWMS versions prior to 3.4.1 also required port 9615 for the Schedd,\nand did not support using shared port for the secondary collectors.\nIf you are upgrading a standalone submit host from version 3.4 or earlier, the default open \nport has changed from 9615 to 9618, and you need to update your firewall rules to reflect this change.\nYou can figure out which port will be used by running the following command:  console\n condor_config_val SHARED_PORT_ARGS   For more detailed information, see  Configuring GlideinWMS Frontend .  As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has a  supported operating system  Obtain root access to the host  Prepare the  required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/other/install-gwms-frontend/#credentials-and-proxies", 
            "text": "The VO Frontend will use two credentials in its interactions with the other GlideinWMS services. At this time, these will be proxy files.   the  VO Frontend proxy  (used to authenticate with the other GlideinWMS services).  one or more GlideinWMS  pilot proxies  (used/delegated to the Factory services and submitted on the GlideinWMS pilot jobs).   The  VO Frontend proxy  and the   pilot proxy  can\nbe the same. By default, the VO Frontend will run as user  frontend  (UID is\nmachine dependent) so these proxies must be owned by the user  frontend .   Note  Both proxies need to be passwordless to allow  automatic proxy renewal .", 
            "title": "Credentials and Proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#vo-frontend-proxy", 
            "text": "The use of a service certificate is recommended. Then you create a proxy from the certificate as explained in the  proxy configuration section .  You must give the Factory operations team the DN of this proxy when you\ninitially setup the Frontend and each time the DN changes .", 
            "title": "VO Frontend proxy"
        }, 
        {
            "location": "/other/install-gwms-frontend/#pilot-proxies", 
            "text": "These proxies are used by the Factory to submit the GlideinWMS pilot jobs.\nTherefore, they must be authorized to access to the CEs (Factory entry points)\nwhere jobs are submitted. There is no need to notify the Factory operation about\nthe DN of these proxies (neither at the initial registration nor for subsequent\nchanges). These additional proxies have no special requirements or controls added by the\nFactory but will probably require VO attributes because of the CEs: if you are\nable to use one of these proxies to submit jobs to the corresponding CEs where the Factory runs\nGlideinWMS pilots for you, then the proxies are OK. You can test each of your proxies using globusrun  or HTCondor-G.  To check the important information about a PEM certificate you can use:  openssl\nx509 -in /etc/grid-security/hostcert.pem -subject -issuer -dates -noout . You\nwill need that to find out information for the configuration files and the\nrequest to the GlideinWMS Factory.", 
            "title": "Pilot proxies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#osg-factory-access", 
            "text": "Before installing the GlideinWMS VO Frontend you need the information about a  Glidein Factory  that you can access:   (recommended) OSG is managing a factory at UCSD  You have another Glidein Factory that you can access   To request access to the OSG Glidein Factory at UCSD you have to send an email to   providing:   Your Name  The VO that is utilizing the VO Frontend  The DN of the proxy you will use to communicate with the Factory (VO Frontend DN, e.g. the host certificate subject if you follow the  proxy configuration section )  You can propose a security name that will have to be confirmed/changed by the Factory managers (see below)   A list of sites where you want to run:    Your VO must be supported on those sites   You can provide a list or piggy back on existing lists, e.g. all the sites supported for the VO. Check with the Factory managers  You can start with one single site   In the reply from the OSG Factory managers you will receive some information needed for the configuration of your VO Frontend   The exact spelling and capitalization of your VO name. Sometime is different from what is commonly used, e.g. OSG VO is \"OSGVO\".  The host of the Factory Collector:  gfactory-1.t2.ucsd.edu  The DN os the factory, e.g.  /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu  The factory identity, e.g.:  gfactory@gfactory-1.t2.ucsd.edu  The identity on the Factory you will be mapped to. Something like:  username@gfactory-1.t2.ucsd.edu  Your security name. A unique name, usually containing your VO name:  My_SecName  A string to add in the main Factory query_expr in the Frontend configuration, e.g.  stringListMember(\" VO \",GLIDEIN_Supported_VOs) . This is used to select the entries you can use. From there you get the correct name of the VO (above in this list).", 
            "title": "OSG Factory access"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installing-glideinwms-frontend", 
            "text": "", 
            "title": "Installing GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installing-htcondor", 
            "text": "If you don't have HTCondor already installed, you can install the HTCondor RPM\nfrom the OSG repository:  root@host #  yum install condor.x86_64  If you already have installed HTCondor using a tarball or a source other than\nthe OSG ROM, you will need to install the empty-condor RPM:  root@host #  yum install empty-condor --enablerepo = osg-empty", 
            "title": "Installing HTCondor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installing-the-vo-frontend-rpm", 
            "text": "Install the RPM and dependencies (be prepared for a lot of dependencies).  root@host #  yum install glideinwms-vofrontend  This will install the current production release verified and tested by OSG with\ndefault HTCondor configuration. This command will install the GlideinWMS\nvofrontend, HTCondor, the OSG client, and all the required dependencies all on one\nnode.  If you wish to install a different version of GlideinWMS, add the \"--enablerepo\"\nargument to the command as follows:   yum install --enablerepo=osg-testing glideinwms-vofrontend : The most recent production release, still in testing phase. This will usually match the current tarball version on the GlideinWMS home page. (The osg-release production version may lag behind the tarball release by a few weeks as it is verified and packaged by OSG). Note that this will also take the osg-testing versions of all dependencies as well.  yum install --enablerepo=osg-upcoming glideinwms-vofrontend : The most recent development series release, i.e. version 3.3 release. This has newer features such as cloud submission support, but is less tested.   Note that these commands will install default HTCondor configurations with all\nGlideinWMS services on one node.", 
            "title": "Installing the VO Frontend RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#installing-glideinwms-frontend-on-multiple-nodes-advanced", 
            "text": "For advanced users expecting heavy usage on their submit node, you may want to\nconsider splitting the user collector, user submit, and vo frontend\nservices. This can be doing using the following three commands (on different\nmachines):  root@host #  yum install glideinwms-vofrontend-standalone root@host #  yum install glideinwms-usercollector root@host #  yum install glideinwms-userschedd  In addition, you will need to perform the following steps:   On the vofrontend and userschedd, modify  CONDOR_HOST  to point to your usercollector. This is in  /etc/condor/config.d/00_gwms_general.config . You can also override this value by placing it in a new config file. (For instance,  /etc/condor/config.d/99_local_custom.config  to avoid rpmsave/rpmnew conflicts on upgrades).  In  /etc/condor/certs/condor_mapfile , you will need to add the DNs of each machine (userschedd, usercollector, vofrontend). Take great care to escape all special characters. Alternatively, you can use the  glidecondor_addDN  to add these values.  In the  /etc/gwms-frontend/frontend.xml  file, change the schedd locations to match the correct server. Also change the collectors tags at the bottom of the file. More details on frontend.xml are in the following sections.", 
            "title": "Installing GlideinWMS Frontend on Multiple Nodes (Advanced)"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-glideinwms-frontend", 
            "text": "After installing the RPM, you need to configure the components of the GlideinWMS VO Frontend:   Edit Frontend configuration options  Edit HTCondor configuration options  Create a HTCondor grid map file  Reconfigure and Start the Frontend", 
            "title": "Configuring GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-the-frontend", 
            "text": "The VO Frontend configuration file is  /etc/gwms-frontend/frontend.xml . The next steps will describe each line that you will need to edit if you are using the OSG Factory at UCSD. The portions to edit are highlighted in red font. If you are using a different Factory more changes are necessary, please check the VO Frontend configuration reference.    The VO you are affiliated with. This will identify those CEs that the GlideinWMS pilot will be authorized to run on using the  pilot proxy  described previously in  this section . Sometimes the whole  query_expr  is provided to you by the Factory operators (see Factory access above):  factory   query_expr= ((stringListMember( VO , GLIDEIN_Supported_VOs)))     Factory collector information. The  username  that you are assigned by the Factory (also called the identity you will be mapped to on the factory, see above) . Note that if you are using a factory different than the production Factory, you will have to change also  DN ,  factory_identity  and  node  attributes. (refer to the information provided to you by the Factory operator):  collector   DN= /DC=org/DC=doegrids/OU=Services/CN=gfactory-1.t2.ucsd.edu \n            comment= Define factory collector globally for simplicity \n            factory_identity= gfactory@gfactory-1.t2.ucsd.edu \n            my_identity= username@gfactory-1.t2.ucsd.edu \n            node= gfactory-1.t2.ucsd.edu /     Frontend security information.   The  classad_proxy  in the security entry is the location of the VO Frontend proxy described previously  here .  The  proxy_DN  is the DN of the  classad_proxy  above.  The  security_name  identifies this VO Frontend to the the Factory, It is provided by the Factory operator.  The  absfname  in the credential entry is the location of the GlideinWMS  pilot  proxy described in the requirements section  here . There can be multiple pilot proxies, or even other kind of keys (e.g. if you use cloud resources).  The type and trust_domain of the credential must match respectively auth_method and trust_domain used in the entry definition in the Factory. If there is no match, between these two attributes in one of the credentials and the corresponding ones in some entry in one of the Factories, then this Frontend cannot trigger glideins. \nBoth the  classad_proxy  and  absfname  files should be owned by  frontend  user. security   classad_proxy= /tmp/vo_proxy   proxy_DN= DN of vo_proxy \n       proxy_selection_plugin= ProxyAll \n       security_name= The security name, this is used by factory \n       sym_key= aes_256_cbc \n       credentials \n         credential   absfname= /tmp/pilot_proxy   security_class= frontend \n         trust_domain= OSG   type= grid_proxy / \n       /credentials  /security       The schedd information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  fullname  attribute is the fully qualified domain name of the host where you installed the VO Frontend ( hostname --fqdn ).\nA secondary schedd is optional. You will need to delete the secondary schedd line if you are not using it. Multiple schedds allow the Frontend to service requests from multiple submit hosts. schedds \n   schedd   DN= Cert DN used by the schedd at fullname: \n         fullname= Hostname of the schedd / \n    schedd   DN= Cert DN used by the second Schedd at fullname: \n          fullname= schedd name@Hostname of second schedd /  /schedds       The User Collector information.   The  DN  of the  VO Frontend Proxy  described previously  here .  The  node  attribute is the full hostname of the collectors ( hostname --fqdn ) and port  The  secondary  attribute indicates whether the element is for the primary or secondary collectors (True/False).\n The default HTCondor configuration of the VO Frontend starts multiple Collector processes on the host ( /etc/condor/config.d/11_gwms_secondary_collectors.config ). The  DN  and  hostname  on the first line are the hostname and the host certificate of the VO Frontend. The  DN  and  hostname  on the second line are the same as the ones in the first one. The hostname (e.g. hostname.domain.tld) is filled automatically during the installation. The secondary collector connection can be defined as sinful string for the sock case , e.g., hostname.domain.tld:9618?sock=collector16.     [Example 1]           ::: xml \n         collector   DN = DN of main collector \n                node = hostname.domain.tld:9618   secondary = False / \n         collector   DN = DN of secondary collectors (usually same as DN in line above) \n                node = hostname.domain.tld:9620-9660   secondary = True /    Note   In GlideinWMS v3.4.1, shared port only configuration is incompatible if talking to older Factories (v3.4 or older). We strongly recommend any user of GlideinWMS Frontend v3.4.1 or newer, to transition to the use of shared port for secondary collectors and CCBs.\nThe shared port configuration is incompatible if your Frontend is talking to Factories v3.4 or older and you'll get an error telling you to wait.\nTo transition to the use of shared port for secondary collectors, you have to change the collectors section in the Frontend configuration. If you are using the default port range for the secondary collectors as shown in [Example 2] below, then you should replace it with port  9618  and the sock-range as shown in [Example 1] above.  If you have a more complex configuration, please read the  detailed GlideinWMS configuration  [Example 2]           ::: xml \n         collector   DN = DN of main collector \n                node = hostname.domain.tld:9618   secondary = False / \n         collector   DN = DN of secondary collectors (usually same as DN in line above) \n                node = \u201c hostname . domain . tld : 9618 ? sock = collector0 - 40  secondary= True /    The CCBs information.\n    If you have a different configuration of the HTCondor Connection Brokering (CCB servers) from the default (usually the section is empty as the User Collectors acts as CCB if needed), you can set the connection in the CCB section the same way that User Collector information previously mentioned. Also, the same rules for transition to shared_port of the connections, apply to the CCBs.      ::: xml \n     ccb   DN = DN of the CCB server \n            node = hostname.domain.tld:9618 / \n     ccb   DN = DN of the CCB server \n            node = \u201c hostname . domain . tld : 9618 ? sock = collector0 - 40  secondary= True /      Warning  The Frontend configuration includes many knobs, some of which are conflicting with a RPM installation where there is only one version of the Frontend installed and it uses well known paths.     Do not change the following in the Frontend configuration (you must leave the default values coming with the RPM installation):   frontend_versioning='False' (in the first line of XML, versioning is useful to install multiple tarball versions)  for RPM installs, work base_dir must be /var/lib/gwms-frontend/vofrontend/ (other scripts like /etc/init.d/gwms-frontend count on that value)", 
            "title": "Configuring the Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#using-a-different-factory", 
            "text": "The configuration above points to the OSG production Factory. If you are using a different Factory, then you have to:   replace  gfactory@gfactory-1.t2.ucsd.edu  and  gfactory-1.t2.ucsd.edu  with the correct values for your Factory. And control also that the name used for the Frontend () matches.  make sure that the Factory is advertising the attributes used in the Factory query expression ( query_expr ).", 
            "title": "Using a Different Factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#configuring-htcondor", 
            "text": "The HTCondor configuration for the Frontend is placed in  /etc/condor/config.d .   00_gwms_general.config  01_gwms_collectors.config  02_gwms_schedds.config  03_gwms_local.config  11_gwms_secondary_collectors.config  90_gwms_dns.config   For most installations create a new file named  /etc/condor/config.d/92_local_condor.config", 
            "title": "Configuring HTCondor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#using-other-htcondor-rpms-eg-uw-madison-htcondor-rpm", 
            "text": "The above procedure will work if you are using the OSG HTCondor RPMS. You can\nverify that you used the OSG HTCondor RPM by using  yum list condor . The\nversion name should include \"osg\", e.g.  8.6.4-3.osg.el7 .  If you are using the UW Madison HTCondor RPMS, be aware of the following changes:   This HTCondor RPM uses a file  /etc/condor/condor_config.local  to add your local machine slot to the user pool.  If you want to disable this behavior (recommended), you should blank out that file or comment out the line in  /etc/condor/condor_config  for LOCAL_CONFIG_FILE. (Make sure that LOCAL_CONFIG_DIR is set to  /etc/condor/config.d )  Note that the variable LOCAL_DIR is set differently in UW Madison and OSG RPMs. This should not cause any more problems in the GlideinWMS RPMs, but please take note if you use this variable in your job submissions or other customizations.   In general if you are using a non OSG RPM or if you added custom configuration\nfiles for HTCondor please check the order of the configuration files:  root@host #  condor_config_val -config Configuration source:      /etc/condor/condor_config  Local configuration sources:      /etc/condor/config.d/00_gwms_general.config      /etc/condor/config.d/01_gwms_collectors.config      /etc/condor/config.d/02_gwms_schedds.config      /etc/condor/config.d/03_gwms_local.config      /etc/condor/config.d/11_gwms_secondary_collectors.config      /etc/condor/config.d/90_gwms_dns.config      /etc/condor/condor_config.local   If, like in the example above, the GlideinWMS configuration files are not the\nlast ones in the list please verify that important configuration options have\nnot been overridden by the other configuration files.", 
            "title": "Using other HTCondor RPMs, e.g. UW Madison HTCondor RPM"
        }, 
        {
            "location": "/other/install-gwms-frontend/#verifying-your-htcondor-configuration", 
            "text": "The GlideinWMS configuration files in  /etc/condor/config.d  should be the last ones in the list. If not, please verify that important configuration options have not been overridden by the other configuration files.    Verify the alll the expected HTCondor daemons are running:  root@host #  condor_config_val -verbose DAEMON_LIST DAEMON_LIST: MASTER, COLLECTOR, NEGOTIATOR, SCHEDD, SHARED_PORT, COLLECTOR0 COLLECTOR1 COLLECTOR2 COLLECTOR3 COLLECTOR4 COLLECTOR5 COLLECTOR6 COLLECTOR7 COLLECTOR8 COLLECTOR9 COLLECTOR10 , COLLECTOR11, COLLECTOR12,  COLLECTOR13, COLLECTOR14, COLLECTOR15, COLLECTOR16, COLLECTOR17, COLLECTOR18, COLLECTOR19, COLLECTOR20, COLLECTOR21, COLLECTOR22, COLLECTOR23,  COLLECTOR24, COLLECTOR25, COLLECTOR26, COLLECTOR27, COLLECTOR28, COLLECTOR29, COLLECTOR30, COLLECTOR31, COLLECTOR32, COLLECTOR33, COLLECTOR34,  COLLECTOR35, COLLECTOR36, COLLECTOR37, COLLECTOR38, COLLECTOR39, COLLECTOR40  Defined in  /etc/condor/config.d/11_gwms_secondary_collectors.config , line 193.     If you don't see all the collectors. shared port and the two schedd, then the\nconfiguration must be corrected. There should be  no   startd  daemons\nlisted.", 
            "title": "Verifying your HTCondor configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#creating-a-htcondor-grid-mapfile", 
            "text": "The HTCondor mapfile ( /etc/condor/certs/condor_mapfile ) is used for\nauthentication between the GlideinWMS pilot running on a remote worker node, and\nthe local collector. HTCondor uses the mapfile to map certificates to pseudo-users\non the local machine. It is important that you map the DN's of:    Each schedd proxy : The  DN  of each schedd that the frontend talks to. Specified in the frontend.xml schedd element  DN  attribute:  schedds \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= YOUR_HOST / \n   schedd   DN= /DC=org/DC=doegrids/OU=Services/CN=YOUR_HOST   fullname= schedd_jobs2@YOUR_HOST /  /schedds     Frontend proxy : The DN of the proxy that the Frontend uses to communicate with the other GlideinWMS services. Specified in the frontend.xml security element  proxy_DN  attribute:  security   classad_proxy= /tmp/vo_proxy   proxy_DN= DN of vo_proxy   ....     Each pilot proxy  The DN of  each  proxy that the frontend forwards to the factory to use with the GlideinWMS pilots.  This allows the !GlideinWMS pilot jobs to communicate with the User Collector. Specified in the frontend.xml proxy  absfname  attribute (you need to specify the  DN  of each of those proxies:  security   ....  proxies \n     proxy   absfname= /tmp/vo_proxy   .... \n    :  /proxies     Below is an example mapfile, by default found in /etc/condor/certs/condor_mapfile . In this example there are lines for each of\nservices mentioned above.  GSI   DN of schedd proxy   schedd  GSI   DN of frontend proxy   frontend  GSI   DN of pilot proxy $   pilot_proxy  GSI   ^/DC=org/DC=doegrids/OU=Services/CN=personal-submit-host2.mydomain.edu$   % RED example_of_format % ENDCOLOR %  GSI   (. * )   anonymous  FS   (. * )   \\ 1", 
            "title": "Creating a HTCondor grid mapfile."
        }, 
        {
            "location": "/other/install-gwms-frontend/#restarting-htcondor", 
            "text": "After configuring HTCondor, be sure to restart HTCondor:  root@host #  service condor restart", 
            "title": "Restarting HTCondor"
        }, 
        {
            "location": "/other/install-gwms-frontend/#proxy-configuration", 
            "text": "GlideinWMS comes with the  gwms-renew-proxies service  that can automatically generate\nand renew the  pilot proxies  and  VO Frontend proxy . To configure this service, modify /etc/gwms-frontend/proxies.ini  using the following instructions:    For each of your  pilot proxies , create a  [PILOT  NAME ]  section, where  NAME  is a descriptive\n   name for the proxy that is unique to your local configuration. In each section, set the  proxy_cert ,  proxy_key ,\n    output , and  vo  corresponding to each pilot proxy:  [PILOT  NAME ]  proxy_cert   =   PATH TO THE PILOT CERTIFICATE  proxy_key   =   PATH TO THE PILOT KEY  output   =   PATH TO CREATE THE PILOT PROXY  vo   =   NAME OF VIRTUAL ORGANIZATION   Additionally, in each  [PILOT  NAME ]  section, you must specify how the proxy's VOMS attributes will be signed by\nsetting  use_voms_server . Choose one of the following options:    To directly sign the VOMS attributes (recommended), you must have access to the  vo 's certificate and key.\n  Specify the paths to the  vo  certificate and key, and optionally, the VOMS attribute (e.g.  /osg/Role=NULL/Capability=NULL \n  for the OSG VO):  use_voms_server   =   false  vo_cert   =   PATH   TO   THE   PILOT   CERTIFICATE  vo_key   =   PATH   TO   THE   PILOT   KEY  fqan   =   VOMS   ATTRIBUTE    Note  If you do not have access to the  vo 's  voms_cert  and  voms_key , contact the VO manager.     To have your proxy's VOMS attributes signed by the  vo 's VOMS server, set  use_voms_server = true \n  and the VOMS attribute (e.g.  /osg/Role=NULL/Capability=NULL  for the OSG VO):  use_voms_server   =   true  fqan   =   VOMS   ATTRIBUTE    Warning  Due to the  retirement of VOMS Admin server \nin the OSG,  use_voms_server = false  is the preferred method for signing VOMS attributes.      Optionally, the proxy renewal  frequency  and  lifetime  (in hours) can be specified in each  [PILOT  NAME ]  section:  #   Default :   1  frequency   =   RENEWAL   FREQUENCY  #   Default :   24  lifetime   =   PROXY   LIFETIME     Configure the location and output of the  VO Frontend proxy  under the  [FRONTEND]  section and set\n   the  proxy_cert ,  proxy_key , and  output  to paths corresponding to your VO Frontend:  [FRONTEND]  proxy_cert   =   PATH TO THE FRONTEND CERTIFICATE  proxy_key   =   PATH TO THE FRONTEND KEY  output   =   PATH TO CREATE THE FRONTEND PROXY    Note  output  must be the same path as the  classad_proxy  specified in  this section     (OPTIONAL)  If you are running the  gwms-frontend  service under a non-default user (default:  frontend ),\n   specify the user as the owner of your proxies under the  [COMMON]  section:   Note  The  [COMMON]  section is required but its contents are optional", 
            "title": "Proxy Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#adding-gratia-accounting-and-a-local-monitoring-page-on-a-production-server", 
            "text": "You must report accounting information if you are running more than a few test jobs on the OSG .    Install the GlideinWMS Gratia Probe on each of your submit hosts in your GlideinWMS installation:  root@host #  yum install gratia-probe-glideinwms    Edit the ProbeConfig located in  /etc/gratia/condor/ProbeConfig . First, edit the  SiteName  and  ProbeName  to be a unique identifier for your GlideinWMS Submit host. There can be multiple probes (with different names) per site. If you haven't already, you should register your GlideinWMS submit host in  OIM . Then you can use the name you used to register the resource.  ProbeName = condor: hostname  SiteName = HCC-GlideinWMW-Frontend   Next, turn the probe on by setting  EnableProbe :  EnableProbe = 1     Reconfigure HTCondor:  root@host #  condor_reconfig", 
            "title": "Adding Gratia Accounting and a Local Monitoring Page on a Production Server"
        }, 
        {
            "location": "/other/install-gwms-frontend/#optional-accounting-configuration", 
            "text": "The following sections contain additional configuration that may be required depending on the customizations you've made to your GlideinWMS frontend installation.", 
            "title": "Optional Accounting Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#users-without-certificates", 
            "text": "If you have users that submit jobs without a certificate explicitly declared in the submit file, you will need to add  MapUnknownToGroup  to the ProbeConfig. In the file  /etc/gratia/condor/ProbeConfig , add the value after the  EnableProbe .       ... \n     SuppressGridLocalRecords = 0 \n     EnableProbe = 1 \n     % RED % MapUnknownToGroup = 1 % ENDCOLOR % \n\n     Title3 = Tuning parameter \n     ...   Further, if you want to record all usage as coming from a single VO, you can configure the probe to override the 'guessed' VO. In the below example, replace the  Engage  with a registered VO that you would like to report as. If you don't have a VO that you are affiliated with, you may use Engage.  ... \n     MapUnknownToGroup = 1 \n     MapGroupToRole = 1 \n     VOOverride = Engage  ...", 
            "title": "Users without Certificates"
        }, 
        {
            "location": "/other/install-gwms-frontend/#non-standard-htcondor-install", 
            "text": "If HTCondor is installed in a non-standard location (i.e., not RPMs, or relocated RPM outside  /usr/bin ), then you need to tell the probe where to find the HTCondor binaries. This can be done with a script with a special attribute in  /etc/gratia/condor/ProbeConfig ,  CondorLocation . Point it to the location of the HTCondor install, such that  CondorLocation/bin/condor_version  exists.", 
            "title": "Non-Standard HTCondor Install"
        }, 
        {
            "location": "/other/install-gwms-frontend/#new-data-directory", 
            "text": "If your  PER_JOB_HISTORY_DIR  HTCondor configuration variable is different from the default value, you must update the value of  DataFolder  in  /etc/gratia/condor/ProbeConfig . To check the value of  PER_JOB_HISTORY_DIR  run the following command:  user@host $  condor_config_val PER_JOB_HISTORY_DIR", 
            "title": "New Data Directory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#different-collector-and-other-customizations", 
            "text": "By default the probe reports to the OSG GRACC. To change that you must edit the configuration file,  /etc/gratia/condor/ProbeConfig , and replace the OSG production host with your desired one:  ... \n     CollectorHost = gratia-osg-prod.opensciencegrid.org:80 \n     SSLHost = gratia-osg-prod.opensciencegrid.org:443 \n     SSLRegistrationHost = gratia-osg-prod.opensciencegrid.org:80  ...", 
            "title": "Different collector and other customizations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required\nfor setting up a small site. If you do not need any of the following special\nconfigurations, skip to  the section on service\nactivation/deactivation .   Allow users to specify where their jobs run  Creating a group to test configuration changes", 
            "title": "Optional Configuration"
        }, 
        {
            "location": "/other/install-gwms-frontend/#allowing-users-to-specify-where-their-jobs-run", 
            "text": "In order to allow users to specify the sites at which their jobs want to run (or\nto test a specific site), a Frontend can be configured to match on DESIRED_Sites  or ignore it if not specified. Modify /etc/gwms-frontend/frontend.xml  using the following instructions:    In the Frontend's global  match  stanza, set the  match_expr :  ((job.get( DESIRED_Sites , nosite )== nosite ) or (glidein[ attrs ][ GLIDEIN_Site ] in job.get( DESIRED_Sites , nosite ).split( , )))     In the same  match  stanza, set the  start_expr :  ( DESIRED_Sites =?= undefined   ||   stringListMember ( GLIDEIN_Site , DESIRED_Sites , , ))     Add the  DESIRED_Sites  attribute to the match attributes list:  match_attrs \n    match_attr   name= DESIRED_Sites   type= string /  /match_attrs     Reconfigure the Frontend:  root@host #  /etc/init.d/gwms-frontend reconfig", 
            "title": "Allowing users to specify where their jobs run"
        }, 
        {
            "location": "/other/install-gwms-frontend/#creating-a-group-for-testing-configuration-changes", 
            "text": "To perform configuration changes without impacting production the recommended\nway is to create an ITB group in  /etc/gwms-frontend/frontend.xml . This\ngroupwould only match jobs that have the  +is_itb=True  ClassAd.    Create a  group  named itb.    Set the group's  start_expr  so that the group's glideins will only match user jobs with  +is_itb=True :  match   match_expr= True   start_expr= (is_itb)     Set the  factory_query_expr  so that this group only communicates with ITB factories:  factory   query_expr= FactoryType=?= itb     Set the group's  collector  stanza to reference the ITB factory, replacing  username@gfactory-1.t2.ucsd.edu  with your factory identity:  collector   DN= /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=glidein-itb.grid.iu.edu   \\ \n           factory_identity= gfactory@glidein-itb.grid.iu.edu   \\ \n           my_identity= username@gfactory-1.t2.ucsd.edu   \\ \n           node= glidein-itb.grid.iu.edu /     Set the job  query_expr  so that only ITB jobs appear in  condor_q :  job   query_expr= (!isUndefined(is_itb)   is_itb)     Reconfigure the Frontend (see the  section below ):  #  on EL7 systems systemctl reload gwms-frontend  #  on EL6 systems service gwms-frontend reconfig", 
            "title": "Creating a group for testing configuration changes"
        }, 
        {
            "location": "/other/install-gwms-frontend/#using-glideinwms", 
            "text": "", 
            "title": "Using GlideinWMS"
        }, 
        {
            "location": "/other/install-gwms-frontend/#managing-glideinwms-services", 
            "text": "In addition to the GlideinWMS service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation  for more info    Gratia  gratia-probes-cron  Accounting software    HTCondor  condor     HTTPD  httpd  GlideinWMS monitoring and staging    GlideinWMS  gwms-renew-proxies  (EL6) or  gwms-renew-proxies.timer  (EL7)  Automatic proxy renewal     gwms-frontend  The main GlideinWMS service          Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To...  On EL6, run the command...  On EL7, run the command...      Start a service  service  SERVICE-NAME  start  systemctl start  SERVICE-NAME    Stop a  service  service  SERVICE-NAME  stop  systemctl stop  SERVICE-NAME    Enable a service to start on boot  chkconfig  SERVICE-NAME  on  systemctl enable  SERVICE-NAME    Disable a service from starting on boot  chkconfig  SERVICE-NAME  off  systemctl disable  SERVICE-NAME", 
            "title": "Managing GlideinWMS Services"
        }, 
        {
            "location": "/other/install-gwms-frontend/#reconfiguring-glideinwms", 
            "text": "After changing the configuration of GlideinWMS, use the following table to find the appropriate command for your\noperating system (run as  root ):     If your operating system is...  Run the following command...      Enterprise Linux 7  systemctl reload gwms-frontend    Enterprise Linux 6  service gwms-frontend reconfig", 
            "title": "Reconfiguring GlideinWMS"
        }, 
        {
            "location": "/other/install-gwms-frontend/#upgrading-glideinwms-frontend", 
            "text": "After upgrading the GlideinWMS RPM, you must issue an upgrade command to GlideinWMS:    If you are using Enterprise Linux 7 :    Stop the  condor  and  gwms-frontend  services as specified in  this section    Issue the upgrade command:  root@host #  /usr/sbin/gwms-frontend upgrade    Start the  condor  and  gwms-frontend  services as specified in  this section      If you are using Enterprise Linux 6 :    Upgrade the GlideinWMS Frontend:  root@host #  service gwms-frontend upgrade    Restart the  condor  service as specified in the  managing GlideinWMS services section", 
            "title": "Upgrading GlideinWMS FrontEnd"
        }, 
        {
            "location": "/other/install-gwms-frontend/#validating-glideinwms-frontend", 
            "text": "The complete validation of the Frontend is the submission of actual jobs.\nHowever, there are a few things that can be checked prior to submitting user\njobs to HTCondor.", 
            "title": "Validating GlideinWMS Frontend"
        }, 
        {
            "location": "/other/install-gwms-frontend/#verifying-services-are-running", 
            "text": "There are a few things that can be checked prior to submitting user jobs to HTCondor.    Verify all HTCondor daemons are started.  user @host   $   condor_config_val   - verbose   DAEMON_LIST  DAEMON_LIST :   MASTER ,    COLLECTOR ,   NEGOTIATOR ,    SCHEDD ,   SHARED_PORT ,   SCHEDDJOBS2   COLLECTOR0   COLLECTOR1   COLLECTOR2  COLLECTOR3   COLLECTOR4   COLLECTOR5   COLLECTOR6   COLLECTOR7   COLLECTOR8   COLLECTOR9   COLLECTOR10   ,   COLLECTOR11 ,  COLLECTOR12 ,   COLLECTOR13 ,   COLLECTOR14 ,   COLLECTOR15 ,   COLLECTOR16 ,   COLLECTOR17 ,   COLLECTOR18 ,   COLLECTOR19 ,   COLLECTOR20 ,  COLLECTOR21 ,   COLLECTOR22 ,   COLLECTOR23 ,   COLLECTOR24 ,   COLLECTOR25 ,   COLLECTOR26 ,   COLLECTOR27 ,   COLLECTOR28 ,   COLLECTOR29 ,  COLLECTOR30 ,   COLLECTOR31 ,   COLLECTOR32 ,   COLLECTOR33 ,   COLLECTOR34 ,   COLLECTOR35 ,   COLLECTOR36 ,   COLLECTOR37 ,   COLLECTOR38 ,  COLLECTOR39 ,   COLLECTOR40  Defined   in   /etc/condor/config.d/11_gwms_secondary_collectors.config ,   line   193.   If you don't see all the  collectors and the two schedds , then the configuration must be corrected. There should be no startd daemons listed    Verify all VO Frontend HTCondor services are communicating.  user@host $  condor_status -any MyType               TargetType           Name  glideresource        None                 MM_fermicloud026@gfactory_inst  Scheduler            None                 fermicloud020.fnal.gov  DaemonMaster         None                 fermicloud020.fnal.gov  Negotiator           None                 fermicloud020.fnal.gov  Collector            None                 frontend_service@fermicloud020.fnal.gov  Scheduler            None                 schedd_jobs2@fermicloud020.fnal.gov     To see the details of the glidein resource use  condor_status -subsystem glideresource -l , including the GlideFactoryName.    Verify that the Factory is seeing correctly the Frontend using  condor_status -pool  FACTORY_HOST  -any -constraint 'FrontendName== \" FRONTEND_NAME_FROM_CONFIG \" ' -l , including the GlideFactoryName.", 
            "title": "Verifying Services Are Running"
        }, 
        {
            "location": "/other/install-gwms-frontend/#glideinwms-job-submission", 
            "text": "HTCondor submit file  glidein-job.sub . This is a simple job printing the hostname of the host where the job is running:  # file   glidein - job . sub  universe   =   vanilla  executable   =   / bin / hostname  output   =   glidein / test . out  error   =   glidein / test . err  requirements   =   IS_GLIDEIN   ==   True  log   =   glidein / test . log  ShouldTransferFiles   =   YES  when_to_transfer_output   =   ON_EXIT  queue   To submit the job:  root@host #  condor_submit glidein-job.sub  Then you can control the job like a normal HTCondor job, e.g. to check the status of the job use  condor_q .", 
            "title": "GlideinWMS Job submission"
        }, 
        {
            "location": "/other/install-gwms-frontend/#monitoring-web-pages", 
            "text": "You should be able to see the jobs also in the GlideinWMS monitoring pages that are\nmade available on the Web: http://gwms-frontend-host.domain/vofrontend/monitor/", 
            "title": "Monitoring Web pages"
        }, 
        {
            "location": "/other/install-gwms-frontend/#troubleshooting-glideinwms", 
            "text": "", 
            "title": "Troubleshooting GlideinWMS"
        }, 
        {
            "location": "/other/install-gwms-frontend/#file-locations", 
            "text": "File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    Logs  /var/log/gwms-frontend/    Startup script  /etc/init.d/gwms-frontend (on EL6) - /usr/bin/gwms-frontend (on EL7)    Web Directory  /var/lib/gwms-frontend/web-area    Web Base  /var/lib/gwms-frontend/web-base    Web configuration  /etc/httpd/conf.d/gwms-frontend.conf    Working Directory  /var/lib/gwms-frontend/vofrontend/    Lock files  /var/lib/gwms-frontend/vofrontend/lock/frontend.lock /var/lib/gwms-frontend/vofrontend/group_*/lock/frontend.lock    Status files  /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml      Note  /var/lib/gwms-frontend  is also the home directory of the  frontend  user", 
            "title": "File Locations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-brief", 
            "text": "Here a short list of files to check when you change the certificates. Note that if you renew a proxy or certificate and the DN remains the same no configuration file needs to change, just put the renewed certificate/proxy in place.     File Description  File Location      Configuration file  /etc/gwms-frontend/frontend.xml    HTCondor certificates map  /etc/condor/certs/condor_mapfile (1)    Host certificate and key (2)  /etc/grid-security/hostcert.pem            /etc/grid-security/hostkey.pem    VO Frontend proxy (from host certificate)  /tmp/vofe_proxy (3)    Pilot proxy  /tmp/pilot_proxy (3)       If using HTCondor RPM installation, e.g. the one coming from OSG. If you have separate/multiple HTCondor hosts (schedds, collectors, negotiators, ..) you may have to check this file on all of them to make sure that the HTCondor authentication works correctly.    Used to create the VO Frontend proxy if following the  instructions above    If using the Frontend configuration and scripts described  above in this document . These paths are the ones specified in the configuration file.    Remember also that when you change DN:   The VO Frontend certificate DN must be communicated to the GlideinWMS Factory ( see above )  The pilot proxy must be able to run jobs at the sites you are using, e.g. by being added to the correct VO in OSG (the Factory forwards the proxy and does not care about the DN)", 
            "title": "Certificates brief"
        }, 
        {
            "location": "/other/install-gwms-frontend/#increase-the-log-level-and-change-rotation-policies", 
            "text": "You can increase the log level of the frontend. To add a log file with all the log information add the following line with all the message types in the  process_log  section of  /etc/gwms-frontend/frontend.xml :  log_retention \n    process_logs \n        process_log   extension= all   max_days= 7.0   max_mbytes= 100.0   min_days= 3.0   msg_types= DEBUG,EXCEPTION,INFO,ERROR,ERR /   You can also change the rotation policy and choose whether compress the rotated files, all in the same section of the config files:   max_bytes is the max size of the log files  max_days it will be rotated.  compression specifies if rotated files are compressed  backup_count is the number of rotated log files kept   Further details are in the  reference documentation .", 
            "title": "Increase the log level and change rotation policies"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-reconfig-failing", 
            "text": "If  service gwms-frontend reconfig  fails at the end with an error like \"Writing back config file failed, Reconfiguring the frontend [FAILED]\", make sure that  /etc/gwms-frontend/  belongs to the  frontend  user. It must be able to write to update the configuration file.", 
            "title": "Frontend reconfig failing"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-failing-to-start", 
            "text": "If the startup script of the frontend is failing, check the log file for errors (probably  /var/log/gwms-frontend/frontend/frontend. TODAY .err.log  and  .debug.log ).  If you find errors like  \"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\"  and  \"IOError: [Errno 9] Bad file descriptor\"  you may have an empty status file ( /var/lib/gwms-frontend/vofrontend/monitor/group_*/frontend_status.xml ) that causes GlideinWMS Frontend not to start. The glideinFrontend crashes after a XML parsing exception visible in the log file (\"Exception occurred: ... 'ExpatError: no element found: line 1, column 0\\n']\").  Remove the status file. Then start the frontend. The Frontend will be fixed in future versions to handle this automatically.", 
            "title": "Frontend failing to start"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates-not-there", 
            "text": "The scripts should send an email warning if there are problems and they fail to generate the proxies. Anyway something could go wrong and you want to check manually. If you are using the scripts to generate automatically the proxies but the proxies are not there (in  /tmp  or wherever you expect them):   make sure that the scripts are there and configured with the correct values  make sure that the scripts are executable  make sure that the scripts are in  frontend  's crontab  make sure that the certificates (or master proxy) used to generate the proxies is not expired", 
            "title": "Certificates not there"
        }, 
        {
            "location": "/other/install-gwms-frontend/#failed-authentication", 
            "text": "If you get a failed authentication error (e.g. \"Failed to talk to factory_pool gfactory-1.t2.ucsd.edu...) then:   check that you have the right x509 certificates mentioned in the security section of  /etc/gwms-frontend/frontend.xml  the owner must be  frontend  (user running the frontend)  the permission must be 600  they must be valid for more than one hour (2/300 hours), at least the non VO part    check that the clock is synchronized (see HostTimeSetup)", 
            "title": "Failed authentication"
        }, 
        {
            "location": "/other/install-gwms-frontend/#frontend-doesnt-trust-factory", 
            "text": "If your frontend complains in the debug log:  code   256 :[ Error: communication error \\n ,  AUTHENTICATE:1003:Failed to authenticate with any method \\n ,  AUTHENTICATE:1004:Failed to authenticate using GSI \\n ,  GSI:5006:Failed to authenticate because the subject  /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu  is not currently trusted by you.  If it should be, add it to GSI_DAEMON_NAME in the condor_config, or use the environment variable override (check the manual). \\n ,  GSI:5004:Failed to gss_assist_gridmap /DC=org/DC=doegrids/OU=Services/CN=devg-3.t2.ucsd.edu to a local user.   A possible solution is to comment/remove the LOCAL_CONFIG_DIR in the file  /var/lib/gwms-frontend/vofrontend/frontend.condor_config .", 
            "title": "Frontend doesn't trust Factory"
        }, 
        {
            "location": "/other/install-gwms-frontend/#no-security-credentials-match-for-factory-pool-not-advertising-request", 
            "text": "You may see a warning like \"No security credentials match for factory pool ..., not advertising request\", if the  trust_domain  and  auth_method  of an entry in the Factory configuration is not matching any of the  trust_domain ,  type  couples in the credentials in the Frontend configuration. This causes the Frontend not to use some Factory entries (the ones not matching) and may end up without entries to send glideins to.  To fix the problem make sure that those attributes match as desired.", 
            "title": "No security credentials match for factory pool ..., not advertising request"
        }, 
        {
            "location": "/other/install-gwms-frontend/#jobs-not-running", 
            "text": "If your jobs remain Idle   Check the frontend log files (see above)  Check the HTCondor log files ( condor_config_val LOG  will give you the correct log directory):  Specifically look the CollectorXXXLog files     Common causes of problems could be:   x509 certificates  missing or expired or too short-lived proxy  incorrect ownership or permission on the certificate/proxy file  missing certificates    If the Frontend http server is down in the glidein logs in the Factory there will be errors like \"Failed to load file 'description.dbceCN.cfg' from  http://FRONTEND_HOST/vofrontend/stage .\"  check that the http server is running and you can reach the URL ( http://FRONTEND_HOST/vofrontend/stage/description.dbceCN.cfg )", 
            "title": "Jobs not running"
        }, 
        {
            "location": "/other/install-gwms-frontend/#advanced-configurations", 
            "text": "GlideinWMS Frontend on a Campus Grid", 
            "title": "Advanced Configurations"
        }, 
        {
            "location": "/other/install-gwms-frontend/#getting-help", 
            "text": "To get assistance about the OSG software please use  this page .  For specific questions about the Frontend configuration (and how to add it in your HTCondor infrastructure) you can email the glideinWMS support   To request access the OSG Glidein Factory (e.g. the UCSD factory) you have to send an email to   (see below).", 
            "title": "Getting Help"
        }, 
        {
            "location": "/other/install-gwms-frontend/#references", 
            "text": "Definitions:   What is a  Virtual Organization   Documents about the Glidein-WMS system and the VO frontend:   http://glideinwms.fnal.gov/", 
            "title": "References"
        }, 
        {
            "location": "/other/install-gwms-frontend/#users", 
            "text": "The Glidein WMS Frontend installation will create the following users unless they are already created.     User  Default uid  Comment      apache  48  Runs httpd to provide the monitoring page (installed via dependencies).    condor  none  HTCondor user (installed via dependencies).    frontend  none  This user runs the glideinWMS VO frontend. It also owns the credentials forwarded to the factory to use for the glideins.    gratia  none  Runs the Gratia probes to collect accounting data (optional see  the Gratia section below )      Warning  UID 48 is reserved by RedHat for user  apache .  If it is already taken by a different username, you will experience errors.", 
            "title": "Users"
        }, 
        {
            "location": "/other/install-gwms-frontend/#certificates", 
            "text": "This document has a  proxy configuration section  that\nuses the host certificate/key and a user certificate to generate the required\nproxies.     Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Here  are instructions to request a host certificate.", 
            "title": "Certificates"
        }, 
        {
            "location": "/other/install-gwms-frontend/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTCondor port range  tcp  LOWPORT, HIGHPORT  YES   contiguous range of ports    GlideinWMS Frontend  tcp  9618, 9620 to 9660  YES   HTCondor Collectors for the GlideinWMS Frontend (received ClassAds from resources and jobs)     The VO frontend must have reliable network connectivity, be on the public\ninternet (no NAT), and preferably with no firewalls. Incoming TCP ports 9618 to 9660 must be open.", 
            "title": "Networking"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/", 
            "text": "Install a CVMFS Stratum 1\n\n\nThis document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG Operations Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).\n\n\n\n\nApplicable versions\n\n\nThe applicable software versions for this document are cvmfs and cvmfs-server \n= 2.4.2.\n\n\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points:\n\n\n\n\nUser IDs and Group IDs:\n If your machine is also going to be a repository server like OSG Operations, the installation will create the same user and group IDs as the \ncvmfs client\n.  If you are installing frontier-squid, the installation will also create the same user id as \nfrontier-squid\n.\n\n\nNetwork ports:\n This installation will host the stratum 1 on ports 80 and 8000 and, if squid is installed, it will host the uncached apache on port 8081.\n\n\nHost choice:\n -  Make sure there is adequate disk space for the repositories that will be served, at \n/srv/cvmfs\n. Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at \n/var/cache\n.\n\n\nSELinux\n - Ensure SELinux is disabled\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare the \nrequired Yum repositories\n\n\n\n\nInstalling\n\n\nAll CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install \nfrontier-squid\n and \nfrontier-awstats\n on the same machine to be able to easily join the WLCG \nMRTG\n and \nawstats\n monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups.  Other than that, it is primarily for monitoring.\n\n\nInstalling cvmfs-server and httpd\n\n\nThe OSG Operations Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers.  Choose the appropriate subsection.\n\n\nInstalling a CVMFS stratum 1 that is also a repository server\n\n\nEL6 does not support a CVMFS repository server with the standard kernel, so use EL7.  EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later.\n\n\nroot@host #\n yum -y install cvmfs-server cvmfs mod_wsgi\n\n\n\n\n\nInstalling CVMFS stratum 1 that is not a repository server\n\n\nIf you're not installing for OSG Operations or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command on either EL6 or EL7:\n\n\nroot@host #\n yum -y install cvmfs-server cvmfs-config mod_wsgi\n\n\n\n\n\nInstalling frontier-squid and frontier-awstats\n\n\nfrontier-awstats\n is not distributed by OSG so these instructions get it from its original source.  Do these commands to install frontier-squid and frontier-awstats:\n\n\nroot@host #\n rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm\n\nroot@host #\n yum -y install frontier-awstats\n\n\n\n\n\nConfiguring\n\n\nConfiguring the system\n\n\nIncrease the default number of open file descriptors:\n\n\nroot@host #\n \necho\n -e \n*\\t\\t-\\tnofile\\t\\t16384\n \n/etc/security/limits.conf \n\nroot@host #\n \nulimit\n -n \n16384\n\n\n\n\n\n\nIn order for this to apply also interactively when logging in over ssh, the option \nUsePAM\n has to be set to \nyes\n in \n/etc/ssh/sshd_config\n.\n\n\nConfiguring cron\n\n\nFirst, create the log directory: \n\n\nroot@host #\n mkdir -p /var/log/cvmfs\n\n\n\n\n\nPut the following in \n/etc/cron.d/cvmfs\n:\n\n\n*/\n5\n \n*\n \n*\n \n*\n \n*\n \nroot\n \ntest\n \n-\nd\n \n/\nsrv\n/\ncvmfs\n \n||\n \nexit\n;cvmfs_server snapshot -ai \n\n\n6\n \n1\n \n*\n \n*\n \n*\n \nroot\n \ncvmfs_server\n \ngc\n \n-\naf\n \n2\n/\ndev\n/\nnull\n \n||\n \ntrue\n\n\n0\n \n9\n \n*\n \n*\n \n*\n \nroot\n \nfind\n \n/\nsrv\n/\ncvmfs\n/*.*/\ndata\n/\ntxn\n \n-\nname\n \n*.*\n \n-\nmtime\n \n+\n2\n \n2\n/\ndev\n/\nnull\n|\nxargs\n \nrm\n \n-\nf\n\n\n\n\n\n\nAlso, put the following in \n/etc/logrotate.d/cvmfs\n:\n\n\n/\nvar\n/\nlog\n/\ncvmfs\n/*.log {\n\n\n    weekly\n\n\n    missingok\n\n\n    notifempty\n\n\n}\n\n\n\n\n\n\nConfiguring apache\n\n\nIf you are installing frontier-squid, create \n/etc/httpd/conf.d/cvmfs.conf\n and put the following lines into it:\n\n\nListen\n \n8081\n \nKeepAlive\n \nOn\n\n\n\n\n\n\nIf you are not installing frontier-squid, instead put the following lines into that file:\n\n\nListen\n \n8000\n \nKeepAlive\n \nOn\n\n\n\n\n\n\nIf you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each \n/etc/httpd/conf.d/cvmfs.\nrepositoryname\n.conf\n that adding a replica creates (this is included in the \nadd_osg_repository script\n), and instead add the following to \n/etc/httpd/conf.d/cvmfs.conf\n:\n\n\nRewriteEngine\n \nOn\n \n\nRewriteRule\n \n^/\ncvmfs\n/\n(\n[\n^\n.\n/\n]\n*\n)\n/\n(\n.\n*\n)\n$ \n/\ncvmfs\n/\n$1\n.\nopensciencegrid\n.\norg\n/\n$2\n \n\nRewriteRule\n \n^/\ncvmfs\n/\n(\n[\n^/\n]\n+\n)\n/\napi\n/\n(\n.\n*\n)\n$ \n/\nvar\n/\nwww\n/\nwsgi\n-\nscripts\n/\ncvmfs\n-\nserver\n/\ncvmfs\n-\napi\n.\nwsgi\n/\n$1\n/\n$2\n\n\nRewriteRule\n \n^/\ncvmfs\n/\n(\n.\n*\n)\n$ \n/\nsrv\n/\ncvmfs\n/\n$1\n \n\nDirectory\n \n/srv/cvmfs\n \n  \nOptions\n \n-\nMultiViews\n \n+\nFollowSymLinks\n \n-\nIndexes\n \n  \nAllowOverride\n \nAll\n \n  \nRequire\n \nall\n \ngranted\n\n\n  \nEnableMMAP\n \nOff\n \nEnableSendFile\n \nOff\n\n\n  \nFilesMatch\n \n^\\.cvmfs\n\n    \nForceType\n \napplication\n/\nx\n-\ncvmfs\n\n  \n/\nFilesMatch\n\n\n  \nHeader\n \nunset\n \nLast\n-\nModified\n \n  \nRequestHeader\n \nunset\n \nIf\n-\nModified\n-\nSince\n\n  \nFileETag\n \nNone\n\n\n  \nExpiresActive\n \nOn\n \n  \nExpiresDefault\n \naccess plus 3 days\n \n  \nExpiresByType\n \ntext\n/\nhtml\n \naccess plus 15 minutes\n \n  \nExpiresByType\n \napplication\n/\nx\n-\ncvmfs\n \naccess plus 61 seconds\n \n  \nExpiresByType\n \napplication\n/\njson\n \naccess plus 61 seconds\n \n\n/\nDirectory\n\n\n\nWSGIDaemonProcess\n \ncvmfs\n-\napi\n \nthreads\n=\n64\n \ndisplay\n-\nname\n=%\n{\nGROUP\n} \\\n    \npython\n-\npath\n=/\nusr\n/\nshare\n/\ncvmfs\n-\nserver\n/\nwebapi\n\n\nDirectory\n \n/\nvar\n/\nwww\n/\nwsgi\n-\nscripts\n/\ncvmfs\n-\nserver\n\n  \nWSGIProcessGroup\n \ncvmfs\n-\napi\n\n  \nWSGIApplicationGroup\n \ncvmfs\n-\napi\n\n  \nOptions\n \nExecCGI\n\n  \nSetHandler\n \nwsgi\n-\nscript\n\n  \nRequire\n \nall\n \ngranted\n\n\n/\nDirectory\n\n\nWSGISocketPrefix\n \n/\nvar\n/\nrun\n/\nwsgi\n \n\n\n\n\n\n\n\nNote\n\n\nOn EL6-based systems (Apache httpd 2.2) replace both instances of \nRequire all granted\n above with the following:\n\n\nOrder\n \nallow\n,\n \ndeny\n\n\nAllow\n \nfrom\n \nall\n\n\n\n\n\n\n\n\nIf you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.\n\n\nThen enable apache.  On EL6 do\n\n\nroot@host #\n chkconfig httpd on \n\nroot@host #\n service httpd start\n\n\n\n\n\nor on EL7 do\n\n\nroot@host #\n systemctl \nenable\n httpd\n\nroot@host #\n systemctl start httpd\n\n\n\n\n\nConfiguring frontier-squid\n\n\nPut the following in \n/etc/squid/customize.sh\n after the existing comment header:\n\n\nawk\n \n--\nfile\n \n`\ndirname\n \n$\n0\n`\n/\ncustomhelps\n.\nawk\n \n--\nsource\n \n{\n\n\n\n# cache only api calls \n\n\ninsertline(\n^http_access deny all\n, \nacl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/\n)\n\n\ninsertline(\n^http_access deny all\n, \ncache deny !CVMFSAPI\n)\n\n\n\n# port 80 is also supported, through an iptables redirect \n\n\nsetoption(\nhttp_port\n, \n8000 accel defaultsite=localhost:8081 no-vhost\n)\n\n\nsetoption(\ncache_peer\n, \nlocalhost parent 8081 0 no-query originserver\n)\n\n\n\n# allow incoming http accesses from anywhere\n\n\n# all requests will be forwarded to the originserver \n\n\ncommentout(\nhttp_access allow NET_LOCAL\n)\n\n\ninsertline(\n^http_access deny all\n, \nhttp_access allow all\n)\n\n\n\n# do not let squid cache DNS entries more than 5 minutes \n\n\nsetoption(\npositive_dns_ttl\n, \n5 minutes\n)\n\n\n\n# set shutdown_lifetime to 0 to avoid giving new connections error\n\n\n# codes, which get cached upstream \n\n\nsetoption(\nshutdown_lifetime\n, \n0 seconds\n)\n\n\n\n# turn off collapsed_forwarding to prevent slow clients from slowing down\n\n\n# faster ones\n\n\nsetoption(\ncollapsed_forwarding\n, \noff\n)\n\n\n\nprint\n\n\n}\n\n\n\n\n\n\nOn an EL7 system, make sure that iptables-services is installed and enabled:\n\n\nroot@host #\n yum -y install iptables-services \n\nroot@host #\n systemctl \nenable\n iptables\n\n\n\n\n\nForward port 80 to port 8000:\n\n\nroot@host #\n iptables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n \n\nroot@host #\n service iptables save\n\n\n\n\n\nOn EL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on EL6):\n\n\nroot@host #\n ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport \n80\n -j REDIRECT --to-ports \n8000\n\n\nroot@host #\n service ip6tables save\n\n\n\n\n\nEnable frontier-squid.  On EL6 do:\n\n\nroot@host #\n chkconfig frontier-squid on\n\nroot@host #\n service frontier-squid start\n\n\n\n\n\nor on EL7 do:\n\n\nroot@host #\n systemctl \nenable\n frontier-squid\n\nroot@host #\n systemctl start frontier-squid\n\n\n\n\n\n\n\nNote\n\n\nThe above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the \ninstructions for running multiple squid workers\n.\n\n\n\n\nVerifying\n\n\nIn order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.\n\n\nAdding an example repository\n\n\nThe OSG Operations Stratum 1 should add a repository replica using the \nadd_osg_repository\n script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub \n\nroot@host #\n add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu\n\n\n\n\n\nIt's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub\n\n\n\n\n\nHowever, non-OSG Operations Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG Operations machine with this as their first command:\n\n\nroot@host #\n cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub\n\n\n\n\n\nThe second command for Stratum 1s that have the httpd configuration as described above in the \nConfiguring apache section\n is this:\n\n\nroot@host #\n rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf\n\n\n\n\n\nThen the next command is this:\n\n\nroot@host #\n cvmfs_server snapshot config-egi.egi.eu\n\n\n\n\n\nWith large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.\n\n\nVerifying that the replica is being served\n\n\nNow to verify that the replication is working, do the following commands:\n\n\nroot@host #\n wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished \n|\n cat -v\n\nroot@host #\n wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished \n|\n cat -v\n\n\n\n\n\nBoth commands should show a short file including gibberish at the end which is the signature.\n\n\nIt is a good idea to familiarize yourself with the log entries at \n/var/log/httpd/access_log\n and also, if you have installed frontier-squid, at \n/var/log/squid/access.log\n. Also, at least 15 minutes after the snapshot is finished, check the log \n/var/log/cvmfs/snapshots.log\n to see that it tried to get an update and got no errors.\n\n\nSetting up monitoring\n\n\nIf you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.\n\n\nFirst, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in \nthe frontier-squid instructions\n. Next, choose any random password and put it in \n/etc/awstats/password-file\n. Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#install-a-cvmfs-stratum-1", 
            "text": "This document describes how to install a CVMFS Stratum 1. There are many different variations on how to do that, but this document focuses on the configuration of the OSG Operations Stratum 1 oasis-replica.opensciencegrid.org. It is applicable to other Stratum 1s as well, very likely with modifications (some of which are suggested in the document below).   Applicable versions  The applicable software versions for this document are cvmfs and cvmfs-server  = 2.4.2.", 
            "title": "Install a CVMFS Stratum 1"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#before-starting", 
            "text": "Before starting the installation process, consider the following points:   User IDs and Group IDs:  If your machine is also going to be a repository server like OSG Operations, the installation will create the same user and group IDs as the  cvmfs client .  If you are installing frontier-squid, the installation will also create the same user id as  frontier-squid .  Network ports:  This installation will host the stratum 1 on ports 80 and 8000 and, if squid is installed, it will host the uncached apache on port 8081.  Host choice:  -  Make sure there is adequate disk space for the repositories that will be served, at  /srv/cvmfs . Do not use xfs as the filesystem type on operating systems older than EL7, because it has been demonstrated to perform poorly for CVMFS repositories; instead use ext3 or ext4. About 10GB should be reserved for apache and squid logs under /var/log on a production server, although they normally will not get that large. A Stratum 1 that is also a repository server should have at least 5GB available at  /var/cache .  SELinux  - Ensure SELinux is disabled   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the host has  a supported operating system  Obtain root access to the host  Prepare the  required Yum repositories", 
            "title": "Before Starting"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing", 
            "text": "All CVMFS Stratum 1s require cvmfs-server software and apache (httpd). It is highly recommended to also install  frontier-squid  and  frontier-awstats  on the same machine to be able to easily join the WLCG  MRTG  and  awstats  monitoring systems. The recommended configuration for frontier-squid below only caches geo api lookups.  Other than that, it is primarily for monitoring.", 
            "title": "Installing"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-server-and-httpd", 
            "text": "The OSG Operations Stratum 1 has to function as a repository server in addition to serving repository replications; most Stratum 1s serve only replications. Instructions are also provided for how to install cvmfs-server on Stratum 1s that do not have to be repository servers.  Choose the appropriate subsection.", 
            "title": "Installing cvmfs-server and httpd"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-a-cvmfs-stratum-1-that-is-also-a-repository-server", 
            "text": "EL6 does not support a CVMFS repository server with the standard kernel, so use EL7.  EL7.2 cannot be reliably used as a repository server, because of bugs in the union filesystem OverlayFS. The bugs are fixed in EL7.3, so use EL7.3 or later.  root@host #  yum -y install cvmfs-server cvmfs mod_wsgi", 
            "title": "Installing a CVMFS stratum 1 that is also a repository server"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-cvmfs-stratum-1-that-is-not-a-repository-server", 
            "text": "If you're not installing for OSG Operations or otherwise want to support serving repositories on the same machine as a Stratum 1, use this command on either EL6 or EL7:  root@host #  yum -y install cvmfs-server cvmfs-config mod_wsgi", 
            "title": "Installing CVMFS stratum 1 that is not a repository server"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#installing-frontier-squid-and-frontier-awstats", 
            "text": "frontier-awstats  is not distributed by OSG so these instructions get it from its original source.  Do these commands to install frontier-squid and frontier-awstats:  root@host #  rpm -i http://frontier.cern.ch/dist/rpms/RPMS/noarch/frontier-release-1.1-1.noarch.rpm root@host #  yum -y install frontier-awstats", 
            "title": "Installing frontier-squid and frontier-awstats"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring", 
            "text": "", 
            "title": "Configuring"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-the-system", 
            "text": "Increase the default number of open file descriptors:  root@host #   echo  -e  *\\t\\t-\\tnofile\\t\\t16384   /etc/security/limits.conf  root@host #   ulimit  -n  16384   In order for this to apply also interactively when logging in over ssh, the option  UsePAM  has to be set to  yes  in  /etc/ssh/sshd_config .", 
            "title": "Configuring the system"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-cron", 
            "text": "First, create the log directory:   root@host #  mkdir -p /var/log/cvmfs  Put the following in  /etc/cron.d/cvmfs :  */ 5   *   *   *   *   root   test   - d   / srv / cvmfs   ||   exit ;cvmfs_server snapshot -ai   6   1   *   *   *   root   cvmfs_server   gc   - af   2 / dev / null   ||   true  0   9   *   *   *   root   find   / srv / cvmfs /*.*/ data / txn   - name   *.*   - mtime   + 2   2 / dev / null | xargs   rm   - f   Also, put the following in  /etc/logrotate.d/cvmfs :  / var / log / cvmfs /*.log {      weekly      missingok      notifempty  }", 
            "title": "Configuring cron"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-apache", 
            "text": "If you are installing frontier-squid, create  /etc/httpd/conf.d/cvmfs.conf  and put the following lines into it:  Listen   8081   KeepAlive   On   If you are not installing frontier-squid, instead put the following lines into that file:  Listen   8000   KeepAlive   On   If you will be serving opensciencegrid.org repositories, you have to allow for old client configurations that access repositories without the domain name added. For that reason, you will need to remove each  /etc/httpd/conf.d/cvmfs. repositoryname .conf  that adding a replica creates (this is included in the  add_osg_repository script ), and instead add the following to  /etc/httpd/conf.d/cvmfs.conf :  RewriteEngine   On   RewriteRule   ^/ cvmfs / ( [ ^ . / ] * ) / ( . * ) $  / cvmfs / $1 . opensciencegrid . org / $2   RewriteRule   ^/ cvmfs / ( [ ^/ ] + ) / api / ( . * ) $  / var / www / wsgi - scripts / cvmfs - server / cvmfs - api . wsgi / $1 / $2  RewriteRule   ^/ cvmfs / ( . * ) $  / srv / cvmfs / $1   Directory   /srv/cvmfs  \n   Options   - MultiViews   + FollowSymLinks   - Indexes  \n   AllowOverride   All  \n   Require   all   granted \n\n   EnableMMAP   Off   EnableSendFile   Off \n\n   FilesMatch   ^\\.cvmfs \n     ForceType   application / x - cvmfs \n   / FilesMatch \n\n   Header   unset   Last - Modified  \n   RequestHeader   unset   If - Modified - Since \n   FileETag   None \n\n   ExpiresActive   On  \n   ExpiresDefault   access plus 3 days  \n   ExpiresByType   text / html   access plus 15 minutes  \n   ExpiresByType   application / x - cvmfs   access plus 61 seconds  \n   ExpiresByType   application / json   access plus 61 seconds   / Directory  WSGIDaemonProcess   cvmfs - api   threads = 64   display - name =% { GROUP } \\\n     python - path =/ usr / share / cvmfs - server / webapi  Directory   / var / www / wsgi - scripts / cvmfs - server \n   WSGIProcessGroup   cvmfs - api \n   WSGIApplicationGroup   cvmfs - api \n   Options   ExecCGI \n   SetHandler   wsgi - script \n   Require   all   granted  / Directory  WSGISocketPrefix   / var / run / wsgi     Note  On EL6-based systems (Apache httpd 2.2) replace both instances of  Require all granted  above with the following:  Order   allow ,   deny  Allow   from   all    If you will be serving cern.ch repositories, it has the same problem; replace opensciencegrid.org above with cern.ch. If you need to serve both opensciencegrid.org and cern.ch contact Dave Dykstra to discuss the options.  Then enable apache.  On EL6 do  root@host #  chkconfig httpd on  root@host #  service httpd start  or on EL7 do  root@host #  systemctl  enable  httpd root@host #  systemctl start httpd", 
            "title": "Configuring apache"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#configuring-frontier-squid", 
            "text": "Put the following in  /etc/squid/customize.sh  after the existing comment header:  awk   -- file   ` dirname   $ 0 ` / customhelps . awk   -- source   {  # cache only api calls   insertline( ^http_access deny all ,  acl CVMFSAPI urlpath_regex ^/cvmfs/[^/]*/api/ )  insertline( ^http_access deny all ,  cache deny !CVMFSAPI )  # port 80 is also supported, through an iptables redirect   setoption( http_port ,  8000 accel defaultsite=localhost:8081 no-vhost )  setoption( cache_peer ,  localhost parent 8081 0 no-query originserver )  # allow incoming http accesses from anywhere  # all requests will be forwarded to the originserver   commentout( http_access allow NET_LOCAL )  insertline( ^http_access deny all ,  http_access allow all )  # do not let squid cache DNS entries more than 5 minutes   setoption( positive_dns_ttl ,  5 minutes )  # set shutdown_lifetime to 0 to avoid giving new connections error  # codes, which get cached upstream   setoption( shutdown_lifetime ,  0 seconds )  # turn off collapsed_forwarding to prevent slow clients from slowing down  # faster ones  setoption( collapsed_forwarding ,  off )  print  }   On an EL7 system, make sure that iptables-services is installed and enabled:  root@host #  yum -y install iptables-services  root@host #  systemctl  enable  iptables  Forward port 80 to port 8000:  root@host #  iptables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000   root@host #  service iptables save  On EL7 also set up the the same port forwarding for IPv6 (unfortunately it is not supported on EL6):  root@host #  ip6tables -t nat -A PREROUTING -p tcp -m tcp --dport  80  -j REDIRECT --to-ports  8000  root@host #  service ip6tables save  Enable frontier-squid.  On EL6 do:  root@host #  chkconfig frontier-squid on root@host #  service frontier-squid start  or on EL7 do:  root@host #  systemctl  enable  frontier-squid root@host #  systemctl start frontier-squid   Note  The above configuration is for a single squid thread, which is fine for 1Gbit/s and possibly 2Gbit/s, but if higher bandwidth is needed, see the  instructions for running multiple squid workers .", 
            "title": "Configuring frontier-squid"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying", 
            "text": "In order to verify that everything is installed correctly, create a repository replica. The repository chosen for the instructions below is one from egi.eu because it is very small, but you can use another one if you prefer.", 
            "title": "Verifying"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#adding-an-example-repository", 
            "text": "The OSG Operations Stratum 1 should add a repository replica using the  add_osg_repository  script from the oasis-goc rpm. Instructions for installing that are elsewhere. That script assumes that the oasis.opensciencegrid.org replica repository was first created, so this instruction creates it but does not download the first snapshot because that would take a lot of space and time. Use these commands to create the oasis replica and to create and download the example replica:  root@host #  cvmfs_server add-replica -o root http://oasis.opensciencegrid.org:8000/cvmfs/oasis.opensciencegrid.org /etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  root@host #  add_osg_repository http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu  It's a good idea for other Stratum 1s to make their own scripts for adding repository replicas, because there's always two or three commands to run, and it's easy to forget the commands after the first one. The first command is this:  root@host #  cvmfs_server add-replica -o root http://cvmfs-stratum0.gridpp.rl.ac.uk:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub  However, non-OSG Operations Stratum 1s (that is, at BNL and FNAL), for the sake of fulfilling an OSG security requirement, need to instead read from the OSG Operations machine with this as their first command:  root@host #  cvmfs_server add-replica -o root http://oasis-replica.opensciencegrid.org:8000/cvmfs/config-egi.egi.eu /etc/cvmfs/keys/egi.eu/egi.eu.pub:/etc/cvmfs/keys/opensciencegrid.org/opensciencegrid.org.pub  The second command for Stratum 1s that have the httpd configuration as described above in the  Configuring apache section  is this:  root@host #  rm -f /etc/httpd/conf.d/cvmfs.config-egi.egi.eu.conf  Then the next command is this:  root@host #  cvmfs_server snapshot config-egi.egi.eu  With large repositories that can take a very long time, but with small repositories it should be very quick and not show any errors.", 
            "title": "Adding an example repository"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#verifying-that-the-replica-is-being-served", 
            "text": "Now to verify that the replication is working, do the following commands:  root@host #  wget -qdO- http://localhost:8000/cvmfs/config-egi.egi.eu/.cvmfspublished  |  cat -v root@host #  wget -qdO- http://localhost:80/cvmfs/config-egi.egi.eu/.cvmfspublished  |  cat -v  Both commands should show a short file including gibberish at the end which is the signature.  It is a good idea to familiarize yourself with the log entries at  /var/log/httpd/access_log  and also, if you have installed frontier-squid, at  /var/log/squid/access.log . Also, at least 15 minutes after the snapshot is finished, check the log  /var/log/cvmfs/snapshots.log  to see that it tried to get an update and got no errors.", 
            "title": "Verifying that the replica is being served"
        }, 
        {
            "location": "/other/install-cvmfs-stratum1/#setting-up-monitoring", 
            "text": "If you installed frontier-squid and frontier-awstats, there is a little more to do to configure monitoring.  First, make sure that your firewall accepts UDP queries from the monitoring server at CERN. Details are in  the frontier-squid instructions . Next, choose any random password and put it in  /etc/awstats/password-file . Then tell Dave Dykstra the fully qualified domain name of your machine and the password you chose, and he'll set up the monitoring servers.", 
            "title": "Setting up monitoring"
        }, 
        {
            "location": "/other/network-performance-toolkit/", 
            "text": "Network Performance Toolkit\n\n\nThis document is for System Administrators and advanced Grid Users. It describes the usage of tools provided by the Open Science Grid to evaluate the network performance between resources.\n\n\nIntroduction\n\n\nThe Network Performance Toolkit is a collection of applications provided by the \nperfSONAR project\n and distributed by the \nOpen Science Grid\n. The \nserver\n components of the Network Performance Toolkit have been installed on dedicated resources of the \nOpen Science Grid\n. Following \nclient\n tools are described in this document:\n\n\n\n\nNetwork Diagnostic Tool (NDT)\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nBandwidth Control tool (BWCTL)\n\n\nNetwork Path and Application Diagnosis (NPAD)\n\n\n\n\nInstallation\n\n\nClient Site Installation\n\n\n\n\nWarning\n\n\nNote that BWCTL is \ndeprecated\n\n\n\n\nThe Network Performance Toolkit is installed with the OSG Client. Specifically, the tools included are: BWCTL, NDT and OWAMP (bwctl-client, bwctl-server, bwctl, ndt, owamp-client). NPAD is currently not in OSG client.\n\n\nIf you just want to install the OSG command line clients you can do the following::\n\n\nyum install bwctl-client\n\n\nyum install owamp-client\n\n\nyum install ndt-client\n\n\nyum install npad-client\n\n\n\n\n\n\nYou may install these utilities separately as RPM using yum by following the \nperfSONAR\n instructions. The packages are in the OSG repository, some of them with a separate client or server version, available for the OSG supported platforms:\n\n\n\n\nNDT: ndt\n\n\nOWAMP: owamp, owamp-client, owamp-server\n\n\nBWCTL: bwctl, bwctl-client, bwctl-server\n\n\nNPAD: npad\n\n\n\n\nServer Site Installation\n\n\nThe \nperfSONAR\n-based tools and services support the following tasks for OSG VO's:\n\n\n\n\nmonitor site-to-site network paths and ensure that these paths remain operational\n\n\ntroubleshoot performance problems quickly and efficiently\n\n\n\n\nThe \nserver site components\n can be brought-up \non demand\n using the netinstall provided by \nperfSONAR project downloads\n. Source packages are provided on the \nperfSONAR home page\n.\n\n\nOnce the Toolkit server has booted you may begin on-demand testing. The server tools will use a generic set of configuration files. The intent is to make it easy to stand up a temporary server when and where it is needed. However, it is expected that a permanently installed server will be customized/configured, allowing it to support both on-demand testing and regularly scheduled monitoring. See the \nperfSONAR home page\n for step-by-step instructions on how to complete this customization process.\n\n\nFinding Target Servers\n\n\nFinding servers against which to run on-demand tests can be a major impediment to effectively using these tools. The \nperfSONAR\n project tackles this problem by running a registration service for participating tools. The Performance Node ISO automatically uses this Lookup Service to advertise the tools' existence. You can also create custom views by making web-service calls to retrieve the data of interest. \n\n\nWe also have requested ALL OSG sites register their perfSONAR Toolkit installations in \nOIM\n. Using this list you can select a \"closest\" relevant instance to use for running on-demand tests. Alternately if you have a perfSONAR toolkit install, the web interface has a \"Global Services\" link you can visit to see ALL perfSONAR instances that have updated the perfSONAR lookup service.\n\n\nUsing the Client Tools\n\n\nThese tools support delay measurements (OWAMP), throughput measurements (BWCTL), and advanced diagnostics (NDT and NPAD). The command syntax for each tool is described in the following sub-sections. Each of the client tools listed above communicates with a companion server process to perform a measurement/test.\n\n\nNetwork Diagnostic Tool (NDT)\n\n\nThe Network Diagnostic Tool (NDT) runs a series of short tests to determine what the current performance is and what, if anything, is limiting that performance. It can distinguish between host configuration and network infrastructure problems. To diagnose the CE/SE configuration and network connection run the \nweb100clt\n command:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nMore details can be obtained by using the \n-l\n command line option to \nweb100ctl\n:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n -l\n\n\n\n\n\nTo increase the output further use:\n\n\n[user@client /opt/npt]$\n web100clt \u2013n \nTarget Server \nfor\n Measurement\n -ll\n\n\n\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nThe One Way Active Measurement Protocol (OWAMP) is an advanced version of the common \nping\n program. The OWAMP client \nowping\n communicates with an OWAMP server and measures the delay in each direction using NTP based time stamps. OWAMP can be used to identify delay, loss, and packet reordering problems inside the network. To measure the delay between the CE/SE and the remote server use the \nowping\n command:\n\n\n[user@client /opt/npt]$\n owping \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nBandwidth Control tool (BWCTL)\n\n\nThe Bandwidth Control tool (BWCTL) is a wrapper for the \niperf\n command, its policy and a daemon. BWCTL improves the usability of \niperf\n by avoiding following problems:\n\n\n\n\nneed for remote access to the target host used for measurement\n\n\nsecurity concerns about leaving an \niperf\n daemon running on the target host\n\n\n\n\nBWCTL supports testing in either direction, or between 2 remote BWCTL servers from a third location. To measure the current throughput from your SE/CE to the remote server use the \nbwctl\n command:\n\n\n[user@client /opt/npt]$\n bwctl \u2013s \nTarget Server \nfor\n Measurement\n\n\n\n\n\n\nThird party tests, between 2 remote BWCTL servers, will let you measure various sections of the end-2-end path, running\n\n\n[user@client /opt/npt]$\n bwctl \u2013c \n1st Server \nfor\n Measurement\n \u2013s \n2nd Server \nfor\n Measurement\n\n\n\n\n\n\nOther useful options are \n-f\n, format, \n-t\n, length of test, and \n-i\n, test interval.\n\n\nNetwork Path and Application Diagnosis (NPAD)\n\n\nNOTE: Network Path and Application Diagnosis (NPAD) is deprecated and won't be in future versions of the OSG distribution\n\n\nThe Network Path and Application Diagnosis (NPAD) tool examines a host and its local network infrastructure to determine what problems, if any, would hinder wide area performance. Issues such as small TCP buffers in switches and routers are detected as well as common host configuration errors. To determine if the CE/SE will achieve maximum performance over a WAN path run the command \ndiag-client\n:\n\n\n[user@client /opt/npt]$\n diag-client \nTarget Server \nfor\n Measurement\n \n8001\n \n10\n \n50\n\n\n\n\n\n\n\n\nNote\n\n\nThe last 2 numeric parameters, after host and port, are the rtt time (in ms) and speed/rate values (in Mbps) you need to achieve. The reason it works this way is that its meant to 'test' local infrastructure. The idea is that if you were testing to an NPAD server that was 5ms away on a 1G network, you would get close to that speed even with network flaws. If you were to supply 80ms and 1G to the server and there truly was a flaw, the NPAD test would tell you it wasn't possible, thus enabling you to fix the problem\n\n\n\n\n\n\nNote\n\n\nThe \ndiag-client\n commands return a partial URL, enabling easy sharing of results between users and site administrators. To view the results, prepend the Toolkit servers name/port to the returned string. The example above would result in this URL: \nhttp://server.this.osg.domain:8002/ServerData/Reports-2011-07/vtbv-ce.uchicago.edu:2011-07-12-18:50:07.html\n.\n\n\n\n\nAdvanced Topic: Scheduled Monitoring\n\n\n(See \nhttp://docs.perfsonar.net/install_quick_start.html\n for more details.)\n\n\nIn addition to the above on-demand tests, the Performance Toolkit server can be configured to continuously monitor the throughput or delay between your site and peer sites of interest. To begin this monitoring, enter the GUI and ensure that your server is a member of the community or communities of interest. Once that is complete, continue on by selecting either the \nperfSONAR-BUOY\n throughput or delay configuration menu item.\n\n\npSB-throughput: This utility will run regularly scheduled BWCTL tests between your Toolkit server and the selected peer servers. Results are stored in a database and displayed on the server's web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views.\n\n\npSB-delay: This utility will run regularly scheduled OWAMP tests between your Toolkit server and the selected peers. Results are stored in a database and displayed on the server\u2019s web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views as required.\n\n\nReferences\n\n\n\n\nOne Way Active Measurement Protocol (OWAMP)\n\n\nOSG/WLCG pages on perfSONAR\n.", 
            "title": "Install the Network Performance Toolkit"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-performance-toolkit", 
            "text": "This document is for System Administrators and advanced Grid Users. It describes the usage of tools provided by the Open Science Grid to evaluate the network performance between resources.", 
            "title": "Network Performance Toolkit"
        }, 
        {
            "location": "/other/network-performance-toolkit/#introduction", 
            "text": "The Network Performance Toolkit is a collection of applications provided by the  perfSONAR project  and distributed by the  Open Science Grid . The  server  components of the Network Performance Toolkit have been installed on dedicated resources of the  Open Science Grid . Following  client  tools are described in this document:   Network Diagnostic Tool (NDT)  One Way Active Measurement Protocol (OWAMP)  Bandwidth Control tool (BWCTL)  Network Path and Application Diagnosis (NPAD)", 
            "title": "Introduction"
        }, 
        {
            "location": "/other/network-performance-toolkit/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#client-site-installation", 
            "text": "Warning  Note that BWCTL is  deprecated   The Network Performance Toolkit is installed with the OSG Client. Specifically, the tools included are: BWCTL, NDT and OWAMP (bwctl-client, bwctl-server, bwctl, ndt, owamp-client). NPAD is currently not in OSG client.  If you just want to install the OSG command line clients you can do the following::  yum install bwctl-client  yum install owamp-client  yum install ndt-client  yum install npad-client   You may install these utilities separately as RPM using yum by following the  perfSONAR  instructions. The packages are in the OSG repository, some of them with a separate client or server version, available for the OSG supported platforms:   NDT: ndt  OWAMP: owamp, owamp-client, owamp-server  BWCTL: bwctl, bwctl-client, bwctl-server  NPAD: npad", 
            "title": "Client Site Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#server-site-installation", 
            "text": "The  perfSONAR -based tools and services support the following tasks for OSG VO's:   monitor site-to-site network paths and ensure that these paths remain operational  troubleshoot performance problems quickly and efficiently   The  server site components  can be brought-up  on demand  using the netinstall provided by  perfSONAR project downloads . Source packages are provided on the  perfSONAR home page .  Once the Toolkit server has booted you may begin on-demand testing. The server tools will use a generic set of configuration files. The intent is to make it easy to stand up a temporary server when and where it is needed. However, it is expected that a permanently installed server will be customized/configured, allowing it to support both on-demand testing and regularly scheduled monitoring. See the  perfSONAR home page  for step-by-step instructions on how to complete this customization process.", 
            "title": "Server Site Installation"
        }, 
        {
            "location": "/other/network-performance-toolkit/#finding-target-servers", 
            "text": "Finding servers against which to run on-demand tests can be a major impediment to effectively using these tools. The  perfSONAR  project tackles this problem by running a registration service for participating tools. The Performance Node ISO automatically uses this Lookup Service to advertise the tools' existence. You can also create custom views by making web-service calls to retrieve the data of interest.   We also have requested ALL OSG sites register their perfSONAR Toolkit installations in  OIM . Using this list you can select a \"closest\" relevant instance to use for running on-demand tests. Alternately if you have a perfSONAR toolkit install, the web interface has a \"Global Services\" link you can visit to see ALL perfSONAR instances that have updated the perfSONAR lookup service.", 
            "title": "Finding Target Servers"
        }, 
        {
            "location": "/other/network-performance-toolkit/#using-the-client-tools", 
            "text": "These tools support delay measurements (OWAMP), throughput measurements (BWCTL), and advanced diagnostics (NDT and NPAD). The command syntax for each tool is described in the following sub-sections. Each of the client tools listed above communicates with a companion server process to perform a measurement/test.", 
            "title": "Using the Client Tools"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-diagnostic-tool-ndt", 
            "text": "The Network Diagnostic Tool (NDT) runs a series of short tests to determine what the current performance is and what, if anything, is limiting that performance. It can distinguish between host configuration and network infrastructure problems. To diagnose the CE/SE configuration and network connection run the  web100clt  command:  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement   More details can be obtained by using the  -l  command line option to  web100ctl :  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement  -l  To increase the output further use:  [user@client /opt/npt]$  web100clt \u2013n  Target Server  for  Measurement  -ll", 
            "title": "Network Diagnostic Tool (NDT)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#one-way-active-measurement-protocol-owamp", 
            "text": "The One Way Active Measurement Protocol (OWAMP) is an advanced version of the common  ping  program. The OWAMP client  owping  communicates with an OWAMP server and measures the delay in each direction using NTP based time stamps. OWAMP can be used to identify delay, loss, and packet reordering problems inside the network. To measure the delay between the CE/SE and the remote server use the  owping  command:  [user@client /opt/npt]$  owping  Target Server  for  Measurement", 
            "title": "One Way Active Measurement Protocol (OWAMP)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#bandwidth-control-tool-bwctl", 
            "text": "The Bandwidth Control tool (BWCTL) is a wrapper for the  iperf  command, its policy and a daemon. BWCTL improves the usability of  iperf  by avoiding following problems:   need for remote access to the target host used for measurement  security concerns about leaving an  iperf  daemon running on the target host   BWCTL supports testing in either direction, or between 2 remote BWCTL servers from a third location. To measure the current throughput from your SE/CE to the remote server use the  bwctl  command:  [user@client /opt/npt]$  bwctl \u2013s  Target Server  for  Measurement   Third party tests, between 2 remote BWCTL servers, will let you measure various sections of the end-2-end path, running  [user@client /opt/npt]$  bwctl \u2013c  1st Server  for  Measurement  \u2013s  2nd Server  for  Measurement   Other useful options are  -f , format,  -t , length of test, and  -i , test interval.", 
            "title": "Bandwidth Control tool (BWCTL)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#network-path-and-application-diagnosis-npad", 
            "text": "NOTE: Network Path and Application Diagnosis (NPAD) is deprecated and won't be in future versions of the OSG distribution  The Network Path and Application Diagnosis (NPAD) tool examines a host and its local network infrastructure to determine what problems, if any, would hinder wide area performance. Issues such as small TCP buffers in switches and routers are detected as well as common host configuration errors. To determine if the CE/SE will achieve maximum performance over a WAN path run the command  diag-client :  [user@client /opt/npt]$  diag-client  Target Server  for  Measurement   8001   10   50    Note  The last 2 numeric parameters, after host and port, are the rtt time (in ms) and speed/rate values (in Mbps) you need to achieve. The reason it works this way is that its meant to 'test' local infrastructure. The idea is that if you were testing to an NPAD server that was 5ms away on a 1G network, you would get close to that speed even with network flaws. If you were to supply 80ms and 1G to the server and there truly was a flaw, the NPAD test would tell you it wasn't possible, thus enabling you to fix the problem    Note  The  diag-client  commands return a partial URL, enabling easy sharing of results between users and site administrators. To view the results, prepend the Toolkit servers name/port to the returned string. The example above would result in this URL:  http://server.this.osg.domain:8002/ServerData/Reports-2011-07/vtbv-ce.uchicago.edu:2011-07-12-18:50:07.html .", 
            "title": "Network Path and Application Diagnosis (NPAD)"
        }, 
        {
            "location": "/other/network-performance-toolkit/#advanced-topic-scheduled-monitoring", 
            "text": "(See  http://docs.perfsonar.net/install_quick_start.html  for more details.)  In addition to the above on-demand tests, the Performance Toolkit server can be configured to continuously monitor the throughput or delay between your site and peer sites of interest. To begin this monitoring, enter the GUI and ensure that your server is a member of the community or communities of interest. Once that is complete, continue on by selecting either the  perfSONAR-BUOY  throughput or delay configuration menu item.  pSB-throughput: This utility will run regularly scheduled BWCTL tests between your Toolkit server and the selected peer servers. Results are stored in a database and displayed on the server's web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views.  pSB-delay: This utility will run regularly scheduled OWAMP tests between your Toolkit server and the selected peers. Results are stored in a database and displayed on the server\u2019s web page. You may also use standard web-service calls to retrieve this data for display on remote web servers. This would allow monitoring of a common core infrastructure at a central site, while each site could keep local/customized views as required.", 
            "title": "Advanced Topic: Scheduled Monitoring"
        }, 
        {
            "location": "/other/network-performance-toolkit/#references", 
            "text": "One Way Active Measurement Protocol (OWAMP)  OSG/WLCG pages on perfSONAR .", 
            "title": "References"
        }, 
        {
            "location": "/other/troubleshooting-gratia/", 
            "text": "Troubleshooting Gratia Accounting\n\n\nThis document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service.\n\n\nGratia/GRACC: The Big Picture\n\n\nGratia is software used in OSG to gather accounting information. The information is collected from individual resources at a site, such as a Compute Element or a a submission host. The program that collects the data is called a \"Gratia probe\". The information is transferred to a GRACC server. Most sites will choose to send the accounting data to the central OSG Gratia server, but you can also use a Gratia server at your site (which can send forward the data to the central OSG Gratia server). Here is a diagram:\n\n\n\n\nDifference between Gratia and GRACC\n\n\nGratia is the legacy name of the OSG Accounting system.  GRACC is the new name of the server and hosted components of the accounting system.  When we refer to Gratia, we mean either the data or the probes on the resources.  If we mention GRACC, we are referring to the hosted components that the OSG maintains.\n\n\n\n\n\n\nThese are the definitions of the major elements in the above figure.\n\n\n\n\nGratia probe\n: A piece of software that collects accounting data from the computer on which it's running, and transmits it to a Gratia server.\n\n\nGRACC server\n: A server that collects Gratia accounting data from one or more sites and can share it with users via a web page.  The GRACC server is hosted by the OSG.\n\n\nReporter\n: A web service running on the GRACC server. Users can connect to the reporter via a web browser to explore the Gratia data.\n\n\nCollector\n: A web service running on the GRACC server that collects data from one or more Gratia probes. Users do not directly interact with the collector.\n\n\n\n\nYou can see the OSG's GRACC website at \nhttps://gracc.opensciencegrid.org\n.\n\n\nYou can see a fancier version of the Gratia data at \nhttps://display.opensciencegrid.org/\n. This is \nnot\n running a Gratia collector, but is a separate service.\n\n\nGratia Probes\n\n\nGratia Probes are periodically run as cron jobs, but different probes will run at different intervals. The cron jobs will always run and you should not remove them. You can find them in \n/etc/cron.d\n.\n\n\nHowever, the cron jobs will only do anything if you have enabled them. You enable them via an init script. For example, to enable them:\n\n\nroot@host #\n service gratia-probes-cron start\n\nEnabling gratia probes cron:                               [  \nOK\n  ]\n\n\n\n\n\n\nTo disable them:\n\n\nroot@host #\n service gratia-probes-cron stop\n\nDisabling gratia probes cron:                               [  \nOK\n  ]\n\n\n\n\n\n\nYou also need to enable individual probes, usually via \nosg-configure\n.  Documentation on using \nosg-configure\n with Gratia \ndocumented elsewhere\n.\n\n\nRunning Gratia Probes\n\n\nWhen the cron jobs are enabled and run, they go through the following process, with minor changes between different Gratia probes:\n\n\n\n\nThe probe is invoked. It reads its configuration from \n/etc/gratia/\nPROBE-NAME\n/ProbeConfig\n.\n\n\nIt collects the accounting information from the underlying system. For example, the Condor probe will read it from the \nPER_JOB_HISTORY_DIR\n, which is usually \n/var/lib/gratia/data\n.\n\n\nIt transforms the data into Gratia records and saves them into \n/var/lib/gratia/tmp/gratiafiles/\n\n\nWhen there are sufficient Gratia records, or when sufficient time has passed, it uploads sets of records in batches to the GRACC server, then removes them from the \ngratiafiles\n directory.\n\n\nAll progress is logged to \n/var/log/gratia\n.\n\n\nIf there are failures in uploading the files to the GRACC server\n\n\nFiles are not removed from \ngratiafiles\n until they are successfully uploaded.\n\n\nErrors are logged to log files in \n/var/log/gratia\n.\n\n\nThe uploads will be tried again later.\n\n\n\n\n\n\n\n\nGratia Probe Configuration\n\n\nIn normal cases, \nosg-configure\n does the editing of the probe configuration files, at least on the CE. The configuration is found in \n/etc/osg/config.d/30-gratia.ini\n and \ndocumented elsewhere\n.\n\n\nIf there are problems or special configuration, you might need to edit the Gratia configuration files yourself. Each probe has a separate configuration file found in \n/etc/gratia/\nPROBE-NAME\n/ProbeConfig\n.\n\n\nThe ProbeConfig files have many details. A few options that you might need to edit are shown before. This is \nnot\n a complete file, but only shows a subset of the options.\n\n\nProbeConfiguration\n \n\n    \nCollectorHost\n=\ngratia-osg-itb.opensciencegrid.org:80\n\n    \nSSLHost\n=\ngratia-osg-itb.opensciencegrid.org:80\n\n    \nSSLRegistrationHost\n=\ngratia-osg-itb.opensciencegrid.org:80\n\n\n    \nProbeName\n=\ncondor:fermicloud084.fnal.gov\n\n    \nSiteName\n=\nWISC_OSG_EDU\n\n    \nEnableProbe\n=\n1\n\n\n/\n\n\n\n\n\n\nThe options you see here are:\n\n\n\n\n\n\n\n\nOption\n\n\nComments\n\n\n\n\n\n\n\n\n\n\nCollectorHost\n\n\nThe GRACC server this probe reports to\n\n\n\n\n\n\nSSLHost\n\n\nThe GRACC server this probe reports to\n\n\n\n\n\n\nSSLRegistrationHost\n\n\nThe GRACC server this probe reports to\n\n\n\n\n\n\nProbeName\n\n\nThe unique name for this probe. Note that it includes the probe type and the host name\n\n\n\n\n\n\nSiteName\n\n\nThe name of your site, as registered in OIM. If your site must be registered in OIM\n\n\n\n\n\n\nEnableProbe\n\n\nThe probe will only run if this is \"1\"\n\n\n\n\n\n\n\n\nAgain, there are many more options in this file. Most of the time you won't need to touch them.\n\n\nAre the Gratia cron jobs running?\n\n\nYou should make sure the Gratia cron jobs are running. The simplest way is with the \nservice\n command:\n\n\nroot@host #\n /sbin/service gratia-probes-cron status\n\ngratia probes cron is enabled.\n\n\n\n\n\n\nIf it is not enabled, enable it as described above.\n\n\nA future release of Gratia will provide status on each of the individual probes, but right now this only ensures that the basic cron job is running. In the meantime, you can check if the individual Gratia probes are enabled. To do this, look at the \nEnableProbe\n option in the \nProbeConfig\n file, as described above. A quick command to do this is shown here. Note that the Condor and GridFTP Transfer probes are enabled while the glexec probe is disabled:\n\n\nroot@host #\n \ncd\n /etc/gratia\n\nroot@host #\n grep -r EnableProbe *\n\ncondor/ProbeConfig:    EnableProbe=\n1\n\n\nglexec/ProbeConfig:    EnableProbe=\n0\n\n\ngridftp-transfer/ProbeConfig:    EnableProbe=\n1\n\n\n\n\n\n\nIf you see no log files in \n/var/log/gratia\n you may have an error in the probe configuration file. Manually run the test for your probe (check \n/etc/cron.d/gratia-probe-condor.cron\n), e.g. \n/usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig\n. If there is an error you may get a suggestion on where it is, e.g.:\n\n\nroot@host #\n /usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig\n\nParse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4\n\n\n\n\n\n\nCorrect the error and restart gratia.\n\n\nHave you configured the resource names correctly?\n\n\nDo the names of your resources match the names in \nOIM\n?\nGratia retrieves the resource name from the \nSite Information\n section of the \n/etc/osg/config.d/40-siteinfo.ini\n\n\n;\n===================================================================\n\n\n;\n                       \nSite\n \nInformation\n\n\n;\n===================================================================\n\n\n\n[\nSite\n \nInformation\n]\n\n\n;\n \nThe\n \ngroup\n \noption\n \nindicates\n \nthe\n \ngroup\n \nthat\n \nthe\n \nOSG\n \nsite\n \nshould\n \nbe\n \nlisted\n \nin\n,\n\n\n;\n \nfor\n \nproduction\n \nsites\n \nthis\n \nshould\n \nbe\n \nOSG\n,\n \nfor\n \nvtb\n \nor\n \nitb\n \ntesting\n \nit\n \nshould\n \nbe\n\n\n;\n \nOSG\n-\nITB\n\n\n;\n \n\n;\n \nYOU\n \nWILL\n \nNEED\n \nTO\n \nCHANGE\n \nTHIS\n\n\ngroup\n \n=\n \nOSG\n\n\n\n;\n \nThe\n \nhost_name\n \nsetting\n \nshould\n \ngive\n \nthe\n \nhost\n \nname\n \nof\n \nthe\n \nCE\n  \nthat\n \nis\n \nbeing\n \n\n;\n \nconfigured\n,\n \nthis\n \nsetting\n \nmust\n \nbe\n \na\n \nvalid\n \ndns\n \nname\n \nthat\n \nresolves\n\n\n;\n \n\n;\n \nYOU\n \nWILL\n \nNEED\n \nTO\n \nCHANGE\n \nTHIS\n\n\nhost_name\n \n=\n \ntusker\n-\ngw1\n.\nunl\n.\nedu\n\n\n\n;\n \nThe\n \nresource\n \nsetting\n \nshould\n \nbe\n \nset\n \nto\n \nthe\n \nsame\n \nvalue\n \nas\n \nused\n \nin\n \nthe\n \nOIM\n \n\n;\n \nregistration\n \nat\n \nthe\n \ngoc\n \n\n;\n \n\n;\n \nYOU\n \nWILL\n \nNEED\n \nTO\n \nCHANGE\n \nTHIS\n\n\nresource\n \n=\n \nTusker\n-\nCE1\n\n\n\n;\n \nThe\n \nresource_group\n \nsetting\n \nshould\n \nbe\n \nset\n \nto\n \nthe\n \nsame\n \nvalue\n \nas\n \nused\n \nin\n \nthe\n \nOIM\n \n\n;\n \nregistration\n \nat\n \nthe\n \ngoc\n \n\n;\n \n\n;\n \nYOU\n \nWILL\n \nNEED\n \nTO\n \nCHANGE\n \nTHIS\n\n\nresource_group\n \n=\n \nTusker\n\n\n\n\n\n\nDo those names match the names that you registered with OIM? If not, edit the names, and rerun \"osg-configure -c\".\n\n\nDid the site name change?\n\n\nWas the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the GRACC operations team to update the name of your site at the GRACC collector. To do this:\n\n\n\n\nOpen a \nsupport ticket\n\n\nSelect \"Software or Service\"\n\n\nSelect \"GRACC Operations\"\n\n\nType a friendly email that asks the GRACC team to change your site name at the collector. Make sure to tell them the old name and the new name.  Below is an example email:\nHello\n \nGRACC\n \nTeam\n,\n\n\n\nPlease\n \nchange\n \nthe\n \nsite\n \nname\n \nof\n \nmy\n \nsite\n \nfrom\n \nInsert\n \nOld\n \nName\n \nto\n \nInsert\n \nNew\n \nName\n.\n\n\n\nThanks\n,\n \n...\n\n\n\n\n\n\n\n\n\n\nIs a site reporting data?\n\n\nYou can see if the OSG GRACC Server is getting data from a site by going to \nGRACC\n:\n\n\n\n\nSpecify the site name in Facility\n\n\n\n\nHTCondor's Gratia Configuration\n\n\n\n\nNote\n\n\nOnly applicable to HTCondor batch sites, not SLURM, PBS, SGE or LSF sites\n\n\n\n\nCondor must be configured to put information about each job into a special directory.  Gratia will read and remove the files in order to collect the accounting information.\n\n\nThe configuration variable is called \nPER_JOB_HISTORY_DIR\n. If you install the OSG RPM for Condor, the Gratia probe will extend its configuration by adding a file to \n/etc/condor/config.d\n, and will set this variable to \n/var/lib/gratia/data\n. If you are using a different installation method, you may need to set the variable yourself. You can check if it's set by using \ncondor_config_val\n, like this:\n\n\nuser@host $\n condor_config_val -v PER_JOB_HISTORY_DIR\n\nPER_JOB_HISTORY_DIR: /var/lib/gratia/data\n\n\n    Defined in \n/etc/condor/config.d/99_gratia.conf\n, line 5.\n\n\n\n\n\n\nIf you set this value, you need to restart condor:\n\n\nroot@host #\n condor_restart\n\nSent \nRestart\n command to local master\n\n\n\n\n\n\nUnlike many Condor settings, a \ncondor_reconfig\n is not sufficient - you must restart!\n\n\nIf you accidentally did not set \nPER_JOB_HISTORY_DIR\n (see above)\n\n\nThe HTCondor Gratia probe will not publish accounting information about jobs without \nPER_JOB_HISTORY_DIR\n. You can have Gratia read the Condor history file and publish data that way. If you know the time period of the missing data, you should specify a start and end times. This reduces the load on the Gratia collector. To do so:\n\n\n%\nBLUE%Preferred method using start and end times\n\n\nroot@host #\n /usr/share/gratia/condor/condor_meter --history --start-time\n=\n2014-06-01\n --end-time\n=\n2014-06-02\n --verbose\n\n2014-06-03 10:00:36 CDT Gratia: RUNNING condor_meter MANUALLY using HTCondor history from 2014-06-01 to 2014-06-02\n\n\n2014-06-03 10:00:36 CDT Gratia: RUNNING: condor_history -l -constraint \n((JobCurrentStartDate \n 1401598800) \n (JobCurrentStartDate \n 1401685200))\n\n\n2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records submitted: 399\n\n\n2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records found: 400\n\n\n2014-06-03 10:00:49 CDT Gratia: RUNNING condor_meter MANUALLY Finished\n\n\n\n%\nBLUE% or \nif\n you need to go back to the beginning of time\n\n\nroot@host #\n /usr/share/gratia/condor/condor_meter --history --verbose\n\n2014-06-03 10:06:19 CDT Gratia: RUNNING condor_meter MANUALLY using all HTCondor history\n\n\n2014-06-03 10:06:19 CDT Gratia: RUNNING: condor_history -l\n\n\n2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records submitted: 13026\n\n\n2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records found: 13027\n\n\n2014-06-03 10:11:38 CDT Gratia: RUNNING condor_meter MANUALLY Finished\n\n\n\n\n\n\nNot much is printed to the screen, but you can see progress in the Gratia log file:\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nInitializing\n \nGratia\n \nwith\n \n/\netc\n/\ngratia\n/\ncondor\n/\nProbeConfig\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nCreating\n \na\n \nProbeDetails\n \nrecord\n \n2012\n-\n04\n-\n04\nT18\n:\n35\n:\n28\nZ\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \n***********************************************************\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nOK\n \n-\n \nHandshake\n \nadded\n \nto\n \nbundle\n \n(\n1\n/\n100\n)\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \n***********************************************************\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nList\n \nof\n \nbackup\n \ndirectories\n: [\nu\n/var/lib/gratia/tmp\n]\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nReprocessing\n \nresponse\n: \nOK\n \n-\n \nReprocessing\n \n0\n \nrecord\n(\ns\n)\n \nuploaded\n, \n0\n \nbundled\n, \n0\n \nfailed\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nAfter\n \nreprocessing\n: \n0\n \nin\n \noutbox\n \n0\n \nin\n \nstaged\n \noutbox\n \n0\n \ntar\n \nfiles\n\n\n13\n:\n35\n:\n28\n \nCDT\n \nGratia\n: \nCreating\n \na\n \nUsageRecord\n \n2012\n-\n04\n-\n04\nT18\n:\n35\n:\n28\nZ\n\n...\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \nProcessing\n \nbundle\n \nfile\n: \n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \nProcessing\n \nbundle\n \nfile\n: \n/\nvar\n/\nlib\n/\ngratia\n/\ntmp\n/\ngratiafiles\n/\n\n    \nsubdir\n.\ncondor_fermicloud084\n.\nfnal\n.\ngov_gratia\n-\nosg\n-\nitb\n.\nopensciencegrid\n.\norg_80\n/\n\n    \noutbox\n/\nr\n.\n18425\n.\ncondor_fermicloud084\n.\nfnal\n.\ngov_gratia\n-\nosg\n-\nitb\n.\nopensciencegrid\n.\norg_80\n.\ngratia\n.\nxml__BSuXo18428\n\n...\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \n***********************************************************\n\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \nRemoving\n \nlog\n \nfiles\n \nolder\n \nthan\n \n31\n \ndays\n \nfrom\n \n/\nvar\n/\nlog\n/\ngratia\n\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \n/\nvar\n/\nlog\n/\ngratia\n \nuses\n \n0\n.\n035\n%\n \nand\n \nthere\n \nis\n \n73\n%\n \nfree\n\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \nRemoving\n \nincomplete\n \ndata\n \nfiles\n \nolder\n \nthan\n \n31\n \ndays\n \nfrom\n \n/\nvar\n/\nlib\n/\ngratia\n/\ndata\n/\n\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \n/\nvar\n/\nlib\n/\ngratia\n/\ndata\n \nuses\n \n0\n%\n \nand\n \nthere\n \nis\n \n73\n%\n \nfree\n\n\n13\n:\n35\n:\n29\n \nCDT\n \nGratia\n: \nEnd\n \nof\n \nexecution\n \nsummary\n: \nnew\n \nrecords\n \nsent\n \nsuccessfully\n: \n37\n\n\n\n\n\n\n\n\nNote\n\n\nCondor rotates history files, so you can only report what Condor has kept. Controlling the Condor history is documented in the Condor manual. In particular, see the options for \nMAX_HISTORY_LOG\n and \nMAX_HISTORY_ROTATIONS\n.\n\n\n\n\nBad Gratia hostname\n\n\nThis is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in \n/var/log/gratia/\n. There is one log file per day, labeled by the date:\n\n\nroot@host #\n \ncd\n /var/log/gratia/\n\nroot@host #\n cat \n2012\n-04-03.log \n\n...\n\n\n%\nRED%You can see that Gratia is using the correct configuration file:\n\n\n15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig\n\n\n\n%\nRED%Here Gratia is removing a file from the Condor PER_JOB_HISTORY_DIR and creating a Gratia accounting record \nfor\n it\n\n\n15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z\n\n\n15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0\n\n\n15:06:55 CDT Gratia: ***********************************************************\n\n\n15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/\n\n\n    subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/\n\n\n    outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606\n\n\n15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0\n\n\n\n%\nRED%Later, Gratia failed to connect to the server due to a bad hostname\n\n\n15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type \nsocket.gaierror\n: (-2, \nName or service not known\n)\n\n\n...\n\n\n15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted:\n\n\n15:06:55 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/\n\n\n    subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/\n\n\n    outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606\n\n\n\n\n\n\nIf you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data. \n\n\nThis can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files.\n\n\nWhen a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as \ncondor\n), the name of the host you're running on, and the name of the GRACC host you're sending the information to. For simplicity, lets call that name \nprobe-records\n, but you'll see what it really looks like below. Within this directory, you'll see some subdirectories:\n\n\n\n\n\n\n\n\nDirectory\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n/var/lib/gratia/tmp/grataifiles/\nprobe-records\n/outbox\n\n\nThe usual location for the accounting records\n\n\n\n\n\n\n/var/lib/gratia/tmp/grataifiles/\nprobe-records\n/staged/store\n\n\nAn overflow location when there are problems\n\n\n\n\n\n\n\n\nWhen you recover old records, you need to:\n\n\n\n\nMove files from the outbox of the incorrect \nprobe-records\n directory into the outbox of the correctly named \nprobe-records\n directory.\n\n\nMove tarred and compressed files from the staged/store of the incorrect \nprobe-records\n directory into the staged/store of the correctly named \nprobe-records\n directory. Then you uncompress them and remove the compressed version.\n\n\n\n\nIn the examples below, the hostname for gratia was \"accidentally\" spelled backwards. Instead of \ngratia-osg-itb.opensciencegrid.org\n, it was \naitarg-osg-itb.opensciencegrid.org\n.\n\n\n\n\n\n\nFirst you need to fix the hostname. For a CE, you can edit \n/etc/osg/config.d/30-gratia.ini\n and rerun \nosg-configure -c\n. In other installations, you have to edit the appropriate \nProbeConfig\n file.\n\n\n\n\n\n\nNext, submit a job via to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example:\n\n\nAs a user: \n\n\nuser@host $\n globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname\n\n\n\n\n\nAs root (adjust for your batch system): \n\n\nroot@host #\n /share/gratia/condor/condor\n\\_\nmeter\n\n\n\n\n\n\n\n\n\nFind the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldly name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is noted in red below, but \nit will be different on your computer\n.\n\n\nuser@host $\n less /var/log/gratia/2012-04-06\n\n...\n\n\n16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted:\n\n\n16:04:29 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/\n\n\n    subdir.condor_fermicloud084.fnal.gov_\naitarg\n-osg-itb.opensciencegrid.org_80/\n\n\n    outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918\n\n\n\n\n\n\n(The filename was wrapped for legibility.)\n\n\nYou can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run.\n\n\nroot@host #\n \ncd\n /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_\naitarg\n-osg-itb.opensciencegrid.org_80/outbox/.\n\nroot@host #\n mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_\ngratia\n-osg-itb.opensciencegrid.org_80/outbox/.\n\n\n\n\n\n\n\n\n\nIf this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in \"staged/store\" instead of \"outbox\" as above:\n\n\n%\nRED%# Find the old files\n\n\nroot@host #\n \ncd\n /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_\naitarg\n-osg-itb.opensciencegrid.org_80/staged/store\n\n\n%\nRED%# Move them to the correct directory\n\n\nroot@host #\n mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_\ngratia\n-osg-itb.opensciencegrid.org_80/outbox/.\n\nroot@host #\n \ncd\n !$\n\n\n%\nRED%# For each tz file:\n\n\nroot@host #\n tar xf tz.1223.... \n[\nname shortened \nfor\n legibility\n]\n\n\nroot@host #\n rm tz.1223....\n\n\n\n\n\nWhen you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron.\n\n\n\n\n\n\nReference: Important Gratia files\n\n\nIf you need to look for more data, you can look at log files for the various services on your CE.\n\n\n\n\n\n\n\n\nFile\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\n/var/log/gratia/\nDATE\n.log\n\n\nLog file that records information about processing and uploading of Gratia accounting data\n\n\n\n\n\n\n/var/log/gratia/gridftpTransfer.log\n\n\nLog file specific to the Gratia GridFTP probe\n\n\n\n\n\n\n/var/lib/gratia/data\n\n\nLocation for Condor and PBS job data before being processed by Gratia\nCondor's \nPER_JOB_HISTORY_DIR\n should be set to this location\n\n\n\n\n\n\n/var/lib/gratia/tmp/gratiafiles\n\n\nLocation for temporary Gratia data as it is being processed, usually empty.\nIf you have files that are more than 30 minutes old in this directory, there may be a problem\n\n\n\n\n\n\n/etc/gratia/\nPROBE-NAME\n/ProbeConfig\n\n\nConfiguration for Gratia probes, one per probe type\nNormally you don't need to edit this\n\n\n\n\n\n\n\n\nNot all RPMs will be on all hosts.  Instead, only the \ngratia-probe-common\n and the one RPM specific to that host will be installed. The most common RPMs you will see are:\n\n\n\n\n\n\n\n\nRPM\n\n\nPurpose\n\n\n\n\n\n\n\n\n\n\ngratia-probe-common\n\n\nCode shared between all Graita probes\n\n\n\n\n\n\ngratia-probe-condor\n\n\nThe probe that tracks Condor usage\n\n\n\n\n\n\ngratia-probe-slurm\n\n\nThe probe that tracks SLURM usage\n\n\n\n\n\n\ngratia-probe-pbs-lsf\n\n\nThe probe that tracks PBS and/or LSF usage\n\n\n\n\n\n\ngratia-probe-gridftp-transfer\n\n\nThe probe that tracks transfers done with GridFTP", 
            "title": "Troubleshooting Gratia"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#troubleshooting-gratia-accounting", 
            "text": "This document will help you troubleshoot problems with the Gratia Accounting, particularly with problems in collecting and reporting accounting information to the central OSG accounting service.", 
            "title": "Troubleshooting Gratia Accounting"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#gratiagracc-the-big-picture", 
            "text": "Gratia is software used in OSG to gather accounting information. The information is collected from individual resources at a site, such as a Compute Element or a a submission host. The program that collects the data is called a \"Gratia probe\". The information is transferred to a GRACC server. Most sites will choose to send the accounting data to the central OSG Gratia server, but you can also use a Gratia server at your site (which can send forward the data to the central OSG Gratia server). Here is a diagram:   Difference between Gratia and GRACC  Gratia is the legacy name of the OSG Accounting system.  GRACC is the new name of the server and hosted components of the accounting system.  When we refer to Gratia, we mean either the data or the probes on the resources.  If we mention GRACC, we are referring to the hosted components that the OSG maintains.    These are the definitions of the major elements in the above figure.   Gratia probe : A piece of software that collects accounting data from the computer on which it's running, and transmits it to a Gratia server.  GRACC server : A server that collects Gratia accounting data from one or more sites and can share it with users via a web page.  The GRACC server is hosted by the OSG.  Reporter : A web service running on the GRACC server. Users can connect to the reporter via a web browser to explore the Gratia data.  Collector : A web service running on the GRACC server that collects data from one or more Gratia probes. Users do not directly interact with the collector.   You can see the OSG's GRACC website at  https://gracc.opensciencegrid.org .  You can see a fancier version of the Gratia data at  https://display.opensciencegrid.org/ . This is  not  running a Gratia collector, but is a separate service.", 
            "title": "Gratia/GRACC: The Big Picture"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#gratia-probes", 
            "text": "Gratia Probes are periodically run as cron jobs, but different probes will run at different intervals. The cron jobs will always run and you should not remove them. You can find them in  /etc/cron.d .  However, the cron jobs will only do anything if you have enabled them. You enable them via an init script. For example, to enable them:  root@host #  service gratia-probes-cron start Enabling gratia probes cron:                               [   OK   ]   To disable them:  root@host #  service gratia-probes-cron stop Disabling gratia probes cron:                               [   OK   ]   You also need to enable individual probes, usually via  osg-configure .  Documentation on using  osg-configure  with Gratia  documented elsewhere .", 
            "title": "Gratia Probes"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#running-gratia-probes", 
            "text": "When the cron jobs are enabled and run, they go through the following process, with minor changes between different Gratia probes:   The probe is invoked. It reads its configuration from  /etc/gratia/ PROBE-NAME /ProbeConfig .  It collects the accounting information from the underlying system. For example, the Condor probe will read it from the  PER_JOB_HISTORY_DIR , which is usually  /var/lib/gratia/data .  It transforms the data into Gratia records and saves them into  /var/lib/gratia/tmp/gratiafiles/  When there are sufficient Gratia records, or when sufficient time has passed, it uploads sets of records in batches to the GRACC server, then removes them from the  gratiafiles  directory.  All progress is logged to  /var/log/gratia .  If there are failures in uploading the files to the GRACC server  Files are not removed from  gratiafiles  until they are successfully uploaded.  Errors are logged to log files in  /var/log/gratia .  The uploads will be tried again later.", 
            "title": "Running Gratia Probes"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#gratia-probe-configuration", 
            "text": "In normal cases,  osg-configure  does the editing of the probe configuration files, at least on the CE. The configuration is found in  /etc/osg/config.d/30-gratia.ini  and  documented elsewhere .  If there are problems or special configuration, you might need to edit the Gratia configuration files yourself. Each probe has a separate configuration file found in  /etc/gratia/ PROBE-NAME /ProbeConfig .  The ProbeConfig files have many details. A few options that you might need to edit are shown before. This is  not  a complete file, but only shows a subset of the options.  ProbeConfiguration  \n\n     CollectorHost = gratia-osg-itb.opensciencegrid.org:80 \n     SSLHost = gratia-osg-itb.opensciencegrid.org:80 \n     SSLRegistrationHost = gratia-osg-itb.opensciencegrid.org:80 \n\n     ProbeName = condor:fermicloud084.fnal.gov \n     SiteName = WISC_OSG_EDU \n     EnableProbe = 1  /   The options you see here are:     Option  Comments      CollectorHost  The GRACC server this probe reports to    SSLHost  The GRACC server this probe reports to    SSLRegistrationHost  The GRACC server this probe reports to    ProbeName  The unique name for this probe. Note that it includes the probe type and the host name    SiteName  The name of your site, as registered in OIM. If your site must be registered in OIM    EnableProbe  The probe will only run if this is \"1\"     Again, there are many more options in this file. Most of the time you won't need to touch them.", 
            "title": "Gratia Probe Configuration"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#are-the-gratia-cron-jobs-running", 
            "text": "You should make sure the Gratia cron jobs are running. The simplest way is with the  service  command:  root@host #  /sbin/service gratia-probes-cron status gratia probes cron is enabled.   If it is not enabled, enable it as described above.  A future release of Gratia will provide status on each of the individual probes, but right now this only ensures that the basic cron job is running. In the meantime, you can check if the individual Gratia probes are enabled. To do this, look at the  EnableProbe  option in the  ProbeConfig  file, as described above. A quick command to do this is shown here. Note that the Condor and GridFTP Transfer probes are enabled while the glexec probe is disabled:  root@host #   cd  /etc/gratia root@host #  grep -r EnableProbe * condor/ProbeConfig:    EnableProbe= 1  glexec/ProbeConfig:    EnableProbe= 0  gridftp-transfer/ProbeConfig:    EnableProbe= 1   If you see no log files in  /var/log/gratia  you may have an error in the probe configuration file. Manually run the test for your probe (check  /etc/cron.d/gratia-probe-condor.cron ), e.g.  /usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig . If there is an error you may get a suggestion on where it is, e.g.:  root@host #  /usr/share/gratia/common/cron_check  /etc/gratia/condor/ProbeConfig Parse error in /etc/gratia/condor/ProbeConfig: not well-formed (invalid token): line 21, column 4   Correct the error and restart gratia.", 
            "title": "Are the Gratia cron jobs running?"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#have-you-configured-the-resource-names-correctly", 
            "text": "Do the names of your resources match the names in  OIM ?\nGratia retrieves the resource name from the  Site Information  section of the  /etc/osg/config.d/40-siteinfo.ini  ; ===================================================================  ;                         Site   Information  ; ===================================================================  [ Site   Information ]  ;   The   group   option   indicates   the   group   that   the   OSG   site   should   be   listed   in ,  ;   for   production   sites   this   should   be   OSG ,   for   vtb   or   itb   testing   it   should   be  ;   OSG - ITB  ;   ;   YOU   WILL   NEED   TO   CHANGE   THIS  group   =   OSG  ;   The   host_name   setting   should   give   the   host   name   of   the   CE    that   is   being   ;   configured ,   this   setting   must   be   a   valid   dns   name   that   resolves  ;   ;   YOU   WILL   NEED   TO   CHANGE   THIS  host_name   =   tusker - gw1 . unl . edu  ;   The   resource   setting   should   be   set   to   the   same   value   as   used   in   the   OIM   ;   registration   at   the   goc   ;   ;   YOU   WILL   NEED   TO   CHANGE   THIS  resource   =   Tusker - CE1  ;   The   resource_group   setting   should   be   set   to   the   same   value   as   used   in   the   OIM   ;   registration   at   the   goc   ;   ;   YOU   WILL   NEED   TO   CHANGE   THIS  resource_group   =   Tusker   Do those names match the names that you registered with OIM? If not, edit the names, and rerun \"osg-configure -c\".", 
            "title": "Have you configured the resource names correctly?"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#did-the-site-name-change", 
            "text": "Was the site previously reporting data, but the site name (not host name, but site name) changed? When the site name changes, you need to ask the GRACC operations team to update the name of your site at the GRACC collector. To do this:   Open a  support ticket  Select \"Software or Service\"  Select \"GRACC Operations\"  Type a friendly email that asks the GRACC team to change your site name at the collector. Make sure to tell them the old name and the new name.  Below is an example email: Hello   GRACC   Team ,  Please   change   the   site   name   of   my   site   from   Insert   Old   Name   to   Insert   New   Name .  Thanks ,   ...", 
            "title": "Did the site name change?"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#is-a-site-reporting-data", 
            "text": "You can see if the OSG GRACC Server is getting data from a site by going to  GRACC :   Specify the site name in Facility", 
            "title": "Is a site reporting data?"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#htcondors-gratia-configuration", 
            "text": "Note  Only applicable to HTCondor batch sites, not SLURM, PBS, SGE or LSF sites   Condor must be configured to put information about each job into a special directory.  Gratia will read and remove the files in order to collect the accounting information.  The configuration variable is called  PER_JOB_HISTORY_DIR . If you install the OSG RPM for Condor, the Gratia probe will extend its configuration by adding a file to  /etc/condor/config.d , and will set this variable to  /var/lib/gratia/data . If you are using a different installation method, you may need to set the variable yourself. You can check if it's set by using  condor_config_val , like this:  user@host $  condor_config_val -v PER_JOB_HISTORY_DIR PER_JOB_HISTORY_DIR: /var/lib/gratia/data      Defined in  /etc/condor/config.d/99_gratia.conf , line 5.   If you set this value, you need to restart condor:  root@host #  condor_restart Sent  Restart  command to local master   Unlike many Condor settings, a  condor_reconfig  is not sufficient - you must restart!", 
            "title": "HTCondor's Gratia Configuration"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#if-you-accidentally-did-not-set-per_job_history_dir-see-above", 
            "text": "The HTCondor Gratia probe will not publish accounting information about jobs without  PER_JOB_HISTORY_DIR . You can have Gratia read the Condor history file and publish data that way. If you know the time period of the missing data, you should specify a start and end times. This reduces the load on the Gratia collector. To do so:  % BLUE%Preferred method using start and end times  root@host #  /usr/share/gratia/condor/condor_meter --history --start-time = 2014-06-01  --end-time = 2014-06-02  --verbose 2014-06-03 10:00:36 CDT Gratia: RUNNING condor_meter MANUALLY using HTCondor history from 2014-06-01 to 2014-06-02  2014-06-03 10:00:36 CDT Gratia: RUNNING: condor_history -l -constraint  ((JobCurrentStartDate   1401598800)   (JobCurrentStartDate   1401685200))  2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records submitted: 399  2014-06-03 10:00:49 CDT Gratia: condor_meter --history: Usage records found: 400  2014-06-03 10:00:49 CDT Gratia: RUNNING condor_meter MANUALLY Finished  % BLUE% or  if  you need to go back to the beginning of time  root@host #  /usr/share/gratia/condor/condor_meter --history --verbose 2014-06-03 10:06:19 CDT Gratia: RUNNING condor_meter MANUALLY using all HTCondor history  2014-06-03 10:06:19 CDT Gratia: RUNNING: condor_history -l  2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records submitted: 13026  2014-06-03 10:11:38 CDT Gratia: condor_meter --history: Usage records found: 13027  2014-06-03 10:11:38 CDT Gratia: RUNNING condor_meter MANUALLY Finished   Not much is printed to the screen, but you can see progress in the Gratia log file:  13 : 35 : 28   CDT   Gratia :  Initializing   Gratia   with   / etc / gratia / condor / ProbeConfig  13 : 35 : 28   CDT   Gratia :  Creating   a   ProbeDetails   record   2012 - 04 - 04 T18 : 35 : 28 Z  13 : 35 : 28   CDT   Gratia :  ***********************************************************  13 : 35 : 28   CDT   Gratia :  OK   -   Handshake   added   to   bundle   ( 1 / 100 )  13 : 35 : 28   CDT   Gratia :  ***********************************************************  13 : 35 : 28   CDT   Gratia :  List   of   backup   directories : [ u /var/lib/gratia/tmp ] 13 : 35 : 28   CDT   Gratia :  Reprocessing   response :  OK   -   Reprocessing   0   record ( s )   uploaded ,  0   bundled ,  0   failed  13 : 35 : 28   CDT   Gratia :  After   reprocessing :  0   in   outbox   0   in   staged   outbox   0   tar   files  13 : 35 : 28   CDT   Gratia :  Creating   a   UsageRecord   2012 - 04 - 04 T18 : 35 : 28 Z \n... 13 : 35 : 29   CDT   Gratia :  Processing   bundle   file :  13 : 35 : 29   CDT   Gratia :  Processing   bundle   file :  / var / lib / gratia / tmp / gratiafiles / \n     subdir . condor_fermicloud084 . fnal . gov_gratia - osg - itb . opensciencegrid . org_80 / \n     outbox / r . 18425 . condor_fermicloud084 . fnal . gov_gratia - osg - itb . opensciencegrid . org_80 . gratia . xml__BSuXo18428 \n... 13 : 35 : 29   CDT   Gratia :  ***********************************************************  13 : 35 : 29   CDT   Gratia :  Removing   log   files   older   than   31   days   from   / var / log / gratia  13 : 35 : 29   CDT   Gratia :  / var / log / gratia   uses   0 . 035 %   and   there   is   73 %   free  13 : 35 : 29   CDT   Gratia :  Removing   incomplete   data   files   older   than   31   days   from   / var / lib / gratia / data /  13 : 35 : 29   CDT   Gratia :  / var / lib / gratia / data   uses   0 %   and   there   is   73 %   free  13 : 35 : 29   CDT   Gratia :  End   of   execution   summary :  new   records   sent   successfully :  37    Note  Condor rotates history files, so you can only report what Condor has kept. Controlling the Condor history is documented in the Condor manual. In particular, see the options for  MAX_HISTORY_LOG  and  MAX_HISTORY_ROTATIONS .", 
            "title": "If you accidentally did not set PER_JOB_HISTORY_DIR (see above)"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#bad-gratia-hostname", 
            "text": "This is an example problem where the configuration was bad: there was an incorrect hostname for the Gratia server. The problem is clearly visible in the Gratia log file, which is located in  /var/log/gratia/ . There is one log file per day, labeled by the date:  root@host #   cd  /var/log/gratia/ root@host #  cat  2012 -04-03.log  ...  % RED%You can see that Gratia is using the correct configuration file:  15:06:55 CDT Gratia: Using config file: /etc/gratia/condor/ProbeConfig  % RED%Here Gratia is removing a file from the Condor PER_JOB_HISTORY_DIR and creating a Gratia accounting record  for  it  15:06:55 CDT Gratia: Creating a UsageRecord 2012-04-03T20:06:55Z  15:06:55 CDT Gratia: Registering transient input file: /var/lib/gratia/data/history.37.0  15:06:55 CDT Gratia: ***********************************************************  15:06:55 CDT Gratia: Saved record to /var/lib/gratia/tmp/gratiafiles/      subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/      outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606  15:06:55 CDT Gratia: Deleting transient input file: /var/lib/gratia/data/history.37.0  % RED%Later, Gratia failed to connect to the server due to a bad hostname  15:06:55 CDT Gratia: Failed to send xml to web service due to an error of type  socket.gaierror : (-2,  Name or service not known )  ...  15:06:55 CDT Gratia: Response indicates failure, the following files will not be deleted:  15:06:55 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/      subdir.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80/      outbox/r.30604.condor_fermicloud084.fnal.gov_ggratia-osg-itb.opensciencegrid.org_80.gratia.xml__wfIgi30606   If you accidentally had a bad Gratia hostname, you probably want to recover your Gratia data.   This can be done, though it's not simple. There are a few things you need to do. But first, you need to understand exactly where Gratia stores files.  When a Gratia extracts accounting information, it creates one file per record and stores it in a directory. The directory is a long name that contains the type of the probe (such as  condor ), the name of the host you're running on, and the name of the GRACC host you're sending the information to. For simplicity, lets call that name  probe-records , but you'll see what it really looks like below. Within this directory, you'll see some subdirectories:     Directory  Purpose      /var/lib/gratia/tmp/grataifiles/ probe-records /outbox  The usual location for the accounting records    /var/lib/gratia/tmp/grataifiles/ probe-records /staged/store  An overflow location when there are problems     When you recover old records, you need to:   Move files from the outbox of the incorrect  probe-records  directory into the outbox of the correctly named  probe-records  directory.  Move tarred and compressed files from the staged/store of the incorrect  probe-records  directory into the staged/store of the correctly named  probe-records  directory. Then you uncompress them and remove the compressed version.   In the examples below, the hostname for gratia was \"accidentally\" spelled backwards. Instead of  gratia-osg-itb.opensciencegrid.org , it was  aitarg-osg-itb.opensciencegrid.org .    First you need to fix the hostname. For a CE, you can edit  /etc/osg/config.d/30-gratia.ini  and rerun  osg-configure -c . In other installations, you have to edit the appropriate  ProbeConfig  file.    Next, submit a job via to your batch system, then run the appropriate Gratia probe (or wait for it to run via cron). This will create the properly named directories on your disk. For example:  As a user:   user@host $  globus-job-run fermicloud084.fnal.gov/jobmanager-condor /bin/hostname  As root (adjust for your batch system):   root@host #  /share/gratia/condor/condor \\_ meter    Find the Gratia records that can be easily uploaded. They are located in a a directory with an unwieldly name that includes your hostname and the incorrect name of the Gratia host. You can see the directory name in the Gratia log: the misspelled name is noted in red below, but  it will be different on your computer .  user@host $  less /var/log/gratia/2012-04-06 ...  16:04:29 CDT Gratia: Response indicates failure, the following files will not be deleted:  16:04:29 CDT Gratia:    /var/lib/gratia/tmp/gratiafiles/      subdir.condor_fermicloud084.fnal.gov_ aitarg -osg-itb.opensciencegrid.org_80/      outbox/r.916.condor_fermicloud084.fnal.gov_aitarg-osg-itb.opensciencegrid.org_80.gratia.xml__JDlHbNb918   (The filename was wrapped for legibility.)  You can simply copy these to the correct directory. Wait for the Gratia cron job to run, or force it to run.  root@host #   cd  /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_ aitarg -osg-itb.opensciencegrid.org_80/outbox/. root@host #  mv * /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_ gratia -osg-itb.opensciencegrid.org_80/outbox/.    If this has been a persistent problem, you might have many records. After a while, they are put into a compressed files in another directory. You can move those files, then uncompress them. This is a long name: note that the path ends in \"staged/store\" instead of \"outbox\" as above:  % RED%# Find the old files  root@host #   cd  /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_ aitarg -osg-itb.opensciencegrid.org_80/staged/store % RED%# Move them to the correct directory  root@host #  mv tz* /var/lib/gratia/tmp/gratiafiles/subdir.condor_fermicloud084.fnal.gov_ gratia -osg-itb.opensciencegrid.org_80/outbox/. root@host #   cd  !$ % RED%# For each tz file:  root@host #  tar xf tz.1223....  [ name shortened  for  legibility ]  root@host #  rm tz.1223....  When you've done this, you can re-run the Gratia probe by hand, or wait for it to run via cron.", 
            "title": "Bad Gratia hostname"
        }, 
        {
            "location": "/other/troubleshooting-gratia/#reference-important-gratia-files", 
            "text": "If you need to look for more data, you can look at log files for the various services on your CE.     File  Purpose      /var/log/gratia/ DATE .log  Log file that records information about processing and uploading of Gratia accounting data    /var/log/gratia/gridftpTransfer.log  Log file specific to the Gratia GridFTP probe    /var/lib/gratia/data  Location for Condor and PBS job data before being processed by Gratia Condor's  PER_JOB_HISTORY_DIR  should be set to this location    /var/lib/gratia/tmp/gratiafiles  Location for temporary Gratia data as it is being processed, usually empty. If you have files that are more than 30 minutes old in this directory, there may be a problem    /etc/gratia/ PROBE-NAME /ProbeConfig  Configuration for Gratia probes, one per probe type Normally you don't need to edit this     Not all RPMs will be on all hosts.  Instead, only the  gratia-probe-common  and the one RPM specific to that host will be installed. The most common RPMs you will see are:     RPM  Purpose      gratia-probe-common  Code shared between all Graita probes    gratia-probe-condor  The probe that tracks Condor usage    gratia-probe-slurm  The probe that tracks SLURM usage    gratia-probe-pbs-lsf  The probe that tracks PBS and/or LSF usage    gratia-probe-gridftp-transfer  The probe that tracks transfers done with GridFTP", 
            "title": "Reference: Important Gratia files"
        }, 
        {
            "location": "/other/schedd-filebeats/", 
            "text": "Warning\n\n\nThis is a technology preview document and will probably change content and location withouth notice.\n\n\n\n\nInstallation of FileBeats for Submit nodes\n\n\nThis document is for frontend administrators. It describes the installation of \nFilebeats\n to continuously upload the HTCondor submit host transfer log to Elastic Search.\n\n\nIntroduction\n\n\nA submit host (HTCondor schedd) is a login node where users submit jobs to the Grid. One interesting log that it produces is the TransferLog. The TransferLogs report all the transfers of files between compute node and submit nodes. In this guide we describe the installation of Filebeats to upload this log to Elastic Search.\n\n\nInstallation\n\n\nFileBeat Installation\n\n\nFor the installation of filebeats follow the  official instruction to set up the repositories and install filebeats as described \nhere\n.\n\n\nConfiguration\n\n\nConfiguration of Filebeats\n\n\nThe configuration of filebeats revolves around this file \n/etc/filebeat/filebeat.yml\n. Bellow are the steps to modify the different sections of this file\n\n\n\n\n\n\nThe \nFilebeat Prospectors\n section, the input should look like this:\n\n\nfilebeat\n.\nprospectors\n:\n\n\n-\n \ntype\n:\n \nlog\n\n  \nenabled\n:\n \ntrue\n\n  \npaths\n:\n\n    \n-\n \n/\nvar\n/\nlog\n/\ncondor\n/\nXferStatsLog\n\n\n\n\n\n\n\n\n\n\nThe output logstash section should look like:\n\n\n#\n-----------------------------\n \nLogstash\n \noutput\n \n--------------------------------\n\n\noutput\n.\nlogstash\n:\n  # \nThe\n \nLogstash\n \nhosts\n\n  \nhosts\n: [\ngracc.opensciencegrid.org:6938\n]\n  # \nOptional\n \nSSL\n. \nBy\n \ndefault\n \nis\n \noff\n. \n  # \nList\n \nof\n \nroot\n \ncertificates\n \nfor\n \nHTTPS\n \nserver\n \nverifications\n\n  \nssl\n.\ncertificate_authorities\n: [\n/etc/grid-security/certificates/InCommon-IGTF-Server-CA.pem\n]\n  #  \nCertificate\n \nfor\n \nSSL\n \nclient\n \nauthentication\n\n  \nssl\n.\ncertificate\n: \n/etc/grid-security/hostcert.pem\n\n  # \nClient\n \nCertificate\n \nKey\n\n  \nssl\n.\nkey\n: \n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\n\n\nComment out all of the \nElasticsearch output\n since we are using LogStash\n\n\n#\n--------------------------\n \nElasticsearch\n \noutput\n \n------------------------------\n\n#\noutput\n.\nelasticsearch\n:\n# \nArray\n \nof\n \nhosts\n \nto\n \nconnect\n \nto\n.\n#\nhosts\n: [\nlocalhost:9200\n]\n\n# \nOptional\n \nprotocol\n \nand\n \nbasic\n \nauth\n \ncredentials\n.\n#\nprotocol\n: \nhttps\n\n#\nusername\n: \nelastic\n\n#\npassword\n: \nchangeme\n\n\n\n\n\n\n\n\n\n\nThe general section should look like this, where \nhostname\n should be replaced by the hostname of the machine you are installing filebeats on.\n\n\n#================================\n \nGeneral\n \n=====================================\n\n\nname\n:\n \n%\nRED\n%\nhostname\n%\nENDCOLOR\n%\n\n\ntags\n:\n \n[\nxfer-log\n]\n\n\n\n\n\n\n\n\n\n\nTest that the configuration is correct by running:\n\n\nroot@host #\n filebeat -configtest -e\n\n\n\n\n\n\n\n\n\nStart the filebeats services:\n\n\nroot@host #\n service filebeat start\n\n\n\n\n\n\n\n\n\nConfiguration of HTCondor\n\n\nFor the configuration of the HTCondor submit host to use the TransferLog follow the next instructions:\n\n\n\n\nNote\n\n\nThe transfer metrics was introduced in HTCondor 8.6 series. You need to be running a version equal or greater than 8.6.1 to enable it.\n\n\n\n\n\n\n\n\nCreate a file named \n/etc/condor/config.d/50-transferLog.config\n with the following contents:\n\n\nSHADOW_DEBUG\n \n=\n D_STATS\n\nSHADOW_STATS_LOG\n \n=\n \n$(\nLOG\n)\n/XferStatsLog\n\nSTARTER_STATS_LOG\n \n=\n \n$(\nLOG\n)\n/XferStatsLog\n\n\n\n\n\n\n\n\n\nReconfigure condor:\n\n\nroot@host #\n condor_reconfig\n\n\n\n\n\n\n\n\n\nMake sure that after a couple of minutes the new log \n/var/log/condor/XferStatsLog\n is present.", 
            "title": "Install Transfer Log Filebeats"
        }, 
        {
            "location": "/other/schedd-filebeats/#installation-of-filebeats-for-submit-nodes", 
            "text": "This document is for frontend administrators. It describes the installation of  Filebeats  to continuously upload the HTCondor submit host transfer log to Elastic Search.", 
            "title": "Installation of FileBeats for Submit nodes"
        }, 
        {
            "location": "/other/schedd-filebeats/#introduction", 
            "text": "A submit host (HTCondor schedd) is a login node where users submit jobs to the Grid. One interesting log that it produces is the TransferLog. The TransferLogs report all the transfers of files between compute node and submit nodes. In this guide we describe the installation of Filebeats to upload this log to Elastic Search.", 
            "title": "Introduction"
        }, 
        {
            "location": "/other/schedd-filebeats/#installation", 
            "text": "", 
            "title": "Installation"
        }, 
        {
            "location": "/other/schedd-filebeats/#filebeat-installation", 
            "text": "For the installation of filebeats follow the  official instruction to set up the repositories and install filebeats as described  here .", 
            "title": "FileBeat Installation"
        }, 
        {
            "location": "/other/schedd-filebeats/#configuration", 
            "text": "", 
            "title": "Configuration"
        }, 
        {
            "location": "/other/schedd-filebeats/#configuration-of-filebeats", 
            "text": "The configuration of filebeats revolves around this file  /etc/filebeat/filebeat.yml . Bellow are the steps to modify the different sections of this file    The  Filebeat Prospectors  section, the input should look like this:  filebeat . prospectors :  -   type :   log \n   enabled :   true \n   paths : \n     -   / var / log / condor / XferStatsLog     The output logstash section should look like:  # -----------------------------   Logstash   output   --------------------------------  output . logstash :\n  #  The   Logstash   hosts \n   hosts : [ gracc.opensciencegrid.org:6938 ]\n  #  Optional   SSL .  By   default   is   off . \n  #  List   of   root   certificates   for   HTTPS   server   verifications \n   ssl . certificate_authorities : [ /etc/grid-security/certificates/InCommon-IGTF-Server-CA.pem ]\n  #   Certificate   for   SSL   client   authentication \n   ssl . certificate :  /etc/grid-security/hostcert.pem \n  #  Client   Certificate   Key \n   ssl . key :  /etc/grid-security/hostkey.pem     Comment out all of the  Elasticsearch output  since we are using LogStash  # --------------------------   Elasticsearch   output   ------------------------------ \n# output . elasticsearch :\n#  Array   of   hosts   to   connect   to .\n# hosts : [ localhost:9200 ]\n\n#  Optional   protocol   and   basic   auth   credentials .\n# protocol :  https \n# username :  elastic \n# password :  changeme     The general section should look like this, where  hostname  should be replaced by the hostname of the machine you are installing filebeats on.  #================================   General   =====================================  name :   % RED % hostname % ENDCOLOR %  tags :   [ xfer-log ]     Test that the configuration is correct by running:  root@host #  filebeat -configtest -e    Start the filebeats services:  root@host #  service filebeat start", 
            "title": "Configuration of Filebeats"
        }, 
        {
            "location": "/other/schedd-filebeats/#configuration-of-htcondor", 
            "text": "For the configuration of the HTCondor submit host to use the TransferLog follow the next instructions:   Note  The transfer metrics was introduced in HTCondor 8.6 series. You need to be running a version equal or greater than 8.6.1 to enable it.     Create a file named  /etc/condor/config.d/50-transferLog.config  with the following contents:  SHADOW_DEBUG   =  D_STATS SHADOW_STATS_LOG   =   $( LOG ) /XferStatsLog STARTER_STATS_LOG   =   $( LOG ) /XferStatsLog    Reconfigure condor:  root@host #  condor_reconfig    Make sure that after a couple of minutes the new log  /var/log/condor/XferStatsLog  is present.", 
            "title": "Configuration of HTCondor"
        }, 
        {
            "location": "/release/notes/", 
            "text": "Release Notes\n\n\n\n\nNote\n\n\nThe 3.3 series was \nend-of-lifed in May 2018\n.\nPlease \nupgrade to the 3.4 series\n at your earliest convenience.\n\n\n\n\nOSG 3.4\n\n\n\n\n\n\n\n\nVersion\n\n\nDate\n\n\nSummary\n\n\n\n\n\n\n\n\n\n\n3.4.31\n\n\n2019-06-13\n\n\nSingularity 3.2.1, GlideinWMS 3.4.5-2, HTCondor 8.6.13-1.4, VO Package v93; Upcoming: HTCondor 8.8.3\n\n\n\n\n\n\n3.4.30-2\n\n\n2019-05-30\n\n\nIGTF 1.99, VO Package v92\n\n\n\n\n\n\n3.4.30\n\n\n2019-05-16\n\n\nBLAHP 1.81.41, VO Package V91, xrootd-voms-plugin 0.6.0, osg-pki-tools 3.3.0; Upcoming: Singularity 3.1.1-1.1, osg-se-hadoop, BLAHP 1.18.41\n\n\n\n\n\n\n3.4.29-2\n\n\n2019-05-07\n\n\nVO Package v90\n\n\n\n\n\n\n3.4.29\n\n\n2019-05-02\n\n\nXCache 1.0.5, MyProxy 6.2.3\n\n\n\n\n\n\n3.4.28-2\n\n\n2019-04-30\n\n\nIGTF 1.98\n\n\n\n\n\n\n3.4.28\n\n\n2019-04-25\n\n\nXRootD 4.9.1, xrootd-hdfs 2.1.4, GlideinWMS 3.4.5, osg-flock 1.1, VO Package v89; Upcoming: HTCondor 8.8.2\n\n\n\n\n\n\n3.4.27-2\n\n\n2019-04-16\n\n\nVO Package v88\n\n\n\n\n\n\n3.4.27\n\n\n2019-04-11\n\n\nGlobus GridFTP uses GCT, CVMFS 2.6.0, HTCondor-CE 3.2.2, cctools 7.0.11, osg-pki-tools 3.2.2; Upcoming: Singularity 3.1.1\n\n\n\n\n\n\n3.4.26-2\n\n\n2019-04-02\n\n\nIGTF 1.97, VO Package v87\n\n\n\n\n\n\n3.4.26\n\n\n2019-03-14\n\n\ncctools 7.0.9, Pegasus 4.9.1, osg-pki-tools 3.1.0; Upcoming: Singularity 3.1.0\n\n\n\n\n\n\n3.4.25\n\n\n2019-03-07\n\n\ngsi-openssh 7.4p1, HTCondor 8.6.13 patched, xrootd-lcmaps 1.7.0; Upcoming: HTCondor 8.8.1\n\n\n\n\n\n\n3.4.24-2\n\n\n2019-03-05\n\n\nIGTF 1.96, VO Package v86\n\n\n\n\n\n\n3.4.24\n\n\n2019-02-21\n\n\nBLAHP 1.18.39, osg-pki-tools 3.0.1, HTCondor-CE 3.2.1, condor-cron 1.14.1; Upcoming: Singularity 3.0.3, HDFS on EL6\n\n\n\n\n\n\n3.4.23\n\n\n2019-01-23\n\n\nGratia probes 1.20.8; Upcoming: Singularity 3.0.2, HTCondor 8.8.0\n\n\n\n\n\n\n3.4.22-2\n\n\n2019-01-14\n\n\nIGTF 1.95\n\n\n\n\n\n\n3.4.22\n\n\n2018-12-20\n\n\nHTCondor-CE 3.2.0, frontier-squid 4.4, Pegasus 4.9.0, CVMFS 2.5.2, IGTF 1.94, VO Package v85\n\n\n\n\n\n\n3.4.21\n\n\n2018-12-12\n\n\nHigh Priority Release: Singularity 2.6.1\n\n\n\n\n\n\n3.4.20\n\n\n2018-11-01\n\n\nGlideinWM 3.4.2, SciTokens 1.2.1, stashcache 0.10, stashcache-client 5.1.0-4, HTCondor 8.6.13; Upcoming: HTCondor 8.7.10\n\n\n\n\n\n\n3.4.19\n\n\n2018-10-25\n\n\nPatched tarball, XRootD 4.8.5, osg-flock 1.0, stashcache 0.9, autopyfactory 2.4.9\n\n\n\n\n\n\n3.4.18-2\n\n\n2018-10-03\n\n\nVO Package v84\n\n\n\n\n\n\n3.4.18\n\n\n2018-09-27\n\n\nXRootD 4.8.4 + HTTP Patches, xrootd-lcmaps 1.4.1, xrootd-hdfs 2.1.3, HTCondor CE 3.1.4, CernVM-FS 2.5.1, Gratia probes 1.20.7, Pegasus 4.8.4, GlideinWMS 3.4, RSV 3.19.8, BLAHP 1.18.38\n\n\n\n\n\n\n3.4.17-3\n\n\n2018-09-26\n\n\nIGTF 1.93\n\n\n\n\n\n\n3.4.17-2\n\n\n2018-09-13\n\n\nVO Package v83\n\n\n\n\n\n\n3.4.17\n\n\n2018-08-16\n\n\nSingularity 2.6.0, HTCondor 8.6.12, Pegasus 4.8.3, HTCondor-CE 3.1.3, xrootd-lcmaps 1.4.0; Upcoming: HTCondor 8.7.9\n\n\n\n\n\n\n3.4.16-2\n\n\n2018-08-08\n\n\nCILogon OpenID Certification Authority Certificate\n\n\n\n\n\n\n3.4.16\n\n\n2018-08-01\n\n\nFrontier Squid 3.5.27-5.1, XRootD 4.8.4, SciTokens 1.2.0\n\n\n\n\n\n\n3.4.15\n\n\n2018-07-06\n\n\nHigh Priority Release: Singularity 2.5.2\n\n\n\n\n\n\n3.4.14-2\n\n\n2018-07-05\n\n\nIGTF 1.92\n\n\n\n\n\n\n3.4.14\n\n\n2018-07-02\n\n\nHDFS 2.6(EL7), osg-configure 2.3.1, RSV 3.19.7, lcmaps-plugins-voms 1.7.1-1.6, Gratia probes 1.20.3, osg-pki-tools 3.0.0, GridFTP-HDFS 1.1.1-1.2(EL7), lcmaps-plugins-verify-proxy 1.5.11, OWAMP 3.5.6 BLAHP 1.18.37; Upcoming: GlideinWMS 3.4, BLAHP 1.18.37\n\n\n\n\n\n\n3.4.13\n\n\n2018-06-12\n\n\nCVMFS 2.5.0, HTCondor 8.6.11, Singularity 2.5.1, Pegasus 4.8.2, voms-proxy-direct; Upcoming: HTCondor 8.7.8\n\n\n\n\n\n\n3.4.12-3\n\n\n2018-05-21\n\n\nIGTF 1.91\n\n\n\n\n\n\n3.4.12-2\n\n\n2018-05-10\n\n\nAdded Let's Encrypt CA, VO Package v79\n\n\n\n\n\n\n3.4.12\n\n\n2018-05-10\n\n\nHTCondor-CE 3.1.2, GlideinWMS 3.2.22.2, XRootD 4.8.3, Gratia probes 1.20, RSV 3.18; Upcoming: GlideinWMS 3.3.3\n\n\n\n\n\n\n3.4.11\n\n\n2018-05-01\n\n\nHigh Priority Release: Singularity 2.5.0\n\n\n\n\n\n\n3.4.10\n\n\n2018-04-18\n\n\nSingularity 2.4.6, HTCondor-CE 3.1.1, HTCondor 8.6.10, gigetcert 1.16, BLAHP 1.18.36, xrootd-lcmaps 1.2.1-3, osg-configure 2.2.4; Upcoming: HTCondor 8.7.7, xrootd-hdfs 2.0.2\n\n\n\n\n\n\n3.4.9-2\n\n\n2018-04-05\n\n\nIGTF 1.90, VO Package v78\n\n\n\n\n\n\n3.4.9\n\n\n2018-03-08\n\n\nXRootD 4.8.1, GlideinWMS 3.2.21, Frontier Squid 3.5.27-3, RSV 3.17.0, osg-release 3.4-4\n\n\n\n\n\n\n3.4.8\n\n\n2018-02-08\n\n\nGlideinWMS 3.2.20-2\n\n\n\n\n\n\n3.4.7\n\n\n2018-02-01\n\n\nSingularity 2.4.2, Pegasus 4.8.1, gratia-probe 1.19.0, perfsonar-tools 4.0.1, HTCondor 8.6.9, frontier-squid 3.5.27-2.1, osg-pki-tools 2.1.4; Upcoming: HDFS 2.6, HTCondor 8.7.6\n\n\n\n\n\n\n3.4.6-2\n\n\n2018-01-24\n\n\nIGTF 1.89\n\n\n\n\n\n\n3.4.6\n\n\n2017-12-21\n\n\nXRootD 4.8.0, CernVM-FS 2.4.4, GlideinWMS 3.2.20, osg-pki-tools 2.1.2, HTCondor-CE 3.0.4, osg-configure 2.2.3\n\n\n\n\n\n\n3.4.5-4\n\n\n2017-12-20\n\n\nVO Package v77\n\n\n\n\n\n\n3.4.5-3\n\n\n2017-11-29\n\n\nIGTF 1.88\n\n\n\n\n\n\n3.4.5-2\n\n\n2017-11-20\n\n\nVO Package v76\n\n\n\n\n\n\n3.4.5\n\n\n2017-11-14\n\n\nosg-pki-tools 2.0.0, BLAHP 1.18.34, HTCondor 8.6.8, XRootD 4.7.1, CMVFS 2.4.2 globus-gridftp-server 12.2-1.2, globus-gridftp-server-control 6.0, RSV 3.16.0; Upcoming: HTCondor 8.7.5\n\n\n\n\n\n\n3.4.4-3\n\n\n2017-11-01\n\n\nIGTF 1.87\n\n\n\n\n\n\n3.4.4-2\n\n\n2017-10-11\n\n\nIGTF 1.86, VO Package v75\n\n\n\n\n\n\n3.4.4\n\n\n2017-10-10\n\n\ngsi-openssh 7.3p1c, Singularity 2.3.2, HTCondor 8.6.6, globus-gridftp-server-control 5.2, osg-ca-scripts 1.1.7-2, osg-configure 2.2.1; Upcoming: HTCondor 8.7.3\n\n\n\n\n\n\n3.4.3\n\n\n2017-09-12\n\n\nCVMFS 2.4.1, Singularity 2.3.1, BLAHP 1.18.33, XRootD 4.7.0, StashCache 0.8, Globus update, osg-ca-scripts 1.1.7, globus-gridftp-osg-extensions 0.4, xrootd-lcmaps 1.3.4, HTCondor-CE 3.0.2\n\n\n\n\n\n\n3.4.2-2\n\n\n2017-08-14\n\n\nIGTF 1.85\n\n\n\n\n\n\n3.4.2\n\n\n2017-08-08\n\n\nHTCondor 8.6.5, HTCondor-CE 3.0.1, condor-cron 1.1.3, osg-configure 2.1.1, BLAHP 1.18.32, osg-ce 3.4-3\n\n\n\n\n\n\n3.4.1-2\n\n\n2017-07-13\n\n\nIGTF 1.84\n\n\n\n\n\n\n3.4.1\n\n\n2017-07-12\n\n\nlcmaps-plugins-verify-proxy 1.5.9-1.2, osg-configure 2.1.0, BLAHP 1.18.30, HTCondor-CE 2.2.1, Gratia probes 1.18.1, CVMFS 2.3.5, globus-gridftp-server 11.8, gridftp-dsi-posix 1.4, HTCondor 8.6.4; Upcoming: HTCondor 8.7.2\n\n\n\n\n\n\n3.4.0-2\n\n\n2017-06-15\n\n\nIGTF 1.83, VO Package v74\n\n\n\n\n\n\n3.4.0\n\n\n2017-06-14\n\n\nHTCondor 8.6.3, Frontier-squid 3.5.24-3.1, lcmaps-plugins-voms 1.7.1, XRootD 4.6.1, GlideinWMS 3.2.19, HTCondor CE 2.2.0, voms-admin-server 1.7.0-1.22, osg-configure 2.0.0, osg-ca-scripts 1.1.6; Upcoming: GlideinWMS 3.3.2", 
            "title": "Release Note Overview"
        }, 
        {
            "location": "/release/notes/#release-notes", 
            "text": "Note  The 3.3 series was  end-of-lifed in May 2018 .\nPlease  upgrade to the 3.4 series  at your earliest convenience.", 
            "title": "Release Notes"
        }, 
        {
            "location": "/release/notes/#osg-34", 
            "text": "Version  Date  Summary      3.4.31  2019-06-13  Singularity 3.2.1, GlideinWMS 3.4.5-2, HTCondor 8.6.13-1.4, VO Package v93; Upcoming: HTCondor 8.8.3    3.4.30-2  2019-05-30  IGTF 1.99, VO Package v92    3.4.30  2019-05-16  BLAHP 1.81.41, VO Package V91, xrootd-voms-plugin 0.6.0, osg-pki-tools 3.3.0; Upcoming: Singularity 3.1.1-1.1, osg-se-hadoop, BLAHP 1.18.41    3.4.29-2  2019-05-07  VO Package v90    3.4.29  2019-05-02  XCache 1.0.5, MyProxy 6.2.3    3.4.28-2  2019-04-30  IGTF 1.98    3.4.28  2019-04-25  XRootD 4.9.1, xrootd-hdfs 2.1.4, GlideinWMS 3.4.5, osg-flock 1.1, VO Package v89; Upcoming: HTCondor 8.8.2    3.4.27-2  2019-04-16  VO Package v88    3.4.27  2019-04-11  Globus GridFTP uses GCT, CVMFS 2.6.0, HTCondor-CE 3.2.2, cctools 7.0.11, osg-pki-tools 3.2.2; Upcoming: Singularity 3.1.1    3.4.26-2  2019-04-02  IGTF 1.97, VO Package v87    3.4.26  2019-03-14  cctools 7.0.9, Pegasus 4.9.1, osg-pki-tools 3.1.0; Upcoming: Singularity 3.1.0    3.4.25  2019-03-07  gsi-openssh 7.4p1, HTCondor 8.6.13 patched, xrootd-lcmaps 1.7.0; Upcoming: HTCondor 8.8.1    3.4.24-2  2019-03-05  IGTF 1.96, VO Package v86    3.4.24  2019-02-21  BLAHP 1.18.39, osg-pki-tools 3.0.1, HTCondor-CE 3.2.1, condor-cron 1.14.1; Upcoming: Singularity 3.0.3, HDFS on EL6    3.4.23  2019-01-23  Gratia probes 1.20.8; Upcoming: Singularity 3.0.2, HTCondor 8.8.0    3.4.22-2  2019-01-14  IGTF 1.95    3.4.22  2018-12-20  HTCondor-CE 3.2.0, frontier-squid 4.4, Pegasus 4.9.0, CVMFS 2.5.2, IGTF 1.94, VO Package v85    3.4.21  2018-12-12  High Priority Release: Singularity 2.6.1    3.4.20  2018-11-01  GlideinWM 3.4.2, SciTokens 1.2.1, stashcache 0.10, stashcache-client 5.1.0-4, HTCondor 8.6.13; Upcoming: HTCondor 8.7.10    3.4.19  2018-10-25  Patched tarball, XRootD 4.8.5, osg-flock 1.0, stashcache 0.9, autopyfactory 2.4.9    3.4.18-2  2018-10-03  VO Package v84    3.4.18  2018-09-27  XRootD 4.8.4 + HTTP Patches, xrootd-lcmaps 1.4.1, xrootd-hdfs 2.1.3, HTCondor CE 3.1.4, CernVM-FS 2.5.1, Gratia probes 1.20.7, Pegasus 4.8.4, GlideinWMS 3.4, RSV 3.19.8, BLAHP 1.18.38    3.4.17-3  2018-09-26  IGTF 1.93    3.4.17-2  2018-09-13  VO Package v83    3.4.17  2018-08-16  Singularity 2.6.0, HTCondor 8.6.12, Pegasus 4.8.3, HTCondor-CE 3.1.3, xrootd-lcmaps 1.4.0; Upcoming: HTCondor 8.7.9    3.4.16-2  2018-08-08  CILogon OpenID Certification Authority Certificate    3.4.16  2018-08-01  Frontier Squid 3.5.27-5.1, XRootD 4.8.4, SciTokens 1.2.0    3.4.15  2018-07-06  High Priority Release: Singularity 2.5.2    3.4.14-2  2018-07-05  IGTF 1.92    3.4.14  2018-07-02  HDFS 2.6(EL7), osg-configure 2.3.1, RSV 3.19.7, lcmaps-plugins-voms 1.7.1-1.6, Gratia probes 1.20.3, osg-pki-tools 3.0.0, GridFTP-HDFS 1.1.1-1.2(EL7), lcmaps-plugins-verify-proxy 1.5.11, OWAMP 3.5.6 BLAHP 1.18.37; Upcoming: GlideinWMS 3.4, BLAHP 1.18.37    3.4.13  2018-06-12  CVMFS 2.5.0, HTCondor 8.6.11, Singularity 2.5.1, Pegasus 4.8.2, voms-proxy-direct; Upcoming: HTCondor 8.7.8    3.4.12-3  2018-05-21  IGTF 1.91    3.4.12-2  2018-05-10  Added Let's Encrypt CA, VO Package v79    3.4.12  2018-05-10  HTCondor-CE 3.1.2, GlideinWMS 3.2.22.2, XRootD 4.8.3, Gratia probes 1.20, RSV 3.18; Upcoming: GlideinWMS 3.3.3    3.4.11  2018-05-01  High Priority Release: Singularity 2.5.0    3.4.10  2018-04-18  Singularity 2.4.6, HTCondor-CE 3.1.1, HTCondor 8.6.10, gigetcert 1.16, BLAHP 1.18.36, xrootd-lcmaps 1.2.1-3, osg-configure 2.2.4; Upcoming: HTCondor 8.7.7, xrootd-hdfs 2.0.2    3.4.9-2  2018-04-05  IGTF 1.90, VO Package v78    3.4.9  2018-03-08  XRootD 4.8.1, GlideinWMS 3.2.21, Frontier Squid 3.5.27-3, RSV 3.17.0, osg-release 3.4-4    3.4.8  2018-02-08  GlideinWMS 3.2.20-2    3.4.7  2018-02-01  Singularity 2.4.2, Pegasus 4.8.1, gratia-probe 1.19.0, perfsonar-tools 4.0.1, HTCondor 8.6.9, frontier-squid 3.5.27-2.1, osg-pki-tools 2.1.4; Upcoming: HDFS 2.6, HTCondor 8.7.6    3.4.6-2  2018-01-24  IGTF 1.89    3.4.6  2017-12-21  XRootD 4.8.0, CernVM-FS 2.4.4, GlideinWMS 3.2.20, osg-pki-tools 2.1.2, HTCondor-CE 3.0.4, osg-configure 2.2.3    3.4.5-4  2017-12-20  VO Package v77    3.4.5-3  2017-11-29  IGTF 1.88    3.4.5-2  2017-11-20  VO Package v76    3.4.5  2017-11-14  osg-pki-tools 2.0.0, BLAHP 1.18.34, HTCondor 8.6.8, XRootD 4.7.1, CMVFS 2.4.2 globus-gridftp-server 12.2-1.2, globus-gridftp-server-control 6.0, RSV 3.16.0; Upcoming: HTCondor 8.7.5    3.4.4-3  2017-11-01  IGTF 1.87    3.4.4-2  2017-10-11  IGTF 1.86, VO Package v75    3.4.4  2017-10-10  gsi-openssh 7.3p1c, Singularity 2.3.2, HTCondor 8.6.6, globus-gridftp-server-control 5.2, osg-ca-scripts 1.1.7-2, osg-configure 2.2.1; Upcoming: HTCondor 8.7.3    3.4.3  2017-09-12  CVMFS 2.4.1, Singularity 2.3.1, BLAHP 1.18.33, XRootD 4.7.0, StashCache 0.8, Globus update, osg-ca-scripts 1.1.7, globus-gridftp-osg-extensions 0.4, xrootd-lcmaps 1.3.4, HTCondor-CE 3.0.2    3.4.2-2  2017-08-14  IGTF 1.85    3.4.2  2017-08-08  HTCondor 8.6.5, HTCondor-CE 3.0.1, condor-cron 1.1.3, osg-configure 2.1.1, BLAHP 1.18.32, osg-ce 3.4-3    3.4.1-2  2017-07-13  IGTF 1.84    3.4.1  2017-07-12  lcmaps-plugins-verify-proxy 1.5.9-1.2, osg-configure 2.1.0, BLAHP 1.18.30, HTCondor-CE 2.2.1, Gratia probes 1.18.1, CVMFS 2.3.5, globus-gridftp-server 11.8, gridftp-dsi-posix 1.4, HTCondor 8.6.4; Upcoming: HTCondor 8.7.2    3.4.0-2  2017-06-15  IGTF 1.83, VO Package v74    3.4.0  2017-06-14  HTCondor 8.6.3, Frontier-squid 3.5.24-3.1, lcmaps-plugins-voms 1.7.1, XRootD 4.6.1, GlideinWMS 3.2.19, HTCondor CE 2.2.0, voms-admin-server 1.7.0-1.22, osg-configure 2.0.0, osg-ca-scripts 1.1.6; Upcoming: GlideinWMS 3.3.2", 
            "title": "OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/", 
            "text": "OSG Software Release 3.4.31\n\n\nRelease Date\n: 2019-06-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nNote\n\n\nThis release contains a major upgrade to Singularity.\nBe on the lookout for any .rpmnew files and merge in any local changes.\nThere are numerous additions to the configuration.\n\n\n\n\n\n\nSingularity 3.2.1\n: Major upgrade from version 2.6.1 (See note above)\n\n\nGlideinWMS 3.4.5-2: Fix problems with the proxy renewal service\n\n\nHTCondor 8.6.13-1.4: Fix problems when upgrading condor-python\n\n\nVO Package v93\n: Add in sPHENIX VO\n\n\nUpcoming repository:\n\n\nHTCondor 8.8.3\n\n\nFixed a bug where jobs were killed instead of peacefully shutting down\n\n\nFixed a bug where a restarted schedd wouldn't connect to its running jobs\n\n\nImproved file transfer performance when sending multiple files\n\n\nFix a bug that prevented interactive submit from working with Singularity\n\n\nOrphaned Docker containers are now cleaned up on execute nodes\n\n\nRestored a deprecated Python interface that is used to read the event log\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nNotes\n\n\nThis section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\nGlideinWMS\n\n\n\n\n\n\nGlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).\n\n\n\n\n\n\nFor new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be \n= 3.4.1.\n\n\n\n\nNote\n\n\nOSG GlideinWMS factories are running at least 3.4.1\n\n\n\n\nIf some of the connected Factories are \n= 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:\n\n\n\n\nhttps://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars\n\n\n\n\n\n\n\n\nUpgrades from \n= 3.4.0 may require merging \n/etc/condor/config.d/*.rpmnew\n files and a restart of HTCondor.\n\n\n\n\n\n\nGlideinWMS \n= 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n   \n/etc/gwms-frontend/frontend.xml\n, changing the address to include the shared port sinful string:\n\n\ncollector\n \nDN\n=\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain\n \ngroup\n=\ndefault\n \nnode\n=\ngwms-frontend.domain:9618?sock=collector0-40\n \nsecondary\n=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\nReplacing \ngwms-frontend-domain\n with the hostname of your GlideinWMS frontend.\n   See the \nGlideinWMS documentation\n for details.\n\n\nKnown Issues\n\n\nNone.\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.13-1.4.osg34.el6\n\n\nglideinwms-3.4.5-2.osg34.el6\n\n\nosg-version-3.4.31-1.osg34.el6\n\n\nsingularity-3.2.1-1.osg34.el6\n\n\nvo-client-93-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.13-1.4.osg34.el7\n\n\nglideinwms-3.4.5-2.osg34.el7\n\n\nosg-version-3.4.31-1.osg34.el7\n\n\nsingularity-3.2.1-1.osg34.el7\n\n\nvo-client-93-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nversion\n \npython2\n-\ncondor\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n31\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n93\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n93\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n93\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n31\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n93\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n93\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n93\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.8.3-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.8.3-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nminicondor\n \npython2\n-\ncondor\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\nminicondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\nminicondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n3\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.31"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#osg-software-release-3431", 
            "text": "Release Date : 2019-06-13", 
            "title": "OSG Software Release 3.4.31"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#summary-of-changes", 
            "text": "This release contains:   Note  This release contains a major upgrade to Singularity.\nBe on the lookout for any .rpmnew files and merge in any local changes.\nThere are numerous additions to the configuration.    Singularity 3.2.1 : Major upgrade from version 2.6.1 (See note above)  GlideinWMS 3.4.5-2: Fix problems with the proxy renewal service  HTCondor 8.6.13-1.4: Fix problems when upgrading condor-python  VO Package v93 : Add in sPHENIX VO  Upcoming repository:  HTCondor 8.8.3  Fixed a bug where jobs were killed instead of peacefully shutting down  Fixed a bug where a restarted schedd wouldn't connect to its running jobs  Improved file transfer performance when sending multiple files  Fix a bug that prevented interactive submit from working with Singularity  Orphaned Docker containers are now cleaned up on execute nodes  Restored a deprecated Python interface that is used to read the event log       These  JIRA tickets  were addressed in this release.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#notes", 
            "text": "This section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found  here .   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7", 
            "title": "Notes"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#glideinwms", 
            "text": "GlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).    For new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be  = 3.4.1.   Note  OSG GlideinWMS factories are running at least 3.4.1   If some of the connected Factories are  = 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:   https://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars     Upgrades from  = 3.4.0 may require merging  /etc/condor/config.d/*.rpmnew  files and a restart of HTCondor.    GlideinWMS  = 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n    /etc/gwms-frontend/frontend.xml , changing the address to include the shared port sinful string:  collector   DN = /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain   group = default   node = gwms-frontend.domain:9618?sock=collector0-40   secondary = True /     Replacing  gwms-frontend-domain  with the hostname of your GlideinWMS frontend.\n   See the  GlideinWMS documentation  for details.", 
            "title": "GlideinWMS"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#known-issues", 
            "text": "None.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-6", 
            "text": "condor-8.6.13-1.4.osg34.el6  glideinwms-3.4.5-2.osg34.el6  osg-version-3.4.31-1.osg34.el6  singularity-3.2.1-1.osg34.el6  vo-client-93-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-7", 
            "text": "condor-8.6.13-1.4.osg34.el7  glideinwms-3.4.5-2.osg34.el7  osg-version-3.4.31-1.osg34.el7  singularity-3.2.1-1.osg34.el7  vo-client-93-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   igtf - ca - certs   osg - ca - certs   osg - version   python2 - condor   singularity   singularity - debuginfo   vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - all - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - bosco - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - classads - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - classads - devel - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - cream - gahp - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - debuginfo - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - kbdd - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - procd - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - std - universe - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - test - 8 . 6 . 13 - 1 . 4 . osg34 . el6  condor - vm - gahp - 8 . 6 . 13 - 1 . 4 . osg34 . el6  glideinwms - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - common - tools - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - condor - common - config - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - factory - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - factory - condor - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - libs - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - minimal - condor - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - usercollector - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - userschedd - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - vofrontend - 3 . 4 . 5 - 2 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 4 . 5 - 2 . osg34 . el6  osg - version - 3 . 4 . 31 - 1 . osg34 . el6  python2 - condor - 8 . 6 . 13 - 1 . 4 . osg34 . el6  singularity - 3 . 2 . 1 - 1 . osg34 . el6  singularity - debuginfo - 3 . 2 . 1 - 1 . osg34 . el6  vo - client - 93 - 1 . osg34 . el6  vo - client - dcache - 93 - 1 . osg34 . el6  vo - client - lcmaps - voms - 93 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - all - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - bosco - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - classads - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - classads - devel - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - cream - gahp - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - debuginfo - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - kbdd - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - procd - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - test - 8 . 6 . 13 - 1 . 4 . osg34 . el7  condor - vm - gahp - 8 . 6 . 13 - 1 . 4 . osg34 . el7  glideinwms - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - common - tools - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - condor - common - config - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - factory - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - factory - condor - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - libs - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - minimal - condor - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - usercollector - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - userschedd - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - vofrontend - 3 . 4 . 5 - 2 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 4 . 5 - 2 . osg34 . el7  osg - version - 3 . 4 . 31 - 1 . osg34 . el7  python2 - condor - 8 . 6 . 13 - 1 . 4 . osg34 . el7  singularity - 3 . 2 . 1 - 1 . osg34 . el7  singularity - debuginfo - 3 . 2 . 1 - 1 . osg34 . el7  vo - client - 93 - 1 . osg34 . el7  vo - client - dcache - 93 - 1 . osg34 . el7  vo - client - lcmaps - voms - 93 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-6_2", 
            "text": "condor-8.8.3-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-7_2", 
            "text": "condor-8.8.3-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   minicondor   python2 - condor   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-6_3", 
            "text": "condor - 8 . 8 . 3 - 1 . osgup . el6  condor - all - 8 . 8 . 3 - 1 . osgup . el6  condor - annex - ec2 - 8 . 8 . 3 - 1 . osgup . el6  condor - bosco - 8 . 8 . 3 - 1 . osgup . el6  condor - classads - 8 . 8 . 3 - 1 . osgup . el6  condor - classads - devel - 8 . 8 . 3 - 1 . osgup . el6  condor - cream - gahp - 8 . 8 . 3 - 1 . osgup . el6  condor - debuginfo - 8 . 8 . 3 - 1 . osgup . el6  condor - kbdd - 8 . 8 . 3 - 1 . osgup . el6  condor - procd - 8 . 8 . 3 - 1 . osgup . el6  condor - std - universe - 8 . 8 . 3 - 1 . osgup . el6  condor - test - 8 . 8 . 3 - 1 . osgup . el6  condor - vm - gahp - 8 . 8 . 3 - 1 . osgup . el6  minicondor - 8 . 8 . 3 - 1 . osgup . el6  python2 - condor - 8 . 8 . 3 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-31/#enterprise-linux-7_3", 
            "text": "condor - 8 . 8 . 3 - 1 . osgup . el7  condor - all - 8 . 8 . 3 - 1 . osgup . el7  condor - annex - ec2 - 8 . 8 . 3 - 1 . osgup . el7  condor - bosco - 8 . 8 . 3 - 1 . osgup . el7  condor - classads - 8 . 8 . 3 - 1 . osgup . el7  condor - classads - devel - 8 . 8 . 3 - 1 . osgup . el7  condor - cream - gahp - 8 . 8 . 3 - 1 . osgup . el7  condor - debuginfo - 8 . 8 . 3 - 1 . osgup . el7  condor - kbdd - 8 . 8 . 3 - 1 . osgup . el7  condor - procd - 8 . 8 . 3 - 1 . osgup . el7  condor - test - 8 . 8 . 3 - 1 . osgup . el7  condor - vm - gahp - 8 . 8 . 3 - 1 . osgup . el7  minicondor - 8 . 8 . 3 - 1 . osgup . el7  python2 - condor - 8 . 8 . 3 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.30-2\n\n\nRelease Date\n: 2019-05-30\n\n\nSummary of changes\n\n\nThis release contains:\n\n\nCA Certificates based on \nIGTF 1.99\n\n\n\n\nWithdrawn superseded HKU CA (HK)\n\n\nWithdrawn discontinued CyGrid CA following migration to TCS (CY)\n\n\n\n\nVO Package v92\n\n\n\n\nAdded nanoHUB back to the VO client\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.99-1.osg34.el6\n\n\nosg-ca-certs-1.82-1.osg34.el6\n\n\nvo-client-92-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.99-1.osg34.el7\n\n\nosg-ca-certs-1.82-1.osg34.el7\n\n\nvo-client-92-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n99\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n82\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n92\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n92\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n92\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n99\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n82\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n92\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n92\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n92\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.30-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#osg-software-stack-data-release-3430-2", 
            "text": "Release Date : 2019-05-30", 
            "title": "OSG Software Stack -- Data Release -- 3.4.30-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#summary-of-changes", 
            "text": "This release contains:  CA Certificates based on  IGTF 1.99   Withdrawn superseded HKU CA (HK)  Withdrawn discontinued CyGrid CA following migration to TCS (CY)   VO Package v92   Added nanoHUB back to the VO client   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.99-1.osg34.el6  osg-ca-certs-1.82-1.osg34.el6  vo-client-92-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.99-1.osg34.el7  osg-ca-certs-1.82-1.osg34.el7  vo-client-92-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 99 - 1 . osg34 . el6  osg - ca - certs - 1 . 82 - 1 . osg34 . el6  vo - client - 92 - 1 . osg34 . el6  vo - client - dcache - 92 - 1 . osg34 . el6  vo - client - lcmaps - voms - 92 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 99 - 1 . osg34 . el7  osg - ca - certs - 1 . 82 - 1 . osg34 . el7  vo - client - 92 - 1 . osg34 . el7  vo - client - dcache - 92 - 1 . osg34 . el7  vo - client - lcmaps - voms - 92 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/", 
            "text": "OSG Software Release 3.4.30\n\n\nRelease Date\n: 2019-05-16\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nDanger\n\n\nThis release contains a \nsecurity fix\n for Singularity in the upcoming repository.\nThe Singularity in OSG 3.4 does not have the vulnerability.\n\n\n\n\n\n\nBLAHP 1.18.41\n\n\nUses better qstat options for completed jobs when using PBS Pro\n\n\nFixed proxy certificate renewal issue for non-HTCondor batch systems\n\n\nUses the new HTCondor environment format\n\n\n\n\n\n\nVO Package v91\n: Update the STAR VO Certificate\n\n\nxrootd-voms-plugin 0.6.0: Fix minor memory leak\n\n\nosg-pki-tools 3.3.0\n: New options to set organization and department codes\n\n\nUpcoming repository:\n\n\nSingularity 3.1.1-1.1: \nSecurity Fix\n\n\nosg-se-hadoop meta-package for EL6\n\n\nBLAHP 1.18.41: See above\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nNotes\n\n\nThis section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\nGlideinWMS\n\n\n\n\n\n\nGlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).\n\n\n\n\n\n\nFor new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be \n= 3.4.1.\n\n\n\n\nNote\n\n\nOSG GlideinWMS factories are running at least 3.4.1\n\n\n\n\nIf some of the connected Factories are \n= 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:\n\n\n\n\nhttps://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars\n\n\n\n\n\n\n\n\nUpgrades from \n= 3.4.0 may require merging \n/etc/condor/config.d/*.rpmnew\n files and a restart of HTCondor.\n\n\n\n\n\n\nGlideinWMS \n= 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n   \n/etc/gwms-frontend/frontend.xml\n, changing the address to include the shared port sinful string:\n\n\ncollector\n \nDN\n=\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain\n \ngroup\n=\ndefault\n \nnode\n=\ngwms-frontend.domain:9618?sock=collector0-40\n \nsecondary\n=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\nReplacing \ngwms-frontend-domain\n with the hostname of your GlideinWMS frontend.\n   See the \nGlideinWMS documentation\n for details.\n\n\nKnown Issues\n\n\nNone.\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.41.bosco-1.osg34.el6\n\n\nosg-pki-tools-3.3.0-1.osg34.el6\n\n\nosg-release-3.4-8.osg34.el6\n\n\nosg-test-3.0.0-1.osg34.el6\n\n\nosg-version-3.4.30-1.osg34.el6\n\n\nvo-client-91-1.osg34.el6\n\n\nxrootd-voms-plugin-0.6.0-2.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.41.bosco-1.osg34.el7\n\n\nosg-pki-tools-3.3.0-1.osg34.el7\n\n\nosg-release-3.4-8.osg34.el7\n\n\nosg-test-3.0.0-1.osg34.el7\n\n\nosg-version-3.4.30-1.osg34.el7\n\n\nvo-client-91-1.osg34.el7\n\n\nxrootd-voms-plugin-0.6.0-2.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nrelease\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n-\nvoms\n-\nplugin\n \nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n \nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n8\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n30\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n91\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n91\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n91\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n8\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n30\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n91\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n91\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n91\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n-\n0\n.\n6\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.41.bosco-1.osgup.el6\n\n\nosg-se-hadoop-3.4-8.osgup.el6\n\n\nsingularity-3.1.1-1.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.41.bosco-1.osgup.el7\n\n\nsingularity-3.1.1-1.1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\navro\n-\ndoc\n \navro\n-\nlibs\n \navro\n-\ntools\n \nbigtop\n-\njsvc\n \nbigtop\n-\njsvc\n-\ndebuginfo\n \nbigtop\n-\nutils\n \nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n \nhadoop\n \nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n \nhadoop\n-\n0\n.\n20\n-\nmapreduce\n \nhadoop\n-\nclient\n \nhadoop\n-\nconf\n-\npseudo\n \nhadoop\n-\ndebuginfo\n \nhadoop\n-\ndoc\n \nhadoop\n-\nhdfs\n \nhadoop\n-\nhdfs\n-\ndatanode\n \nhadoop\n-\nhdfs\n-\nfuse\n \nhadoop\n-\nhdfs\n-\njournalnode\n \nhadoop\n-\nhdfs\n-\nnamenode\n \nhadoop\n-\nhdfs\n-\nnfs3\n \nhadoop\n-\nhdfs\n-\nsecondarynamenode\n \nhadoop\n-\nhdfs\n-\nzkfc\n \nhadoop\n-\nhttpfs\n \nhadoop\n-\nkms\n \nhadoop\n-\nkms\n-\nserver\n \nhadoop\n-\nlibhdfs\n \nhadoop\n-\nlibhdfs\n-\ndevel\n \nhadoop\n-\nmapreduce\n \nhadoop\n-\nyarn\n \nminicondor\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nhdfs\n \nosg\n-\ngridftp\n-\nxrootd\n \nosg\n-\nse\n-\nhadoop\n \nosg\n-\nse\n-\nhadoop\n-\nclient\n \nosg\n-\nse\n-\nhadoop\n-\ndatanode\n \nosg\n-\nse\n-\nhadoop\n-\nnamenode\n \nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n \npython2\n-\ncondor\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nzookeeper\n \nzookeeper\n-\ndebuginfo\n \nzookeeper\n-\nnative\n \nzookeeper\n-\nserver\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nosg\n-\nse\n-\nhadoop\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nosg\n-\nse\n-\nhadoop\n-\nclient\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nosg\n-\nse\n-\nhadoop\n-\ndatanode\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nosg\n-\nse\n-\nhadoop\n-\nnamenode\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nsingularity\n-\n3\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n41\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\n3\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.30"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#osg-software-release-3430", 
            "text": "Release Date : 2019-05-16", 
            "title": "OSG Software Release 3.4.30"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#summary-of-changes", 
            "text": "This release contains:   Danger  This release contains a  security fix  for Singularity in the upcoming repository.\nThe Singularity in OSG 3.4 does not have the vulnerability.    BLAHP 1.18.41  Uses better qstat options for completed jobs when using PBS Pro  Fixed proxy certificate renewal issue for non-HTCondor batch systems  Uses the new HTCondor environment format    VO Package v91 : Update the STAR VO Certificate  xrootd-voms-plugin 0.6.0: Fix minor memory leak  osg-pki-tools 3.3.0 : New options to set organization and department codes  Upcoming repository:  Singularity 3.1.1-1.1:  Security Fix  osg-se-hadoop meta-package for EL6  BLAHP 1.18.41: See above     These  JIRA tickets  were addressed in this release.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#notes", 
            "text": "This section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found  here .   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7", 
            "title": "Notes"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#glideinwms", 
            "text": "GlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).    For new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be  = 3.4.1.   Note  OSG GlideinWMS factories are running at least 3.4.1   If some of the connected Factories are  = 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:   https://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars     Upgrades from  = 3.4.0 may require merging  /etc/condor/config.d/*.rpmnew  files and a restart of HTCondor.    GlideinWMS  = 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n    /etc/gwms-frontend/frontend.xml , changing the address to include the shared port sinful string:  collector   DN = /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain   group = default   node = gwms-frontend.domain:9618?sock=collector0-40   secondary = True /     Replacing  gwms-frontend-domain  with the hostname of your GlideinWMS frontend.\n   See the  GlideinWMS documentation  for details.", 
            "title": "GlideinWMS"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#known-issues", 
            "text": "None.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-6", 
            "text": "blahp-1.18.41.bosco-1.osg34.el6  osg-pki-tools-3.3.0-1.osg34.el6  osg-release-3.4-8.osg34.el6  osg-test-3.0.0-1.osg34.el6  osg-version-3.4.30-1.osg34.el6  vo-client-91-1.osg34.el6  xrootd-voms-plugin-0.6.0-2.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-7", 
            "text": "blahp-1.18.41.bosco-1.osg34.el7  osg-pki-tools-3.3.0-1.osg34.el7  osg-release-3.4-8.osg34.el7  osg-test-3.0.0-1.osg34.el7  osg-version-3.4.30-1.osg34.el7  vo-client-91-1.osg34.el7  xrootd-voms-plugin-0.6.0-2.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   osg - pki - tools   osg - release   osg - test   osg - test - log - viewer   osg - version   vo - client   vo - client - dcache   vo - client - lcmaps - voms   xrootd - voms - plugin   xrootd - voms - plugin - debuginfo   xrootd - voms - plugin - devel   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 41 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 41 . bosco - 1 . osg34 . el6  osg - pki - tools - 3 . 3 . 0 - 1 . osg34 . el6  osg - release - 3 . 4 - 8 . osg34 . el6  osg - test - 3 . 0 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 3 . 0 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 30 - 1 . osg34 . el6  vo - client - 91 - 1 . osg34 . el6  vo - client - dcache - 91 - 1 . osg34 . el6  vo - client - lcmaps - voms - 91 - 1 . osg34 . el6  xrootd - voms - plugin - 0 . 6 . 0 - 2 . osg34 . el6  xrootd - voms - plugin - debuginfo - 0 . 6 . 0 - 2 . osg34 . el6  xrootd - voms - plugin - devel - 0 . 6 . 0 - 2 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 41 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 41 . bosco - 1 . osg34 . el7  osg - pki - tools - 3 . 3 . 0 - 1 . osg34 . el7  osg - release - 3 . 4 - 8 . osg34 . el7  osg - test - 3 . 0 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 3 . 0 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 30 - 1 . osg34 . el7  vo - client - 91 - 1 . osg34 . el7  vo - client - dcache - 91 - 1 . osg34 . el7  vo - client - lcmaps - voms - 91 - 1 . osg34 . el7  xrootd - voms - plugin - 0 . 6 . 0 - 2 . osg34 . el7  xrootd - voms - plugin - debuginfo - 0 . 6 . 0 - 2 . osg34 . el7  xrootd - voms - plugin - devel - 0 . 6 . 0 - 2 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-6_2", 
            "text": "blahp-1.18.41.bosco-1.osgup.el6  osg-se-hadoop-3.4-8.osgup.el6  singularity-3.1.1-1.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-7_2", 
            "text": "blahp-1.18.41.bosco-1.osgup.el7  singularity-3.1.1-1.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  avro - doc   avro - libs   avro - tools   bigtop - jsvc   bigtop - jsvc - debuginfo   bigtop - utils   blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   hadoop   hadoop - 0 . 20 - conf - pseudo   hadoop - 0 . 20 - mapreduce   hadoop - client   hadoop - conf - pseudo   hadoop - debuginfo   hadoop - doc   hadoop - hdfs   hadoop - hdfs - datanode   hadoop - hdfs - fuse   hadoop - hdfs - journalnode   hadoop - hdfs - namenode   hadoop - hdfs - nfs3   hadoop - hdfs - secondarynamenode   hadoop - hdfs - zkfc   hadoop - httpfs   hadoop - kms   hadoop - kms - server   hadoop - libhdfs   hadoop - libhdfs - devel   hadoop - mapreduce   hadoop - yarn   minicondor   osg - gridftp   osg - gridftp - hdfs   osg - gridftp - xrootd   osg - se - hadoop   osg - se - hadoop - client   osg - se - hadoop - datanode   osg - se - hadoop - namenode   osg - se - hadoop - secondarynamenode   python2 - condor   singularity   singularity - debuginfo   zookeeper   zookeeper - debuginfo   zookeeper - native   zookeeper - server   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 41 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 41 . bosco - 1 . osgup . el6  osg - se - hadoop - 3 . 4 - 8 . osgup . el6  osg - se - hadoop - client - 3 . 4 - 8 . osgup . el6  osg - se - hadoop - datanode - 3 . 4 - 8 . osgup . el6  osg - se - hadoop - namenode - 3 . 4 - 8 . osgup . el6  osg - se - hadoop - secondarynamenode - 3 . 4 - 8 . osgup . el6  singularity - 3 . 1 . 1 - 1 . 1 . osgup . el6  singularity - debuginfo - 3 . 1 . 1 - 1 . 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-30/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 41 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 41 . bosco - 1 . osgup . el7  singularity - 3 . 1 . 1 - 1 . 1 . osgup . el7  singularity - debuginfo - 3 . 1 . 1 - 1 . 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.29-2\n\n\nRelease Date\n: 2019-05-07\n\n\nSummary of changes\n\n\nThis release contains:\n\n\nVO Package v90\n\n\n\n\nAdd voms.cnaf.infn.it VIRGO VOMS server\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-90-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-90-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nvo\n-\nclient\n-\n90\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n90\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n90\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nvo\n-\nclient\n-\n90\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n90\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n90\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.29-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#osg-software-stack-data-release-3429-2", 
            "text": "Release Date : 2019-05-07", 
            "title": "OSG Software Stack -- Data Release -- 3.4.29-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#summary-of-changes", 
            "text": "This release contains:  VO Package v90   Add voms.cnaf.infn.it VIRGO VOMS server   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#enterprise-linux-6", 
            "text": "vo-client-90-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#enterprise-linux-7", 
            "text": "vo-client-90-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#enterprise-linux-6_1", 
            "text": "vo - client - 90 - 1 . osg34 . el6  vo - client - dcache - 90 - 1 . osg34 . el6  vo - client - lcmaps - voms - 90 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-29-2/#enterprise-linux-7_1", 
            "text": "vo - client - 90 - 1 . osg34 . el7  vo - client - dcache - 90 - 1 . osg34 . el7  vo - client - lcmaps - voms - 90 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/", 
            "text": "OSG Software Release 3.4.29\n\n\nRelease Date\n: 2019-05-02\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\n\n\nXCache 1.0.5 is a complete overhaul of the packaging and configuration\n    for the StashCache cache and origin services, based on improvements\n    available in XRootD 4.9.1.  XCache is also the basis of upcoming work\n    for connecting caches to the CMS and ATLAS data federations.\n\n\nThe configuration has been rewritten to use \nconfig.d\n-style directories\ninstead of single config files, and supporting services have been added\nto do the following:\n\n\n\n\nupdate authorization for both caches and origins based on data in the\n    OSG Topology service\n\n\nautomatically renew cache proxies\n\n\n\n\nThis \noverview document\n\ncontains links to instructions for setting up new\ncaches and origins, and instructions for VOs on how to get their data\ninto the StashCache Federation.\n\n\n\n\nNote\n\n\n\n\n\n\nXCache is only available for EL7, and EL7 is now a requirement for\n    a cache or origin to join the StashCache Data Federation.\n\n\n\n\n\n\nBecause of the extensive changes to the configuration, sites upgrading\n    their caches or origins to this version should consider the upgrade to\n    be the same amount of work as a reinstall of the service.\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate MyProxy to use the Grid Community Toolkit (GCT)\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nNotes\n\n\nThis section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\nGlideinWMS\n\n\n\n\n\n\nGlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).\n\n\n\n\n\n\nFor new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be \n= 3.4.1.\n\n\n\n\nNote\n\n\nOSG GlideinWMS factories are running at least 3.4.1\n\n\n\n\nIf some of the connected Factories are \n= 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:\n\n\n\n\nhttps://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars\n\n\n\n\n\n\n\n\nUpgrades from \n= 3.4.0 may require merging \n/etc/condor/config.d/*.rpmnew\n files and a restart of HTCondor.\n\n\n\n\n\n\nGlideinWMS \n= 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n   \n/etc/gwms-frontend/frontend.xml\n, changing the address to include the shared port sinful string:\n\n\ncollector\n \nDN\n=\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain\n \ngroup\n=\ndefault\n \nnode\n=\ngwms-frontend.domain:9618?sock=collector0-40\n \nsecondary\n=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\nReplacing \ngwms-frontend-domain\n with the hostname of your GlideinWMS frontend.\n   See the \nGlideinWMS documentation\n for details. \n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nmyproxy-6.2.3-1.1.osg34.el6\n\n\nosg-version-3.4.29-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nmyproxy-6.2.3-1.1.osg34.el7\n\n\nosg-version-3.4.29-1.osg34.el7\n\n\nxcache-1.0.5-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nmyproxy\n \nmyproxy\n-\nadmin\n \nmyproxy\n-\ndebuginfo\n \nmyproxy\n-\ndevel\n \nmyproxy\n-\ndoc\n \nmyproxy\n-\nlibs\n \nmyproxy\n-\nserver\n \nmyproxy\n-\nvoms\n \nosg\n-\nversion\n \nstash\n-\ncache\n \nstash\n-\norigin\n \nxcache\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nmyproxy\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nserver\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n29\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nmyproxy\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nserver\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n2\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n29\n-\n1\n.\nosg34\n.\nel7\n\n\nstash\n-\ncache\n-\n1\n.\n0\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nstash\n-\norigin\n-\n1\n.\n0\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxcache\n-\n1\n.\n0\n.\n5\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.29"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#osg-software-release-3429", 
            "text": "Release Date : 2019-05-02", 
            "title": "OSG Software Release 3.4.29"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#summary-of-changes", 
            "text": "This release contains:    XCache 1.0.5 is a complete overhaul of the packaging and configuration\n    for the StashCache cache and origin services, based on improvements\n    available in XRootD 4.9.1.  XCache is also the basis of upcoming work\n    for connecting caches to the CMS and ATLAS data federations.  The configuration has been rewritten to use  config.d -style directories\ninstead of single config files, and supporting services have been added\nto do the following:   update authorization for both caches and origins based on data in the\n    OSG Topology service  automatically renew cache proxies   This  overview document \ncontains links to instructions for setting up new\ncaches and origins, and instructions for VOs on how to get their data\ninto the StashCache Federation.   Note    XCache is only available for EL7, and EL7 is now a requirement for\n    a cache or origin to join the StashCache Data Federation.    Because of the extensive changes to the configuration, sites upgrading\n    their caches or origins to this version should consider the upgrade to\n    be the same amount of work as a reinstall of the service.       Update MyProxy to use the Grid Community Toolkit (GCT)    These  JIRA tickets  were addressed in this release.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#notes", 
            "text": "This section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found  here .   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7", 
            "title": "Notes"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#glideinwms", 
            "text": "GlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).    For new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be  = 3.4.1.   Note  OSG GlideinWMS factories are running at least 3.4.1   If some of the connected Factories are  = 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:   https://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars     Upgrades from  = 3.4.0 may require merging  /etc/condor/config.d/*.rpmnew  files and a restart of HTCondor.    GlideinWMS  = 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n    /etc/gwms-frontend/frontend.xml , changing the address to include the shared port sinful string:  collector   DN = /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain   group = default   node = gwms-frontend.domain:9618?sock=collector0-40   secondary = True /     Replacing  gwms-frontend-domain  with the hostname of your GlideinWMS frontend.\n   See the  GlideinWMS documentation  for details.", 
            "title": "GlideinWMS"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#enterprise-linux-6", 
            "text": "myproxy-6.2.3-1.1.osg34.el6  osg-version-3.4.29-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#enterprise-linux-7", 
            "text": "myproxy-6.2.3-1.1.osg34.el7  osg-version-3.4.29-1.osg34.el7  xcache-1.0.5-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  myproxy   myproxy - admin   myproxy - debuginfo   myproxy - devel   myproxy - doc   myproxy - libs   myproxy - server   myproxy - voms   osg - version   stash - cache   stash - origin   xcache   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#enterprise-linux-6_1", 
            "text": "myproxy - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - admin - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - debuginfo - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - devel - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - doc - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - libs - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - server - 6 . 2 . 3 - 1 . 1 . osg34 . el6  myproxy - voms - 6 . 2 . 3 - 1 . 1 . osg34 . el6  osg - version - 3 . 4 . 29 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-29/#enterprise-linux-7_1", 
            "text": "myproxy - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - admin - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - debuginfo - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - devel - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - doc - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - libs - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - server - 6 . 2 . 3 - 1 . 1 . osg34 . el7  myproxy - voms - 6 . 2 . 3 - 1 . 1 . osg34 . el7  osg - version - 3 . 4 . 29 - 1 . osg34 . el7  stash - cache - 1 . 0 . 5 - 1 . osg34 . el7  stash - origin - 1 . 0 . 5 - 1 . osg34 . el7  xcache - 1 . 0 . 5 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.28-2\n\n\nRelease Date\n: 2019-04-30\n\n\nSummary of changes\n\n\nThis release contains:\n\n\nCA Certificates based on \nIGTF 1.98\n\n\n\n\nWithdrawn superseded IRAN-GRID authority (IR)\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.98-1.osg34.el6\n\n\nosg-ca-certs-1.81-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.98-1.osg34.el7\n\n\nosg-ca-certs-1.81-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n98\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n81\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n98\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n81\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.28-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#osg-software-stack-data-release-3428-2", 
            "text": "Release Date : 2019-04-30", 
            "title": "OSG Software Stack -- Data Release -- 3.4.28-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#summary-of-changes", 
            "text": "This release contains:  CA Certificates based on  IGTF 1.98   Withdrawn superseded IRAN-GRID authority (IR)   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.98-1.osg34.el6  osg-ca-certs-1.81-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.98-1.osg34.el7  osg-ca-certs-1.81-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 98 - 1 . osg34 . el6  osg - ca - certs - 1 . 81 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 98 - 1 . osg34 . el7  osg - ca - certs - 1 . 81 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/", 
            "text": "OSG Software Release 3.4.28\n\n\nRelease Date\n: 2019-04-25\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nXRootD 4.9.1\n: Updated from XRootD 4.8.5\n\n\nIntegrated plugins for macaroons and third-party copy\n\n\nAdded support for Subject Alternative Names\n\n\nAdded support for multiple configuration files via the \ncontinue\n statement\n\n\nVarious bug fixes. See detailed release notes for more information.\n\n\n\n\n\n\nxrootd-hdfs 2.1.4\n: Bugfix release\n\n\nGlideinWMS 3.4.5\n: Updated from GlideinWMS 3.4.2\n\n\nFrontend configurations now use shared port (i.e. only port 9618 is required)\n\n\nAdded a scaling factor for all Glidein limits in factory entries\n\n\nAdded the ability to completely disable Glidein removal\n\n\nIncludes unprivileged Singularity and preserve important system files\n\n\nAdded option to ignore entries in downtime when considering Glidein matches\n\n\nTracks jobs that spawn multiple nodes, e.g. HPC submission \n\n\nPropagates attributes controlled by the Frontend to Factory and Glidein submission\n\n\nVarious bug fixes. See detailed release notes for more information.\n\n\n\n\n\n\nOSG Flock 1.1: Added new \nflock.opensciencegrid.org\n host certificate information\n\n\nVO Package v89\n\n\nAdded new GLOW certificate information\n\n\nRemoved retired SBGrid certificate information\n\n\n\n\n\n\nUpcoming Repository: \nHTCondor 8.8.2\n\n\nAdded support for \ncondor_ssh_to_job\n for jobs running under non-setuid Singularity\n\n\nAdded new Python bindings function to output ClassAds as JSON\n\n\nVarious bug fixes. See detailed release notes for more information.\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nNotes\n\n\nThis section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\nGlideinWMS\n\n\n\n\n\n\nGlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).\n\n\n\n\n\n\nFor new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be \n= 3.4.1.\n\n\n\n\nNote\n\n\nOSG GlideinWMS factories are running at least 3.4.1\n\n\n\n\nIf some of the connected Factories are \n= 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:\n\n\n\n\nhttps://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity\n\n\nhttps://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars\n\n\n\n\n\n\n\n\nUpgrades from \n= 3.4.0 may require merging \n/etc/condor/config.d/*.rpmnew\n files and a restart of HTCondor.\n\n\n\n\n\n\nGlideinWMS \n= 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n   \n/etc/gwms-frontend/frontend.xml\n, changing the address to include the shared port sinful string:\n\n\ncollector\n \nDN\n=\n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain\n \ngroup\n=\ndefault\n \nnode\n=\ngwms-frontend.domain:9618?sock=collector0-40\n \nsecondary\n=\nTrue\n/\n\n\n\n\n\n\n\n\n\n\nReplacing \ngwms-frontend-domain\n with the hostname of your GlideinWMS frontend.\n   See the \nGlideinWMS documentation\n for details. \n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.4.5-1.osg34.el6\n\n\nosg-flock-1.1-1.osg34.el6\n\n\nosg-version-3.4.28-1.osg34.el6\n\n\nvo-client-89-1.osg34.el6\n\n\nxrootd-4.9.1-1.osg34.el6\n\n\nxrootd-lcmaps-1.7.0-1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.4.5-1.osg34.el7\n\n\nosg-flock-1.1-1.osg34.el7\n\n\nosg-version-3.4.28-1.osg34.el7\n\n\nvo-client-89-1.osg34.el7\n\n\nxrootd-4.9.1-1.osg34.el7\n\n\nxrootd-hdfs-2.1.4-1.osg34.el7\n\n\nxrootd-lcmaps-1.7.0-1.1.osg34.el7\n\n\nxrootd-multiuser-0.4.2-3.osg34.el7\n\n\nxrootd-scitokens-0.6.0-3.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nosg\n-\nflock\n \nosg\n-\nversion\n \npython2\n-\nxrootd\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nhdfs\n \nxrootd\n-\nhdfs\n-\ndebuginfo\n \nxrootd\n-\nhdfs\n-\ndevel\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n \nxrootd\n-\nlibs\n \nxrootd\n-\nmultiuser\n \nxrootd\n-\nmultiuser\n-\ndebuginfo\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nscitokens\n \nxrootd\n-\nscitokens\n-\ndebuginfo\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nflock\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n28\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n89\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n89\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n89\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n7\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nflock\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n28\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n89\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n89\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n89\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\n2\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndebuginfo\n-\n2\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndevel\n-\n2\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n7\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nmultiuser\n-\n0\n.\n4\n.\n2\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nmultiuser\n-\ndebuginfo\n-\n0\n.\n4\n.\n2\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nscitokens\n-\n0\n.\n6\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nscitokens\n-\ndebuginfo\n-\n0\n.\n6\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.8.2-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.8.2-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nminicondor\n \npython2\n-\ncondor\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\nminicondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\nminicondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n2\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.28"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#osg-software-release-3428", 
            "text": "Release Date : 2019-04-25", 
            "title": "OSG Software Release 3.4.28"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#summary-of-changes", 
            "text": "This release contains:   XRootD 4.9.1 : Updated from XRootD 4.8.5  Integrated plugins for macaroons and third-party copy  Added support for Subject Alternative Names  Added support for multiple configuration files via the  continue  statement  Various bug fixes. See detailed release notes for more information.    xrootd-hdfs 2.1.4 : Bugfix release  GlideinWMS 3.4.5 : Updated from GlideinWMS 3.4.2  Frontend configurations now use shared port (i.e. only port 9618 is required)  Added a scaling factor for all Glidein limits in factory entries  Added the ability to completely disable Glidein removal  Includes unprivileged Singularity and preserve important system files  Added option to ignore entries in downtime when considering Glidein matches  Tracks jobs that spawn multiple nodes, e.g. HPC submission   Propagates attributes controlled by the Frontend to Factory and Glidein submission  Various bug fixes. See detailed release notes for more information.    OSG Flock 1.1: Added new  flock.opensciencegrid.org  host certificate information  VO Package v89  Added new GLOW certificate information  Removed retired SBGrid certificate information    Upcoming Repository:  HTCondor 8.8.2  Added support for  condor_ssh_to_job  for jobs running under non-setuid Singularity  Added new Python bindings function to output ClassAds as JSON  Various bug fixes. See detailed release notes for more information.     These  JIRA tickets  were addressed in this release.", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#notes", 
            "text": "This section describes important upgrade notes and/or caveats for packages available in the OSG release repositories.\nDetailed changes are below. All of the documentation can be found  here .   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7", 
            "title": "Notes"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#glideinwms", 
            "text": "GlideinWMS 3.4.5 is the last release supporting Globus GRAM (a.k.a. GT2/GT5).    For new Singularity features introduced in GlideinWMS 3.4.1, all factories and frontends need to be  = 3.4.1.   Note  OSG GlideinWMS factories are running at least 3.4.1   If some of the connected Factories are  = 3.4.1 you will see an error during reconfig/upgrade if you try to use\nfeatures that require a newer Factory.\nTo start using Singularity via GlideinWMS, see:   https://glideinwms.fnal.gov/doc.prd/frontend/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/configuration.html#singularity  https://glideinwms.fnal.gov/doc.prd/factory/custom_vars.html#singularity_vars     Upgrades from  = 3.4.0 may require merging  /etc/condor/config.d/*.rpmnew  files and a restart of HTCondor.    GlideinWMS  = 3.4.5 uses shared port, requiring only port 9618.\n   To ease the transition to shared port, the User Collector secondary collectors and CCBs support both shared and\n   separate, individual ports.\n   To start using shared port, change the secondary collectors lines and the CCBs lines (if any) in\n    /etc/gwms-frontend/frontend.xml , changing the address to include the shared port sinful string:  collector   DN = /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=gwms-frontend.domain   group = default   node = gwms-frontend.domain:9618?sock=collector0-40   secondary = True /     Replacing  gwms-frontend-domain  with the hostname of your GlideinWMS frontend.\n   See the  GlideinWMS documentation  for details.", 
            "title": "GlideinWMS"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-6", 
            "text": "glideinwms-3.4.5-1.osg34.el6  osg-flock-1.1-1.osg34.el6  osg-version-3.4.28-1.osg34.el6  vo-client-89-1.osg34.el6  xrootd-4.9.1-1.osg34.el6  xrootd-lcmaps-1.7.0-1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-7", 
            "text": "glideinwms-3.4.5-1.osg34.el7  osg-flock-1.1-1.osg34.el7  osg-version-3.4.28-1.osg34.el7  vo-client-89-1.osg34.el7  xrootd-4.9.1-1.osg34.el7  xrootd-hdfs-2.1.4-1.osg34.el7  xrootd-lcmaps-1.7.0-1.1.osg34.el7  xrootd-multiuser-0.4.2-3.osg34.el7  xrootd-scitokens-0.6.0-3.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   osg - flock   osg - version   python2 - xrootd   vo - client   vo - client - dcache   vo - client - lcmaps - voms   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - hdfs   xrootd - hdfs - debuginfo   xrootd - hdfs - devel   xrootd - lcmaps   xrootd - lcmaps - debuginfo   xrootd - libs   xrootd - multiuser   xrootd - multiuser - debuginfo   xrootd - private - devel   xrootd - scitokens   xrootd - scitokens - debuginfo   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-6_1", 
            "text": "glideinwms - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - common - tools - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - condor - common - config - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - factory - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - factory - condor - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - libs - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - minimal - condor - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - usercollector - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - userschedd - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - vofrontend - 3 . 4 . 5 - 1 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 4 . 5 - 1 . osg34 . el6  osg - flock - 1 . 1 - 1 . osg34 . el6  osg - version - 3 . 4 . 28 - 1 . osg34 . el6  python2 - xrootd - 4 . 9 . 1 - 1 . osg34 . el6  vo - client - 89 - 1 . osg34 . el6  vo - client - dcache - 89 - 1 . osg34 . el6  vo - client - lcmaps - voms - 89 - 1 . osg34 . el6  xrootd - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - client - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - client - devel - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - client - libs - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - devel - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - doc - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - fuse - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - lcmaps - 1 . 7 . 0 - 1 . 1 . osg34 . el6  xrootd - lcmaps - debuginfo - 1 . 7 . 0 - 1 . 1 . osg34 . el6  xrootd - libs - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - private - devel - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - selinux - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - server - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - server - devel - 4 . 9 . 1 - 1 . osg34 . el6  xrootd - server - libs - 4 . 9 . 1 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-7_1", 
            "text": "glideinwms - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - common - tools - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - condor - common - config - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - factory - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - factory - condor - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - libs - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - minimal - condor - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - usercollector - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - userschedd - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - vofrontend - 3 . 4 . 5 - 1 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 4 . 5 - 1 . osg34 . el7  osg - flock - 1 . 1 - 1 . osg34 . el7  osg - version - 3 . 4 . 28 - 1 . osg34 . el7  python2 - xrootd - 4 . 9 . 1 - 1 . osg34 . el7  vo - client - 89 - 1 . osg34 . el7  vo - client - dcache - 89 - 1 . osg34 . el7  vo - client - lcmaps - voms - 89 - 1 . osg34 . el7  xrootd - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - client - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - client - devel - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - client - libs - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - devel - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - doc - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - fuse - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - hdfs - 2 . 1 . 4 - 1 . osg34 . el7  xrootd - hdfs - debuginfo - 2 . 1 . 4 - 1 . osg34 . el7  xrootd - hdfs - devel - 2 . 1 . 4 - 1 . osg34 . el7  xrootd - lcmaps - 1 . 7 . 0 - 1 . 1 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 7 . 0 - 1 . 1 . osg34 . el7  xrootd - libs - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - multiuser - 0 . 4 . 2 - 3 . osg34 . el7  xrootd - multiuser - debuginfo - 0 . 4 . 2 - 3 . osg34 . el7  xrootd - private - devel - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - scitokens - 0 . 6 . 0 - 3 . osg34 . el7  xrootd - scitokens - debuginfo - 0 . 6 . 0 - 3 . osg34 . el7  xrootd - selinux - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - server - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - server - devel - 4 . 9 . 1 - 1 . osg34 . el7  xrootd - server - libs - 4 . 9 . 1 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-6_2", 
            "text": "condor-8.8.2-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-7_2", 
            "text": "condor-8.8.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   minicondor   python2 - condor   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-6_3", 
            "text": "condor - 8 . 8 . 2 - 1 . osgup . el6  condor - all - 8 . 8 . 2 - 1 . osgup . el6  condor - annex - ec2 - 8 . 8 . 2 - 1 . osgup . el6  condor - bosco - 8 . 8 . 2 - 1 . osgup . el6  condor - classads - 8 . 8 . 2 - 1 . osgup . el6  condor - classads - devel - 8 . 8 . 2 - 1 . osgup . el6  condor - cream - gahp - 8 . 8 . 2 - 1 . osgup . el6  condor - debuginfo - 8 . 8 . 2 - 1 . osgup . el6  condor - kbdd - 8 . 8 . 2 - 1 . osgup . el6  condor - procd - 8 . 8 . 2 - 1 . osgup . el6  condor - std - universe - 8 . 8 . 2 - 1 . osgup . el6  condor - test - 8 . 8 . 2 - 1 . osgup . el6  condor - vm - gahp - 8 . 8 . 2 - 1 . osgup . el6  minicondor - 8 . 8 . 2 - 1 . osgup . el6  python2 - condor - 8 . 8 . 2 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-28/#enterprise-linux-7_3", 
            "text": "condor - 8 . 8 . 2 - 1 . osgup . el7  condor - all - 8 . 8 . 2 - 1 . osgup . el7  condor - annex - ec2 - 8 . 8 . 2 - 1 . osgup . el7  condor - bosco - 8 . 8 . 2 - 1 . osgup . el7  condor - classads - 8 . 8 . 2 - 1 . osgup . el7  condor - classads - devel - 8 . 8 . 2 - 1 . osgup . el7  condor - cream - gahp - 8 . 8 . 2 - 1 . osgup . el7  condor - debuginfo - 8 . 8 . 2 - 1 . osgup . el7  condor - kbdd - 8 . 8 . 2 - 1 . osgup . el7  condor - procd - 8 . 8 . 2 - 1 . osgup . el7  condor - test - 8 . 8 . 2 - 1 . osgup . el7  condor - vm - gahp - 8 . 8 . 2 - 1 . osgup . el7  minicondor - 8 . 8 . 2 - 1 . osgup . el7  python2 - condor - 8 . 8 . 2 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.27-2\n\n\nRelease Date\n: 2019-04-16\n\n\nSummary of changes\n\n\nThis release contains:\n\n\nVO Package v88\n\n\n\n\nUpdated certificate info for the FNAL and OSG VOs\n\n\nAdd gluex.phys.uconn.edu VOMS server\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-88-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-88-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nvo\n-\nclient\n-\n88\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n88\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n88\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nvo\n-\nclient\n-\n88\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n88\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n88\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.27-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#osg-software-stack-data-release-3427-2", 
            "text": "Release Date : 2019-04-16", 
            "title": "OSG Software Stack -- Data Release -- 3.4.27-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#summary-of-changes", 
            "text": "This release contains:  VO Package v88   Updated certificate info for the FNAL and OSG VOs  Add gluex.phys.uconn.edu VOMS server   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#enterprise-linux-6", 
            "text": "vo-client-88-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#enterprise-linux-7", 
            "text": "vo-client-88-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#enterprise-linux-6_1", 
            "text": "vo - client - 88 - 1 . osg34 . el6  vo - client - dcache - 88 - 1 . osg34 . el6  vo - client - lcmaps - voms - 88 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27-2/#enterprise-linux-7_1", 
            "text": "vo - client - 88 - 1 . osg34 . el7  vo - client - dcache - 88 - 1 . osg34 . el7  vo - client - lcmaps - voms - 88 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/", 
            "text": "OSG Software Release 3.4.27\n\n\nRelease Date\n: 2019-04-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCVMFS 2.6.0\n: performance improvements, new functionality, and bug fixes\n\n\nCooperative Computing Tools\n 7.0.11: \nBug fix release\n\n\nosg-pki-tools 3.2.2\n: Update from 3.1.0, bug fixes\n\n\nHTCondor-CE 3.2.2\n: Takes advantage of HTCondor's new multi-line syntax\n\n\nUpdate Globus GridFTP packages to use the Grid Community Toolkit (GCT)\n\n\nUpcoming Repository\n\n\nSingularity 3.1.1\n: Bug fix release\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncctools-7.0.11-1.osg34.el6\n\n\ncvmfs-2.6.0-1.osg34.el6\n\n\nglobus-ftp-client-9.1-2.1.osg34.el6\n\n\nglobus-gridftp-server-13.9-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-8.0-1.osg34.el6\n\n\nhtcondor-ce-3.2.2-1.osg34.el6\n\n\nkoji-1.11.1-1.1.osg34.el6\n\n\nosg-build-1.14.2-1.osg34.el6\n\n\nosg-oasis-12-1.osg34.el6\n\n\nosg-pki-tools-3.2.2-1.osg34.el6\n\n\nosg-version-3.4.27-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncctools-7.0.11-1.osg34.el7\n\n\ncvmfs-2.6.0-1.osg34.el7\n\n\nglobus-ftp-client-9.1-2.1.osg34.el7\n\n\nglobus-gridftp-server-13.9-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-8.0-1.osg34.el7\n\n\nhtcondor-ce-3.2.2-1.osg34.el7\n\n\nkoji-1.11.1-1.1.osg34.el7\n\n\nosg-build-1.14.2-1.osg34.el7\n\n\nosg-oasis-12-1.osg34.el7\n\n\nosg-pki-tools-3.2.2-1.osg34.el7\n\n\nosg-version-3.4.27-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncctools\n \ncctools\n-\ndebuginfo\n \ncctools\n-\ndevel\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nducc\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nshrinkwrap\n \ncvmfs\n-\nunittests\n \nglobus\n-\nftp\n-\nclient\n \nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n \nglobus\n-\nftp\n-\nclient\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n-\ndoc\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nkoji\n \nkoji\n-\nbuilder\n \nkoji\n-\nhub\n \nkoji\n-\nhub\n-\nplugins\n \nkoji\n-\nutils\n \nkoji\n-\nvm\n \nkoji\n-\nweb\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\noasis\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nversion\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncctools\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndebuginfo\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndevel\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nshrinkwrap\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n12\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n27\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncctools\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndebuginfo\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndevel\n-\n7\n.\n0\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nducc\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nshrinkwrap\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n9\n.\n1\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n13\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n12\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n27\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nsingularity-3.1.1-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nsingularity-3.1.1-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nsingularity\n \nsingularity\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nsingularity\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nsingularity\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.27"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#osg-software-release-3427", 
            "text": "Release Date : 2019-04-11", 
            "title": "OSG Software Release 3.4.27"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#summary-of-changes", 
            "text": "This release contains:   CVMFS 2.6.0 : performance improvements, new functionality, and bug fixes  Cooperative Computing Tools  7.0.11:  Bug fix release  osg-pki-tools 3.2.2 : Update from 3.1.0, bug fixes  HTCondor-CE 3.2.2 : Takes advantage of HTCondor's new multi-line syntax  Update Globus GridFTP packages to use the Grid Community Toolkit (GCT)  Upcoming Repository  Singularity 3.1.1 : Bug fix release     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-6", 
            "text": "cctools-7.0.11-1.osg34.el6  cvmfs-2.6.0-1.osg34.el6  globus-ftp-client-9.1-2.1.osg34.el6  globus-gridftp-server-13.9-1.1.osg34.el6  globus-gridftp-server-control-8.0-1.osg34.el6  htcondor-ce-3.2.2-1.osg34.el6  koji-1.11.1-1.1.osg34.el6  osg-build-1.14.2-1.osg34.el6  osg-oasis-12-1.osg34.el6  osg-pki-tools-3.2.2-1.osg34.el6  osg-version-3.4.27-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-7", 
            "text": "cctools-7.0.11-1.osg34.el7  cvmfs-2.6.0-1.osg34.el7  globus-ftp-client-9.1-2.1.osg34.el7  globus-gridftp-server-13.9-1.1.osg34.el7  globus-gridftp-server-control-8.0-1.osg34.el7  htcondor-ce-3.2.2-1.osg34.el7  koji-1.11.1-1.1.osg34.el7  osg-build-1.14.2-1.osg34.el7  osg-oasis-12-1.osg34.el7  osg-pki-tools-3.2.2-1.osg34.el7  osg-version-3.4.27-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cctools   cctools - debuginfo   cctools - devel   cvmfs   cvmfs - devel   cvmfs - ducc   cvmfs - server   cvmfs - shrinkwrap   cvmfs - unittests   globus - ftp - client   globus - ftp - client - debuginfo   globus - ftp - client - devel   globus - ftp - client - doc   globus - gridftp - server   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   koji   koji - builder   koji - hub   koji - hub - plugins   koji - utils   koji - vm   koji - web   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - certs   osg - oasis   osg - pki - tools   osg - version   vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-6_1", 
            "text": "cctools - 7 . 0 . 11 - 1 . osg34 . el6  cctools - debuginfo - 7 . 0 . 11 - 1 . osg34 . el6  cctools - devel - 7 . 0 . 11 - 1 . osg34 . el6  cvmfs - 2 . 6 . 0 - 1 . osg34 . el6  cvmfs - devel - 2 . 6 . 0 - 1 . osg34 . el6  cvmfs - server - 2 . 6 . 0 - 1 . osg34 . el6  cvmfs - shrinkwrap - 2 . 6 . 0 - 1 . osg34 . el6  cvmfs - unittests - 2 . 6 . 0 - 1 . osg34 . el6  globus - ftp - client - 9 . 1 - 2 . 1 . osg34 . el6  globus - ftp - client - debuginfo - 9 . 1 - 2 . 1 . osg34 . el6  globus - ftp - client - devel - 9 . 1 - 2 . 1 . osg34 . el6  globus - ftp - client - doc - 9 . 1 - 2 . 1 . osg34 . el6  globus - gridftp - server - 13 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - 8 . 0 - 1 . osg34 . el6  globus - gridftp - server - control - debuginfo - 8 . 0 - 1 . osg34 . el6  globus - gridftp - server - control - devel - 8 . 0 - 1 . osg34 . el6  globus - gridftp - server - debuginfo - 13 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - devel - 13 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - progs - 13 . 9 - 1 . 1 . osg34 . el6  htcondor - ce - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - client - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 2 . 2 - 1 . osg34 . el6  htcondor - ce - view - 3 . 2 . 2 - 1 . osg34 . el6  koji - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - builder - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - hub - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - hub - plugins - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - utils - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - vm - 1 . 11 . 1 - 1 . 1 . osg34 . el6  koji - web - 1 . 11 . 1 - 1 . 1 . osg34 . el6  osg - build - 1 . 14 . 2 - 1 . osg34 . el6  osg - build - base - 1 . 14 . 2 - 1 . osg34 . el6  osg - build - koji - 1 . 14 . 2 - 1 . osg34 . el6  osg - build - mock - 1 . 14 . 2 - 1 . osg34 . el6  osg - build - tests - 1 . 14 . 2 - 1 . osg34 . el6  osg - oasis - 12 - 1 . osg34 . el6  osg - pki - tools - 3 . 2 . 2 - 1 . osg34 . el6  osg - version - 3 . 4 . 27 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-7_1", 
            "text": "cctools - 7 . 0 . 11 - 1 . osg34 . el7  cctools - debuginfo - 7 . 0 . 11 - 1 . osg34 . el7  cctools - devel - 7 . 0 . 11 - 1 . osg34 . el7  cvmfs - 2 . 6 . 0 - 1 . osg34 . el7  cvmfs - devel - 2 . 6 . 0 - 1 . osg34 . el7  cvmfs - ducc - 2 . 6 . 0 - 1 . osg34 . el7  cvmfs - server - 2 . 6 . 0 - 1 . osg34 . el7  cvmfs - shrinkwrap - 2 . 6 . 0 - 1 . osg34 . el7  cvmfs - unittests - 2 . 6 . 0 - 1 . osg34 . el7  globus - ftp - client - 9 . 1 - 2 . 1 . osg34 . el7  globus - ftp - client - debuginfo - 9 . 1 - 2 . 1 . osg34 . el7  globus - ftp - client - devel - 9 . 1 - 2 . 1 . osg34 . el7  globus - ftp - client - doc - 9 . 1 - 2 . 1 . osg34 . el7  globus - gridftp - server - 13 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - 8 . 0 - 1 . osg34 . el7  globus - gridftp - server - control - debuginfo - 8 . 0 - 1 . osg34 . el7  globus - gridftp - server - control - devel - 8 . 0 - 1 . osg34 . el7  globus - gridftp - server - debuginfo - 13 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - devel - 13 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - progs - 13 . 9 - 1 . 1 . osg34 . el7  htcondor - ce - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - client - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 2 . 2 - 1 . osg34 . el7  htcondor - ce - view - 3 . 2 . 2 - 1 . osg34 . el7  koji - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - builder - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - hub - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - hub - plugins - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - utils - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - vm - 1 . 11 . 1 - 1 . 1 . osg34 . el7  koji - web - 1 . 11 . 1 - 1 . 1 . osg34 . el7  osg - build - 1 . 14 . 2 - 1 . osg34 . el7  osg - build - base - 1 . 14 . 2 - 1 . osg34 . el7  osg - build - koji - 1 . 14 . 2 - 1 . osg34 . el7  osg - build - mock - 1 . 14 . 2 - 1 . osg34 . el7  osg - build - tests - 1 . 14 . 2 - 1 . osg34 . el7  osg - oasis - 12 - 1 . osg34 . el7  osg - pki - tools - 3 . 2 . 2 - 1 . osg34 . el7  osg - version - 3 . 4 . 27 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-6_2", 
            "text": "singularity-3.1.1-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-7_2", 
            "text": "singularity-3.1.1-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  singularity   singularity - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-6_3", 
            "text": "singularity - 3 . 1 . 1 - 1 . osgup . el6  singularity - debuginfo - 3 . 1 . 1 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-27/#enterprise-linux-7_3", 
            "text": "singularity - 3 . 1 . 1 - 1 . osgup . el7  singularity - debuginfo - 3 . 1 . 1 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.26-2\n\n\nRelease Date\n: 2019-04-02\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.97\n\n\nTemporarily withdraw EG-GRID 4a96b1ea for network availability reasons (EG)\n\n\n\n\n\n\nVO Package v87\n\n\nAdd new hcc cert\n\n\nRetire unused VOs: \ndzero\n, \nSBGrid\n, \nsuragrid\n \n \ndream\n\n\nUpdate voms.hep.wisc.edu \nlz\n VO certificate\n\n\nUpdate FNAL voms2 VO configuration\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.97-1.osg34.el6\n\n\nosg-ca-certs-1.80-1.osg34.el6\n\n\nvo-client-87-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.97-1.osg34.el7\n\n\nosg-ca-certs-1.80-1.osg34.el7\n\n\nvo-client-87-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n97\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n80\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n87\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n87\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n87\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n97\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n80\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n87\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n87\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n87\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.26-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#osg-software-stack-data-release-3426-2", 
            "text": "Release Date : 2019-04-02", 
            "title": "OSG Software Stack -- Data Release -- 3.4.26-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.97  Temporarily withdraw EG-GRID 4a96b1ea for network availability reasons (EG)    VO Package v87  Add new hcc cert  Retire unused VOs:  dzero ,  SBGrid ,  suragrid     dream  Update voms.hep.wisc.edu  lz  VO certificate  Update FNAL voms2 VO configuration     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.97-1.osg34.el6  osg-ca-certs-1.80-1.osg34.el6  vo-client-87-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.97-1.osg34.el7  osg-ca-certs-1.80-1.osg34.el7  vo-client-87-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 97 - 1 . osg34 . el6  osg - ca - certs - 1 . 80 - 1 . osg34 . el6  vo - client - 87 - 1 . osg34 . el6  vo - client - dcache - 87 - 1 . osg34 . el6  vo - client - lcmaps - voms - 87 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 97 - 1 . osg34 . el7  osg - ca - certs - 1 . 80 - 1 . osg34 . el7  vo - client - 87 - 1 . osg34 . el7  vo - client - dcache - 87 - 1 . osg34 . el7  vo - client - lcmaps - voms - 87 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/", 
            "text": "OSG Software Release 3.4.26\n\n\nRelease Date\n: 2019-03-07\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCooperative Computing Tools\n 7.0.9: Upgrade from version 4.4.3\n\n\nPegasus 4.9.1\n\n\nBug fix release\n\n\nWhen using containers, transfers happen within the container\n\n\n\n\n\n\nosg-pki-tools 3.1.0\n:\n    Added \nosg-incommon-cert-request\n for use by administrators with an InCommon account that can create certificates.\n\n\nUpcoming Repository\n\n\nSingularity 3.1.0\n: Feature Release, new OCI compliant variant of Singularity runtime\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncctools-7.0.9-2.osg34.el6\n\n\nosg-pki-tools-3.1.0-1.1.osg34.el6\n\n\nosg-version-3.4.26-1.osg34.el6\n\n\npegasus-4.9.1-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncctools-7.0.9-2.osg34.el7\n\n\nosg-pki-tools-3.1.0-1.1.osg34.el7\n\n\nosg-version-3.4.26-1.osg34.el7\n\n\npegasus-4.9.1-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncctools\n \ncctools\n-\ndebuginfo\n \ncctools\n-\ndevel\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nversion\n \npegasus\n \npegasus\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncctools\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndebuginfo\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndevel\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n1\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n26\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncctools\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndebuginfo\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndevel\n-\n7\n.\n0\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n1\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n26\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n9\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nsingularity-3.1.0-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nsingularity-3.1.0-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nsingularity\n \nsingularity\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nsingularity\n-\n3\n.\n1\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nsingularity\n-\n3\n.\n1\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n1\n.\n0\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.26"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#osg-software-release-3426", 
            "text": "Release Date : 2019-03-07", 
            "title": "OSG Software Release 3.4.26"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#summary-of-changes", 
            "text": "This release contains:   Cooperative Computing Tools  7.0.9: Upgrade from version 4.4.3  Pegasus 4.9.1  Bug fix release  When using containers, transfers happen within the container    osg-pki-tools 3.1.0 :\n    Added  osg-incommon-cert-request  for use by administrators with an InCommon account that can create certificates.  Upcoming Repository  Singularity 3.1.0 : Feature Release, new OCI compliant variant of Singularity runtime     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-6", 
            "text": "cctools-7.0.9-2.osg34.el6  osg-pki-tools-3.1.0-1.1.osg34.el6  osg-version-3.4.26-1.osg34.el6  pegasus-4.9.1-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-7", 
            "text": "cctools-7.0.9-2.osg34.el7  osg-pki-tools-3.1.0-1.1.osg34.el7  osg-version-3.4.26-1.osg34.el7  pegasus-4.9.1-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cctools   cctools - debuginfo   cctools - devel   osg - pki - tools   osg - version   pegasus   pegasus - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-6_1", 
            "text": "cctools - 7 . 0 . 9 - 2 . osg34 . el6  cctools - debuginfo - 7 . 0 . 9 - 2 . osg34 . el6  cctools - devel - 7 . 0 . 9 - 2 . osg34 . el6  osg - pki - tools - 3 . 1 . 0 - 1 . 1 . osg34 . el6  osg - version - 3 . 4 . 26 - 1 . osg34 . el6  pegasus - 4 . 9 . 1 - 1 . osg34 . el6  pegasus - debuginfo - 4 . 9 . 1 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-7_1", 
            "text": "cctools - 7 . 0 . 9 - 2 . osg34 . el7  cctools - debuginfo - 7 . 0 . 9 - 2 . osg34 . el7  cctools - devel - 7 . 0 . 9 - 2 . osg34 . el7  osg - pki - tools - 3 . 1 . 0 - 1 . 1 . osg34 . el7  osg - version - 3 . 4 . 26 - 1 . osg34 . el7  pegasus - 4 . 9 . 1 - 1 . osg34 . el7  pegasus - debuginfo - 4 . 9 . 1 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-6_2", 
            "text": "singularity-3.1.0-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-7_2", 
            "text": "singularity-3.1.0-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  singularity   singularity - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-6_3", 
            "text": "singularity - 3 . 1 . 0 - 1 . osgup . el6  singularity - debuginfo - 3 . 1 . 0 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-26/#enterprise-linux-7_3", 
            "text": "singularity - 3 . 1 . 0 - 1 . osgup . el7  singularity - debuginfo - 3 . 1 . 0 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/", 
            "text": "OSG Software Release 3.4.25\n\n\nRelease Date\n: 2019-03-07\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\ngsi-openssh 7.4p1: Update from \nOpenSSH 7.3\n\n\nHTCondor 8.6.13: Fixed duplicate accounting records when HTCondor-CE has a HTCondor backend\n\n\nxrootd-lcmaps 1.7.0: Added --no-authz flag for StashCache cache servers\n\n\nUpcoming Repository\n\n\nHTCondor 8.8.1\n: Bug fix release\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is only supported on EL7\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.13-1.2.osg34.el6\n\n\nkoji-1.11.1-1.osg34.el6\n\n\nosg-tested-internal-3.4-7.osg34.el6\n\n\nosg-version-3.4.25-1.osg34.el6\n\n\nxrootd-lcmaps-1.7.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.13-1.2.osg34.el7\n\n\ngsi-openssh-7.4p1-2.3.osg34.el7\n\n\nkoji-1.11.1-1.osg34.el7\n\n\nosg-tested-internal-3.4-7.osg34.el7\n\n\nosg-version-3.4.25-1.osg34.el7\n\n\nxrootd-lcmaps-1.7.0-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ngsi\n-\nopenssh\n \ngsi\n-\nopenssh\n-\nclients\n \ngsi\n-\nopenssh\n-\ndebuginfo\n \ngsi\n-\nopenssh\n-\nserver\n \nigtf\n-\nca\n-\ncerts\n \nkoji\n \nkoji\n-\nbuilder\n \nkoji\n-\nhub\n \nkoji\n-\nhub\n-\nplugins\n \nkoji\n-\nutils\n \nkoji\n-\nvm\n \nkoji\n-\nweb\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\ntested\n-\ninternal\n \nosg\n-\nversion\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nkoji\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n25\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\n7\n.\n4\np1\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nclients\n-\n7\n.\n4\np1\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\ndebuginfo\n-\n7\n.\n4\np1\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nserver\n-\n7\n.\n4\np1\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\nkoji\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n25\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.8.1-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.8.1-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nminicondor\n \npython2\n-\ncondor\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\nminicondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\nminicondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n1\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.25"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#osg-software-release-3425", 
            "text": "Release Date : 2019-03-07", 
            "title": "OSG Software Release 3.4.25"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#summary-of-changes", 
            "text": "This release contains:   gsi-openssh 7.4p1: Update from  OpenSSH 7.3  HTCondor 8.6.13: Fixed duplicate accounting records when HTCondor-CE has a HTCondor backend  xrootd-lcmaps 1.7.0: Added --no-authz flag for StashCache cache servers  Upcoming Repository  HTCondor 8.8.1 : Bug fix release     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is only supported on EL7    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-6", 
            "text": "condor-8.6.13-1.2.osg34.el6  koji-1.11.1-1.osg34.el6  osg-tested-internal-3.4-7.osg34.el6  osg-version-3.4.25-1.osg34.el6  xrootd-lcmaps-1.7.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-7", 
            "text": "condor-8.6.13-1.2.osg34.el7  gsi-openssh-7.4p1-2.3.osg34.el7  koji-1.11.1-1.osg34.el7  osg-tested-internal-3.4-7.osg34.el7  osg-version-3.4.25-1.osg34.el7  xrootd-lcmaps-1.7.0-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   gsi - openssh   gsi - openssh - clients   gsi - openssh - debuginfo   gsi - openssh - server   igtf - ca - certs   koji   koji - builder   koji - hub   koji - hub - plugins   koji - utils   koji - vm   koji - web   osg - ca - certs   osg - tested - internal   osg - version   vo - client   vo - client - dcache   vo - client - lcmaps - voms   xrootd - lcmaps   xrootd - lcmaps - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - all - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - bosco - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - classads - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - classads - devel - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - cream - gahp - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - debuginfo - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - kbdd - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - procd - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - python - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - std - universe - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - test - 8 . 6 . 13 - 1 . 2 . osg34 . el6  condor - vm - gahp - 8 . 6 . 13 - 1 . 2 . osg34 . el6  koji - 1 . 11 . 1 - 1 . osg34 . el6  koji - builder - 1 . 11 . 1 - 1 . osg34 . el6  koji - hub - 1 . 11 . 1 - 1 . osg34 . el6  koji - hub - plugins - 1 . 11 . 1 - 1 . osg34 . el6  koji - utils - 1 . 11 . 1 - 1 . osg34 . el6  koji - vm - 1 . 11 . 1 - 1 . osg34 . el6  koji - web - 1 . 11 . 1 - 1 . osg34 . el6  osg - tested - internal - 3 . 4 - 7 . osg34 . el6  osg - version - 3 . 4 . 25 - 1 . osg34 . el6  xrootd - lcmaps - 1 . 7 . 0 - 1 . osg34 . el6  xrootd - lcmaps - debuginfo - 1 . 7 . 0 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - all - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - bosco - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - classads - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - classads - devel - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - cream - gahp - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - debuginfo - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - kbdd - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - procd - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - python - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - test - 8 . 6 . 13 - 1 . 2 . osg34 . el7  condor - vm - gahp - 8 . 6 . 13 - 1 . 2 . osg34 . el7  gsi - openssh - 7 . 4 p1 - 2 . 3 . osg34 . el7  gsi - openssh - clients - 7 . 4 p1 - 2 . 3 . osg34 . el7  gsi - openssh - debuginfo - 7 . 4 p1 - 2 . 3 . osg34 . el7  gsi - openssh - server - 7 . 4 p1 - 2 . 3 . osg34 . el7  koji - 1 . 11 . 1 - 1 . osg34 . el7  koji - builder - 1 . 11 . 1 - 1 . osg34 . el7  koji - hub - 1 . 11 . 1 - 1 . osg34 . el7  koji - hub - plugins - 1 . 11 . 1 - 1 . osg34 . el7  koji - utils - 1 . 11 . 1 - 1 . osg34 . el7  koji - vm - 1 . 11 . 1 - 1 . osg34 . el7  koji - web - 1 . 11 . 1 - 1 . osg34 . el7  osg - tested - internal - 3 . 4 - 7 . osg34 . el7  osg - version - 3 . 4 . 25 - 1 . osg34 . el7  xrootd - lcmaps - 1 . 7 . 0 - 1 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 7 . 0 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-6_2", 
            "text": "condor-8.8.1-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-7_2", 
            "text": "condor-8.8.1-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   minicondor   python2 - condor   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-6_3", 
            "text": "condor - 8 . 8 . 1 - 1 . osgup . el6  condor - all - 8 . 8 . 1 - 1 . osgup . el6  condor - annex - ec2 - 8 . 8 . 1 - 1 . osgup . el6  condor - bosco - 8 . 8 . 1 - 1 . osgup . el6  condor - classads - 8 . 8 . 1 - 1 . osgup . el6  condor - classads - devel - 8 . 8 . 1 - 1 . osgup . el6  condor - cream - gahp - 8 . 8 . 1 - 1 . osgup . el6  condor - debuginfo - 8 . 8 . 1 - 1 . osgup . el6  condor - kbdd - 8 . 8 . 1 - 1 . osgup . el6  condor - procd - 8 . 8 . 1 - 1 . osgup . el6  condor - std - universe - 8 . 8 . 1 - 1 . osgup . el6  condor - test - 8 . 8 . 1 - 1 . osgup . el6  condor - vm - gahp - 8 . 8 . 1 - 1 . osgup . el6  minicondor - 8 . 8 . 1 - 1 . osgup . el6  python2 - condor - 8 . 8 . 1 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-25/#enterprise-linux-7_3", 
            "text": "condor - 8 . 8 . 1 - 1 . osgup . el7  condor - all - 8 . 8 . 1 - 1 . osgup . el7  condor - annex - ec2 - 8 . 8 . 1 - 1 . osgup . el7  condor - bosco - 8 . 8 . 1 - 1 . osgup . el7  condor - classads - 8 . 8 . 1 - 1 . osgup . el7  condor - classads - devel - 8 . 8 . 1 - 1 . osgup . el7  condor - cream - gahp - 8 . 8 . 1 - 1 . osgup . el7  condor - debuginfo - 8 . 8 . 1 - 1 . osgup . el7  condor - kbdd - 8 . 8 . 1 - 1 . osgup . el7  condor - procd - 8 . 8 . 1 - 1 . osgup . el7  condor - test - 8 . 8 . 1 - 1 . osgup . el7  condor - vm - gahp - 8 . 8 . 1 - 1 . osgup . el7  minicondor - 8 . 8 . 1 - 1 . osgup . el7  python2 - condor - 8 . 8 . 1 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.24-2\n\n\nRelease Date\n: 2019-03-05\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.96\n\n\nWithdrawn superseded QuoVadis-Grid-ICA (1st gen) CA (BM)\n\n\nAdded new trust anchor MD-Grid-CA-T for rollover of existing CA (MD)\n\n\nDiscontinued expiring 2009 series MD-Grid-CA (MD)\n\n\n\n\n\n\nVO Package v86\n\n\nRetire the \nCIGI\n VO\n\n\nAdd backup \nlz\n VOMS Admin Server\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.96-1.osg34.el6\n\n\nosg-ca-certs-1.79-1.osg34.el6\n\n\nvo-client-86-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.96-1.osg34.el7\n\n\nosg-ca-certs-1.79-1.osg34.el7\n\n\nvo-client-86-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n96\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n79\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n86\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n86\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n86\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n96\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n79\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n86\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n86\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n86\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.24-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#osg-software-stack-data-release-3424-2", 
            "text": "Release Date : 2019-03-05", 
            "title": "OSG Software Stack -- Data Release -- 3.4.24-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.96  Withdrawn superseded QuoVadis-Grid-ICA (1st gen) CA (BM)  Added new trust anchor MD-Grid-CA-T for rollover of existing CA (MD)  Discontinued expiring 2009 series MD-Grid-CA (MD)    VO Package v86  Retire the  CIGI  VO  Add backup  lz  VOMS Admin Server     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.96-1.osg34.el6  osg-ca-certs-1.79-1.osg34.el6  vo-client-86-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.96-1.osg34.el7  osg-ca-certs-1.79-1.osg34.el7  vo-client-86-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   vo - client   vo - client - dcache   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 96 - 1 . osg34 . el6  osg - ca - certs - 1 . 79 - 1 . osg34 . el6  vo - client - 86 - 1 . osg34 . el6  vo - client - dcache - 86 - 1 . osg34 . el6  vo - client - lcmaps - voms - 86 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 96 - 1 . osg34 . el7  osg - ca - certs - 1 . 79 - 1 . osg34 . el7  vo - client - 86 - 1 . osg34 . el7  vo - client - dcache - 86 - 1 . osg34 . el7  vo - client - lcmaps - voms - 86 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/", 
            "text": "OSG Software Release 3.4.24\n\n\nRelease Date\n: 2019-02-21\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nBLAHP 1.18.39: Now propagates signals down to the payload job\n\n\nosg-pki-tools 3.0.1: Now accepts multi-word states or provinces\n\n\nHTCondor-CE 3.2.1\n: Minor adjustments to work with new HTCondor versions\n\n\ncondor-cron 1.1.4: Minor adjustments to work with new HTCondor versions\n\n\nosg-release 3.4-1: New osg-rolling repository (disabled by default) allows early access to fully tested software\n\n\nUpcoming Repository\n\n\nSingularity 3.0.3\n (\nchange Log\n)\n\n\n\n\nLimited client HDFS functionality to mount HDFS on EL6 hosts via FUSE\n\n\n\n\nInfo\n\n\nUpgrade to EL7 to update name nodes, data nodes, XRootD, or GridFTP hosts.\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.39.bosco-1.osg34.el6\n\n\ncondor-cron-1.1.4-1.osg34.el6\n\n\nhtcondor-ce-3.2.1-1.osg34.el6\n\n\nosg-build-1.14.1-1.osg34.el6\n\n\nosg-pki-tools-3.0.1-1.osg34.el6\n\n\nosg-release-3.4-7.osg34.el6\n\n\nosg-test-2.3.1-1.osg34.el6\n\n\nosg-version-3.4.24-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.39.bosco-1.osg34.el7\n\n\ncondor-cron-1.1.4-1.osg34.el7\n\n\nhtcondor-ce-3.2.1-1.osg34.el7\n\n\nosg-build-1.14.1-1.osg34.el7\n\n\nosg-pki-tools-3.0.1-1.osg34.el7\n\n\nosg-release-3.4-7.osg34.el7\n\n\nosg-test-2.3.1-1.osg34.el7\n\n\nosg-version-3.4.24-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n-\ncron\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nrelease\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n24\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n14\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n24\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\navro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osgup.el6\n\n\nbigtop-jsvc-0.3.0-1.2.osgup.el6\n\n\nbigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osgup.el6\n\n\nblahp-1.18.39.bosco-1.osgup.el6\n\n\nhadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osgup.el6\n\n\nsingularity-3.0.3-1.osgup.el6\n\n\nzookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.39.bosco-1.osgup.el7\n\n\nsingularity-3.0.3-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\navro\n-\ndoc\n \navro\n-\nlibs\n \navro\n-\ntools\n \nbigtop\n-\njsvc\n \nbigtop\n-\njsvc\n-\ndebuginfo\n \nbigtop\n-\nutils\n \nblahp\n \nblahp\n-\ndebuginfo\n \nhadoop\n \nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n \nhadoop\n-\n0\n.\n20\n-\nmapreduce\n \nhadoop\n-\nclient\n \nhadoop\n-\nconf\n-\npseudo\n \nhadoop\n-\ndebuginfo\n \nhadoop\n-\ndoc\n \nhadoop\n-\nhdfs\n \nhadoop\n-\nhdfs\n-\ndatanode\n \nhadoop\n-\nhdfs\n-\nfuse\n \nhadoop\n-\nhdfs\n-\njournalnode\n \nhadoop\n-\nhdfs\n-\nnamenode\n \nhadoop\n-\nhdfs\n-\nnfs3\n \nhadoop\n-\nhdfs\n-\nsecondarynamenode\n \nhadoop\n-\nhdfs\n-\nzkfc\n \nhadoop\n-\nhttpfs\n \nhadoop\n-\nkms\n \nhadoop\n-\nkms\n-\nserver\n \nhadoop\n-\nlibhdfs\n \nhadoop\n-\nlibhdfs\n-\ndevel\n \nhadoop\n-\nmapreduce\n \nhadoop\n-\nyarn\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nzookeeper\n \nzookeeper\n-\ndebuginfo\n \nzookeeper\n-\nnative\n \nzookeeper\n-\nserver\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\navro\n-\ndoc\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n2\n.\nosgup\n.\nel6\n\n\navro\n-\nlibs\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n2\n.\nosgup\n.\nel6\n\n\navro\n-\ntools\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n2\n.\nosgup\n.\nel6\n\n\nbigtop\n-\njsvc\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosgup\n.\nel6\n\n\nbigtop\n-\njsvc\n-\ndebuginfo\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosgup\n.\nel6\n\n\nbigtop\n-\nutils\n-\n0\n.\n7\n.\n0\n+\ncdh5\n.\n13\n.\n0\n+\n0\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\n0\n.\n20\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nclient\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\ndebuginfo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\ndoc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\ndatanode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\nfuse\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\njournalnode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\nnamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\nnfs3\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\nsecondarynamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhdfs\n-\nzkfc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nhttpfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nkms\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nkms\n-\nserver\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nlibhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nlibhdfs\n-\ndevel\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nhadoop\n-\nyarn\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n8\n.\nosgup\n.\nel6\n\n\nsingularity\n-\n3\n.\n0\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n0\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\nzookeeper\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosgup\n.\nel6\n\n\nzookeeper\n-\ndebuginfo\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosgup\n.\nel6\n\n\nzookeeper\n-\nnative\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosgup\n.\nel6\n\n\nzookeeper\n-\nserver\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n39\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\n3\n.\n0\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n0\n.\n3\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.24"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#osg-software-release-3424", 
            "text": "Release Date : 2019-02-21", 
            "title": "OSG Software Release 3.4.24"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#summary-of-changes", 
            "text": "This release contains:   BLAHP 1.18.39: Now propagates signals down to the payload job  osg-pki-tools 3.0.1: Now accepts multi-word states or provinces  HTCondor-CE 3.2.1 : Minor adjustments to work with new HTCondor versions  condor-cron 1.1.4: Minor adjustments to work with new HTCondor versions  osg-release 3.4-1: New osg-rolling repository (disabled by default) allows early access to fully tested software  Upcoming Repository  Singularity 3.0.3  ( change Log )   Limited client HDFS functionality to mount HDFS on EL6 hosts via FUSE   Info  Upgrade to EL7 to update name nodes, data nodes, XRootD, or GridFTP hosts.       These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-6", 
            "text": "blahp-1.18.39.bosco-1.osg34.el6  condor-cron-1.1.4-1.osg34.el6  htcondor-ce-3.2.1-1.osg34.el6  osg-build-1.14.1-1.osg34.el6  osg-pki-tools-3.0.1-1.osg34.el6  osg-release-3.4-7.osg34.el6  osg-test-2.3.1-1.osg34.el6  osg-version-3.4.24-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-7", 
            "text": "blahp-1.18.39.bosco-1.osg34.el7  condor-cron-1.1.4-1.osg34.el7  htcondor-ce-3.2.1-1.osg34.el7  osg-build-1.14.1-1.osg34.el7  osg-pki-tools-3.0.1-1.osg34.el7  osg-release-3.4-7.osg34.el7  osg-test-2.3.1-1.osg34.el7  osg-version-3.4.24-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor - cron   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - pki - tools   osg - release   osg - test   osg - test - log - viewer   osg - version   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 39 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 39 . bosco - 1 . osg34 . el6  condor - cron - 1 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - client - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - view - 3 . 2 . 1 - 1 . osg34 . el6  osg - build - 1 . 14 . 1 - 1 . osg34 . el6  osg - build - base - 1 . 14 . 1 - 1 . osg34 . el6  osg - build - koji - 1 . 14 . 1 - 1 . osg34 . el6  osg - build - mock - 1 . 14 . 1 - 1 . osg34 . el6  osg - build - tests - 1 . 14 . 1 - 1 . osg34 . el6  osg - pki - tools - 3 . 0 . 1 - 1 . osg34 . el6  osg - release - 3 . 4 - 7 . osg34 . el6  osg - test - 2 . 3 . 1 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 3 . 1 - 1 . osg34 . el6  osg - version - 3 . 4 . 24 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 39 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 39 . bosco - 1 . osg34 . el7  condor - cron - 1 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - client - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - view - 3 . 2 . 1 - 1 . osg34 . el7  osg - build - 1 . 14 . 1 - 1 . osg34 . el7  osg - build - base - 1 . 14 . 1 - 1 . osg34 . el7  osg - build - koji - 1 . 14 . 1 - 1 . osg34 . el7  osg - build - mock - 1 . 14 . 1 - 1 . osg34 . el7  osg - build - tests - 1 . 14 . 1 - 1 . osg34 . el7  osg - pki - tools - 3 . 0 . 1 - 1 . osg34 . el7  osg - release - 3 . 4 - 7 . osg34 . el7  osg - test - 2 . 3 . 1 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 3 . 1 - 1 . osg34 . el7  osg - version - 3 . 4 . 24 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-6_2", 
            "text": "avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.2.osgup.el6  bigtop-jsvc-0.3.0-1.2.osgup.el6  bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osgup.el6  blahp-1.18.39.bosco-1.osgup.el6  hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.8.osgup.el6  singularity-3.0.3-1.osgup.el6  zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-7_2", 
            "text": "blahp-1.18.39.bosco-1.osgup.el7  singularity-3.0.3-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  avro - doc   avro - libs   avro - tools   bigtop - jsvc   bigtop - jsvc - debuginfo   bigtop - utils   blahp   blahp - debuginfo   hadoop   hadoop - 0 . 20 - conf - pseudo   hadoop - 0 . 20 - mapreduce   hadoop - client   hadoop - conf - pseudo   hadoop - debuginfo   hadoop - doc   hadoop - hdfs   hadoop - hdfs - datanode   hadoop - hdfs - fuse   hadoop - hdfs - journalnode   hadoop - hdfs - namenode   hadoop - hdfs - nfs3   hadoop - hdfs - secondarynamenode   hadoop - hdfs - zkfc   hadoop - httpfs   hadoop - kms   hadoop - kms - server   hadoop - libhdfs   hadoop - libhdfs - devel   hadoop - mapreduce   hadoop - yarn   singularity   singularity - debuginfo   zookeeper   zookeeper - debuginfo   zookeeper - native   zookeeper - server   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-6_3", 
            "text": "avro - doc - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 2 . osgup . el6  avro - libs - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 2 . osgup . el6  avro - tools - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 2 . osgup . el6  bigtop - jsvc - 0 . 3 . 0 - 1 . 2 . osgup . el6  bigtop - jsvc - debuginfo - 0 . 3 . 0 - 1 . 2 . osgup . el6  bigtop - utils - 0 . 7 . 0 + cdh5 . 13 . 0 + 0 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osgup . el6  blahp - 1 . 18 . 39 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 39 . bosco - 1 . osgup . el6  hadoop - 0 . 20 - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - 0 . 20 - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - client - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - debuginfo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - doc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - datanode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - fuse - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - journalnode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - namenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - nfs3 - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - secondarynamenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - hdfs - zkfc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - httpfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - kms - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - kms - server - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - libhdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - libhdfs - devel - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  hadoop - yarn - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 8 . osgup . el6  singularity - 3 . 0 . 3 - 1 . osgup . el6  singularity - debuginfo - 3 . 0 . 3 - 1 . osgup . el6  zookeeper - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osgup . el6  zookeeper - debuginfo - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osgup . el6  zookeeper - native - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osgup . el6  zookeeper - server - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-24/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 39 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 39 . bosco - 1 . osgup . el7  singularity - 3 . 0 . 3 - 1 . osgup . el7  singularity - debuginfo - 3 . 0 . 3 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/", 
            "text": "OSG Software Release 3.4.23\n\n\nRelease Date\n: 2019-01-23\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nGratia probes 1.20.8\n\n\nInterpret CPU expressions for Hosted CEs (needed for proper accounting)\n\n\nMinor change properly set \nProcessor\n field for Slurm (no impact on accounting)\n\n\nAdded unit tests for the HTCondor probe processors field\n\n\n\n\n\n\nUpcoming Repository\n\n\nSingularity 3.0.2\n\n\nReimplemented in Go\n\n\nMany changes\n\n\n\n\n\n\n\n\nHTCondor 8.8.0\n\n\n\n\nJob Router: follow first match by default, easier to write routing rules\n\n\n\n\n\n\nSites with multiple routes\n\n\nIf instead you prefer round-robin matching to to distribute jobs over routes,\nadd \nJOB_ROUTER_ROUND_ROBIN_SELECTION = True\n to a file in the \n/etc/condor-ce/config.d\n directory\nand run \ncondor_ce_reconfig\n.\n\n\n\n\n\n\nPerformance improvements in the collector\n\n\nTracks GPU resources\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ngratia-probe-1.20.8-1.osg34.el6\n\n\nosg-version-3.4.23-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ngratia-probe-1.20.8-1.osg34.el7\n\n\nosg-version-3.4.23-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nversion\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n23\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n23\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.8.0-1.osgup.el6\n\n\nsingularity-3.0.2-1.3.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.8.0-1.osgup.el7\n\n\nsingularity-3.0.2-1.3.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n \nminicondor\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nhdfs\n \nosg\n-\ngridftp\n-\nxrootd\n \npython2\n-\ncondor\n \nsingularity\n \nsingularity\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\nminicondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel6\n\n\nsingularity\n-\n3\n.\n0\n.\n2\n-\n1\n.\n3\n.\nosgup\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n0\n.\n2\n-\n1\n.\n3\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\nminicondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\npython2\n-\ncondor\n-\n8\n.\n8\n.\n0\n-\n1\n.\nosgup\n.\nel7\n\n\nsingularity\n-\n3\n.\n0\n.\n2\n-\n1\n.\n3\n.\nosgup\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n3\n.\n0\n.\n2\n-\n1\n.\n3\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.23"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#osg-software-release-3423", 
            "text": "Release Date : 2019-01-23", 
            "title": "OSG Software Release 3.4.23"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#summary-of-changes", 
            "text": "This release contains:   Gratia probes 1.20.8  Interpret CPU expressions for Hosted CEs (needed for proper accounting)  Minor change properly set  Processor  field for Slurm (no impact on accounting)  Added unit tests for the HTCondor probe processors field    Upcoming Repository  Singularity 3.0.2  Reimplemented in Go  Many changes     HTCondor 8.8.0   Job Router: follow first match by default, easier to write routing rules    Sites with multiple routes  If instead you prefer round-robin matching to to distribute jobs over routes,\nadd  JOB_ROUTER_ROUND_ROBIN_SELECTION = True  to a file in the  /etc/condor-ce/config.d  directory\nand run  condor_ce_reconfig .    Performance improvements in the collector  Tracks GPU resources       These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-6", 
            "text": "gratia-probe-1.20.8-1.osg34.el6  osg-version-3.4.23-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-7", 
            "text": "gratia-probe-1.20.8-1.osg34.el7  osg-version-3.4.23-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glideinwms   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   igtf - ca - certs   osg - ca - certs   osg - version   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-6_1", 
            "text": "gratia - probe - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - common - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - condor - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - metric - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - services - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - sge - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 20 . 8 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 20 . 8 - 1 . osg34 . el6  osg - version - 3 . 4 . 23 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-7_1", 
            "text": "gratia - probe - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - common - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - condor - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - metric - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - services - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - sge - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 20 . 8 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 20 . 8 - 1 . osg34 . el7  osg - version - 3 . 4 . 23 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-6_2", 
            "text": "condor-8.8.0-1.osgup.el6  singularity-3.0.2-1.3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-7_2", 
            "text": "condor-8.8.0-1.osgup.el7  singularity-3.0.2-1.3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - std - universe   condor - test   condor - vm - gahp   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   minicondor   osg - gridftp   osg - gridftp - hdfs   osg - gridftp - xrootd   python2 - condor   singularity   singularity - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-6_3", 
            "text": "condor - 8 . 8 . 0 - 1 . osgup . el6  condor - all - 8 . 8 . 0 - 1 . osgup . el6  condor - annex - ec2 - 8 . 8 . 0 - 1 . osgup . el6  condor - bosco - 8 . 8 . 0 - 1 . osgup . el6  condor - classads - 8 . 8 . 0 - 1 . osgup . el6  condor - classads - devel - 8 . 8 . 0 - 1 . osgup . el6  condor - cream - gahp - 8 . 8 . 0 - 1 . osgup . el6  condor - debuginfo - 8 . 8 . 0 - 1 . osgup . el6  condor - kbdd - 8 . 8 . 0 - 1 . osgup . el6  condor - procd - 8 . 8 . 0 - 1 . osgup . el6  condor - std - universe - 8 . 8 . 0 - 1 . osgup . el6  condor - test - 8 . 8 . 0 - 1 . osgup . el6  condor - vm - gahp - 8 . 8 . 0 - 1 . osgup . el6  minicondor - 8 . 8 . 0 - 1 . osgup . el6  python2 - condor - 8 . 8 . 0 - 1 . osgup . el6  singularity - 3 . 0 . 2 - 1 . 3 . osgup . el6  singularity - debuginfo - 3 . 0 . 2 - 1 . 3 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-23/#enterprise-linux-7_3", 
            "text": "condor - 8 . 8 . 0 - 1 . osgup . el7  condor - all - 8 . 8 . 0 - 1 . osgup . el7  condor - annex - ec2 - 8 . 8 . 0 - 1 . osgup . el7  condor - bosco - 8 . 8 . 0 - 1 . osgup . el7  condor - classads - 8 . 8 . 0 - 1 . osgup . el7  condor - classads - devel - 8 . 8 . 0 - 1 . osgup . el7  condor - cream - gahp - 8 . 8 . 0 - 1 . osgup . el7  condor - debuginfo - 8 . 8 . 0 - 1 . osgup . el7  condor - kbdd - 8 . 8 . 0 - 1 . osgup . el7  condor - procd - 8 . 8 . 0 - 1 . osgup . el7  condor - test - 8 . 8 . 0 - 1 . osgup . el7  condor - vm - gahp - 8 . 8 . 0 - 1 . osgup . el7  minicondor - 8 . 8 . 0 - 1 . osgup . el7  python2 - condor - 8 . 8 . 0 - 1 . osgup . el7  singularity - 3 . 0 . 2 - 1 . 3 . osgup . el7  singularity - debuginfo - 3 . 0 . 2 - 1 . 3 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.22-2\n\n\nRelease Date\n: 2019-01-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.95\n\n\nUpdated namespaces and signing_policy files for CILogon Silver CA to permit DNs without \"/C=US\" (US)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.95-1.osg34.el6\n\n\nosg-ca-certs-1.78-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.95-1.osg34.el7\n\n\nosg-ca-certs-1.78-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n95\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n78\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n95\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n78\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.22-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#osg-software-stack-data-release-3422-2", 
            "text": "Release Date : 2019-01-14", 
            "title": "OSG Software Stack -- Data Release -- 3.4.22-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.95  Updated namespaces and signing_policy files for CILogon Silver CA to permit DNs without \"/C=US\" (US)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.95-1.osg34.el6  osg-ca-certs-1.78-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.95-1.osg34.el7  osg-ca-certs-1.78-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 95 - 1 . osg34 . el6  osg - ca - certs - 1 . 78 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-22-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 95 - 1 . osg34 . el7  osg - ca - certs - 1 . 78 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/", 
            "text": "OSG Software Release 3.4.22\n\n\nRelease Date\n: 2018-12-20\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor-CE 3.2.0\n: \nAccept pilot jobs authenticated with host certificates signed by a VO\n\n\nfrontier-squid 4.4\n: Update from 3.5.28\n\n\nPegasus 4.9.0\n: Update from 4.8.4\n\n\nHTCondor 8.6.13: \nPatched memory leak when remote daemon is offline\n\n\nosg-ca-scripts 1.2.4: Drop MD5 checksum requirement, prefer SHA256\n\n\nstashcache-client 5.2.0: Can install stashcp locally, without using CVMFS\n\n\nCVMFS 2.5.2\n:  Bug fix release\n\n\nosg-wn-client 3.4-5: Include stashcp on the worker-node\n\n\nCertificate Authority certificates based on \nIGTF 1.94\n\n\nextended validity period for the ArmeSFo CA (AM)\n\n\nwithdrawn expiring DFN-SLCS CA (DE)\n\n\n\n\n\n\nVO Package v85\n\n\nUpdate \nINFN\n CA DN\n\n\nAdd backup VOMS server for \nenmr.eu\n and \nglast.org\n\n\n\n\n\n\n\n\n\n\nImportant HTCondor-CE update\n\n\nThis release of HTCondor-CE adds support for pilot jobs authenticating\nwith host certificates signed by a VO.  Due to the expiration of service\ncertificates starting in April 2019, it is important to update to this\nversion to continue to receive OSG jobs.\nBe sure to watch out for \n/etc/condor-ce/condor_mapfile.rpmnew\n and merge\nany changes into \n/etc/condor-ce/condor_mapfile\n.\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.13-1.1.osg34.el6\n\n\ncvmfs-2.5.2-1.osg34.el6\n\n\ncvmfs-gateway-0.3.1-1.1.osg34.el6\n\n\nfrontier-squid-4.4-1.1.osg34.el6\n\n\nhtcondor-ce-3.2.0-1.osg34.el6\n\n\nigtf-ca-certs-1.94-2.osg34.el6\n\n\nosg-ca-certs-1.77-1.osg34.el6\n\n\nosg-ca-scripts-1.2.4-1.osg34.el6\n\n\nosg-oasis-11-1.osg34.el6\n\n\nosg-test-2.3.0-1.osg34.el6\n\n\nosg-version-3.4.22-1.osg34.el6\n\n\nosg-wn-client-3.4-5.osg34.el6\n\n\npegasus-4.9.0-1.osg34.el6\n\n\nstashcache-client-5.2.0-1.osg34.el6\n\n\nvo-client-85-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.13-1.1.osg34.el7\n\n\ncvmfs-2.5.2-1.osg34.el7\n\n\ncvmfs-gateway-0.3.1-1.1.osg34.el7\n\n\nfrontier-squid-4.4-1.1.osg34.el7\n\n\nhtcondor-ce-3.2.0-1.osg34.el7\n\n\nigtf-ca-certs-1.94-2.osg34.el7\n\n\nosg-ca-certs-1.77-1.osg34.el7\n\n\nosg-ca-scripts-1.2.4-1.osg34.el7\n\n\nosg-oasis-11-1.osg34.el7\n\n\nosg-test-2.3.0-1.osg34.el7\n\n\nosg-version-3.4.22-1.osg34.el7\n\n\nosg-wn-client-3.4-5.osg34.el7\n\n\npegasus-4.9.0-1.osg34.el7\n\n\nstashcache-client-5.2.0-1.osg34.el7\n\n\nvo-client-85-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\ngateway\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\noasis\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nosg\n-\nwn\n-\nclient\n \npegasus\n \npegasus\n-\ndebuginfo\n \nstashcache\n-\nclient\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ngateway\n-\n0\n.\n3\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\n4\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n4\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n94\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n77\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n85\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n11\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n22\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n9\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n9\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nstashcache\n-\nclient\n-\n5\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n85\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n85\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n85\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n85\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ngateway\n-\n0\n.\n3\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\n4\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n4\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n94\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n77\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n85\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n11\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n22\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n9\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n9\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\nclient\n-\n5\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n85\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n85\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n85\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n85\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.22"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#osg-software-release-3422", 
            "text": "Release Date : 2018-12-20", 
            "title": "OSG Software Release 3.4.22"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#summary-of-changes", 
            "text": "This release contains:   HTCondor-CE 3.2.0 :  Accept pilot jobs authenticated with host certificates signed by a VO  frontier-squid 4.4 : Update from 3.5.28  Pegasus 4.9.0 : Update from 4.8.4  HTCondor 8.6.13:  Patched memory leak when remote daemon is offline  osg-ca-scripts 1.2.4: Drop MD5 checksum requirement, prefer SHA256  stashcache-client 5.2.0: Can install stashcp locally, without using CVMFS  CVMFS 2.5.2 :  Bug fix release  osg-wn-client 3.4-5: Include stashcp on the worker-node  Certificate Authority certificates based on  IGTF 1.94  extended validity period for the ArmeSFo CA (AM)  withdrawn expiring DFN-SLCS CA (DE)    VO Package v85  Update  INFN  CA DN  Add backup VOMS server for  enmr.eu  and  glast.org      Important HTCondor-CE update  This release of HTCondor-CE adds support for pilot jobs authenticating\nwith host certificates signed by a VO.  Due to the expiration of service\ncertificates starting in April 2019, it is important to update to this\nversion to continue to receive OSG jobs.\nBe sure to watch out for  /etc/condor-ce/condor_mapfile.rpmnew  and merge\nany changes into  /etc/condor-ce/condor_mapfile .   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#enterprise-linux-6", 
            "text": "condor-8.6.13-1.1.osg34.el6  cvmfs-2.5.2-1.osg34.el6  cvmfs-gateway-0.3.1-1.1.osg34.el6  frontier-squid-4.4-1.1.osg34.el6  htcondor-ce-3.2.0-1.osg34.el6  igtf-ca-certs-1.94-2.osg34.el6  osg-ca-certs-1.77-1.osg34.el6  osg-ca-scripts-1.2.4-1.osg34.el6  osg-oasis-11-1.osg34.el6  osg-test-2.3.0-1.osg34.el6  osg-version-3.4.22-1.osg34.el6  osg-wn-client-3.4-5.osg34.el6  pegasus-4.9.0-1.osg34.el6  stashcache-client-5.2.0-1.osg34.el6  vo-client-85-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#enterprise-linux-7", 
            "text": "condor-8.6.13-1.1.osg34.el7  cvmfs-2.5.2-1.osg34.el7  cvmfs-gateway-0.3.1-1.1.osg34.el7  frontier-squid-4.4-1.1.osg34.el7  htcondor-ce-3.2.0-1.osg34.el7  igtf-ca-certs-1.94-2.osg34.el7  osg-ca-certs-1.77-1.osg34.el7  osg-ca-scripts-1.2.4-1.osg34.el7  osg-oasis-11-1.osg34.el7  osg-test-2.3.0-1.osg34.el7  osg-version-3.4.22-1.osg34.el7  osg-wn-client-3.4-5.osg34.el7  pegasus-4.9.0-1.osg34.el7  stashcache-client-5.2.0-1.osg34.el7  vo-client-85-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs   cvmfs - devel   cvmfs - gateway   cvmfs - server   cvmfs - unittests   frontier - squid   frontier - squid - debuginfo   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   osg - ca - certs   osg - ca - scripts   osg - gums - config   osg - oasis   osg - test   osg - test - log - viewer   osg - version   osg - wn - client   pegasus   pegasus - debuginfo   stashcache - client   vo - client   vo - client - dcache   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - all - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - bosco - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - classads - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - kbdd - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - procd - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - python - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - std - universe - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - test - 8 . 6 . 13 - 1 . 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 13 - 1 . 1 . osg34 . el6  cvmfs - 2 . 5 . 2 - 1 . osg34 . el6  cvmfs - devel - 2 . 5 . 2 - 1 . osg34 . el6  cvmfs - gateway - 0 . 3 . 1 - 1 . 1 . osg34 . el6  cvmfs - server - 2 . 5 . 2 - 1 . osg34 . el6  cvmfs - unittests - 2 . 5 . 2 - 1 . osg34 . el6  frontier - squid - 4 . 4 - 1 . 1 . osg34 . el6  frontier - squid - debuginfo - 4 . 4 - 1 . 1 . osg34 . el6  htcondor - ce - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - client - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - view - 3 . 2 . 0 - 1 . osg34 . el6  igtf - ca - certs - 1 . 94 - 2 . osg34 . el6  osg - ca - certs - 1 . 77 - 1 . osg34 . el6  osg - ca - scripts - 1 . 2 . 4 - 1 . osg34 . el6  osg - gums - config - 85 - 1 . osg34 . el6  osg - oasis - 11 - 1 . osg34 . el6  osg - test - 2 . 3 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 3 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 22 - 1 . osg34 . el6  osg - wn - client - 3 . 4 - 5 . osg34 . el6  pegasus - 4 . 9 . 0 - 1 . osg34 . el6  pegasus - debuginfo - 4 . 9 . 0 - 1 . osg34 . el6  stashcache - client - 5 . 2 . 0 - 1 . osg34 . el6  vo - client - 85 - 1 . osg34 . el6  vo - client - dcache - 85 - 1 . osg34 . el6  vo - client - edgmkgridmap - 85 - 1 . osg34 . el6  vo - client - lcmaps - voms - 85 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-22/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - all - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - bosco - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - classads - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - kbdd - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - procd - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - python - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - test - 8 . 6 . 13 - 1 . 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 13 - 1 . 1 . osg34 . el7  cvmfs - 2 . 5 . 2 - 1 . osg34 . el7  cvmfs - devel - 2 . 5 . 2 - 1 . osg34 . el7  cvmfs - gateway - 0 . 3 . 1 - 1 . 1 . osg34 . el7  cvmfs - server - 2 . 5 . 2 - 1 . osg34 . el7  cvmfs - unittests - 2 . 5 . 2 - 1 . osg34 . el7  frontier - squid - 4 . 4 - 1 . 1 . osg34 . el7  frontier - squid - debuginfo - 4 . 4 - 1 . 1 . osg34 . el7  htcondor - ce - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - client - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - view - 3 . 2 . 0 - 1 . osg34 . el7  igtf - ca - certs - 1 . 94 - 2 . osg34 . el7  osg - ca - certs - 1 . 77 - 1 . osg34 . el7  osg - ca - scripts - 1 . 2 . 4 - 1 . osg34 . el7  osg - gums - config - 85 - 1 . osg34 . el7  osg - oasis - 11 - 1 . osg34 . el7  osg - test - 2 . 3 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 3 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 22 - 1 . osg34 . el7  osg - wn - client - 3 . 4 - 5 . osg34 . el7  pegasus - 4 . 9 . 0 - 1 . osg34 . el7  pegasus - debuginfo - 4 . 9 . 0 - 1 . osg34 . el7  stashcache - client - 5 . 2 . 0 - 1 . osg34 . el7  vo - client - 85 - 1 . osg34 . el7  vo - client - dcache - 85 - 1 . osg34 . el7  vo - client - edgmkgridmap - 85 - 1 . osg34 . el7  vo - client - lcmaps - voms - 85 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/", 
            "text": "OSG Software Release 3.4.21\n\n\nRelease Date\n: 2018-12-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nSingularity 2.6.1\n: Fixed high severity security issue\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nosg-version-3.4.21-1.osg34.el6\n\n\nsingularity-2.6.1-1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nosg-version-3.4.21-1.osg34.el7\n\n\nsingularity-2.6.1-1.1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\nversion\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n21\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n21\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n6\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.21"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#osg-software-release-3421", 
            "text": "Release Date : 2018-12-12", 
            "title": "OSG Software Release 3.4.21"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#summary-of-changes", 
            "text": "This release contains:   Singularity 2.6.1 : Fixed high severity security issue   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#enterprise-linux-6", 
            "text": "osg-version-3.4.21-1.osg34.el6  singularity-2.6.1-1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#enterprise-linux-7", 
            "text": "osg-version-3.4.21-1.osg34.el7  singularity-2.6.1-1.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - version   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#enterprise-linux-6_1", 
            "text": "osg - version - 3 . 4 . 21 - 1 . osg34 . el6  singularity - 2 . 6 . 1 - 1 . 1 . osg34 . el6  singularity - debuginfo - 2 . 6 . 1 - 1 . 1 . osg34 . el6  singularity - devel - 2 . 6 . 1 - 1 . 1 . osg34 . el6  singularity - runtime - 2 . 6 . 1 - 1 . 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-21/#enterprise-linux-7_1", 
            "text": "osg - version - 3 . 4 . 21 - 1 . osg34 . el7  singularity - 2 . 6 . 1 - 1 . 1 . osg34 . el7  singularity - debuginfo - 2 . 6 . 1 - 1 . 1 . osg34 . el7  singularity - devel - 2 . 6 . 1 - 1 . 1 . osg34 . el7  singularity - runtime - 2 . 6 . 1 - 1 . 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/", 
            "text": "OSG Software Release 3.4.20\n\n\nRelease Date\n: 2018-11-01\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nGlideinWMS 3.4.2\n: improved Singularity support\n\n\nSciTokens 1.2.1\n: bug fix for multiple audience support in the verifier\n\n\nStashCache-Daemon: improved statistics collection\n\n\nStashCache-Client: new RPM packaging for stashcp\n\n\nHTCondor 8.6.13\n: bug fix release\n\n\nFixed a bug where job start date was not recorded for hosted CEs\n\n\nMade the Python 'in' operator case-insensitive for ClassAd attributes\n\n\nPython bindings are now built for the Debian and Ubuntu platforms\n\n\nFixed a memory leak in the Python bindings\n\n\nFixed a bug where absolute paths failed for output/error files on Windows\n\n\nFixed a bug using Condor-C to run Condor-C jobs\n\n\nFixed a bug where Singularity could not be used if Docker was not present\n\n\n\n\n\n\nUpcoming: \nHTCondor 8.7.10\n: final 8.7.x release\n\n\nCan now interactively submit Docker jobs\n\n\nThe administrator can now add arguments to the Singularity command line\n\n\nThe MUNGE security method is now supported on all Linux platforms\n\n\nThe grid universe can create and manage VM instances in Microsoft Azure\n\n\nAdded a single-node package to facilitate using a personal HTCondor\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.13-1.osg34.el6\n\n\nglideinwms-3.4.2-1.osg34.el6\n\n\nosg-version-3.4.20-1.osg34.el6\n\n\nstashcache-client-5.1.0-4.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.13-1.osg34.el7\n\n\nglideinwms-3.4.2-1.osg34.el7\n\n\nosg-version-3.4.20-1.osg34.el7\n\n\npython-jwt-1.6.1-1.osg34.el7\n\n\npython-scitokens-1.2.1-2.osg34.el7\n\n\nstashcache-0.10-1.osg34.el7\n\n\nstashcache-client-5.1.0-4.osg34.el7\n\n\nxrootd-scitokens-0.6.0-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nosg\n-\nversion\n \npython2\n-\njwt\n \npython2\n-\nscitokens\n \npython34\n-\njwt\n \nstashcache\n-\ncache\n-\nserver\n \nstashcache\n-\ncache\n-\nserver\n-\nauth\n \nstashcache\n-\nclient\n \nstashcache\n-\ndaemon\n \nstashcache\n-\norigin\n-\nserver\n \nxrootd\n-\nscitokens\n \nxrootd\n-\nscitokens\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nstashcache\n-\nclient\n-\n5\n.\n1\n.\n0\n-\n4\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\njwt\n-\n1\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nscitokens\n-\n1\n.\n2\n.\n1\n-\n2\n.\nosg34\n.\nel7\n\n\npython34\n-\njwt\n-\n1\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\npython\n-\njwt\n-\n1\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\npython\n-\nscitokens\n-\n1\n.\n2\n.\n1\n-\n2\n.\nosg34\n.\nel7\n\n\nstashcache\n-\n0\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\n0\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\nauth\n-\n0\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\nclient\n-\n5\n.\n1\n.\n0\n-\n4\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ndaemon\n-\n0\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\norigin\n-\nserver\n-\n0\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nscitokens\n-\n0\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nscitokens\n-\ndebuginfo\n-\n0\n.\n6\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.7.10-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.7.10-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\nconfig\n-\nsingle\n-\nnode\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nconfig\n-\nsingle\n-\nnode\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nconfig\n-\nsingle\n-\nnode\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n10\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.20"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#osg-software-release-3420", 
            "text": "Release Date : 2018-11-01", 
            "title": "OSG Software Release 3.4.20"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#summary-of-changes", 
            "text": "This release contains:   GlideinWMS 3.4.2 : improved Singularity support  SciTokens 1.2.1 : bug fix for multiple audience support in the verifier  StashCache-Daemon: improved statistics collection  StashCache-Client: new RPM packaging for stashcp  HTCondor 8.6.13 : bug fix release  Fixed a bug where job start date was not recorded for hosted CEs  Made the Python 'in' operator case-insensitive for ClassAd attributes  Python bindings are now built for the Debian and Ubuntu platforms  Fixed a memory leak in the Python bindings  Fixed a bug where absolute paths failed for output/error files on Windows  Fixed a bug using Condor-C to run Condor-C jobs  Fixed a bug where Singularity could not be used if Docker was not present    Upcoming:  HTCondor 8.7.10 : final 8.7.x release  Can now interactively submit Docker jobs  The administrator can now add arguments to the Singularity command line  The MUNGE security method is now supported on all Linux platforms  The grid universe can create and manage VM instances in Microsoft Azure  Added a single-node package to facilitate using a personal HTCondor     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-6", 
            "text": "condor-8.6.13-1.osg34.el6  glideinwms-3.4.2-1.osg34.el6  osg-version-3.4.20-1.osg34.el6  stashcache-client-5.1.0-4.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-7", 
            "text": "condor-8.6.13-1.osg34.el7  glideinwms-3.4.2-1.osg34.el7  osg-version-3.4.20-1.osg34.el7  python-jwt-1.6.1-1.osg34.el7  python-scitokens-1.2.1-2.osg34.el7  stashcache-0.10-1.osg34.el7  stashcache-client-5.1.0-4.osg34.el7  xrootd-scitokens-0.6.0-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   osg - version   python2 - jwt   python2 - scitokens   python34 - jwt   stashcache - cache - server   stashcache - cache - server - auth   stashcache - client   stashcache - daemon   stashcache - origin - server   xrootd - scitokens   xrootd - scitokens - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 13 - 1 . osg34 . el6  condor - all - 8 . 6 . 13 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 13 - 1 . osg34 . el6  condor - classads - 8 . 6 . 13 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 13 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 13 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 13 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 13 - 1 . osg34 . el6  condor - procd - 8 . 6 . 13 - 1 . osg34 . el6  condor - python - 8 . 6 . 13 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 13 - 1 . osg34 . el6  condor - test - 8 . 6 . 13 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 13 - 1 . osg34 . el6  glideinwms - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - common - tools - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - condor - common - config - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - factory - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - factory - condor - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - libs - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - minimal - condor - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - usercollector - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - userschedd - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - vofrontend - 3 . 4 . 2 - 1 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 4 . 2 - 1 . osg34 . el6  osg - version - 3 . 4 . 20 - 1 . osg34 . el6  stashcache - client - 5 . 1 . 0 - 4 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 13 - 1 . osg34 . el7  condor - all - 8 . 6 . 13 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 13 - 1 . osg34 . el7  condor - classads - 8 . 6 . 13 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 13 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 13 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 13 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 13 - 1 . osg34 . el7  condor - procd - 8 . 6 . 13 - 1 . osg34 . el7  condor - python - 8 . 6 . 13 - 1 . osg34 . el7  condor - test - 8 . 6 . 13 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 13 - 1 . osg34 . el7  glideinwms - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - common - tools - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - condor - common - config - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - factory - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - factory - condor - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - libs - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - minimal - condor - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - usercollector - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - userschedd - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - vofrontend - 3 . 4 . 2 - 1 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 4 . 2 - 1 . osg34 . el7  osg - version - 3 . 4 . 20 - 1 . osg34 . el7  python2 - jwt - 1 . 6 . 1 - 1 . osg34 . el7  python2 - scitokens - 1 . 2 . 1 - 2 . osg34 . el7  python34 - jwt - 1 . 6 . 1 - 1 . osg34 . el7  python - jwt - 1 . 6 . 1 - 1 . osg34 . el7  python - scitokens - 1 . 2 . 1 - 2 . osg34 . el7  stashcache - 0 . 10 - 1 . osg34 . el7  stashcache - cache - server - 0 . 10 - 1 . osg34 . el7  stashcache - cache - server - auth - 0 . 10 - 1 . osg34 . el7  stashcache - client - 5 . 1 . 0 - 4 . osg34 . el7  stashcache - daemon - 0 . 10 - 1 . osg34 . el7  stashcache - origin - server - 0 . 10 - 1 . osg34 . el7  xrootd - scitokens - 0 . 6 . 0 - 1 . osg34 . el7  xrootd - scitokens - debuginfo - 0 . 6 . 0 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-6_2", 
            "text": "condor-8.7.10-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-7_2", 
            "text": "condor-8.7.10-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - config - single - node   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-6_3", 
            "text": "condor - 8 . 7 . 10 - 1 . osgup . el6  condor - all - 8 . 7 . 10 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 10 - 1 . osgup . el6  condor - bosco - 8 . 7 . 10 - 1 . osgup . el6  condor - classads - 8 . 7 . 10 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 10 - 1 . osgup . el6  condor - config - single - node - 8 . 7 . 10 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 10 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 10 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 10 - 1 . osgup . el6  condor - procd - 8 . 7 . 10 - 1 . osgup . el6  condor - python - 8 . 7 . 10 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 10 - 1 . osgup . el6  condor - test - 8 . 7 . 10 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 10 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-20/#enterprise-linux-7_3", 
            "text": "condor - 8 . 7 . 10 - 1 . osgup . el7  condor - all - 8 . 7 . 10 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 10 - 1 . osgup . el7  condor - bosco - 8 . 7 . 10 - 1 . osgup . el7  condor - classads - 8 . 7 . 10 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 10 - 1 . osgup . el7  condor - config - single - node - 8 . 7 . 10 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 10 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 10 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 10 - 1 . osgup . el7  condor - procd - 8 . 7 . 10 - 1 . osgup . el7  condor - python - 8 . 7 . 10 - 1 . osgup . el7  condor - test - 8 . 7 . 10 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 10 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/", 
            "text": "OSG Software Release 3.4.19\n\n\nRelease Date\n: 2018-10-25\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nAdded workaround to the OSG worker node tarball client to avoid TLS failures with GFAL\n\n\nUpdated to \nXRootD 4.8.5\n - Bug fix release\n\n\nReleased \nosg-flock\n package for easier installation of OSG submit hosts\n\n\nImproved StashCache packaging with better default configuration\n\n\nReleased \nautopyfactory 2.4.9\n which is currently in use at CERN\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nautopyfactory-2.4.9-2.osg34.el6\n\n\nosg-flock-1.0-1.osg34.el6\n\n\nosg-version-3.4.19-1.osg34.el6\n\n\nxrootd-4.8.5-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nautopyfactory-2.4.9-2.osg34.el7\n\n\nosg-flock-1.0-1.osg34.el7\n\n\nosg-version-3.4.19-1.osg34.el7\n\n\nstashcache-0.9-1.osg34.el7\n\n\nxrootd-4.8.5-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nautopyfactory\n-\natlas\n \nautopyfactory\n-\ncloud\n \nautopyfactory\n-\ncommon\n \nautopyfactory\n-\npanda\n \nautopyfactory\n-\nplugins\n-\ncloud\n \nautopyfactory\n-\nplugins\n-\nlocal\n \nautopyfactory\n-\nplugins\n-\nmonitor\n \nautopyfactory\n-\nplugins\n-\npanda\n \nautopyfactory\n-\nplugins\n-\nremote\n \nautopyfactory\n-\nplugins\n-\nscheds\n \nautopyfactory\n-\nproxymanager\n \nautopyfactory\n-\nremote\n \nautopyfactory\n-\nwms\n \nosg\n-\nflock\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\nversion\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nstashcache\n-\ncache\n-\nserver\n \nstashcache\n-\ncache\n-\nserver\n-\nauth\n \nstashcache\n-\ndaemon\n \nstashcache\n-\norigin\n-\nserver\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nautopyfactory\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\natlas\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\ncloud\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\ncommon\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\npanda\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\ncloud\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nlocal\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nmonitor\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\npanda\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nremote\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nscheds\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nproxymanager\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nremote\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nwms\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nflock\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n19\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nautopyfactory\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\natlas\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\ncloud\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\ncommon\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\npanda\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\ncloud\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nlocal\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nmonitor\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\npanda\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nremote\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nscheds\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nproxymanager\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nremote\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nwms\n-\n2\n.\n4\n.\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nflock\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n19\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\n0\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\n0\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\nauth\n-\n0\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ndaemon\n-\n0\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\norigin\n-\nserver\n-\n0\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n5\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.19"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#osg-software-release-3419", 
            "text": "Release Date : 2018-10-25", 
            "title": "OSG Software Release 3.4.19"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#summary-of-changes", 
            "text": "This release contains:   Added workaround to the OSG worker node tarball client to avoid TLS failures with GFAL  Updated to  XRootD 4.8.5  - Bug fix release  Released  osg-flock  package for easier installation of OSG submit hosts  Improved StashCache packaging with better default configuration  Released  autopyfactory 2.4.9  which is currently in use at CERN   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#enterprise-linux-6", 
            "text": "autopyfactory-2.4.9-2.osg34.el6  osg-flock-1.0-1.osg34.el6  osg-version-3.4.19-1.osg34.el6  xrootd-4.8.5-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#enterprise-linux-7", 
            "text": "autopyfactory-2.4.9-2.osg34.el7  osg-flock-1.0-1.osg34.el7  osg-version-3.4.19-1.osg34.el7  stashcache-0.9-1.osg34.el7  xrootd-4.8.5-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  autopyfactory - atlas   autopyfactory - cloud   autopyfactory - common   autopyfactory - panda   autopyfactory - plugins - cloud   autopyfactory - plugins - local   autopyfactory - plugins - monitor   autopyfactory - plugins - panda   autopyfactory - plugins - remote   autopyfactory - plugins - scheds   autopyfactory - proxymanager   autopyfactory - remote   autopyfactory - wms   osg - flock   osg - gums - config   osg - version   python2 - xrootd   python3 - xrootd   stashcache - cache - server   stashcache - cache - server - auth   stashcache - daemon   stashcache - origin - server   vo - client   vo - client - dcache   vo - client - edgmkgridmap   vo - client - lcmaps - voms   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#enterprise-linux-6_1", 
            "text": "autopyfactory - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - atlas - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - cloud - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - common - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - panda - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - cloud - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - local - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - monitor - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - panda - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - remote - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - plugins - scheds - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - proxymanager - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - remote - 2 . 4 . 9 - 2 . osg34 . el6  autopyfactory - wms - 2 . 4 . 9 - 2 . osg34 . el6  osg - flock - 1 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 19 - 1 . osg34 . el6  python2 - xrootd - 4 . 8 . 5 - 1 . osg34 . el6  python3 - xrootd - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - client - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - client - devel - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - client - libs - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - devel - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - doc - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - fuse - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - libs - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - private - devel - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - selinux - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - server - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - server - devel - 4 . 8 . 5 - 1 . osg34 . el6  xrootd - server - libs - 4 . 8 . 5 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-19/#enterprise-linux-7_1", 
            "text": "autopyfactory - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - atlas - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - cloud - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - common - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - panda - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - cloud - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - local - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - monitor - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - panda - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - remote - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - plugins - scheds - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - proxymanager - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - remote - 2 . 4 . 9 - 2 . osg34 . el7  autopyfactory - wms - 2 . 4 . 9 - 2 . osg34 . el7  osg - flock - 1 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 19 - 1 . osg34 . el7  python2 - xrootd - 4 . 8 . 5 - 1 . osg34 . el7  python3 - xrootd - 4 . 8 . 5 - 1 . osg34 . el7  stashcache - 0 . 9 - 1 . osg34 . el7  stashcache - cache - server - 0 . 9 - 1 . osg34 . el7  stashcache - cache - server - auth - 0 . 9 - 1 . osg34 . el7  stashcache - daemon - 0 . 9 - 1 . osg34 . el7  stashcache - origin - server - 0 . 9 - 1 . osg34 . el7  xrootd - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - client - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - client - devel - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - client - libs - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - devel - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - doc - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - fuse - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - private - devel - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - selinux - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - server - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - server - devel - 4 . 8 . 5 - 1 . osg34 . el7  xrootd - server - libs - 4 . 8 . 5 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.18-2\n\n\nRelease Date\n: 2018-10-03\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v84\n\n\nAdd VO configuration for dteam VO\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-84-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-84-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\ngums\n-\nconfig\n-\n84\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n84\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n84\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n84\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n84\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\ngums\n-\nconfig\n-\n84\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n84\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n84\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n84\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n84\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.18-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#osg-software-stack-data-release-3418-2", 
            "text": "Release Date : 2018-10-03", 
            "title": "OSG Software Stack -- Data Release -- 3.4.18-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#summary-of-changes", 
            "text": "This release contains:   VO Package v84  Add VO configuration for dteam VO     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#enterprise-linux-6", 
            "text": "vo-client-84-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#enterprise-linux-7", 
            "text": "vo-client-84-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - gums - config   vo - client   vo - client - dcache   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#enterprise-linux-6_1", 
            "text": "osg - gums - config - 84 - 1 . osg34 . el6  vo - client - 84 - 1 . osg34 . el6  vo - client - dcache - 84 - 1 . osg34 . el6  vo - client - edgmkgridmap - 84 - 1 . osg34 . el6  vo - client - lcmaps - voms - 84 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18-2/#enterprise-linux-7_1", 
            "text": "osg - gums - config - 84 - 1 . osg34 . el7  vo - client - 84 - 1 . osg34 . el7  vo - client - dcache - 84 - 1 . osg34 . el7  vo - client - edgmkgridmap - 84 - 1 . osg34 . el7  vo - client - lcmaps - voms - 84 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/", 
            "text": "OSG Software Release 3.4.18\n\n\nRelease Date\n: 2018-09-27\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.4 Repository\n\n\nXRootD: Patches to the XrdHttp implementation, particularly relevant for StashCache\n\n\nxrootd-lcmaps 1.4.1\n: Fixed crash when using both HTTP and XRootD protocols\n\n\nxrootd-hdfs 2.1.3\n: Fixed two checksum handlings bugs (Update from 2.0.2)\n\n\nHTCondor-CE 3.1.4\n\n\ncondor_ce_trace\n now works with a mix of EL6 and EL7 hosts\n\n\ncondor_ce_q\n now shows jobs for all users\n\n\nRussian Data Intensive Grid (RDIG) CEs now report to the central collector\n\n\nCreates all required directories at installation time\n\n\n\n\n\n\nCernVM-FS 2.5.1\n: Bug fixes and improvement for clients and servers\n\n\nGratia probes 1.20.7\n\n\nHandles Unicode characters properly\n\n\nUnhandled exceptions in the Slurm probe are now logged\n\n\n\n\n\n\nPegasus 4.8.4\n: Pegasus workflows should now appear in Gracc\n\n\nUpdated Globus Packages from EPEL (globus-gridftp-server and globus-gridftp-server-control)\n\n\nGlideinWMS 3.4\n: Moved from the Upcoming Repository\n\n\nRSV 3.19.8\n: Fixed persistent warning about the Gracc server being down\n\n\nBLAHP 1.18.38\n: Updated default worker node configuration with respect to proxy renewals\n\n\n\n\n\n\nUpcoming Repository\n\n\nBLAHP 1.18.38\n: Updated default worker node configuration with respect to proxy renewals\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.38.bosco-1.osg34.el6\n\n\ncvmfs-2.5.1-1.osg34.el6\n\n\nglideinwms-3.4-1.1.osg34.el6\n\n\nglobus-gridftp-server-12.9-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-7.0-1.osg34.el6\n\n\ngratia-probe-1.20.7-1.osg34.el6\n\n\nhtcondor-ce-3.1.4-1.osg34.el6\n\n\nosg-oasis-10-1.osg34.el6\n\n\nosg-version-3.4.18-1.osg34.el6\n\n\npegasus-4.8.4-1.osg34.el6\n\n\nrsv-3.19.8-1.osg34.el6\n\n\nxrootd-4.8.4-3.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.38.bosco-1.osg34.el7\n\n\ncvmfs-2.5.1-1.osg34.el7\n\n\nglideinwms-3.4-1.1.osg34.el7\n\n\nglobus-gridftp-server-12.9-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-7.0-1.osg34.el7\n\n\ngratia-probe-1.20.7-1.osg34.el7\n\n\nhtcondor-ce-3.1.4-1.osg34.el7\n\n\nosg-oasis-10-1.osg34.el7\n\n\nosg-version-3.4.18-1.osg34.el7\n\n\npegasus-4.8.4-1.osg34.el7\n\n\nrsv-3.19.8-1.osg34.el7\n\n\nxrootd-4.8.4-3.osg34.el7\n\n\nxrootd-hdfs-2.1.3-1.osg34.el7\n\n\nxrootd-lcmaps-1.4.1-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\noasis\n \nosg\n-\nversion\n \npegasus\n \npegasus\n-\ndebuginfo\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nhdfs\n \nxrootd\n-\nhdfs\n-\ndebuginfo\n \nxrootd\n-\nhdfs\n-\ndevel\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n10\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n18\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n18\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n19\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\n2\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndebuginfo\n-\n2\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndevel\n-\n2\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.38.bosco-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.38.bosco-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n38\n.\nbosco\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.18"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#osg-software-release-3418", 
            "text": "Release Date : 2018-09-27", 
            "title": "OSG Software Release 3.4.18"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#summary-of-changes", 
            "text": "This release contains:   OSG 3.4 Repository  XRootD: Patches to the XrdHttp implementation, particularly relevant for StashCache  xrootd-lcmaps 1.4.1 : Fixed crash when using both HTTP and XRootD protocols  xrootd-hdfs 2.1.3 : Fixed two checksum handlings bugs (Update from 2.0.2)  HTCondor-CE 3.1.4  condor_ce_trace  now works with a mix of EL6 and EL7 hosts  condor_ce_q  now shows jobs for all users  Russian Data Intensive Grid (RDIG) CEs now report to the central collector  Creates all required directories at installation time    CernVM-FS 2.5.1 : Bug fixes and improvement for clients and servers  Gratia probes 1.20.7  Handles Unicode characters properly  Unhandled exceptions in the Slurm probe are now logged    Pegasus 4.8.4 : Pegasus workflows should now appear in Gracc  Updated Globus Packages from EPEL (globus-gridftp-server and globus-gridftp-server-control)  GlideinWMS 3.4 : Moved from the Upcoming Repository  RSV 3.19.8 : Fixed persistent warning about the Gracc server being down  BLAHP 1.18.38 : Updated default worker node configuration with respect to proxy renewals    Upcoming Repository  BLAHP 1.18.38 : Updated default worker node configuration with respect to proxy renewals     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-6", 
            "text": "blahp-1.18.38.bosco-1.osg34.el6  cvmfs-2.5.1-1.osg34.el6  glideinwms-3.4-1.1.osg34.el6  globus-gridftp-server-12.9-1.1.osg34.el6  globus-gridftp-server-control-7.0-1.osg34.el6  gratia-probe-1.20.7-1.osg34.el6  htcondor-ce-3.1.4-1.osg34.el6  osg-oasis-10-1.osg34.el6  osg-version-3.4.18-1.osg34.el6  pegasus-4.8.4-1.osg34.el6  rsv-3.19.8-1.osg34.el6  xrootd-4.8.4-3.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-7", 
            "text": "blahp-1.18.38.bosco-1.osg34.el7  cvmfs-2.5.1-1.osg34.el7  glideinwms-3.4-1.1.osg34.el7  globus-gridftp-server-12.9-1.1.osg34.el7  globus-gridftp-server-control-7.0-1.osg34.el7  gratia-probe-1.20.7-1.osg34.el7  htcondor-ce-3.1.4-1.osg34.el7  osg-oasis-10-1.osg34.el7  osg-version-3.4.18-1.osg34.el7  pegasus-4.8.4-1.osg34.el7  rsv-3.19.8-1.osg34.el7  xrootd-4.8.4-3.osg34.el7  xrootd-hdfs-2.1.3-1.osg34.el7  xrootd-lcmaps-1.4.1-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   globus - gridftp - server   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glideinwms   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   osg - ca - certs   osg - gums - config   osg - oasis   osg - version   pegasus   pegasus - debuginfo   python2 - xrootd   python3 - xrootd   rsv   rsv - consumers   rsv - core   rsv - metrics   vo - client   vo - client - dcache   vo - client - edgmkgridmap   vo - client - lcmaps - voms   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - hdfs   xrootd - hdfs - debuginfo   xrootd - hdfs - devel   xrootd - lcmaps   xrootd - lcmaps - debuginfo   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 38 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 38 . bosco - 1 . osg34 . el6  cvmfs - 2 . 5 . 1 - 1 . osg34 . el6  cvmfs - devel - 2 . 5 . 1 - 1 . osg34 . el6  cvmfs - server - 2 . 5 . 1 - 1 . osg34 . el6  cvmfs - unittests - 2 . 5 . 1 - 1 . osg34 . el6  glideinwms - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - common - tools - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - condor - common - config - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - factory - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - factory - condor - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - libs - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - minimal - condor - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - usercollector - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - userschedd - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - vofrontend - 3 . 4 - 1 . 1 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 4 - 1 . 1 . osg34 . el6  globus - gridftp - server - 12 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - 7 . 0 - 1 . osg34 . el6  globus - gridftp - server - control - debuginfo - 7 . 0 - 1 . osg34 . el6  globus - gridftp - server - control - devel - 7 . 0 - 1 . osg34 . el6  globus - gridftp - server - debuginfo - 12 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - devel - 12 . 9 - 1 . 1 . osg34 . el6  globus - gridftp - server - progs - 12 . 9 - 1 . 1 . osg34 . el6  gratia - probe - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - common - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - condor - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - metric - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - services - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - sge - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 20 . 7 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 20 . 7 - 1 . osg34 . el6  htcondor - ce - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - client - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 1 . 4 - 1 . osg34 . el6  htcondor - ce - view - 3 . 1 . 4 - 1 . osg34 . el6  osg - oasis - 10 - 1 . osg34 . el6  osg - version - 3 . 4 . 18 - 1 . osg34 . el6  pegasus - 4 . 8 . 4 - 1 . osg34 . el6  pegasus - debuginfo - 4 . 8 . 4 - 1 . osg34 . el6  python2 - xrootd - 4 . 8 . 4 - 3 . osg34 . el6  python3 - xrootd - 4 . 8 . 4 - 3 . osg34 . el6  rsv - 3 . 19 . 8 - 1 . osg34 . el6  rsv - consumers - 3 . 19 . 8 - 1 . osg34 . el6  rsv - core - 3 . 19 . 8 - 1 . osg34 . el6  rsv - metrics - 3 . 19 . 8 - 1 . osg34 . el6  xrootd - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - client - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - client - devel - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - client - libs - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - devel - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - doc - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - fuse - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - libs - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - private - devel - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - selinux - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - server - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - server - devel - 4 . 8 . 4 - 3 . osg34 . el6  xrootd - server - libs - 4 . 8 . 4 - 3 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 38 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 38 . bosco - 1 . osg34 . el7  cvmfs - 2 . 5 . 1 - 1 . osg34 . el7  cvmfs - devel - 2 . 5 . 1 - 1 . osg34 . el7  cvmfs - server - 2 . 5 . 1 - 1 . osg34 . el7  cvmfs - unittests - 2 . 5 . 1 - 1 . osg34 . el7  glideinwms - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - common - tools - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - condor - common - config - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - factory - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - factory - condor - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - libs - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - minimal - condor - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - usercollector - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - userschedd - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - vofrontend - 3 . 4 - 1 . 1 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 4 - 1 . 1 . osg34 . el7  globus - gridftp - server - 12 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - 7 . 0 - 1 . osg34 . el7  globus - gridftp - server - control - debuginfo - 7 . 0 - 1 . osg34 . el7  globus - gridftp - server - control - devel - 7 . 0 - 1 . osg34 . el7  globus - gridftp - server - debuginfo - 12 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - devel - 12 . 9 - 1 . 1 . osg34 . el7  globus - gridftp - server - progs - 12 . 9 - 1 . 1 . osg34 . el7  gratia - probe - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - common - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - condor - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - metric - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - services - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - sge - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 20 . 7 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 20 . 7 - 1 . osg34 . el7  htcondor - ce - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - client - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 1 . 4 - 1 . osg34 . el7  htcondor - ce - view - 3 . 1 . 4 - 1 . osg34 . el7  osg - oasis - 10 - 1 . osg34 . el7  osg - version - 3 . 4 . 18 - 1 . osg34 . el7  pegasus - 4 . 8 . 4 - 1 . osg34 . el7  pegasus - debuginfo - 4 . 8 . 4 - 1 . osg34 . el7  python2 - xrootd - 4 . 8 . 4 - 3 . osg34 . el7  python3 - xrootd - 4 . 8 . 4 - 3 . osg34 . el7  rsv - 3 . 19 . 8 - 1 . osg34 . el7  rsv - consumers - 3 . 19 . 8 - 1 . osg34 . el7  rsv - core - 3 . 19 . 8 - 1 . osg34 . el7  rsv - metrics - 3 . 19 . 8 - 1 . osg34 . el7  xrootd - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - client - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - client - devel - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - client - libs - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - devel - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - doc - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - fuse - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - hdfs - 2 . 1 . 3 - 1 . osg34 . el7  xrootd - hdfs - debuginfo - 2 . 1 . 3 - 1 . osg34 . el7  xrootd - hdfs - devel - 2 . 1 . 3 - 1 . osg34 . el7  xrootd - lcmaps - 1 . 4 . 1 - 1 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 4 . 1 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - private - devel - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - selinux - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - server - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - server - devel - 4 . 8 . 4 - 3 . osg34 . el7  xrootd - server - libs - 4 . 8 . 4 - 3 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-6_2", 
            "text": "blahp-1.18.38.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-7_2", 
            "text": "blahp-1.18.38.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 38 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 38 . bosco - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-18/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 38 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 38 . bosco - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.17-3\n\n\nRelease Date\n: 2018-09-26\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.93\n\n\nUpdated contact information for HellasGrid-CA (GR)\n\n\nRemoved superseded IGCA CA (IN)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.93-1.osg34.el6\n\n\nosg-ca-certs-1.75-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.93-1.osg34.el7\n\n\nosg-ca-certs-1.75-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n93\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n75\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n93\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n75\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.17-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#osg-software-stack-data-release-3417-3", 
            "text": "Release Date : 2018-09-26", 
            "title": "OSG Software Stack -- Data Release -- 3.4.17-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.93  Updated contact information for HellasGrid-CA (GR)  Removed superseded IGCA CA (IN)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.93-1.osg34.el6  osg-ca-certs-1.75-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.93-1.osg34.el7  osg-ca-certs-1.75-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 93 - 1 . osg34 . el6  osg - ca - certs - 1 . 75 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-3/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 93 - 1 . osg34 . el7  osg - ca - certs - 1 . 75 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.17-2\n\n\nRelease Date\n: 2018-09-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v83\n\n\nAdded a default VO mapfile for dCache\n\n\nUpdated the SuperCDMS VOMS certificate information\n\n\nLSST VOMS Admin server moved from Fermilab to SLAC\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-83-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-83-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\ndcache\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\ngums\n-\nconfig\n-\n83\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n83\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\ndcache\n-\n83\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n83\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n83\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\ngums\n-\nconfig\n-\n83\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n83\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\ndcache\n-\n83\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n83\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n83\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.17-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#osg-software-stack-data-release-3417-2", 
            "text": "Release Date : 2018-09-13", 
            "title": "OSG Software Stack -- Data Release -- 3.4.17-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#summary-of-changes", 
            "text": "This release contains:   VO Package v83  Added a default VO mapfile for dCache  Updated the SuperCDMS VOMS certificate information  LSST VOMS Admin server moved from Fermilab to SLAC     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#enterprise-linux-6", 
            "text": "vo-client-83-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#enterprise-linux-7", 
            "text": "vo-client-83-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - gums - config   vo - client   vo - client - dcache   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#enterprise-linux-6_1", 
            "text": "osg - gums - config - 83 - 1 . osg34 . el6  vo - client - 83 - 1 . osg34 . el6  vo - client - dcache - 83 - 1 . osg34 . el6  vo - client - edgmkgridmap - 83 - 1 . osg34 . el6  vo - client - lcmaps - voms - 83 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17-2/#enterprise-linux-7_1", 
            "text": "osg - gums - config - 83 - 1 . osg34 . el7  vo - client - 83 - 1 . osg34 . el7  vo - client - dcache - 83 - 1 . osg34 . el7  vo - client - edgmkgridmap - 83 - 1 . osg34 . el7  vo - client - lcmaps - voms - 83 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/", 
            "text": "OSG Software Release 3.4.17\n\n\nRelease Date\n: 2018-08-16\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.4 Repository\n\n\nSingularity 2.6.0\n: Bug fix release\n\n\nThis release includes an extra option called \"underlay\" that is not\n    in the upstream release nor enabled by default but is supported by\n    OSG and recommended to be enabled instead of the \"overlay\" option.\n    \nSee the documentation for details.\n\n\n\n\n\n\nHTCondor 8.6.12\n: Bug fix release\n\n\nFixed memory leak when SSL authentication fails\n\n\nCan now set job environment variables in Singularity containers\n\n\n\n\n\n\nCVMFS X.509 helper 1.1: Fixed file descriptor leak\n\n\nGratia probes 1.20.4: Fixed problem where some Slurm jobs weren't reported\n\n\nPegasus 4.8.3\n: Minor bug fix release\n\n\nHTCondor-CE 3.1.3\n: Fix for \ncondor_ce_info_status\n using the wrong port for the central collector\n\n\nxrootd-lcmaps 1.4.0: Fixed the ability to specify an alternate policy name in lcmaps.db (EL7 Only)\n\n\nNew xrootd-multiuser 0.4.2 plugin (EL7 only)\n\n\n\n\n\n\nUpcoming Repository\n\n\nHTCondor 8.7.9\n\n\nFixed handling of CNAMEs in NETWORK_HOSTNAME\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.12-1.osg34.el6\n\n\ncvmfs-x509-helper-1.1-1.osg34.el6\n\n\ngratia-probe-1.20.4-1.osg34.el6\n\n\nhtcondor-ce-3.1.3-1.osg34.el6\n\n\nosg-oasis-9-3.osg34.el6\n\n\nosg-test-2.2.1-1.osg34.el6\n\n\nosg-version-3.4.17-1.osg34.el6\n\n\npegasus-4.8.3-1.osg34.el6\n\n\nsingularity-2.6.0-1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.12-1.osg34.el7\n\n\ncvmfs-x509-helper-1.1-1.osg34.el7\n\n\ngratia-probe-1.20.4-1.osg34.el7\n\n\nhtcondor-ce-3.1.3-1.osg34.el7\n\n\nosg-oasis-9-3.osg34.el7\n\n\nosg-test-2.2.1-1.osg34.el7\n\n\nosg-version-3.4.17-1.osg34.el7\n\n\npegasus-4.8.3-1.osg34.el7\n\n\nsingularity-2.6.0-1.1.osg34.el7\n\n\nxrootd-lcmaps-1.4.0-1.osg34.el7\n\n\nxrootd-multiuser-0.4.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n-\nx509\n-\nhelper\n \ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nosg\n-\noasis\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \npegasus\n \npegasus\n-\ndebuginfo\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n \nxrootd\n-\nmultiuser\n \nxrootd\n-\nmultiuser\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n9\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n17\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n-\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n9\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n17\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n6\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nmultiuser\n-\n0\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nmultiuser\n-\ndebuginfo\n-\n0\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.37.bosco-2.osgup.el6\n\n\ncondor-8.7.9-1.osgup.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.6.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.37.bosco-2.osgup.el7\n\n\ncondor-8.7.9-1.osgup.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.6.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nhdfs\n \nosg\n-\ngridftp\n-\nxrootd\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n2\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n6\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n6\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n2\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n9\n-\n1\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n6\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n6\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.17"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#osg-software-release-3417", 
            "text": "Release Date : 2018-08-16", 
            "title": "OSG Software Release 3.4.17"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#summary-of-changes", 
            "text": "This release contains:   OSG 3.4 Repository  Singularity 2.6.0 : Bug fix release  This release includes an extra option called \"underlay\" that is not\n    in the upstream release nor enabled by default but is supported by\n    OSG and recommended to be enabled instead of the \"overlay\" option.\n     See the documentation for details.    HTCondor 8.6.12 : Bug fix release  Fixed memory leak when SSL authentication fails  Can now set job environment variables in Singularity containers    CVMFS X.509 helper 1.1: Fixed file descriptor leak  Gratia probes 1.20.4: Fixed problem where some Slurm jobs weren't reported  Pegasus 4.8.3 : Minor bug fix release  HTCondor-CE 3.1.3 : Fix for  condor_ce_info_status  using the wrong port for the central collector  xrootd-lcmaps 1.4.0: Fixed the ability to specify an alternate policy name in lcmaps.db (EL7 Only)  New xrootd-multiuser 0.4.2 plugin (EL7 only)    Upcoming Repository  HTCondor 8.7.9  Fixed handling of CNAMEs in NETWORK_HOSTNAME       These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-6", 
            "text": "condor-8.6.12-1.osg34.el6  cvmfs-x509-helper-1.1-1.osg34.el6  gratia-probe-1.20.4-1.osg34.el6  htcondor-ce-3.1.3-1.osg34.el6  osg-oasis-9-3.osg34.el6  osg-test-2.2.1-1.osg34.el6  osg-version-3.4.17-1.osg34.el6  pegasus-4.8.3-1.osg34.el6  singularity-2.6.0-1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-7", 
            "text": "condor-8.6.12-1.osg34.el7  cvmfs-x509-helper-1.1-1.osg34.el7  gratia-probe-1.20.4-1.osg34.el7  htcondor-ce-3.1.3-1.osg34.el7  osg-oasis-9-3.osg34.el7  osg-test-2.2.1-1.osg34.el7  osg-version-3.4.17-1.osg34.el7  pegasus-4.8.3-1.osg34.el7  singularity-2.6.0-1.1.osg34.el7  xrootd-lcmaps-1.4.0-1.osg34.el7  xrootd-multiuser-0.4.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cilogon - openid - ca - cert   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs - x509 - helper   cvmfs - x509 - helper - debuginfo   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glideinwms   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   osg - oasis   osg - test   osg - test - log - viewer   osg - version   pegasus   pegasus - debuginfo   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   xrootd - lcmaps   xrootd - lcmaps - debuginfo   xrootd - multiuser   xrootd - multiuser - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 12 - 1 . osg34 . el6  condor - all - 8 . 6 . 12 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 12 - 1 . osg34 . el6  condor - classads - 8 . 6 . 12 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 12 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 12 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 12 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 12 - 1 . osg34 . el6  condor - procd - 8 . 6 . 12 - 1 . osg34 . el6  condor - python - 8 . 6 . 12 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 12 - 1 . osg34 . el6  condor - test - 8 . 6 . 12 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 12 - 1 . osg34 . el6  cvmfs - x509 - helper - 1 . 1 - 1 . osg34 . el6  cvmfs - x509 - helper - debuginfo - 1 . 1 - 1 . osg34 . el6  gratia - probe - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - common - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - condor - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - metric - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - services - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - sge - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 20 . 4 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 20 . 4 - 1 . osg34 . el6  htcondor - ce - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - client - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 1 . 3 - 1 . osg34 . el6  htcondor - ce - view - 3 . 1 . 3 - 1 . osg34 . el6  osg - oasis - 9 - 3 . osg34 . el6  osg - test - 2 . 2 . 1 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 2 . 1 - 1 . osg34 . el6  osg - version - 3 . 4 . 17 - 1 . osg34 . el6  pegasus - 4 . 8 . 3 - 1 . osg34 . el6  pegasus - debuginfo - 4 . 8 . 3 - 1 . osg34 . el6  singularity - 2 . 6 . 0 - 1 . 1 . osg34 . el6  singularity - debuginfo - 2 . 6 . 0 - 1 . 1 . osg34 . el6  singularity - devel - 2 . 6 . 0 - 1 . 1 . osg34 . el6  singularity - runtime - 2 . 6 . 0 - 1 . 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 12 - 1 . osg34 . el7  condor - all - 8 . 6 . 12 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 12 - 1 . osg34 . el7  condor - classads - 8 . 6 . 12 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 12 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 12 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 12 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 12 - 1 . osg34 . el7  condor - procd - 8 . 6 . 12 - 1 . osg34 . el7  condor - python - 8 . 6 . 12 - 1 . osg34 . el7  condor - test - 8 . 6 . 12 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 12 - 1 . osg34 . el7  cvmfs - x509 - helper - 1 . 1 - 1 . osg34 . el7  cvmfs - x509 - helper - debuginfo - 1 . 1 - 1 . osg34 . el7  gratia - probe - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - common - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - condor - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - metric - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - services - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - sge - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 20 . 4 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 20 . 4 - 1 . osg34 . el7  htcondor - ce - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - client - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 1 . 3 - 1 . osg34 . el7  htcondor - ce - view - 3 . 1 . 3 - 1 . osg34 . el7  osg - oasis - 9 - 3 . osg34 . el7  osg - test - 2 . 2 . 1 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 2 . 1 - 1 . osg34 . el7  osg - version - 3 . 4 . 17 - 1 . osg34 . el7  pegasus - 4 . 8 . 3 - 1 . osg34 . el7  pegasus - debuginfo - 4 . 8 . 3 - 1 . osg34 . el7  singularity - 2 . 6 . 0 - 1 . 1 . osg34 . el7  singularity - debuginfo - 2 . 6 . 0 - 1 . 1 . osg34 . el7  singularity - devel - 2 . 6 . 0 - 1 . 1 . osg34 . el7  singularity - runtime - 2 . 6 . 0 - 1 . 1 . osg34 . el7  xrootd - lcmaps - 1 . 4 . 0 - 1 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 4 . 0 - 1 . osg34 . el7  xrootd - multiuser - 0 . 4 . 2 - 1 . osg34 . el7  xrootd - multiuser - debuginfo - 0 . 4 . 2 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-6_2", 
            "text": "blahp-1.18.37.bosco-2.osgup.el6  condor-8.7.9-1.osgup.el6  glite-ce-cream-client-api-c-1.15.4-2.6.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-7_2", 
            "text": "blahp-1.18.37.bosco-2.osgup.el7  condor-8.7.9-1.osgup.el7  glite-ce-cream-client-api-c-1.15.4-2.6.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   osg - gridftp   osg - gridftp - hdfs   osg - gridftp - xrootd   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 37 . bosco - 2 . osgup . el6  blahp - debuginfo - 1 . 18 . 37 . bosco - 2 . osgup . el6  condor - 8 . 7 . 9 - 1 . osgup . el6  condor - all - 8 . 7 . 9 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 9 - 1 . osgup . el6  condor - bosco - 8 . 7 . 9 - 1 . osgup . el6  condor - classads - 8 . 7 . 9 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 9 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 9 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 9 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 9 - 1 . osgup . el6  condor - procd - 8 . 7 . 9 - 1 . osgup . el6  condor - python - 8 . 7 . 9 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 9 - 1 . osgup . el6  condor - test - 8 . 7 . 9 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 9 - 1 . osgup . el6  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 6 . osgup . el6  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 6 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-17/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 37 . bosco - 2 . osgup . el7  blahp - debuginfo - 1 . 18 . 37 . bosco - 2 . osgup . el7  condor - 8 . 7 . 9 - 1 . osgup . el7  condor - all - 8 . 7 . 9 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 9 - 1 . osgup . el7  condor - bosco - 8 . 7 . 9 - 1 . osgup . el7  condor - classads - 8 . 7 . 9 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 9 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 9 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 9 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 9 - 1 . osgup . el7  condor - procd - 8 . 7 . 9 - 1 . osgup . el7  condor - python - 8 . 7 . 9 - 1 . osgup . el7  condor - test - 8 . 7 . 9 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 9 - 1 . osgup . el7  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 6 . osgup . el7  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 6 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.16-2\n\n\nRelease Date\n: 2018-08-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCILogon OpenID Certification Authority Certificate\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncilogon-openid-ca-cert-1.1-4.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncilogon-openid-ca-cert-1.1-4.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n-\n1\n.\n1\n-\n4\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n-\n1\n.\n1\n-\n4\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.16-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#osg-software-stack-data-release-3416-2", 
            "text": "Release Date : 2018-08-08", 
            "title": "OSG Software Stack -- Data Release -- 3.4.16-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#summary-of-changes", 
            "text": "This release contains:   CILogon OpenID Certification Authority Certificate   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#enterprise-linux-6", 
            "text": "cilogon-openid-ca-cert-1.1-4.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#enterprise-linux-7", 
            "text": "cilogon-openid-ca-cert-1.1-4.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  cilogon - openid - ca - cert   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#enterprise-linux-6_1", 
            "text": "cilogon - openid - ca - cert - 1 . 1 - 4 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-16-2/#enterprise-linux-7_1", 
            "text": "cilogon - openid - ca - cert - 1 . 1 - 4 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/", 
            "text": "OSG Software Release 3.4.16\n\n\nRelease Date\n: 2018-08-01\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nFrontier Squid 3.5.27-5.2\n: Improve performance by not doing a reverse DNS lookup for every client connection\n\n\nXRootD 4.8.4\n: Bug fix release\n\n\nSciTokens 1.2.0\n: Initial SciTokens Support (EL7 only)\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nfrontier-squid-3.5.27-5.2.osg34.el6\n\n\nosg-version-3.4.16-1.osg34.el6\n\n\nxrootd-4.8.4-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nfrontier-squid-3.5.27-5.2.osg34.el7\n\n\nosg-version-3.4.16-1.osg34.el7\n\n\npython-scitokens-1.2.0-2.osg34.el7\n\n\nxrootd-4.8.4-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \nosg\n-\nversion\n \npython2\n-\nscitokens\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n5\n.\n2\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n5\n.\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n16\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n5\n.\n2\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n5\n.\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n16\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nscitokens\n-\n1\n.\n2\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\npython\n-\nscitokens\n-\n1\n.\n2\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n4\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.16"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#osg-software-release-3416", 
            "text": "Release Date : 2018-08-01", 
            "title": "OSG Software Release 3.4.16"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#summary-of-changes", 
            "text": "This release contains:   Frontier Squid 3.5.27-5.2 : Improve performance by not doing a reverse DNS lookup for every client connection  XRootD 4.8.4 : Bug fix release  SciTokens 1.2.0 : Initial SciTokens Support (EL7 only)   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#enterprise-linux-6", 
            "text": "frontier-squid-3.5.27-5.2.osg34.el6  osg-version-3.4.16-1.osg34.el6  xrootd-4.8.4-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#enterprise-linux-7", 
            "text": "frontier-squid-3.5.27-5.2.osg34.el7  osg-version-3.4.16-1.osg34.el7  python-scitokens-1.2.0-2.osg34.el7  xrootd-4.8.4-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  frontier - squid   frontier - squid - debuginfo   osg - version   python2 - scitokens   python2 - xrootd   python3 - xrootd   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#enterprise-linux-6_1", 
            "text": "frontier - squid - 3 . 5 . 27 - 5 . 2 . osg34 . el6  frontier - squid - debuginfo - 3 . 5 . 27 - 5 . 2 . osg34 . el6  osg - version - 3 . 4 . 16 - 1 . osg34 . el6  python2 - xrootd - 4 . 8 . 4 - 1 . osg34 . el6  python3 - xrootd - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - client - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - client - devel - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - client - libs - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - devel - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - doc - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - fuse - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - libs - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - private - devel - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - selinux - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - server - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - server - devel - 4 . 8 . 4 - 1 . osg34 . el6  xrootd - server - libs - 4 . 8 . 4 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-16/#enterprise-linux-7_1", 
            "text": "frontier - squid - 3 . 5 . 27 - 5 . 2 . osg34 . el7  frontier - squid - debuginfo - 3 . 5 . 27 - 5 . 2 . osg34 . el7  osg - version - 3 . 4 . 16 - 1 . osg34 . el7  python2 - scitokens - 1 . 2 . 0 - 2 . osg34 . el7  python2 - xrootd - 4 . 8 . 4 - 1 . osg34 . el7  python3 - xrootd - 4 . 8 . 4 - 1 . osg34 . el7  python - scitokens - 1 . 2 . 0 - 2 . osg34 . el7  xrootd - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - client - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - client - devel - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - client - libs - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - devel - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - doc - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - fuse - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - private - devel - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - selinux - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - server - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - server - devel - 4 . 8 . 4 - 1 . osg34 . el7  xrootd - server - libs - 4 . 8 . 4 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/", 
            "text": "OSG Software Release 3.4.15\n\n\nRelease Date\n: 2018-07-06\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nSingularity 2.5.2\n\n\nThis release contains fixes for an EL7 \nhigh severity\n security issue affecting Singularity 2.3.0 through 2.5.1\n    on kernels that support overlay file systems (CVE-2018-12021). A malicious user with network access to the host\n    system (e.g. ssh) could exploit this vulnerability to access sensitive information on disk and bypass directory\n    image restrictions like those preventing the root file system from being mounted into the container.\n\n\nSingularity 2.5.2 should be installed immediately, and all previous versions of Singularity should be removed.\n    The vulnerability addressed in this release affects kernels that support overlayfs. If you are unable to upgrade\n    immediately, you should set \nenable overlay = no\n in \nsingularity.conf\n.\n\n\nIn addition, this release contains a large number of bug fixes.\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nosg-version-3.4.15-1.osg34.el6\n\n\nsingularity-2.5.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nosg-version-3.4.15-1.osg34.el7\n\n\nsingularity-2.5.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\nversion\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n15\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n15\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n2\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.15"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#osg-software-release-3415", 
            "text": "Release Date : 2018-07-06", 
            "title": "OSG Software Release 3.4.15"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#summary-of-changes", 
            "text": "This release contains:   Singularity 2.5.2  This release contains fixes for an EL7  high severity  security issue affecting Singularity 2.3.0 through 2.5.1\n    on kernels that support overlay file systems (CVE-2018-12021). A malicious user with network access to the host\n    system (e.g. ssh) could exploit this vulnerability to access sensitive information on disk and bypass directory\n    image restrictions like those preventing the root file system from being mounted into the container.  Singularity 2.5.2 should be installed immediately, and all previous versions of Singularity should be removed.\n    The vulnerability addressed in this release affects kernels that support overlayfs. If you are unable to upgrade\n    immediately, you should set  enable overlay = no  in  singularity.conf .  In addition, this release contains a large number of bug fixes.     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#enterprise-linux-6", 
            "text": "osg-version-3.4.15-1.osg34.el6  singularity-2.5.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#enterprise-linux-7", 
            "text": "osg-version-3.4.15-1.osg34.el7  singularity-2.5.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - version   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#enterprise-linux-6_1", 
            "text": "osg - version - 3 . 4 . 15 - 1 . osg34 . el6  singularity - 2 . 5 . 2 - 1 . osg34 . el6  singularity - debuginfo - 2 . 5 . 2 - 1 . osg34 . el6  singularity - devel - 2 . 5 . 2 - 1 . osg34 . el6  singularity - runtime - 2 . 5 . 2 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-15/#enterprise-linux-7_1", 
            "text": "osg - version - 3 . 4 . 15 - 1 . osg34 . el7  singularity - 2 . 5 . 2 - 1 . osg34 . el7  singularity - debuginfo - 2 . 5 . 2 - 1 . osg34 . el7  singularity - devel - 2 . 5 . 2 - 1 . osg34 . el7  singularity - runtime - 2 . 5 . 2 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.14-2\n\n\nRelease Date\n: 2018-07-05\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.92\n\n\nAdded HKU CA 2 trust anchor during transitioning period (HK)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.92-1.osg34.el6\n\n\nosg-ca-certs-1.74-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.92-1.osg34.el7\n\n\nosg-ca-certs-1.74-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n92\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n74\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n92\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n74\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.14-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#osg-software-stack-data-release-3414-2", 
            "text": "Release Date : 2018-07-05", 
            "title": "OSG Software Stack -- Data Release -- 3.4.14-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.92  Added HKU CA 2 trust anchor during transitioning period (HK)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.92-1.osg34.el6  osg-ca-certs-1.74-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.92-1.osg34.el7  osg-ca-certs-1.74-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 92 - 1 . osg34 . el6  osg - ca - certs - 1 . 74 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 92 - 1 . osg34 . el7  osg - ca - certs - 1 . 74 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/", 
            "text": "OSG Software Release 3.4.14\n\n\nRelease Date\n: 2018-07-02\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\n\n\nOSG 3.4 respository\n\n\n\n\nHadoop Distributed File System 2.6\n on El7 (Based on cloudera CDH 5)\n\n\nosg-configure 2.3.1: automatically turn off BLAHP proxy renewal on HTCondor-CE\n\n\nRSV 3.19.7: works with Apache 2.4, fixed CA and CRL probes\n\n\nlcmaps-plugins-voms 1.7.1-1.6: fixes issues with llrun\n\n\nGratia probes 1.20.3: improve performance with PBS/LSF batch systems\n\n\n\n\nosg-pki-tools 3.0.0: \ngenerate CSRs for CAs\n\n\n\n\nNote\n\n\nThe OSG CA has been retired\n\n\n\n\n\n\n\n\nGridFTP-HDFS 1.1.1-1.2: increase control channel timeout to avoid checksum failures\n\n\n\n\nlcmaps-plugins-verify-proxy: fix rare crash\n\n\nOne-Way Ping (OWAMP) 3.5.6\n: Minor bug fixes\n\n\nBLAHP: Improved security\n\n\nUpcoming repository\n\n\n\n\nGlideinWMS 3.4\n\n\n\n\nCode modernization to Python 2.7 idioms (Python 2.6 is still supported)\n\n\nSupport for Google CE and the policy plugins.\n\n\nGlidein lifetime is no longer based on the length of the proxy\n\n\nNew option to kill glideins when the number of job requests drop\n\n\nEstimate in advance how many cores will be provided to glideins\n\n\nAdd entry monitoring breakdown for metasites\n\n\nReview Factory and Frontend tools, especially glidien_off and manual_glidein_submit.py\n\n\nSingularity improvements\n\n\nUse PATH and module when SINGULARITY_BIN does not contain the correct path\n\n\nRun in a separate session to support restricted-access CVMFS\n\n\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nAll nodes\n\n\nGLIDEIN_ToDie is no longer shortened depending on the X509 proxy lifetime, to get the old behavior set GLIDEIN_Ignore_X509_Duration to False\n\n\nThe type of the GLIDEIN_CPU attr is String (to accomodate the keywords auto, slot, node). It was incorrectly documented as type Int. Make sure your configuration uses the correct type or you may get a reconfig/upgrade error.\n\n\n\n\n\n\nFactory only:\n\n\nIf you use HTCondor 8.7.2 or later with the GlideinWMS Factory, then GRAM gateways are no longer supported\n\n\n'entry_sets' should be considered an experimental feature: the implementation will change in 3.4.1 and there may be errors and manual interventions required when upgrading to new versions of GlideinWMS if metasites are configured\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBLAHP: Improved security\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.37.bosco-1.osg34.el6\n\n\nglobus-gridftp-server-12.2-1.3.osg34.el6\n\n\ngratia-probe-1.20.3-2.osg34.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.11-1.1.osg34.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.6.osg34.el6\n\n\nosg-build-1.13.0.1-1.osg34.el6\n\n\nosg-configure-2.3.1-1.osg34.el6\n\n\nosg-pki-tools-3.0.0-1.osg34.el6\n\n\nosg-version-3.4.14-1.osg34.el6\n\n\nowamp-3.5.6-1.osg34.el6\n\n\nrsv-3.19.7-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\navro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.1.osg34.el7\n\n\nbigtop-jsvc-0.3.0-1.2.osg34.el7\n\n\nbigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg34.el7\n\n\nblahp-1.18.37.bosco-1.osg34.el7\n\n\nglobus-gridftp-server-12.2-1.3.osg34.el7\n\n\ngratia-probe-1.20.3-2.osg34.el7\n\n\ngridftp-hdfs-1.1.1-1.2.osg34.el7\n\n\n[hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.7.osg34.el7](https://koji.chtc.wisc.edu/koji/search?match=glob\ntype=build\nterms=hadoop-2.6.0%2Bcdh5.12.1%2B2540-1.cdh5.12.1.p0.3.7.osg34.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.11-1.1.osg34.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.6.osg34.el7\n\n\nosg-build-1.13.0.1-1.osg34.el7\n\n\nosg-configure-2.3.1-1.osg34.el7\n\n\nosg-gridftp-hdfs-3.4-3.osg34.el7\n\n\nosg-pki-tools-3.0.0-1.osg34.el7\n\n\nosg-se-hadoop-3.4-6.osg34.el7\n\n\nosg-version-3.4.14-1.osg34.el7\n\n\nowamp-3.5.6-1.osg34.el7\n\n\nrsv-3.19.7-1.osg34.el7\n\n\nxrootd-hdfs-2.0.2-1.1.osg34.el7\n\n\nzookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\navro\n-\ndoc\n \navro\n-\nlibs\n \navro\n-\ntools\n \nbigtop\n-\njsvc\n \nbigtop\n-\njsvc\n-\ndebuginfo\n \nbigtop\n-\nutils\n \nblahp\n \nblahp\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \ngridftp\n-\nhdfs\n \ngridftp\n-\nhdfs\n-\ndebuginfo\n \nhadoop\n \nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n \nhadoop\n-\n0\n.\n20\n-\nmapreduce\n \nhadoop\n-\nclient\n \nhadoop\n-\nconf\n-\npseudo\n \nhadoop\n-\ndebuginfo\n \nhadoop\n-\ndoc\n \nhadoop\n-\nhdfs\n \nhadoop\n-\nhdfs\n-\ndatanode\n \nhadoop\n-\nhdfs\n-\nfuse\n \nhadoop\n-\nhdfs\n-\njournalnode\n \nhadoop\n-\nhdfs\n-\nnamenode\n \nhadoop\n-\nhdfs\n-\nnfs3\n \nhadoop\n-\nhdfs\n-\nsecondarynamenode\n \nhadoop\n-\nhdfs\n-\nzkfc\n \nhadoop\n-\nhttpfs\n \nhadoop\n-\nkms\n \nhadoop\n-\nkms\n-\nserver\n \nhadoop\n-\nlibhdfs\n \nhadoop\n-\nlibhdfs\n-\ndevel\n \nhadoop\n-\nmapreduce\n \nhadoop\n-\nyarn\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n \nlcmaps\n-\nplugins\n-\nvoms\n \nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nsiteinfo\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ngridftp\n-\nhdfs\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nse\n-\nhadoop\n \nosg\n-\nse\n-\nhadoop\n-\nclient\n \nosg\n-\nse\n-\nhadoop\n-\ndatanode\n \nosg\n-\nse\n-\nhadoop\n-\ngridftp\n \nosg\n-\nse\n-\nhadoop\n-\nnamenode\n \nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n \nosg\n-\nversion\n \nowamp\n \nowamp\n-\nclient\n \nowamp\n-\ndebuginfo\n \nowamp\n-\nserver\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n \nxrootd\n-\nhdfs\n \nxrootd\n-\nhdfs\n-\ndebuginfo\n \nxrootd\n-\nhdfs\n-\ndevel\n \nzookeeper\n \nzookeeper\n-\ndebuginfo\n \nzookeeper\n-\nnative\n \nzookeeper\n-\nserver\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n11\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n11\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n14\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\nclient\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\nserver\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\navro\n-\ndoc\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosg34\n.\nel7\n\n\navro\n-\nlibs\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosg34\n.\nel7\n\n\navro\n-\ntools\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosg34\n.\nel7\n\n\nbigtop\n-\njsvc\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nbigtop\n-\njsvc\n-\ndebuginfo\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nbigtop\n-\nutils\n-\n0\n.\n7\n.\n0\n+\ncdh5\n.\n13\n.\n0\n+\n0\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\ngridftp\n-\nhdfs\n-\n1\n.\n1\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngridftp\n-\nhdfs\n-\ndebuginfo\n-\n1\n.\n1\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\n0\n.\n20\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nclient\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\ndebuginfo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\ndoc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\ndatanode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nfuse\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\njournalnode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nnamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nnfs3\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nsecondarynamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nzkfc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nhttpfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nkms\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nkms\n-\nserver\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nlibhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nlibhdfs\n-\ndevel\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nhadoop\n-\nyarn\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n11\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n11\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n13\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n3\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\nhdfs\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n3\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nclient\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ndatanode\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ngridftp\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nnamenode\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n14\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\nclient\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\nserver\n-\n3\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n19\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\n2\n.\n0\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndebuginfo\n-\n2\n.\n0\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndevel\n-\n2\n.\n0\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nzookeeper\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosg34\n.\nel7\n\n\nzookeeper\n-\ndebuginfo\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosg34\n.\nel7\n\n\nzookeeper\n-\nnative\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosg34\n.\nel7\n\n\nzookeeper\n-\nserver\n-\n3\n.\n4\n.\n5\n+\ncdh5\n.\n14\n.\n2\n+\n142\n-\n1\n.\ncdh5\n.\n14\n.\n2\n.\np0\n.\n11\n.\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.37.bosco-1.osgup.el6\n\n\nglideinwms-3.4-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.37.bosco-1.osgup.el7\n\n\nglideinwms-3.4-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n37\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.14"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#osg-software-release-3414", 
            "text": "Release Date : 2018-07-02", 
            "title": "OSG Software Release 3.4.14"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#summary-of-changes", 
            "text": "This release contains:    OSG 3.4 respository   Hadoop Distributed File System 2.6  on El7 (Based on cloudera CDH 5)  osg-configure 2.3.1: automatically turn off BLAHP proxy renewal on HTCondor-CE  RSV 3.19.7: works with Apache 2.4, fixed CA and CRL probes  lcmaps-plugins-voms 1.7.1-1.6: fixes issues with llrun  Gratia probes 1.20.3: improve performance with PBS/LSF batch systems   osg-pki-tools 3.0.0:  generate CSRs for CAs   Note  The OSG CA has been retired     GridFTP-HDFS 1.1.1-1.2: increase control channel timeout to avoid checksum failures   lcmaps-plugins-verify-proxy: fix rare crash  One-Way Ping (OWAMP) 3.5.6 : Minor bug fixes  BLAHP: Improved security  Upcoming repository   GlideinWMS 3.4   Code modernization to Python 2.7 idioms (Python 2.6 is still supported)  Support for Google CE and the policy plugins.  Glidein lifetime is no longer based on the length of the proxy  New option to kill glideins when the number of job requests drop  Estimate in advance how many cores will be provided to glideins  Add entry monitoring breakdown for metasites  Review Factory and Frontend tools, especially glidien_off and manual_glidein_submit.py  Singularity improvements  Use PATH and module when SINGULARITY_BIN does not contain the correct path  Run in a separate session to support restricted-access CVMFS      Notes   All nodes  GLIDEIN_ToDie is no longer shortened depending on the X509 proxy lifetime, to get the old behavior set GLIDEIN_Ignore_X509_Duration to False  The type of the GLIDEIN_CPU attr is String (to accomodate the keywords auto, slot, node). It was incorrectly documented as type Int. Make sure your configuration uses the correct type or you may get a reconfig/upgrade error.    Factory only:  If you use HTCondor 8.7.2 or later with the GlideinWMS Factory, then GRAM gateways are no longer supported  'entry_sets' should be considered an experimental feature: the implementation will change in 3.4.1 and there may be errors and manual interventions required when upgrading to new versions of GlideinWMS if metasites are configured        BLAHP: Improved security      These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-6", 
            "text": "blahp-1.18.37.bosco-1.osg34.el6  globus-gridftp-server-12.2-1.3.osg34.el6  gratia-probe-1.20.3-2.osg34.el6  lcmaps-plugins-verify-proxy-1.5.11-1.1.osg34.el6  lcmaps-plugins-voms-1.7.1-1.6.osg34.el6  osg-build-1.13.0.1-1.osg34.el6  osg-configure-2.3.1-1.osg34.el6  osg-pki-tools-3.0.0-1.osg34.el6  osg-version-3.4.14-1.osg34.el6  owamp-3.5.6-1.osg34.el6  rsv-3.19.7-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-7", 
            "text": "avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.1.osg34.el7  bigtop-jsvc-0.3.0-1.2.osg34.el7  bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osg34.el7  blahp-1.18.37.bosco-1.osg34.el7  globus-gridftp-server-12.2-1.3.osg34.el7  gratia-probe-1.20.3-2.osg34.el7  gridftp-hdfs-1.1.1-1.2.osg34.el7  [hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.7.osg34.el7](https://koji.chtc.wisc.edu/koji/search?match=glob type=build terms=hadoop-2.6.0%2Bcdh5.12.1%2B2540-1.cdh5.12.1.p0.3.7.osg34.el7  lcmaps-plugins-verify-proxy-1.5.11-1.1.osg34.el7  lcmaps-plugins-voms-1.7.1-1.6.osg34.el7  osg-build-1.13.0.1-1.osg34.el7  osg-configure-2.3.1-1.osg34.el7  osg-gridftp-hdfs-3.4-3.osg34.el7  osg-pki-tools-3.0.0-1.osg34.el7  osg-se-hadoop-3.4-6.osg34.el7  osg-version-3.4.14-1.osg34.el7  owamp-3.5.6-1.osg34.el7  rsv-3.19.7-1.osg34.el7  xrootd-hdfs-2.0.2-1.1.osg34.el7  zookeeper-3.4.5+cdh5.14.2+142-1.cdh5.14.2.p0.11.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  avro - doc   avro - libs   avro - tools   bigtop - jsvc   bigtop - jsvc - debuginfo   bigtop - utils   blahp   blahp - debuginfo   globus - gridftp - server   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glideinwms   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   gridftp - hdfs   gridftp - hdfs - debuginfo   hadoop   hadoop - 0 . 20 - conf - pseudo   hadoop - 0 . 20 - mapreduce   hadoop - client   hadoop - conf - pseudo   hadoop - debuginfo   hadoop - doc   hadoop - hdfs   hadoop - hdfs - datanode   hadoop - hdfs - fuse   hadoop - hdfs - journalnode   hadoop - hdfs - namenode   hadoop - hdfs - nfs3   hadoop - hdfs - secondarynamenode   hadoop - hdfs - zkfc   hadoop - httpfs   hadoop - kms   hadoop - kms - server   hadoop - libhdfs   hadoop - libhdfs - devel   hadoop - mapreduce   hadoop - yarn   lcmaps - plugins - verify - proxy   lcmaps - plugins - verify - proxy - debuginfo   lcmaps - plugins - voms   lcmaps - plugins - voms - debuginfo   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - misc   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - siteinfo   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - gridftp - hdfs   osg - pki - tools   osg - se - hadoop   osg - se - hadoop - client   osg - se - hadoop - datanode   osg - se - hadoop - gridftp   osg - se - hadoop - namenode   osg - se - hadoop - secondarynamenode   osg - version   owamp   owamp - client   owamp - debuginfo   owamp - server   rsv   rsv - consumers   rsv - core   rsv - metrics   xrootd - hdfs   xrootd - hdfs - debuginfo   xrootd - hdfs - devel   zookeeper   zookeeper - debuginfo   zookeeper - native   zookeeper - server   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 37 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 37 . bosco - 1 . osg34 . el6  globus - gridftp - server - 12 . 2 - 1 . 3 . osg34 . el6  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 3 . osg34 . el6  globus - gridftp - server - devel - 12 . 2 - 1 . 3 . osg34 . el6  globus - gridftp - server - progs - 12 . 2 - 1 . 3 . osg34 . el6  gratia - probe - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - common - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - condor - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - condor - events - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - dcache - storage - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - debuginfo - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - enstore - storage - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - glideinwms - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - lsf - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - metric - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - onevm - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - services - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - sge - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - slurm - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 20 . 3 - 2 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 20 . 3 - 2 . osg34 . el6  lcmaps - plugins - verify - proxy - 1 . 5 . 11 - 1 . 1 . osg34 . el6  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 11 - 1 . 1 . osg34 . el6  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 6 . osg34 . el6  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 6 . osg34 . el6  osg - build - 1 . 13 . 0 . 1 - 1 . osg34 . el6  osg - build - base - 1 . 13 . 0 . 1 - 1 . osg34 . el6  osg - build - koji - 1 . 13 . 0 . 1 - 1 . osg34 . el6  osg - build - mock - 1 . 13 . 0 . 1 - 1 . osg34 . el6  osg - build - tests - 1 . 13 . 0 . 1 - 1 . osg34 . el6  osg - configure - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - bosco - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - ce - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - condor - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - gateway - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - gip - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - gratia - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - lsf - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - misc - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - pbs - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - rsv - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - sge - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - siteinfo - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - slurm - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - squid - 2 . 3 . 1 - 1 . osg34 . el6  osg - configure - tests - 2 . 3 . 1 - 1 . osg34 . el6  osg - pki - tools - 3 . 0 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 14 - 1 . osg34 . el6  owamp - 3 . 5 . 6 - 1 . osg34 . el6  owamp - client - 3 . 5 . 6 - 1 . osg34 . el6  owamp - debuginfo - 3 . 5 . 6 - 1 . osg34 . el6  owamp - server - 3 . 5 . 6 - 1 . osg34 . el6  rsv - 3 . 19 . 7 - 1 . osg34 . el6  rsv - consumers - 3 . 19 . 7 - 1 . osg34 . el6  rsv - core - 3 . 19 . 7 - 1 . osg34 . el6  rsv - metrics - 3 . 19 . 7 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-7_1", 
            "text": "avro - doc - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osg34 . el7  avro - libs - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osg34 . el7  avro - tools - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osg34 . el7  bigtop - jsvc - 0 . 3 . 0 - 1 . 2 . osg34 . el7  bigtop - jsvc - debuginfo - 0 . 3 . 0 - 1 . 2 . osg34 . el7  bigtop - utils - 0 . 7 . 0 + cdh5 . 13 . 0 + 0 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osg34 . el7  blahp - 1 . 18 . 37 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 37 . bosco - 1 . osg34 . el7  globus - gridftp - server - 12 . 2 - 1 . 3 . osg34 . el7  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 3 . osg34 . el7  globus - gridftp - server - devel - 12 . 2 - 1 . 3 . osg34 . el7  globus - gridftp - server - progs - 12 . 2 - 1 . 3 . osg34 . el7  gratia - probe - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - common - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - condor - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - condor - events - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - dcache - storage - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - debuginfo - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - enstore - storage - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - glideinwms - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - lsf - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - metric - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - onevm - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - services - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - sge - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - slurm - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 20 . 3 - 2 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 20 . 3 - 2 . osg34 . el7  gridftp - hdfs - 1 . 1 . 1 - 1 . 2 . osg34 . el7  gridftp - hdfs - debuginfo - 1 . 1 . 1 - 1 . 2 . osg34 . el7  hadoop - 0 . 20 - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - 0 . 20 - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - client - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - debuginfo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - doc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - datanode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - fuse - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - journalnode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - namenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - nfs3 - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - secondarynamenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - hdfs - zkfc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - httpfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - kms - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - kms - server - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - libhdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - libhdfs - devel - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  hadoop - yarn - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osg34 . el7  lcmaps - plugins - verify - proxy - 1 . 5 . 11 - 1 . 1 . osg34 . el7  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 11 - 1 . 1 . osg34 . el7  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 6 . osg34 . el7  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 6 . osg34 . el7  osg - build - 1 . 13 . 0 . 1 - 1 . osg34 . el7  osg - build - base - 1 . 13 . 0 . 1 - 1 . osg34 . el7  osg - build - koji - 1 . 13 . 0 . 1 - 1 . osg34 . el7  osg - build - mock - 1 . 13 . 0 . 1 - 1 . osg34 . el7  osg - build - tests - 1 . 13 . 0 . 1 - 1 . osg34 . el7  osg - configure - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - bosco - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - ce - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - condor - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - gateway - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - gip - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - gratia - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - lsf - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - misc - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - pbs - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - rsv - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - sge - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - siteinfo - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - slurm - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - squid - 2 . 3 . 1 - 1 . osg34 . el7  osg - configure - tests - 2 . 3 . 1 - 1 . osg34 . el7  osg - gridftp - hdfs - 3 . 4 - 3 . osg34 . el7  osg - pki - tools - 3 . 0 . 0 - 1 . osg34 . el7  osg - se - hadoop - 3 . 4 - 6 . osg34 . el7  osg - se - hadoop - client - 3 . 4 - 6 . osg34 . el7  osg - se - hadoop - datanode - 3 . 4 - 6 . osg34 . el7  osg - se - hadoop - gridftp - 3 . 4 - 6 . osg34 . el7  osg - se - hadoop - namenode - 3 . 4 - 6 . osg34 . el7  osg - se - hadoop - secondarynamenode - 3 . 4 - 6 . osg34 . el7  osg - version - 3 . 4 . 14 - 1 . osg34 . el7  owamp - 3 . 5 . 6 - 1 . osg34 . el7  owamp - client - 3 . 5 . 6 - 1 . osg34 . el7  owamp - debuginfo - 3 . 5 . 6 - 1 . osg34 . el7  owamp - server - 3 . 5 . 6 - 1 . osg34 . el7  rsv - 3 . 19 . 7 - 1 . osg34 . el7  rsv - consumers - 3 . 19 . 7 - 1 . osg34 . el7  rsv - core - 3 . 19 . 7 - 1 . osg34 . el7  rsv - metrics - 3 . 19 . 7 - 1 . osg34 . el7  xrootd - hdfs - 2 . 0 . 2 - 1 . 1 . osg34 . el7  xrootd - hdfs - debuginfo - 2 . 0 . 2 - 1 . 1 . osg34 . el7  xrootd - hdfs - devel - 2 . 0 . 2 - 1 . 1 . osg34 . el7  zookeeper - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osg34 . el7  zookeeper - debuginfo - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osg34 . el7  zookeeper - native - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osg34 . el7  zookeeper - server - 3 . 4 . 5 + cdh5 . 14 . 2 + 142 - 1 . cdh5 . 14 . 2 . p0 . 11 . 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-6_2", 
            "text": "blahp-1.18.37.bosco-1.osgup.el6  glideinwms-3.4-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-7_2", 
            "text": "blahp-1.18.37.bosco-1.osgup.el7  glideinwms-3.4-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 37 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 37 . bosco - 1 . osgup . el6  glideinwms - 3 . 4 - 1 . osgup . el6  glideinwms - common - tools - 3 . 4 - 1 . osgup . el6  glideinwms - condor - common - config - 3 . 4 - 1 . osgup . el6  glideinwms - factory - 3 . 4 - 1 . osgup . el6  glideinwms - factory - condor - 3 . 4 - 1 . osgup . el6  glideinwms - glidecondor - tools - 3 . 4 - 1 . osgup . el6  glideinwms - libs - 3 . 4 - 1 . osgup . el6  glideinwms - minimal - condor - 3 . 4 - 1 . osgup . el6  glideinwms - usercollector - 3 . 4 - 1 . osgup . el6  glideinwms - userschedd - 3 . 4 - 1 . osgup . el6  glideinwms - vofrontend - 3 . 4 - 1 . osgup . el6  glideinwms - vofrontend - standalone - 3 . 4 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-14/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 37 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 37 . bosco - 1 . osgup . el7  glideinwms - 3 . 4 - 1 . osgup . el7  glideinwms - common - tools - 3 . 4 - 1 . osgup . el7  glideinwms - condor - common - config - 3 . 4 - 1 . osgup . el7  glideinwms - factory - 3 . 4 - 1 . osgup . el7  glideinwms - factory - condor - 3 . 4 - 1 . osgup . el7  glideinwms - glidecondor - tools - 3 . 4 - 1 . osgup . el7  glideinwms - libs - 3 . 4 - 1 . osgup . el7  glideinwms - minimal - condor - 3 . 4 - 1 . osgup . el7  glideinwms - usercollector - 3 . 4 - 1 . osgup . el7  glideinwms - userschedd - 3 . 4 - 1 . osgup . el7  glideinwms - vofrontend - 3 . 4 - 1 . osgup . el7  glideinwms - vofrontend - standalone - 3 . 4 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/", 
            "text": "OSG Software Release 3.4.13\n\n\nRelease Date\n: 2018-06-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCVMFS 2.5.0\n: performance improvements, new functionality, and bug fixes\n\n\nHTCondor 8.6.11\n\n\nCan now do an interactive submit of a Singularity job\n\n\nShared port daemon is more resilient when starved for TCP ports\n\n\nThe Windows installer configures the environment for the Python bindings\n\n\nFixed several other minor problems\n\n\n\n\n\n\nSingularity 2.5.1\n: Minor bug fix release\n\n\nPegasus 4.8.2\n: Minor bug fix release\n\n\nVOMS: added voms-proxy-direct for use when signing VOMS attributes directly\n\n\nUpcoming repository:\n\n\nHTCondor 8.7.8\n\n\nThe condor annex can easily use multiple regions simultaneously\n\n\nHTCondor now uses CUDA_VISIBLE_DEVICES to tell which GPU devices to manage\n\n\nHTCondor now reports GPU memory utilization\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.11-1.osg34.el6\n\n\ncvmfs-2.5.0-1.osg34.el6\n\n\nosg-oasis-9-2.osg34.el6\n\n\nosg-release-3.4-5.osg34.el6\n\n\nosg-tested-internal-3.4-6.osg34.el6\n\n\nosg-version-3.4.13-1.osg34.el6\n\n\npegasus-4.8.2-1.osg34.el6\n\n\nsingularity-2.5.1-1.osg34.el6\n\n\nvoms-2.0.14-1.4.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.11-1.osg34.el7\n\n\ncvmfs-2.5.0-1.osg34.el7\n\n\nosg-oasis-9-2.osg34.el7\n\n\nosg-release-3.4-5.osg34.el7\n\n\nosg-tested-internal-3.4-6.osg34.el7\n\n\nosg-version-3.4.13-1.osg34.el7\n\n\npegasus-4.8.2-1.osg34.el7\n\n\nsingularity-2.5.1-1.osg34.el7\n\n\nvoms-2.0.14-1.4.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nosg\n-\noasis\n \nosg\n-\nrelease\n \nosg\n-\ntested\n-\ninternal\n \nosg\n-\nversion\n \npegasus\n \npegasus\n-\ndebuginfo\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n \nvoms\n \nvoms\n-\nclients\n-\ncpp\n \nvoms\n-\ndebuginfo\n \nvoms\n-\ndevel\n \nvoms\n-\ndoc\n \nvoms\n-\nserver\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n9\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel6\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n13\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n8\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nvoms\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nvoms\n-\nclients\n-\ncpp\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndebuginfo\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndevel\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndoc\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nvoms\n-\nserver\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n9\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel7\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n6\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n13\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n8\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nvoms\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nvoms\n-\nclients\n-\ncpp\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndebuginfo\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndevel\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndoc\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nvoms\n-\nserver\n-\n2\n.\n0\n.\n14\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.36.bosco-2.osgup.el6\n\n\ncondor-8.7.8-2.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.36.bosco-2.osgup.el7\n\n\ncondor-8.7.8-2.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n2\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n2\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n8\n-\n2\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.13"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#osg-software-release-3413", 
            "text": "Release Date : 2018-06-12", 
            "title": "OSG Software Release 3.4.13"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#summary-of-changes", 
            "text": "This release contains:   CVMFS 2.5.0 : performance improvements, new functionality, and bug fixes  HTCondor 8.6.11  Can now do an interactive submit of a Singularity job  Shared port daemon is more resilient when starved for TCP ports  The Windows installer configures the environment for the Python bindings  Fixed several other minor problems    Singularity 2.5.1 : Minor bug fix release  Pegasus 4.8.2 : Minor bug fix release  VOMS: added voms-proxy-direct for use when signing VOMS attributes directly  Upcoming repository:  HTCondor 8.7.8  The condor annex can easily use multiple regions simultaneously  HTCondor now uses CUDA_VISIBLE_DEVICES to tell which GPU devices to manage  HTCondor now reports GPU memory utilization       These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-6", 
            "text": "condor-8.6.11-1.osg34.el6  cvmfs-2.5.0-1.osg34.el6  osg-oasis-9-2.osg34.el6  osg-release-3.4-5.osg34.el6  osg-tested-internal-3.4-6.osg34.el6  osg-version-3.4.13-1.osg34.el6  pegasus-4.8.2-1.osg34.el6  singularity-2.5.1-1.osg34.el6  voms-2.0.14-1.4.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-7", 
            "text": "condor-8.6.11-1.osg34.el7  cvmfs-2.5.0-1.osg34.el7  osg-oasis-9-2.osg34.el7  osg-release-3.4-5.osg34.el7  osg-tested-internal-3.4-6.osg34.el7  osg-version-3.4.13-1.osg34.el7  pegasus-4.8.2-1.osg34.el7  singularity-2.5.1-1.osg34.el7  voms-2.0.14-1.4.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   osg - oasis   osg - release   osg - tested - internal   osg - version   pegasus   pegasus - debuginfo   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   voms   voms - clients - cpp   voms - debuginfo   voms - devel   voms - doc   voms - server   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 11 - 1 . osg34 . el6  condor - all - 8 . 6 . 11 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 11 - 1 . osg34 . el6  condor - classads - 8 . 6 . 11 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 11 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 11 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 11 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 11 - 1 . osg34 . el6  condor - procd - 8 . 6 . 11 - 1 . osg34 . el6  condor - python - 8 . 6 . 11 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 11 - 1 . osg34 . el6  condor - test - 8 . 6 . 11 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 11 - 1 . osg34 . el6  cvmfs - 2 . 5 . 0 - 1 . osg34 . el6  cvmfs - devel - 2 . 5 . 0 - 1 . osg34 . el6  cvmfs - server - 2 . 5 . 0 - 1 . osg34 . el6  cvmfs - unittests - 2 . 5 . 0 - 1 . osg34 . el6  osg - oasis - 9 - 2 . osg34 . el6  osg - release - 3 . 4 - 5 . osg34 . el6  osg - tested - internal - 3 . 4 - 6 . osg34 . el6  osg - version - 3 . 4 . 13 - 1 . osg34 . el6  pegasus - 4 . 8 . 2 - 1 . osg34 . el6  pegasus - debuginfo - 4 . 8 . 2 - 1 . osg34 . el6  singularity - 2 . 5 . 1 - 1 . osg34 . el6  singularity - debuginfo - 2 . 5 . 1 - 1 . osg34 . el6  singularity - devel - 2 . 5 . 1 - 1 . osg34 . el6  singularity - runtime - 2 . 5 . 1 - 1 . osg34 . el6  voms - 2 . 0 . 14 - 1 . 4 . osg34 . el6  voms - clients - cpp - 2 . 0 . 14 - 1 . 4 . osg34 . el6  voms - debuginfo - 2 . 0 . 14 - 1 . 4 . osg34 . el6  voms - devel - 2 . 0 . 14 - 1 . 4 . osg34 . el6  voms - doc - 2 . 0 . 14 - 1 . 4 . osg34 . el6  voms - server - 2 . 0 . 14 - 1 . 4 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 11 - 1 . osg34 . el7  condor - all - 8 . 6 . 11 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 11 - 1 . osg34 . el7  condor - classads - 8 . 6 . 11 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 11 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 11 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 11 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 11 - 1 . osg34 . el7  condor - procd - 8 . 6 . 11 - 1 . osg34 . el7  condor - python - 8 . 6 . 11 - 1 . osg34 . el7  condor - test - 8 . 6 . 11 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 11 - 1 . osg34 . el7  cvmfs - 2 . 5 . 0 - 1 . osg34 . el7  cvmfs - devel - 2 . 5 . 0 - 1 . osg34 . el7  cvmfs - server - 2 . 5 . 0 - 1 . osg34 . el7  cvmfs - unittests - 2 . 5 . 0 - 1 . osg34 . el7  osg - oasis - 9 - 2 . osg34 . el7  osg - release - 3 . 4 - 5 . osg34 . el7  osg - tested - internal - 3 . 4 - 6 . osg34 . el7  osg - version - 3 . 4 . 13 - 1 . osg34 . el7  pegasus - 4 . 8 . 2 - 1 . osg34 . el7  pegasus - debuginfo - 4 . 8 . 2 - 1 . osg34 . el7  singularity - 2 . 5 . 1 - 1 . osg34 . el7  singularity - debuginfo - 2 . 5 . 1 - 1 . osg34 . el7  singularity - devel - 2 . 5 . 1 - 1 . osg34 . el7  singularity - runtime - 2 . 5 . 1 - 1 . osg34 . el7  voms - 2 . 0 . 14 - 1 . 4 . osg34 . el7  voms - clients - cpp - 2 . 0 . 14 - 1 . 4 . osg34 . el7  voms - debuginfo - 2 . 0 . 14 - 1 . 4 . osg34 . el7  voms - devel - 2 . 0 . 14 - 1 . 4 . osg34 . el7  voms - doc - 2 . 0 . 14 - 1 . 4 . osg34 . el7  voms - server - 2 . 0 . 14 - 1 . 4 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-6_2", 
            "text": "blahp-1.18.36.bosco-2.osgup.el6  condor-8.7.8-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-7_2", 
            "text": "blahp-1.18.36.bosco-2.osgup.el7  condor-8.7.8-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 36 . bosco - 2 . osgup . el6  blahp - debuginfo - 1 . 18 . 36 . bosco - 2 . osgup . el6  condor - 8 . 7 . 8 - 2 . osgup . el6  condor - all - 8 . 7 . 8 - 2 . osgup . el6  condor - annex - ec2 - 8 . 7 . 8 - 2 . osgup . el6  condor - bosco - 8 . 7 . 8 - 2 . osgup . el6  condor - classads - 8 . 7 . 8 - 2 . osgup . el6  condor - classads - devel - 8 . 7 . 8 - 2 . osgup . el6  condor - cream - gahp - 8 . 7 . 8 - 2 . osgup . el6  condor - debuginfo - 8 . 7 . 8 - 2 . osgup . el6  condor - kbdd - 8 . 7 . 8 - 2 . osgup . el6  condor - procd - 8 . 7 . 8 - 2 . osgup . el6  condor - python - 8 . 7 . 8 - 2 . osgup . el6  condor - std - universe - 8 . 7 . 8 - 2 . osgup . el6  condor - test - 8 . 7 . 8 - 2 . osgup . el6  condor - vm - gahp - 8 . 7 . 8 - 2 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-13/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 36 . bosco - 2 . osgup . el7  blahp - debuginfo - 1 . 18 . 36 . bosco - 2 . osgup . el7  condor - 8 . 7 . 8 - 2 . osgup . el7  condor - all - 8 . 7 . 8 - 2 . osgup . el7  condor - annex - ec2 - 8 . 7 . 8 - 2 . osgup . el7  condor - bosco - 8 . 7 . 8 - 2 . osgup . el7  condor - classads - 8 . 7 . 8 - 2 . osgup . el7  condor - classads - devel - 8 . 7 . 8 - 2 . osgup . el7  condor - cream - gahp - 8 . 7 . 8 - 2 . osgup . el7  condor - debuginfo - 8 . 7 . 8 - 2 . osgup . el7  condor - kbdd - 8 . 7 . 8 - 2 . osgup . el7  condor - procd - 8 . 7 . 8 - 2 . osgup . el7  condor - python - 8 . 7 . 8 - 2 . osgup . el7  condor - test - 8 . 7 . 8 - 2 . osgup . el7  condor - vm - gahp - 8 . 7 . 8 - 2 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.12-3\n\n\nRelease Date\n: 2018-05-21\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.91\n\n\nUpdated MREN CA with extended validity period (ME)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.91-1.osg34.el6\n\n\nosg-ca-certs-1.73-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.91-1.osg34.el7\n\n\nosg-ca-certs-1.73-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n91\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n73\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n91\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n73\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.12-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#osg-software-stack-data-release-3412-3", 
            "text": "Release Date : 2018-05-21", 
            "title": "OSG Software Stack -- Data Release -- 3.4.12-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.91  Updated MREN CA with extended validity period (ME)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.91-1.osg34.el6  osg-ca-certs-1.73-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.91-1.osg34.el7  osg-ca-certs-1.73-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 91 - 1 . osg34 . el6  osg - ca - certs - 1 . 73 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-3/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 91 - 1 . osg34 . el7  osg - ca - certs - 1 . 73 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.12-2\n\n\nRelease Date\n: 2018-05-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated OSG CA Certificates\n\n\nAdded Let's Encrypt\n as a certificate authority\n\n\n\n\n\n\nVO Package v79\n\n\nAdded new InCommon DN/CA for voms.opensciencegrid.org\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nosg-ca-certs-1.72-1.osg34.el6\n\n\nvo-client-79-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nosg-ca-certs-1.72-1.osg34.el7\n\n\nvo-client-79-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\nca\n-\ncerts\n \nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n72\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n79\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n79\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n79\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n79\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n72\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n79\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n79\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n79\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n79\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.12-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#osg-software-stack-data-release-3412-2", 
            "text": "Release Date : 2018-05-10", 
            "title": "OSG Software Stack -- Data Release -- 3.4.12-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#summary-of-changes", 
            "text": "This release contains:   Updated OSG CA Certificates  Added Let's Encrypt  as a certificate authority    VO Package v79  Added new InCommon DN/CA for voms.opensciencegrid.org     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#enterprise-linux-6", 
            "text": "osg-ca-certs-1.72-1.osg34.el6  vo-client-79-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#enterprise-linux-7", 
            "text": "osg-ca-certs-1.72-1.osg34.el7  vo-client-79-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - ca - certs   osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#enterprise-linux-6_1", 
            "text": "osg - ca - certs - 1 . 72 - 1 . osg34 . el6  osg - gums - config - 79 - 1 . osg34 . el6  vo - client - 79 - 1 . osg34 . el6  vo - client - edgmkgridmap - 79 - 1 . osg34 . el6  vo - client - lcmaps - voms - 79 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12-2/#enterprise-linux-7_1", 
            "text": "osg - ca - certs - 1 . 72 - 1 . osg34 . el7  osg - gums - config - 79 - 1 . osg34 . el7  vo - client - 79 - 1 . osg34 . el7  vo - client - edgmkgridmap - 79 - 1 . osg34 . el7  vo - client - lcmaps - voms - 79 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/", 
            "text": "OSG Software Release 3.4.12\n\n\nRelease Date\n: 2018-05-10\n\n\n\n\nRequired Actions\n\n\nDue to the retirement of \ngrid.iu.edu\n hosts (see \nthis document\n\nfor details), some software packages require updates to reference new hosts.\n\n\n\n\n\n\nUpdate all packages that may contain references to \ngrid.iu.edu\n:\n   (Yum will only update already installed packages.)\n\n\nroot@host #\n yum update osg-ca-certs-updater osg-ca-scripts osg-release osg-release-itb \n\\\n\n            osg-test\n\\*\n rsv\n\\*\n\n\n\n\n\n\n\n\n\n\nIf you have \nrsv\n installed, see \nthis section\n below for rsv-specific instructions.\n\n\n\n\n\n\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated references to grid.iu.edu with opensciencegrid.org in the following packages:\n\n\nosg-ca-certs-updater\n\n\nosg-ca-scripts\n\n\nosg-release\n\n\nosg-release-itb\n\n\nosg-test\n\n\nrsv\n\n\n\n\n\n\nHTCondor-CE 3.1.2\n: Added mapping for Let's Encrypt\n\n\nGlideinWMS 3.2.22.2\n\n\nImproved interoperation with Singularity\n\n\nImproved proxy renewal support\n\n\n\n\n\n\nXRootD 4.8.3\n\n\nGratia probes 1.20\n\n\nMore flexible parsing of PBS wall time\n\n\nFixed bug in interaction with Slurm\n\n\nMade ProjectName comparision case insensitive\n\n\nDropped GRAM and glexec probes\n\n\n\n\n\n\nRSV 3.18\n\n\nEnhanced java version probe to detect Tomcat on Enterprize Linux 7\n\n\nDisable deprecated probes by default\n\n\nDrop gratia-consumer probe\n\n\n\n\n\n\nUpcoming:\n\n\nGlideinWMS 3.3.3\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore (\n_\n). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nDue to the central RSV service retirement (see \nthis document\n for details),\nthe new version of RSV will disable the \ngratia-consumer\n component that reports to the central service.\nPlease update \nall\n RSV packages by running the following command on your RSV host:\n\n\nroot@host #\n yum update rsv\n\\*\n\n\n\n\n\n\nAdditionally, if you are using osg-configure, please edit \n/etc/osg/config.d/30-rsv.ini\n and set the following:\n\n\nenable_gratia\n \n=\n \nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.2.22.2-4.osg34.el6\n\n\ngratia-probe-1.20.1-1.osg34.el6\n\n\nhtcondor-ce-3.1.2-1.osg34.el6\n\n\nosg-ca-certs-updater-1.8-1.osg34.el6\n\n\nosg-ca-scripts-1.2.3-1.osg34.el6\n\n\nosg-configure-2.3.0-1.osg34.el6\n\n\nosg-test-2.2.0-1.osg34.el6\n\n\nosg-version-3.4.12-1.osg34.el6\n\n\nrsv-3.18.0-1.osg34.el6\n\n\nxrootd-4.8.3-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.2.22.2-4.osg34.el7\n\n\ngratia-probe-1.20.1-1.osg34.el7\n\n\nhtcondor-ce-3.1.2-1.osg34.el7\n\n\nosg-ca-certs-updater-1.8-1.osg34.el7\n\n\nosg-ca-scripts-1.2.3-1.osg34.el7\n\n\nosg-configure-2.3.0-1.osg34.el7\n\n\nosg-test-2.2.0-1.osg34.el7\n\n\nosg-version-3.4.12-1.osg34.el7\n\n\nrsv-3.18.0-1.osg34.el7\n\n\nxrootd-4.8.3-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nosg\n-\nca\n-\ncerts\n-\nupdater\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nsiteinfo\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n12\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n22\n.\n2\n-\n4\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n20\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n3\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n12\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n18\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.3.3-3.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.3.3-3.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n3\n.\n3\n-\n3\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.12"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#osg-software-release-3412", 
            "text": "Release Date : 2018-05-10   Required Actions  Due to the retirement of  grid.iu.edu  hosts (see  this document \nfor details), some software packages require updates to reference new hosts.    Update all packages that may contain references to  grid.iu.edu :\n   (Yum will only update already installed packages.)  root@host #  yum update osg-ca-certs-updater osg-ca-scripts osg-release osg-release-itb  \\ \n            osg-test \\*  rsv \\*     If you have  rsv  installed, see  this section  below for rsv-specific instructions.", 
            "title": "OSG Software Release 3.4.12"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#summary-of-changes", 
            "text": "This release contains:   Updated references to grid.iu.edu with opensciencegrid.org in the following packages:  osg-ca-certs-updater  osg-ca-scripts  osg-release  osg-release-itb  osg-test  rsv    HTCondor-CE 3.1.2 : Added mapping for Let's Encrypt  GlideinWMS 3.2.22.2  Improved interoperation with Singularity  Improved proxy renewal support    XRootD 4.8.3  Gratia probes 1.20  More flexible parsing of PBS wall time  Fixed bug in interaction with Slurm  Made ProjectName comparision case insensitive  Dropped GRAM and glexec probes    RSV 3.18  Enhanced java version probe to detect Tomcat on Enterprize Linux 7  Disable deprecated probes by default  Drop gratia-consumer probe    Upcoming:  GlideinWMS 3.3.3     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ( _ ). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#known-issues", 
            "text": "Due to the central RSV service retirement (see  this document  for details),\nthe new version of RSV will disable the  gratia-consumer  component that reports to the central service.\nPlease update  all  RSV packages by running the following command on your RSV host:  root@host #  yum update rsv \\*   Additionally, if you are using osg-configure, please edit  /etc/osg/config.d/30-rsv.ini  and set the following:  enable_gratia   =   False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-6", 
            "text": "glideinwms-3.2.22.2-4.osg34.el6  gratia-probe-1.20.1-1.osg34.el6  htcondor-ce-3.1.2-1.osg34.el6  osg-ca-certs-updater-1.8-1.osg34.el6  osg-ca-scripts-1.2.3-1.osg34.el6  osg-configure-2.3.0-1.osg34.el6  osg-test-2.2.0-1.osg34.el6  osg-version-3.4.12-1.osg34.el6  rsv-3.18.0-1.osg34.el6  xrootd-4.8.3-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-7", 
            "text": "glideinwms-3.2.22.2-4.osg34.el7  gratia-probe-1.20.1-1.osg34.el7  htcondor-ce-3.1.2-1.osg34.el7  osg-ca-certs-updater-1.8-1.osg34.el7  osg-ca-scripts-1.2.3-1.osg34.el7  osg-configure-2.3.0-1.osg34.el7  osg-test-2.2.0-1.osg34.el7  osg-version-3.4.12-1.osg34.el7  rsv-3.18.0-1.osg34.el7  xrootd-4.8.3-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glideinwms   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   osg - ca - certs - updater   osg - ca - scripts   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - misc   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - siteinfo   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - test   osg - test - log - viewer   osg - version   python2 - xrootd   python3 - xrootd   rsv   rsv - consumers   rsv - core   rsv - metrics   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-6_1", 
            "text": "glideinwms - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - common - tools - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - condor - common - config - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - factory - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - factory - condor - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - libs - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - minimal - condor - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - usercollector - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - userschedd - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - vofrontend - 3 . 2 . 22 . 2 - 4 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 2 . 22 . 2 - 4 . osg34 . el6  gratia - probe - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - common - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - condor - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - metric - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - services - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - sge - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 20 . 1 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 20 . 1 - 1 . osg34 . el6  htcondor - ce - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - client - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 1 . 2 - 1 . osg34 . el6  htcondor - ce - view - 3 . 1 . 2 - 1 . osg34 . el6  osg - ca - certs - updater - 1 . 8 - 1 . osg34 . el6  osg - ca - scripts - 1 . 2 . 3 - 1 . osg34 . el6  osg - configure - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - bosco - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - ce - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - condor - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - gateway - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - gip - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - gratia - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - lsf - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - misc - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - pbs - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - rsv - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - sge - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - siteinfo - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - slurm - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - squid - 2 . 3 . 0 - 1 . osg34 . el6  osg - configure - tests - 2 . 3 . 0 - 1 . osg34 . el6  osg - test - 2 . 2 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 2 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 12 - 1 . osg34 . el6  python2 - xrootd - 4 . 8 . 3 - 1 . osg34 . el6  python3 - xrootd - 4 . 8 . 3 - 1 . osg34 . el6  rsv - 3 . 18 . 0 - 1 . osg34 . el6  rsv - consumers - 3 . 18 . 0 - 1 . osg34 . el6  rsv - core - 3 . 18 . 0 - 1 . osg34 . el6  rsv - metrics - 3 . 18 . 0 - 1 . osg34 . el6  xrootd - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - client - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - client - devel - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - client - libs - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - devel - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - doc - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - fuse - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - libs - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - private - devel - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - selinux - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - server - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - server - devel - 4 . 8 . 3 - 1 . osg34 . el6  xrootd - server - libs - 4 . 8 . 3 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-7_1", 
            "text": "glideinwms - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - common - tools - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - condor - common - config - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - factory - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - factory - condor - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - libs - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - minimal - condor - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - usercollector - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - userschedd - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - vofrontend - 3 . 2 . 22 . 2 - 4 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 2 . 22 . 2 - 4 . osg34 . el7  gratia - probe - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - common - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - condor - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - metric - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - services - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - sge - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 20 . 1 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 20 . 1 - 1 . osg34 . el7  htcondor - ce - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - client - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 1 . 2 - 1 . osg34 . el7  htcondor - ce - view - 3 . 1 . 2 - 1 . osg34 . el7  osg - ca - certs - updater - 1 . 8 - 1 . osg34 . el7  osg - ca - scripts - 1 . 2 . 3 - 1 . osg34 . el7  osg - configure - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - bosco - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - ce - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - condor - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - gateway - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - gip - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - gratia - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - lsf - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - misc - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - pbs - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - rsv - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - sge - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - siteinfo - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - slurm - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - squid - 2 . 3 . 0 - 1 . osg34 . el7  osg - configure - tests - 2 . 3 . 0 - 1 . osg34 . el7  osg - test - 2 . 2 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 2 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 12 - 1 . osg34 . el7  python2 - xrootd - 4 . 8 . 3 - 1 . osg34 . el7  python3 - xrootd - 4 . 8 . 3 - 1 . osg34 . el7  rsv - 3 . 18 . 0 - 1 . osg34 . el7  rsv - consumers - 3 . 18 . 0 - 1 . osg34 . el7  rsv - core - 3 . 18 . 0 - 1 . osg34 . el7  rsv - metrics - 3 . 18 . 0 - 1 . osg34 . el7  xrootd - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - client - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - client - devel - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - client - libs - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - devel - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - doc - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - fuse - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - private - devel - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - selinux - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - server - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - server - devel - 4 . 8 . 3 - 1 . osg34 . el7  xrootd - server - libs - 4 . 8 . 3 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-6_2", 
            "text": "glideinwms-3.3.3-3.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-7_2", 
            "text": "glideinwms-3.3.3-3.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-6_3", 
            "text": "glideinwms - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - common - tools - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - condor - common - config - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - factory - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - factory - condor - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - glidecondor - tools - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - libs - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - minimal - condor - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - usercollector - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - userschedd - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - vofrontend - 3 . 3 . 3 - 3 . osgup . el6  glideinwms - vofrontend - standalone - 3 . 3 . 3 - 3 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-12/#enterprise-linux-7_3", 
            "text": "glideinwms - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - common - tools - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - condor - common - config - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - factory - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - factory - condor - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - glidecondor - tools - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - libs - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - minimal - condor - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - usercollector - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - userschedd - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - vofrontend - 3 . 3 . 3 - 3 . osgup . el7  glideinwms - vofrontend - standalone - 3 . 3 . 3 - 3 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/", 
            "text": "OSG Software Release 3.4.11\n\n\nRelease Date\n: 2018-05-01\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nSingularity 2.5.0\n\n\nThis release includes fixes for several high and medium severity security issues.\n  It also contains a number of bug fixes including the much awaited docker aufs whiteout file fix.\n  It's a new release instead of a point release because it adds a new dependency to handle this bug,\n  includes some new (albeit minor) feature enhancements, and changes the behavior of a few environment variables.\n\n\nSingularity 2.5 should be installed immediately and all previous versions of Singularity should be removed.\n  Many of the vulnerabilities fixed in this release are expected to affect all Linux distributions\n  regardless of whether they implement overlayfs.\n  There are no mitigations or workarounds for these issues outside of updating Singularity.\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore (\n_\n). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nosg-version-3.4.11-1.osg34.el6\n\n\nsingularity-2.5.0-1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nosg-version-3.4.11-1.osg34.el7\n\n\nsingularity-2.5.0-1.1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\nversion\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n11\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n11\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n5\n.\n0\n-\n1\n.\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.11"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#osg-software-release-3411", 
            "text": "Release Date : 2018-05-01", 
            "title": "OSG Software Release 3.4.11"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#summary-of-changes", 
            "text": "This release contains:   Singularity 2.5.0  This release includes fixes for several high and medium severity security issues.\n  It also contains a number of bug fixes including the much awaited docker aufs whiteout file fix.\n  It's a new release instead of a point release because it adds a new dependency to handle this bug,\n  includes some new (albeit minor) feature enhancements, and changes the behavior of a few environment variables.  Singularity 2.5 should be installed immediately and all previous versions of Singularity should be removed.\n  Many of the vulnerabilities fixed in this release are expected to affect all Linux distributions\n  regardless of whether they implement overlayfs.\n  There are no mitigations or workarounds for these issues outside of updating Singularity.     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ( _ ). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#known-issues", 
            "text": "", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#enterprise-linux-6", 
            "text": "osg-version-3.4.11-1.osg34.el6  singularity-2.5.0-1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#enterprise-linux-7", 
            "text": "osg-version-3.4.11-1.osg34.el7  singularity-2.5.0-1.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - version   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#enterprise-linux-6_1", 
            "text": "osg - version - 3 . 4 . 11 - 1 . osg34 . el6  singularity - 2 . 5 . 0 - 1 . 1 . osg34 . el6  singularity - debuginfo - 2 . 5 . 0 - 1 . 1 . osg34 . el6  singularity - devel - 2 . 5 . 0 - 1 . 1 . osg34 . el6  singularity - runtime - 2 . 5 . 0 - 1 . 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-11/#enterprise-linux-7_1", 
            "text": "osg - version - 3 . 4 . 11 - 1 . osg34 . el7  singularity - 2 . 5 . 0 - 1 . 1 . osg34 . el7  singularity - debuginfo - 2 . 5 . 0 - 1 . 1 . osg34 . el7  singularity - devel - 2 . 5 . 0 - 1 . 1 . osg34 . el7  singularity - runtime - 2 . 5 . 0 - 1 . 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/", 
            "text": "OSG Software Release 3.4.10\n\n\nRelease Date\n: 2018-04-18\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\n\n\nSingularity 2.4.6\n\n\n\n\nCMS Sites\n\n\nIf you support the CMS VO, do not update Singularity until CMS announces that it is safe to do so\n\n\n\n\n\n\nAddresses a high severity security issue with bind mounts on hosts using overlayfs\n\n\n\n\n/tmp\n and \n/var/tmp\n are automatically scratch-mounted in containers started with the \n--contain\n option.\n    If you are invoking singularity with \n--scratch /tmp --scratch /var/tmp --contain\n,\n    this is redundant and will result in the following error:\n\n\nERROR\n  \n:\n \nNot\n \nmounting\n \nrequested\n \nscratch\n \ndirectory\n \n(\nalready\n \nmounted\n \nin\n \ncontainer\n):\n \n/\ntmp\n\n\nABORT\n  \n:\n \nRetval\n \n=\n \n255\n\n\n\n\n\n\nTo fix this, drop any \n--scratch /tmp\n and/or \n--scratch /var/tmp\n options.\n\n\n\n\n\n\n\n\n\n\nHTCondor-CE 3.1.1\n: now accepts InCommon certificates\n\n\n\n\nHTCondor 8.6.10\n: fixed handling of grid jobs when submit fails and other fixes\n\n\ncigetcert 1.16\n: first release in the OSG Software Stack\n\n\nBLAHP 1.18.36\n\n\nIf \nqsub\n fails, the BLAHP now honors the \nblah_debug_save_submit_info\n setting\n\n\nThe BLAHP now checks that input files are present before submitting to the batch system.\n\n\n\n\n\n\nxrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS\n\n\nfrontier-squid: fixed startup problem under SELinux\n\n\nosg-configure 2.2.4:\n\n\nUpdate comments in storage section to identify that value to be used for OASIS\n\n\nProperly handle exceptions when using illegal characters in configuration files\n\n\nosg-configure no longer requires all site information for a GridFTP server\n\n\nWarns when the deprecated \nsite_name\n attribute is used\n\n\nNo longer creates and \n/etc/condor-ce\n directory on an SE or standalone GridFTP or XRootD node\n\n\nValidates properly formed environment variables in the local settings\n\n\n\n\n\n\nUpcoming:\n\n\nHTCondor 8.7.7\n\n\nxrootd-hdfs 2.0.2: Improved write support\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore (\n_\n). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nSingularity 2.4.6 has \nexhibited slow startup times\n\n    on systems with a high number of maximum open file descriptors.\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nI2util-1.6-1.osg34.el6\n\n\nblahp-1.18.36.bosco-1.osg34.el6\n\n\nbwctl-1.6.6-1.osg34.el6\n\n\ncigetcert-1.16-2.osg34.el6\n\n\ncondor-8.6.10-1.osg34.el6\n\n\nfrontier-squid-3.5.27-3.1.1.osg34.el6\n\n\nhtcondor-ce-3.1.1-1.osg34.el6\n\n\nosg-build-1.12.2-1.osg34.el6\n\n\nosg-configure-2.2.4-1.osg34.el6\n\n\nosg-version-3.4.10-1.osg34.el6\n\n\nowamp-3.5.4-1.osg34.el6\n\n\nsingularity-2.4.6-1.osg34.el6\n\n\nxrootd-lcmaps-1.2.1-3.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nI2util-1.6-1.osg34.el7\n\n\nblahp-1.18.36.bosco-1.osg34.el7\n\n\nbwctl-1.6.6-1.osg34.el7\n\n\ncigetcert-1.16-2.osg34.el7\n\n\ncondor-8.6.10-1.osg34.el7\n\n\nfrontier-squid-3.5.27-3.1.1.osg34.el7\n\n\nhtcondor-ce-3.1.1-1.osg34.el7\n\n\nosg-build-1.12.2-1.osg34.el7\n\n\nosg-configure-2.2.4-1.osg34.el7\n\n\nosg-version-3.4.10-1.osg34.el7\n\n\nowamp-3.5.4-1.osg34.el7\n\n\nsingularity-2.4.6-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \nbwctl\n \nbwctl\n-\nclient\n \nbwctl\n-\ndebuginfo\n \nbwctl\n-\ndevel\n \nbwctl\n-\nserver\n \ncigetcert\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nI2util\n \nI2util\n-\ndebuginfo\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nsiteinfo\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\nversion\n \nowamp\n \nowamp\n-\nclient\n \nowamp\n-\ndebuginfo\n \nowamp\n-\nserver\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nbwctl\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nclient\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nserver\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncigetcert\n-\n1\n.\n16\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nI2util\n-\n1\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nI2util\n-\ndebuginfo\n-\n1\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n10\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\nclient\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\nserver\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n2\n.\n1\n-\n3\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n2\n.\n1\n-\n3\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nbwctl\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nclient\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nserver\n-\n1\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncigetcert\n-\n1\n.\n16\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nI2util\n-\n1\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nI2util\n-\ndebuginfo\n-\n1\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n12\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n10\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\nclient\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\nserver\n-\n3\n.\n5\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.36.bosco-1.osgup.el6\n\n\ncondor-8.7.7-1.osgup.el6\n\n\nosg-gridftp-3.4-8.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.36.bosco-1.osgup.el7\n\n\ncondor-8.7.7-1.osgup.el7\n\n\nosg-gridftp-3.4-8.osgup.el7\n\n\nosg-se-hadoop-3.4-5.osgup.el7\n\n\nxrootd-hdfs-2.0.2-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nhdfs\n \nosg\n-\ngridftp\n-\nxrootd\n \nosg\n-\nse\n-\nhadoop\n \nosg\n-\nse\n-\nhadoop\n-\nclient\n \nosg\n-\nse\n-\nhadoop\n-\ndatanode\n \nosg\n-\nse\n-\nhadoop\n-\ngridftp\n \nosg\n-\nse\n-\nhadoop\n-\nnamenode\n \nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n \nxrootd\n-\nhdfs\n \nxrootd\n-\nhdfs\n-\ndebuginfo\n \nxrootd\n-\nhdfs\n-\ndevel\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n36\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n7\n-\n1\n.\nosgup\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel7\n\n\nosg\n-\ngridftp\n-\nhdfs\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel7\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n8\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nclient\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ndatanode\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ngridftp\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nnamenode\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n-\n3\n.\n4\n-\n5\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\n2\n.\n0\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndebuginfo\n-\n2\n.\n0\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndevel\n-\n2\n.\n0\n.\n2\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.10"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#osg-software-release-3410", 
            "text": "Release Date : 2018-04-18", 
            "title": "OSG Software Release 3.4.10"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#summary-of-changes", 
            "text": "This release contains:    Singularity 2.4.6   CMS Sites  If you support the CMS VO, do not update Singularity until CMS announces that it is safe to do so    Addresses a high severity security issue with bind mounts on hosts using overlayfs   /tmp  and  /var/tmp  are automatically scratch-mounted in containers started with the  --contain  option.\n    If you are invoking singularity with  --scratch /tmp --scratch /var/tmp --contain ,\n    this is redundant and will result in the following error:  ERROR    :   Not   mounting   requested   scratch   directory   ( already   mounted   in   container ):   / tmp  ABORT    :   Retval   =   255   To fix this, drop any  --scratch /tmp  and/or  --scratch /var/tmp  options.      HTCondor-CE 3.1.1 : now accepts InCommon certificates   HTCondor 8.6.10 : fixed handling of grid jobs when submit fails and other fixes  cigetcert 1.16 : first release in the OSG Software Stack  BLAHP 1.18.36  If  qsub  fails, the BLAHP now honors the  blah_debug_save_submit_info  setting  The BLAHP now checks that input files are present before submitting to the batch system.    xrootd-lcmaps 1.2.1-3: fixed crashes on Enterprise Linux 6 when request were made using HTTPS  frontier-squid: fixed startup problem under SELinux  osg-configure 2.2.4:  Update comments in storage section to identify that value to be used for OASIS  Properly handle exceptions when using illegal characters in configuration files  osg-configure no longer requires all site information for a GridFTP server  Warns when the deprecated  site_name  attribute is used  No longer creates and  /etc/condor-ce  directory on an SE or standalone GridFTP or XRootD node  Validates properly formed environment variables in the local settings    Upcoming:  HTCondor 8.7.7  xrootd-hdfs 2.0.2: Improved write support     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ( _ ). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#known-issues", 
            "text": "Singularity 2.4.6 has  exhibited slow startup times \n    on systems with a high number of maximum open file descriptors.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-6", 
            "text": "I2util-1.6-1.osg34.el6  blahp-1.18.36.bosco-1.osg34.el6  bwctl-1.6.6-1.osg34.el6  cigetcert-1.16-2.osg34.el6  condor-8.6.10-1.osg34.el6  frontier-squid-3.5.27-3.1.1.osg34.el6  htcondor-ce-3.1.1-1.osg34.el6  osg-build-1.12.2-1.osg34.el6  osg-configure-2.2.4-1.osg34.el6  osg-version-3.4.10-1.osg34.el6  owamp-3.5.4-1.osg34.el6  singularity-2.4.6-1.osg34.el6  xrootd-lcmaps-1.2.1-3.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-7", 
            "text": "I2util-1.6-1.osg34.el7  blahp-1.18.36.bosco-1.osg34.el7  bwctl-1.6.6-1.osg34.el7  cigetcert-1.16-2.osg34.el7  condor-8.6.10-1.osg34.el7  frontier-squid-3.5.27-3.1.1.osg34.el7  htcondor-ce-3.1.1-1.osg34.el7  osg-build-1.12.2-1.osg34.el7  osg-configure-2.2.4-1.osg34.el7  osg-version-3.4.10-1.osg34.el7  owamp-3.5.4-1.osg34.el7  singularity-2.4.6-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   bwctl   bwctl - client   bwctl - debuginfo   bwctl - devel   bwctl - server   cigetcert   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   frontier - squid   frontier - squid - debuginfo   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   I2util   I2util - debuginfo   igtf - ca - certs   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - certs   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - misc   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - siteinfo   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - gums - config   osg - version   owamp   owamp - client   owamp - debuginfo   owamp - server   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   xrootd - lcmaps   xrootd - lcmaps - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 36 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 36 . bosco - 1 . osg34 . el6  bwctl - 1 . 6 . 6 - 1 . osg34 . el6  bwctl - client - 1 . 6 . 6 - 1 . osg34 . el6  bwctl - debuginfo - 1 . 6 . 6 - 1 . osg34 . el6  bwctl - devel - 1 . 6 . 6 - 1 . osg34 . el6  bwctl - server - 1 . 6 . 6 - 1 . osg34 . el6  cigetcert - 1 . 16 - 2 . osg34 . el6  condor - 8 . 6 . 10 - 1 . osg34 . el6  condor - all - 8 . 6 . 10 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 10 - 1 . osg34 . el6  condor - classads - 8 . 6 . 10 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 10 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 10 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 10 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 10 - 1 . osg34 . el6  condor - procd - 8 . 6 . 10 - 1 . osg34 . el6  condor - python - 8 . 6 . 10 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 10 - 1 . osg34 . el6  condor - test - 8 . 6 . 10 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 10 - 1 . osg34 . el6  frontier - squid - 3 . 5 . 27 - 3 . 1 . 1 . osg34 . el6  frontier - squid - debuginfo - 3 . 5 . 27 - 3 . 1 . 1 . osg34 . el6  htcondor - ce - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - client - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 1 . 1 - 1 . osg34 . el6  htcondor - ce - view - 3 . 1 . 1 - 1 . osg34 . el6  I2util - 1 . 6 - 1 . osg34 . el6  I2util - debuginfo - 1 . 6 - 1 . osg34 . el6  osg - build - 1 . 12 . 2 - 1 . osg34 . el6  osg - build - base - 1 . 12 . 2 - 1 . osg34 . el6  osg - build - koji - 1 . 12 . 2 - 1 . osg34 . el6  osg - build - mock - 1 . 12 . 2 - 1 . osg34 . el6  osg - build - tests - 1 . 12 . 2 - 1 . osg34 . el6  osg - configure - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - bosco - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - ce - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - condor - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - gateway - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - gip - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - gratia - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - lsf - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - misc - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - pbs - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - rsv - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - sge - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - siteinfo - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - slurm - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - squid - 2 . 2 . 4 - 1 . osg34 . el6  osg - configure - tests - 2 . 2 . 4 - 1 . osg34 . el6  osg - version - 3 . 4 . 10 - 1 . osg34 . el6  owamp - 3 . 5 . 4 - 1 . osg34 . el6  owamp - client - 3 . 5 . 4 - 1 . osg34 . el6  owamp - debuginfo - 3 . 5 . 4 - 1 . osg34 . el6  owamp - server - 3 . 5 . 4 - 1 . osg34 . el6  singularity - 2 . 4 . 6 - 1 . osg34 . el6  singularity - debuginfo - 2 . 4 . 6 - 1 . osg34 . el6  singularity - devel - 2 . 4 . 6 - 1 . osg34 . el6  singularity - runtime - 2 . 4 . 6 - 1 . osg34 . el6  xrootd - lcmaps - 1 . 2 . 1 - 3 . osg34 . el6  xrootd - lcmaps - debuginfo - 1 . 2 . 1 - 3 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 36 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 36 . bosco - 1 . osg34 . el7  bwctl - 1 . 6 . 6 - 1 . osg34 . el7  bwctl - client - 1 . 6 . 6 - 1 . osg34 . el7  bwctl - debuginfo - 1 . 6 . 6 - 1 . osg34 . el7  bwctl - devel - 1 . 6 . 6 - 1 . osg34 . el7  bwctl - server - 1 . 6 . 6 - 1 . osg34 . el7  cigetcert - 1 . 16 - 2 . osg34 . el7  condor - 8 . 6 . 10 - 1 . osg34 . el7  condor - all - 8 . 6 . 10 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 10 - 1 . osg34 . el7  condor - classads - 8 . 6 . 10 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 10 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 10 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 10 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 10 - 1 . osg34 . el7  condor - procd - 8 . 6 . 10 - 1 . osg34 . el7  condor - python - 8 . 6 . 10 - 1 . osg34 . el7  condor - test - 8 . 6 . 10 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 10 - 1 . osg34 . el7  frontier - squid - 3 . 5 . 27 - 3 . 1 . 1 . osg34 . el7  frontier - squid - debuginfo - 3 . 5 . 27 - 3 . 1 . 1 . osg34 . el7  htcondor - ce - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - client - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 1 . 1 - 1 . osg34 . el7  htcondor - ce - view - 3 . 1 . 1 - 1 . osg34 . el7  I2util - 1 . 6 - 1 . osg34 . el7  I2util - debuginfo - 1 . 6 - 1 . osg34 . el7  osg - build - 1 . 12 . 2 - 1 . osg34 . el7  osg - build - base - 1 . 12 . 2 - 1 . osg34 . el7  osg - build - koji - 1 . 12 . 2 - 1 . osg34 . el7  osg - build - mock - 1 . 12 . 2 - 1 . osg34 . el7  osg - build - tests - 1 . 12 . 2 - 1 . osg34 . el7  osg - configure - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - bosco - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - ce - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - condor - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - gateway - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - gip - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - gratia - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - lsf - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - misc - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - pbs - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - rsv - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - sge - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - siteinfo - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - slurm - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - squid - 2 . 2 . 4 - 1 . osg34 . el7  osg - configure - tests - 2 . 2 . 4 - 1 . osg34 . el7  osg - version - 3 . 4 . 10 - 1 . osg34 . el7  owamp - 3 . 5 . 4 - 1 . osg34 . el7  owamp - client - 3 . 5 . 4 - 1 . osg34 . el7  owamp - debuginfo - 3 . 5 . 4 - 1 . osg34 . el7  owamp - server - 3 . 5 . 4 - 1 . osg34 . el7  singularity - 2 . 4 . 6 - 1 . osg34 . el7  singularity - debuginfo - 2 . 4 . 6 - 1 . osg34 . el7  singularity - devel - 2 . 4 . 6 - 1 . osg34 . el7  singularity - runtime - 2 . 4 . 6 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-6_2", 
            "text": "blahp-1.18.36.bosco-1.osgup.el6  condor-8.7.7-1.osgup.el6  osg-gridftp-3.4-8.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-7_2", 
            "text": "blahp-1.18.36.bosco-1.osgup.el7  condor-8.7.7-1.osgup.el7  osg-gridftp-3.4-8.osgup.el7  osg-se-hadoop-3.4-5.osgup.el7  xrootd-hdfs-2.0.2-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   osg - gridftp   osg - gridftp - hdfs   osg - gridftp - xrootd   osg - se - hadoop   osg - se - hadoop - client   osg - se - hadoop - datanode   osg - se - hadoop - gridftp   osg - se - hadoop - namenode   osg - se - hadoop - secondarynamenode   xrootd - hdfs   xrootd - hdfs - debuginfo   xrootd - hdfs - devel   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 36 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 36 . bosco - 1 . osgup . el6  condor - 8 . 7 . 7 - 1 . osgup . el6  condor - all - 8 . 7 . 7 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 7 - 1 . osgup . el6  condor - bosco - 8 . 7 . 7 - 1 . osgup . el6  condor - classads - 8 . 7 . 7 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 7 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 7 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 7 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 7 - 1 . osgup . el6  condor - procd - 8 . 7 . 7 - 1 . osgup . el6  condor - python - 8 . 7 . 7 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 7 - 1 . osgup . el6  condor - test - 8 . 7 . 7 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 7 - 1 . osgup . el6  osg - gridftp - 3 . 4 - 8 . osgup . el6  osg - gridftp - xrootd - 3 . 4 - 8 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-10/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 36 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 36 . bosco - 1 . osgup . el7  condor - 8 . 7 . 7 - 1 . osgup . el7  condor - all - 8 . 7 . 7 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 7 - 1 . osgup . el7  condor - bosco - 8 . 7 . 7 - 1 . osgup . el7  condor - classads - 8 . 7 . 7 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 7 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 7 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 7 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 7 - 1 . osgup . el7  condor - procd - 8 . 7 . 7 - 1 . osgup . el7  condor - python - 8 . 7 . 7 - 1 . osgup . el7  condor - test - 8 . 7 . 7 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 7 - 1 . osgup . el7  osg - gridftp - 3 . 4 - 8 . osgup . el7  osg - gridftp - hdfs - 3 . 4 - 8 . osgup . el7  osg - gridftp - xrootd - 3 . 4 - 8 . osgup . el7  osg - se - hadoop - 3 . 4 - 5 . osgup . el7  osg - se - hadoop - client - 3 . 4 - 5 . osgup . el7  osg - se - hadoop - datanode - 3 . 4 - 5 . osgup . el7  osg - se - hadoop - gridftp - 3 . 4 - 5 . osgup . el7  osg - se - hadoop - namenode - 3 . 4 - 5 . osgup . el7  osg - se - hadoop - secondarynamenode - 3 . 4 - 5 . osgup . el7  xrootd - hdfs - 2 . 0 . 2 - 1 . osgup . el7  xrootd - hdfs - debuginfo - 2 . 0 . 2 - 1 . osgup . el7  xrootd - hdfs - devel - 2 . 0 . 2 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.9-2\n\n\nRelease Date\n: 2018-04-05\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.90\n\n\nAdded new Grid-FR hierarchy for Renater (AC-GRID-FR series) (FR)\n\n\nAdded new GARUDAINDIA2 root for key roll-over IGCA (IN)\n\n\nUpdated contact metadata for UNAM trust anchors (MX)\n\n\n\n\n\n\nVO Package v78\n\n\nUpdate ATLAS default mappings\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.90-1.osg34.el6\n\n\nosg-ca-certs-1.70-1.osg34.el6\n\n\nvo-client-78-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.90-1.osg34.el7\n\n\nosg-ca-certs-1.70-1.osg34.el7\n\n\nvo-client-78-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n90\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n70\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n78\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n78\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n78\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n78\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n90\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n70\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n78\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n78\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n78\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n78\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.9-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#osg-software-stack-data-release-349-2", 
            "text": "Release Date : 2018-04-05", 
            "title": "OSG Software Stack -- Data Release -- 3.4.9-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.90  Added new Grid-FR hierarchy for Renater (AC-GRID-FR series) (FR)  Added new GARUDAINDIA2 root for key roll-over IGCA (IN)  Updated contact metadata for UNAM trust anchors (MX)    VO Package v78  Update ATLAS default mappings     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.90-1.osg34.el6  osg-ca-certs-1.70-1.osg34.el6  vo-client-78-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.90-1.osg34.el7  osg-ca-certs-1.70-1.osg34.el7  vo-client-78-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 90 - 1 . osg34 . el6  osg - ca - certs - 1 . 70 - 1 . osg34 . el6  osg - gums - config - 78 - 1 . osg34 . el6  vo - client - 78 - 1 . osg34 . el6  vo - client - edgmkgridmap - 78 - 1 . osg34 . el6  vo - client - lcmaps - voms - 78 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-9-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 90 - 1 . osg34 . el7  osg - ca - certs - 1 . 70 - 1 . osg34 . el7  osg - gums - config - 78 - 1 . osg34 . el7  vo - client - 78 - 1 . osg34 . el7  vo - client - edgmkgridmap - 78 - 1 . osg34 . el7  vo - client - lcmaps - voms - 78 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/", 
            "text": "OSG Software Release 3.4.9\n\n\nRelease Date\n: 2018-03-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nXRootD 4.8.1\n: Several important bug fixes\n\n\nGlideinWMS 3.2.21\n\n\nEnhanced Singularity support\n\n\nAutomatic proxy renewal\n\n\n\n\n\n\nFrontier Squid 3.5.27-3\n\n\nHonor \"Pragma: no-cache\" directive\n\n\nIncluded patches from recent Squid security advisories\n\n\n\n\n\n\nRSV 3.17.0\n\n\nEnhanced security by verifying certificates with SHA256\n\n\nUse GRACC (rather than the retired Gratia service) to verify the last upload time of job records\n\n\nAdded CREAM and nordugrid support back\n\n\nChange the Java probe to not emit the pipe character which is a problem for the Gratia service\n\n\n\n\n\n\nosg-release: Use HTTPS protocol for OSG repositories\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore (\n_\n). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nNone.\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nfrontier-squid-3.5.27-3.1.osg34.el6\n\n\nglideinwms-3.2.21-2.osg34.el6\n\n\nosg-gridftp-3.4-7.osg34.el6\n\n\nosg-release-3.4-4.osg34.el6\n\n\nosg-release-itb-3.4-4.osg34.el6\n\n\nosg-test-2.1.0-1.osg34.el6\n\n\nosg-version-3.4.9-1.osg34.el6\n\n\nrsv-3.17.0-1.osg34.el6\n\n\nxrootd-4.8.1-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nfrontier-squid-3.5.27-3.1.osg34.el7\n\n\nglideinwms-3.2.21-2.osg34.el7\n\n\nosg-gridftp-3.4-7.osg34.el7\n\n\nosg-release-3.4-4.osg34.el7\n\n\nosg-release-itb-3.4-4.osg34.el7\n\n\nosg-test-2.1.0-1.osg34.el7\n\n\nosg-version-3.4.9-1.osg34.el7\n\n\nrsv-3.17.0-1.osg34.el7\n\n\nxrootd-4.8.1-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nxrootd\n \nosg\n-\nrelease\n \nosg\n-\nrelease\n-\nitb\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\nitb\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n9\n-\n1\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n3\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n21\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\nitb\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n9\n-\n1\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n17\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 7\n\n\n\n\nosg-gridftp-hdfs-3.4-2.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\ngridftp\n-\nhdfs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 7\n\n\nosg\n-\ngridftp\n-\nhdfs\n-\n3\n.\n4\n-\n2\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.9"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#osg-software-release-349", 
            "text": "Release Date : 2018-03-08", 
            "title": "OSG Software Release 3.4.9"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#summary-of-changes", 
            "text": "This release contains:   XRootD 4.8.1 : Several important bug fixes  GlideinWMS 3.2.21  Enhanced Singularity support  Automatic proxy renewal    Frontier Squid 3.5.27-3  Honor \"Pragma: no-cache\" directive  Included patches from recent Squid security advisories    RSV 3.17.0  Enhanced security by verifying certificates with SHA256  Use GRACC (rather than the retired Gratia service) to verify the last upload time of job records  Added CREAM and nordugrid support back  Change the Java probe to not emit the pipe character which is a problem for the Gratia service    osg-release: Use HTTPS protocol for OSG repositories   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ( _ ). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#known-issues", 
            "text": "None.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-6", 
            "text": "frontier-squid-3.5.27-3.1.osg34.el6  glideinwms-3.2.21-2.osg34.el6  osg-gridftp-3.4-7.osg34.el6  osg-release-3.4-4.osg34.el6  osg-release-itb-3.4-4.osg34.el6  osg-test-2.1.0-1.osg34.el6  osg-version-3.4.9-1.osg34.el6  rsv-3.17.0-1.osg34.el6  xrootd-4.8.1-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-7", 
            "text": "frontier-squid-3.5.27-3.1.osg34.el7  glideinwms-3.2.21-2.osg34.el7  osg-gridftp-3.4-7.osg34.el7  osg-release-3.4-4.osg34.el7  osg-release-itb-3.4-4.osg34.el7  osg-test-2.1.0-1.osg34.el7  osg-version-3.4.9-1.osg34.el7  rsv-3.17.0-1.osg34.el7  xrootd-4.8.1-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  frontier - squid   frontier - squid - debuginfo   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   osg - gridftp   osg - gridftp - xrootd   osg - release   osg - release - itb   osg - test   osg - test - log - viewer   osg - version   python2 - xrootd   python3 - xrootd   rsv   rsv - consumers   rsv - core   rsv - metrics   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-6_1", 
            "text": "frontier - squid - 3 . 5 . 27 - 3 . 1 . osg34 . el6  frontier - squid - debuginfo - 3 . 5 . 27 - 3 . 1 . osg34 . el6  glideinwms - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - common - tools - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - condor - common - config - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - factory - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - factory - condor - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - libs - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - minimal - condor - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - usercollector - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - userschedd - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - vofrontend - 3 . 2 . 21 - 2 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 2 . 21 - 2 . osg34 . el6  osg - gridftp - 3 . 4 - 7 . osg34 . el6  osg - gridftp - xrootd - 3 . 4 - 7 . osg34 . el6  osg - release - 3 . 4 - 4 . osg34 . el6  osg - release - itb - 3 . 4 - 4 . osg34 . el6  osg - test - 2 . 1 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 1 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 9 - 1 . osg34 . el6  python2 - xrootd - 4 . 8 . 1 - 1 . osg34 . el6  python3 - xrootd - 4 . 8 . 1 - 1 . osg34 . el6  rsv - 3 . 17 . 0 - 1 . osg34 . el6  rsv - consumers - 3 . 17 . 0 - 1 . osg34 . el6  rsv - core - 3 . 17 . 0 - 1 . osg34 . el6  rsv - metrics - 3 . 17 . 0 - 1 . osg34 . el6  xrootd - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - client - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - client - devel - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - client - libs - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - devel - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - doc - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - fuse - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - libs - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - private - devel - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - selinux - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - server - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - server - devel - 4 . 8 . 1 - 1 . osg34 . el6  xrootd - server - libs - 4 . 8 . 1 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-7_1", 
            "text": "frontier - squid - 3 . 5 . 27 - 3 . 1 . osg34 . el7  frontier - squid - debuginfo - 3 . 5 . 27 - 3 . 1 . osg34 . el7  glideinwms - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - common - tools - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - condor - common - config - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - factory - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - factory - condor - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - libs - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - minimal - condor - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - usercollector - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - userschedd - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - vofrontend - 3 . 2 . 21 - 2 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 2 . 21 - 2 . osg34 . el7  osg - gridftp - 3 . 4 - 7 . osg34 . el7  osg - gridftp - xrootd - 3 . 4 - 7 . osg34 . el7  osg - release - 3 . 4 - 4 . osg34 . el7  osg - release - itb - 3 . 4 - 4 . osg34 . el7  osg - test - 2 . 1 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 1 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 9 - 1 . osg34 . el7  python2 - xrootd - 4 . 8 . 1 - 1 . osg34 . el7  python3 - xrootd - 4 . 8 . 1 - 1 . osg34 . el7  rsv - 3 . 17 . 0 - 1 . osg34 . el7  rsv - consumers - 3 . 17 . 0 - 1 . osg34 . el7  rsv - core - 3 . 17 . 0 - 1 . osg34 . el7  rsv - metrics - 3 . 17 . 0 - 1 . osg34 . el7  xrootd - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - client - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - client - devel - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - client - libs - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - devel - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - doc - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - fuse - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - private - devel - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - selinux - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - server - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - server - devel - 4 . 8 . 1 - 1 . osg34 . el7  xrootd - server - libs - 4 . 8 . 1 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-7_2", 
            "text": "osg-gridftp-hdfs-3.4-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - gridftp - hdfs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-9/#enterprise-linux-7_3", 
            "text": "osg - gridftp - hdfs - 3 . 4 - 2 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/", 
            "text": "OSG Software Release 3.4.8\n\n\nRelease Date\n: 2018-02-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCritical GlideinWMS bug fix for heavily loaded frontends (\nhttps://cdcvs.fnal.gov/redmine/issues/18748\n)\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore (\n_\n). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.2.20-2.osg34.el6\n\n\nosg-version-3.4.8-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.2.20-2.osg34.el7\n\n\nosg-version-3.4.8-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nosg\n-\nversion\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n20\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n8\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.8"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#osg-software-release-348", 
            "text": "Release Date : 2018-02-08", 
            "title": "OSG Software Release 3.4.8"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#summary-of-changes", 
            "text": "This release contains:   Critical GlideinWMS bug fix for heavily loaded frontends ( https://cdcvs.fnal.gov/redmine/issues/18748 )   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ( _ ). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#enterprise-linux-6", 
            "text": "glideinwms-3.2.20-2.osg34.el6  osg-version-3.4.8-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#enterprise-linux-7", 
            "text": "glideinwms-3.2.20-2.osg34.el7  osg-version-3.4.8-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   osg - version   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#enterprise-linux-6_1", 
            "text": "glideinwms - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - common - tools - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - condor - common - config - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - factory - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - factory - condor - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - libs - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - minimal - condor - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - usercollector - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - userschedd - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - vofrontend - 3 . 2 . 20 - 2 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 2 . 20 - 2 . osg34 . el6  osg - version - 3 . 4 . 8 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-8/#enterprise-linux-7_1", 
            "text": "glideinwms - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - common - tools - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - condor - common - config - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - factory - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - factory - condor - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - libs - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - minimal - condor - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - usercollector - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - userschedd - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - vofrontend - 3 . 2 . 20 - 2 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 2 . 20 - 2 . osg34 . el7  osg - version - 3 . 4 . 8 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/", 
            "text": "OSG Software Release 3.4.7\n\n\nRelease Date\n: 2018-02-01\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHDFS 2.6 and related tools (GridFTP-HDFS and XRootD-HDFS) are now available in the Upcoming Repository for Enterprise Linux 7\n\n\nSingularity 2.4.2\n: Feature upgrade from Singularity 2.3.2\n\n\nSee \nSingularity 2.4 release notes\n for a list of new features and improvements\n\n\n\n\n\n\nPegasus 4.8.1\n: Feature upgrade from Pegasus 4.7.4\n\n\nSee \nPegasus 4.8.0 release notes\n for a list of new features and improvements\n\n\n\n\n\n\nUpdated gratia-probe to account for GPUs\n\n\nperfSONAR-tools meta-package including updated BWCTL, OWAMP, and nuttcp packages\n\n\nHTCondor 8.6.9\n: Bug fix release\n\n\nfrontier-squid 3.5.27-2.1\n: Bug fix release\n\n\nMinor bug fix for osg-user-cert-renew\n\n\nHTCondor 8.7.6\n in the Upcoming respository\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ('_'). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nI2util-1.2-2.1.osg34.el6\n\n\nbwctl-1.5.2-5.osg34.el6\n\n\ncondor-8.6.9-1.1.osg34.el6\n\n\nfrontier-squid-3.5.27-2.1.osg34.el6\n\n\ngratia-probe-1.19.0-1.osg34.el6\n\n\nnuttcp-8.1.4-1.osg34.el6\n\n\nosg-build-1.11.2-1.osg34.el6\n\n\nosg-ca-generator-1.3.2-1.osg34.el6\n\n\nosg-ce-3.4-4.osg34.el6\n\n\nosg-gridftp-3.4-5.osg34.el6\n\n\nosg-gridftp-xrootd-3.4-2.osg34.el6\n\n\nosg-pki-tools-2.1.4-1.osg34.el6\n\n\nosg-test-2.0.1-1.osg34.el6\n\n\nosg-version-3.4.7-1.osg34.el6\n\n\nosg-wn-client-3.4-4.osg34.el6\n\n\nowamp-3.4-10.osg34.el6\n\n\npegasus-4.8.1-1.2.osg34.el6\n\n\nperfsonar-tools-4.0.1-1.3.osg34.el6\n\n\nsingularity-2.4.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nI2util-1.2-2.1.osg34.el7\n\n\nbwctl-1.5.2-5.osg34.el7\n\n\ncondor-8.6.9-1.1.osg34.el7\n\n\nfrontier-squid-3.5.27-2.1.osg34.el7\n\n\ngratia-probe-1.19.0-1.osg34.el7\n\n\nnuttcp-8.1.4-1.osg34.el7\n\n\nosg-build-1.11.2-1.osg34.el7\n\n\nosg-ca-generator-1.3.2-1.osg34.el7\n\n\nosg-ce-3.4-4.osg34.el7\n\n\nosg-gridftp-3.4-5.osg34.el7\n\n\nosg-gridftp-xrootd-3.4-2.osg34.el7\n\n\nosg-pki-tools-2.1.4-1.osg34.el7\n\n\nosg-test-2.0.1-1.osg34.el7\n\n\nosg-version-3.4.7-1.osg34.el7\n\n\nosg-wn-client-3.4-4.osg34.el7\n\n\nowamp-3.4-10.osg34.el7\n\n\npegasus-4.8.1-1.2.osg34.el7\n\n\nperfsonar-tools-4.0.1-1.3.osg34.el7\n\n\nsingularity-2.4.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nbwctl\n \nbwctl\n-\nclient\n \nbwctl\n-\ndebuginfo\n \nbwctl\n-\ndevel\n \nbwctl\n-\nserver\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglexec\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngram\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nI2util\n \nI2util\n-\ndebuginfo\n \nigtf\n-\nca\n-\ncerts\n \nnuttcp\n \nnuttcp\n-\ndebuginfo\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ngenerator\n \nosg\n-\nce\n \nosg\n-\nce\n-\nbosco\n \nosg\n-\nce\n-\ncondor\n \nosg\n-\nce\n-\nlsf\n \nosg\n-\nce\n-\npbs\n \nosg\n-\nce\n-\nsge\n \nosg\n-\nce\n-\nslurm\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nxrootd\n \nosg\n-\npki\n-\ntools\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nosg\n-\nwn\n-\nclient\n \nowamp\n \nowamp\n-\nclient\n \nowamp\n-\ndebuginfo\n \nowamp\n-\nserver\n \npegasus\n \npegasus\n-\ndebuginfo\n \nperfsonar\n-\ntools\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nbwctl\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nclient\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndevel\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nserver\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nI2util\n-\n1\n.\n2\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nI2util\n-\ndebuginfo\n-\n1\n.\n2\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nnuttcp\n-\n8\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nnuttcp\n-\ndebuginfo\n-\n8\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ngenerator\n-\n1\n.\n3\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nowamp\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel6\n\n\nowamp\n-\nclient\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel6\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel6\n\n\nowamp\n-\nserver\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n8\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nperfsonar\n-\ntools\n-\n4\n.\n0\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nbwctl\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nclient\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndevel\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nserver\n-\n1\n.\n5\n.\n2\n-\n5\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n27\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n27\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n19\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nI2util\n-\n1\n.\n2\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nI2util\n-\ndebuginfo\n-\n1\n.\n2\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nnuttcp\n-\n8\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nnuttcp\n-\ndebuginfo\n-\n8\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ngenerator\n-\n1\n.\n3\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nowamp\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel7\n\n\nowamp\n-\nclient\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel7\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel7\n\n\nowamp\n-\nserver\n-\n3\n.\n4\n-\n10\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n8\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n8\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nperfsonar\n-\ntools\n-\n4\n.\n0\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.7.6-1.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\navro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.1.osgup.el7\n\n\nbigtop-jsvc-0.3.0-1.2.osgup.el7\n\n\nbigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osgup.el7\n\n\ncondor-8.7.6-1.1.osgup.el7\n\n\ngridftp-hdfs-1.1.1-1.1.osgup.el7\n\n\nhadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.7.osgup.el7\n\n\nosg-gridftp-hdfs-3.4-1.osgup.el7\n\n\nosg-se-hadoop-3.4-3.osgup.el7\n\n\nxrootd-hdfs-1.9.2-5.osgup.el7\n\n\nzookeeper-3.4.3+15-1.cdh4.0.1.p0.6.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\navro\n-\ndoc\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosgup\n.\nel7\n\n\navro\n-\nlibs\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosgup\n.\nel7\n\n\navro\n-\ntools\n-\n1\n.\n7\n.\n6\n+\ncdh5\n.\n13\n.\n0\n+\n135\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosgup\n.\nel7\n\n\nbigtop\n-\njsvc\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosgup\n.\nel7\n\n\nbigtop\n-\njsvc\n-\ndebuginfo\n-\n0\n.\n3\n.\n0\n-\n1\n.\n2\n.\nosgup\n.\nel7\n\n\nbigtop\n-\nutils\n-\n0\n.\n7\n.\n0\n+\ncdh5\n.\n13\n.\n0\n+\n0\n-\n1\n.\ncdh5\n.\n13\n.\n0\n.\np0\n.\n34\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n6\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ngridftp\n-\nhdfs\n-\n1\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\ngridftp\n-\nhdfs\n-\ndebuginfo\n-\n1\n.\n1\n.\n1\n-\n1\n.\n1\n.\nosgup\n.\nel7\n\n\nhadoop\n-\n0\n.\n20\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\n0\n.\n20\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nclient\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nconf\n-\npseudo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\ndebuginfo\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\ndoc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\ndatanode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nfuse\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\njournalnode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nnamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nnfs3\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nsecondarynamenode\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhdfs\n-\nzkfc\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nhttpfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nkms\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nkms\n-\nserver\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nlibhdfs\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nlibhdfs\n-\ndevel\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nmapreduce\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nhadoop\n-\nyarn\n-\n2\n.\n6\n.\n0\n+\ncdh5\n.\n12\n.\n1\n+\n2540\n-\n1\n.\ncdh5\n.\n12\n.\n1\n.\np0\n.\n3\n.\n7\n.\nosgup\n.\nel7\n\n\nosg\n-\ngridftp\n-\nhdfs\n-\n3\n.\n4\n-\n1\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nclient\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ndatanode\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\ngridftp\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nnamenode\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nosg\n-\nse\n-\nhadoop\n-\nsecondarynamenode\n-\n3\n.\n4\n-\n3\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\n1\n.\n9\n.\n2\n-\n5\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndebuginfo\n-\n1\n.\n9\n.\n2\n-\n5\n.\nosgup\n.\nel7\n\n\nxrootd\n-\nhdfs\n-\ndevel\n-\n1\n.\n9\n.\n2\n-\n5\n.\nosgup\n.\nel7\n\n\nzookeeper\n-\n3\n.\n4\n.\n3\n+\n15\n-\n1\n.\ncdh4\n.\n0\n.\n1\n.\np0\n.\n6\n.\nosgup\n.\nel7\n\n\nzookeeper\n-\nserver\n-\n3\n.\n4\n.\n3\n+\n15\n-\n1\n.\ncdh4\n.\n0\n.\n1\n.\np0\n.\n6\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#osg-software-release-347", 
            "text": "Release Date : 2018-02-01", 
            "title": "OSG Software Release 3.4.7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#summary-of-changes", 
            "text": "This release contains:   HDFS 2.6 and related tools (GridFTP-HDFS and XRootD-HDFS) are now available in the Upcoming Repository for Enterprise Linux 7  Singularity 2.4.2 : Feature upgrade from Singularity 2.3.2  See  Singularity 2.4 release notes  for a list of new features and improvements    Pegasus 4.8.1 : Feature upgrade from Pegasus 4.7.4  See  Pegasus 4.8.0 release notes  for a list of new features and improvements    Updated gratia-probe to account for GPUs  perfSONAR-tools meta-package including updated BWCTL, OWAMP, and nuttcp packages  HTCondor 8.6.9 : Bug fix release  frontier-squid 3.5.27-2.1 : Bug fix release  Minor bug fix for osg-user-cert-renew  HTCondor 8.7.6  in the Upcoming respository   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ('_'). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-6", 
            "text": "I2util-1.2-2.1.osg34.el6  bwctl-1.5.2-5.osg34.el6  condor-8.6.9-1.1.osg34.el6  frontier-squid-3.5.27-2.1.osg34.el6  gratia-probe-1.19.0-1.osg34.el6  nuttcp-8.1.4-1.osg34.el6  osg-build-1.11.2-1.osg34.el6  osg-ca-generator-1.3.2-1.osg34.el6  osg-ce-3.4-4.osg34.el6  osg-gridftp-3.4-5.osg34.el6  osg-gridftp-xrootd-3.4-2.osg34.el6  osg-pki-tools-2.1.4-1.osg34.el6  osg-test-2.0.1-1.osg34.el6  osg-version-3.4.7-1.osg34.el6  osg-wn-client-3.4-4.osg34.el6  owamp-3.4-10.osg34.el6  pegasus-4.8.1-1.2.osg34.el6  perfsonar-tools-4.0.1-1.3.osg34.el6  singularity-2.4.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-7", 
            "text": "I2util-1.2-2.1.osg34.el7  bwctl-1.5.2-5.osg34.el7  condor-8.6.9-1.1.osg34.el7  frontier-squid-3.5.27-2.1.osg34.el7  gratia-probe-1.19.0-1.osg34.el7  nuttcp-8.1.4-1.osg34.el7  osg-build-1.11.2-1.osg34.el7  osg-ca-generator-1.3.2-1.osg34.el7  osg-ce-3.4-4.osg34.el7  osg-gridftp-3.4-5.osg34.el7  osg-gridftp-xrootd-3.4-2.osg34.el7  osg-pki-tools-2.1.4-1.osg34.el7  osg-test-2.0.1-1.osg34.el7  osg-version-3.4.7-1.osg34.el7  osg-wn-client-3.4-4.osg34.el7  owamp-3.4-10.osg34.el7  pegasus-4.8.1-1.2.osg34.el7  perfsonar-tools-4.0.1-1.3.osg34.el7  singularity-2.4.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  bwctl   bwctl - client   bwctl - debuginfo   bwctl - devel   bwctl - server   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   frontier - squid   frontier - squid - debuginfo   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glexec   gratia - probe - glideinwms   gratia - probe - gram   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   I2util   I2util - debuginfo   igtf - ca - certs   nuttcp   nuttcp - debuginfo   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - certs   osg - ca - generator   osg - ce   osg - ce - bosco   osg - ce - condor   osg - ce - lsf   osg - ce - pbs   osg - ce - sge   osg - ce - slurm   osg - gridftp   osg - gridftp - xrootd   osg - pki - tools   osg - test   osg - test - log - viewer   osg - version   osg - wn - client   owamp   owamp - client   owamp - debuginfo   owamp - server   pegasus   pegasus - debuginfo   perfsonar - tools   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-6_1", 
            "text": "bwctl - 1 . 5 . 2 - 5 . osg34 . el6  bwctl - client - 1 . 5 . 2 - 5 . osg34 . el6  bwctl - debuginfo - 1 . 5 . 2 - 5 . osg34 . el6  bwctl - devel - 1 . 5 . 2 - 5 . osg34 . el6  bwctl - server - 1 . 5 . 2 - 5 . osg34 . el6  condor - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - all - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - bosco - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - classads - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - kbdd - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - procd - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - python - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - std - universe - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - test - 8 . 6 . 9 - 1 . 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 9 - 1 . 1 . osg34 . el6  frontier - squid - 3 . 5 . 27 - 2 . 1 . osg34 . el6  frontier - squid - debuginfo - 3 . 5 . 27 - 2 . 1 . osg34 . el6  gratia - probe - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - common - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - condor - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - glexec - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - gram - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - metric - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - services - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - sge - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 19 . 0 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 19 . 0 - 1 . osg34 . el6  I2util - 1 . 2 - 2 . 1 . osg34 . el6  I2util - debuginfo - 1 . 2 - 2 . 1 . osg34 . el6  nuttcp - 8 . 1 . 4 - 1 . osg34 . el6  nuttcp - debuginfo - 8 . 1 . 4 - 1 . osg34 . el6  osg - build - 1 . 11 . 2 - 1 . osg34 . el6  osg - build - base - 1 . 11 . 2 - 1 . osg34 . el6  osg - build - koji - 1 . 11 . 2 - 1 . osg34 . el6  osg - build - mock - 1 . 11 . 2 - 1 . osg34 . el6  osg - build - tests - 1 . 11 . 2 - 1 . osg34 . el6  osg - ca - generator - 1 . 3 . 2 - 1 . osg34 . el6  osg - ce - 3 . 4 - 4 . osg34 . el6  osg - ce - bosco - 3 . 4 - 4 . osg34 . el6  osg - ce - condor - 3 . 4 - 4 . osg34 . el6  osg - ce - lsf - 3 . 4 - 4 . osg34 . el6  osg - ce - pbs - 3 . 4 - 4 . osg34 . el6  osg - ce - sge - 3 . 4 - 4 . osg34 . el6  osg - ce - slurm - 3 . 4 - 4 . osg34 . el6  osg - gridftp - 3 . 4 - 5 . osg34 . el6  osg - gridftp - xrootd - 3 . 4 - 2 . osg34 . el6  osg - pki - tools - 2 . 1 . 4 - 1 . osg34 . el6  osg - test - 2 . 0 . 1 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 0 . 1 - 1 . osg34 . el6  osg - version - 3 . 4 . 7 - 1 . osg34 . el6  osg - wn - client - 3 . 4 - 4 . osg34 . el6  owamp - 3 . 4 - 10 . osg34 . el6  owamp - client - 3 . 4 - 10 . osg34 . el6  owamp - debuginfo - 3 . 4 - 10 . osg34 . el6  owamp - server - 3 . 4 - 10 . osg34 . el6  pegasus - 4 . 8 . 1 - 1 . 2 . osg34 . el6  pegasus - debuginfo - 4 . 8 . 1 - 1 . 2 . osg34 . el6  perfsonar - tools - 4 . 0 . 1 - 1 . 3 . osg34 . el6  singularity - 2 . 4 . 2 - 1 . osg34 . el6  singularity - debuginfo - 2 . 4 . 2 - 1 . osg34 . el6  singularity - devel - 2 . 4 . 2 - 1 . osg34 . el6  singularity - runtime - 2 . 4 . 2 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-7_1", 
            "text": "bwctl - 1 . 5 . 2 - 5 . osg34 . el7  bwctl - client - 1 . 5 . 2 - 5 . osg34 . el7  bwctl - debuginfo - 1 . 5 . 2 - 5 . osg34 . el7  bwctl - devel - 1 . 5 . 2 - 5 . osg34 . el7  bwctl - server - 1 . 5 . 2 - 5 . osg34 . el7  condor - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - all - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - bosco - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - classads - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - kbdd - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - procd - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - python - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - test - 8 . 6 . 9 - 1 . 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 9 - 1 . 1 . osg34 . el7  frontier - squid - 3 . 5 . 27 - 2 . 1 . osg34 . el7  frontier - squid - debuginfo - 3 . 5 . 27 - 2 . 1 . osg34 . el7  gratia - probe - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - common - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - condor - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - glexec - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - gram - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - metric - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - services - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - sge - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 19 . 0 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 19 . 0 - 1 . osg34 . el7  I2util - 1 . 2 - 2 . 1 . osg34 . el7  I2util - debuginfo - 1 . 2 - 2 . 1 . osg34 . el7  nuttcp - 8 . 1 . 4 - 1 . osg34 . el7  nuttcp - debuginfo - 8 . 1 . 4 - 1 . osg34 . el7  osg - build - 1 . 11 . 2 - 1 . osg34 . el7  osg - build - base - 1 . 11 . 2 - 1 . osg34 . el7  osg - build - koji - 1 . 11 . 2 - 1 . osg34 . el7  osg - build - mock - 1 . 11 . 2 - 1 . osg34 . el7  osg - build - tests - 1 . 11 . 2 - 1 . osg34 . el7  osg - ca - generator - 1 . 3 . 2 - 1 . osg34 . el7  osg - ce - 3 . 4 - 4 . osg34 . el7  osg - ce - bosco - 3 . 4 - 4 . osg34 . el7  osg - ce - condor - 3 . 4 - 4 . osg34 . el7  osg - ce - lsf - 3 . 4 - 4 . osg34 . el7  osg - ce - pbs - 3 . 4 - 4 . osg34 . el7  osg - ce - sge - 3 . 4 - 4 . osg34 . el7  osg - ce - slurm - 3 . 4 - 4 . osg34 . el7  osg - gridftp - 3 . 4 - 5 . osg34 . el7  osg - gridftp - xrootd - 3 . 4 - 2 . osg34 . el7  osg - pki - tools - 2 . 1 . 4 - 1 . osg34 . el7  osg - test - 2 . 0 . 1 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 0 . 1 - 1 . osg34 . el7  osg - version - 3 . 4 . 7 - 1 . osg34 . el7  osg - wn - client - 3 . 4 - 4 . osg34 . el7  owamp - 3 . 4 - 10 . osg34 . el7  owamp - client - 3 . 4 - 10 . osg34 . el7  owamp - debuginfo - 3 . 4 - 10 . osg34 . el7  owamp - server - 3 . 4 - 10 . osg34 . el7  pegasus - 4 . 8 . 1 - 1 . 2 . osg34 . el7  pegasus - debuginfo - 4 . 8 . 1 - 1 . 2 . osg34 . el7  perfsonar - tools - 4 . 0 . 1 - 1 . 3 . osg34 . el7  singularity - 2 . 4 . 2 - 1 . osg34 . el7  singularity - debuginfo - 2 . 4 . 2 - 1 . osg34 . el7  singularity - devel - 2 . 4 . 2 - 1 . osg34 . el7  singularity - runtime - 2 . 4 . 2 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-6_2", 
            "text": "condor-8.7.6-1.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-7_2", 
            "text": "avro-libs-1.7.6+cdh5.13.0+135-1.cdh5.13.0.p0.34.1.osgup.el7  bigtop-jsvc-0.3.0-1.2.osgup.el7  bigtop-utils-0.7.0+cdh5.13.0+0-1.cdh5.13.0.p0.34.1.osgup.el7  condor-8.7.6-1.1.osgup.el7  gridftp-hdfs-1.1.1-1.1.osgup.el7  hadoop-2.6.0+cdh5.12.1+2540-1.cdh5.12.1.p0.3.7.osgup.el7  osg-gridftp-hdfs-3.4-1.osgup.el7  osg-se-hadoop-3.4-3.osgup.el7  xrootd-hdfs-1.9.2-5.osgup.el7  zookeeper-3.4.3+15-1.cdh4.0.1.p0.6.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-6_3", 
            "text": "condor - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - all - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - bosco - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - classads - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - classads - devel - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - debuginfo - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - kbdd - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - procd - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - python - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - std - universe - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - test - 8 . 7 . 6 - 1 . 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 6 - 1 . 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-7/#enterprise-linux-7_3", 
            "text": "avro - doc - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osgup . el7  avro - libs - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osgup . el7  avro - tools - 1 . 7 . 6 + cdh5 . 13 . 0 + 135 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osgup . el7  bigtop - jsvc - 0 . 3 . 0 - 1 . 2 . osgup . el7  bigtop - jsvc - debuginfo - 0 . 3 . 0 - 1 . 2 . osgup . el7  bigtop - utils - 0 . 7 . 0 + cdh5 . 13 . 0 + 0 - 1 . cdh5 . 13 . 0 . p0 . 34 . 1 . osgup . el7  condor - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - all - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - bosco - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - classads - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - classads - devel - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - debuginfo - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - kbdd - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - procd - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - python - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - test - 8 . 7 . 6 - 1 . 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 6 - 1 . 1 . osgup . el7  gridftp - hdfs - 1 . 1 . 1 - 1 . 1 . osgup . el7  gridftp - hdfs - debuginfo - 1 . 1 . 1 - 1 . 1 . osgup . el7  hadoop - 0 . 20 - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - 0 . 20 - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - client - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - conf - pseudo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - debuginfo - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - doc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - datanode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - fuse - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - journalnode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - namenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - nfs3 - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - secondarynamenode - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - hdfs - zkfc - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - httpfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - kms - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - kms - server - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - libhdfs - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - libhdfs - devel - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - mapreduce - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  hadoop - yarn - 2 . 6 . 0 + cdh5 . 12 . 1 + 2540 - 1 . cdh5 . 12 . 1 . p0 . 3 . 7 . osgup . el7  osg - gridftp - hdfs - 3 . 4 - 1 . osgup . el7  osg - se - hadoop - 3 . 4 - 3 . osgup . el7  osg - se - hadoop - client - 3 . 4 - 3 . osgup . el7  osg - se - hadoop - datanode - 3 . 4 - 3 . osgup . el7  osg - se - hadoop - gridftp - 3 . 4 - 3 . osgup . el7  osg - se - hadoop - namenode - 3 . 4 - 3 . osgup . el7  osg - se - hadoop - secondarynamenode - 3 . 4 - 3 . osgup . el7  xrootd - hdfs - 1 . 9 . 2 - 5 . osgup . el7  xrootd - hdfs - debuginfo - 1 . 9 . 2 - 5 . osgup . el7  xrootd - hdfs - devel - 1 . 9 . 2 - 5 . osgup . el7  zookeeper - 3 . 4 . 3 + 15 - 1 . cdh4 . 0 . 1 . p0 . 6 . osgup . el7  zookeeper - server - 3 . 4 . 3 + 15 - 1 . cdh4 . 0 . 1 . p0 . 6 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.6-2\n\n\nRelease Date\n: 2018-01-24\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.89\n\n\nDiscontinued expiring UGRID (2008) root CA (UA)\n\n\n\n\n\n\nSHA256 checksums are provided in addition to the MD5 checksums\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension).\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.89-2.osg34.el6\n\n\nosg-ca-certs-1.69-2.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.89-2.osg34.el7\n\n\nosg-ca-certs-1.69-2.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n89\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n69\n-\n2\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n89\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n69\n-\n2\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.6-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#osg-software-stack-data-release-346-2", 
            "text": "Release Date : 2018-01-24", 
            "title": "OSG Software Stack -- Data Release -- 3.4.6-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.89  Discontinued expiring UGRID (2008) root CA (UA)    SHA256 checksums are provided in addition to the MD5 checksums   These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension).", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.89-2.osg34.el6  osg-ca-certs-1.69-2.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.89-2.osg34.el7  osg-ca-certs-1.69-2.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 89 - 2 . osg34 . el6  osg - ca - certs - 1 . 69 - 2 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 89 - 2 . osg34 . el7  osg - ca - certs - 1 . 69 - 2 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/", 
            "text": "OSG Software Release 3.4.6\n\n\nRelease Date\n: 2017-12-21\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nXRootD 4.8.0\n: fixed caching issues and multiple segfaults\n\n\nCernVM-FS 2.4.4\n: Update from 2.4.2\n\n\nbug fixes for servers and clients\n\n\n\n\n\n\nGlideinWMS 3.2.20\n: improved factory performance, Singularity support\n\n\nosg-pki-tools 2.1.2\n: use HTTPS for all OIM connections, service certificate fix (Update from 2.0.0)\n\n\nosg-wn-client: add gfal2-http plugin\n\n\nHTCondor-CE 3.0.4\n: Update from 3.0.2\n\n\nosg-gridftp: add osg-configure-gratia\n\n\nosg-configure 2.2.3\n: minor bug fixes\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\nOSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ('_'). This new format first appeared in OSG PKI tools version 2.1.2.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.35.bosco-1.osg34.el6\n\n\ncvmfs-2.4.4-1.osg34.el6\n\n\nglideinwms-3.2.20-1.osg34.el6\n\n\nhtcondor-ce-3.0.4-1.osg34.el6\n\n\nosg-ca-certs-updater-1.7-1.osg34.el6\n\n\nosg-ca-scripts-1.2.2-1.osg34.el6\n\n\nosg-configure-2.2.3-1.osg34.el6\n\n\nosg-gridftp-3.4-4.osg34.el6\n\n\nosg-oasis-8-5.osg34.el6\n\n\nosg-pki-tools-2.1.2-1.osg34.el6\n\n\nosg-system-profiler-1.4.2-1.osg34.el6\n\n\nosg-version-3.4.6-1.osg34.el6\n\n\nosg-wn-client-3.4-3.osg34.el6\n\n\nxrootd-4.8.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.35.bosco-1.osg34.el7\n\n\ncvmfs-2.4.4-1.osg34.el7\n\n\nglideinwms-3.2.20-1.osg34.el7\n\n\nhtcondor-ce-3.0.4-1.osg34.el7\n\n\nosg-ca-certs-updater-1.7-1.osg34.el7\n\n\nosg-ca-scripts-1.2.2-1.osg34.el7\n\n\nosg-configure-2.2.3-1.osg34.el7\n\n\nosg-gridftp-3.4-4.osg34.el7\n\n\nosg-oasis-8-5.osg34.el7\n\n\nosg-pki-tools-2.1.2-1.osg34.el7\n\n\nosg-system-profiler-1.4.2-1.osg34.el7\n\n\nosg-version-3.4.6-1.osg34.el7\n\n\nosg-wn-client-3.4-3.osg34.el7\n\n\nxrootd-4.8.0-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n-\nupdater\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nsiteinfo\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ngridftp\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\noasis\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nsystem\n-\nprofiler\n \nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n \nosg\n-\nversion\n \nosg\n-\nwn\n-\nclient\n \npython2\n-\nxrootd\n \npython3\n-\nxrootd\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n8\n-\n5\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsiteinfo\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n8\n-\n5\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\npython2\n-\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\npython3\n-\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n8\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.35.bosco-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.35.bosco-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n35\n.\nbosco\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#osg-software-release-346", 
            "text": "Release Date : 2017-12-21", 
            "title": "OSG Software Release 3.4.6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#summary-of-changes", 
            "text": "This release contains:   XRootD 4.8.0 : fixed caching issues and multiple segfaults  CernVM-FS 2.4.4 : Update from 2.4.2  bug fixes for servers and clients    GlideinWMS 3.2.20 : improved factory performance, Singularity support  osg-pki-tools 2.1.2 : use HTTPS for all OIM connections, service certificate fix (Update from 2.0.0)  osg-wn-client: add gfal2-http plugin  HTCondor-CE 3.0.4 : Update from 3.0.2  osg-gridftp: add osg-configure-gratia  osg-configure 2.2.3 : minor bug fixes   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.  OSG PKI tools now create service certificate files where the service tag is separated from the hostname with an underscore ('_'). This new format first appeared in OSG PKI tools version 2.1.2.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-6", 
            "text": "blahp-1.18.35.bosco-1.osg34.el6  cvmfs-2.4.4-1.osg34.el6  glideinwms-3.2.20-1.osg34.el6  htcondor-ce-3.0.4-1.osg34.el6  osg-ca-certs-updater-1.7-1.osg34.el6  osg-ca-scripts-1.2.2-1.osg34.el6  osg-configure-2.2.3-1.osg34.el6  osg-gridftp-3.4-4.osg34.el6  osg-oasis-8-5.osg34.el6  osg-pki-tools-2.1.2-1.osg34.el6  osg-system-profiler-1.4.2-1.osg34.el6  osg-version-3.4.6-1.osg34.el6  osg-wn-client-3.4-3.osg34.el6  xrootd-4.8.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-7", 
            "text": "blahp-1.18.35.bosco-1.osg34.el7  cvmfs-2.4.4-1.osg34.el7  glideinwms-3.2.20-1.osg34.el7  htcondor-ce-3.0.4-1.osg34.el7  osg-ca-certs-updater-1.7-1.osg34.el7  osg-ca-scripts-1.2.2-1.osg34.el7  osg-configure-2.2.3-1.osg34.el7  osg-gridftp-3.4-4.osg34.el7  osg-oasis-8-5.osg34.el7  osg-pki-tools-2.1.2-1.osg34.el7  osg-system-profiler-1.4.2-1.osg34.el7  osg-version-3.4.6-1.osg34.el7  osg-wn-client-3.4-3.osg34.el7  xrootd-4.8.0-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   osg - ca - certs   osg - ca - certs - updater   osg - ca - scripts   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - misc   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - siteinfo   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - gridftp   osg - gums - config   osg - oasis   osg - pki - tools   osg - system - profiler   osg - system - profiler - viewer   osg - version   osg - wn - client   python2 - xrootd   python3 - xrootd   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 35 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 35 . bosco - 1 . osg34 . el6  cvmfs - 2 . 4 . 4 - 1 . osg34 . el6  cvmfs - devel - 2 . 4 . 4 - 1 . osg34 . el6  cvmfs - server - 2 . 4 . 4 - 1 . osg34 . el6  cvmfs - unittests - 2 . 4 . 4 - 1 . osg34 . el6  glideinwms - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - common - tools - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - condor - common - config - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - factory - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - factory - condor - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - libs - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - minimal - condor - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - usercollector - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - userschedd - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - vofrontend - 3 . 2 . 20 - 1 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 2 . 20 - 1 . osg34 . el6  htcondor - ce - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - client - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 0 . 4 - 1 . osg34 . el6  htcondor - ce - view - 3 . 0 . 4 - 1 . osg34 . el6  osg - ca - certs - updater - 1 . 7 - 1 . osg34 . el6  osg - ca - scripts - 1 . 2 . 2 - 1 . osg34 . el6  osg - configure - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - bosco - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - ce - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - condor - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - gateway - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - gip - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - gratia - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - lsf - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - misc - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - pbs - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - rsv - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - sge - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - siteinfo - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - slurm - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - squid - 2 . 2 . 3 - 1 . osg34 . el6  osg - configure - tests - 2 . 2 . 3 - 1 . osg34 . el6  osg - gridftp - 3 . 4 - 4 . osg34 . el6  osg - oasis - 8 - 5 . osg34 . el6  osg - pki - tools - 2 . 1 . 2 - 1 . osg34 . el6  osg - system - profiler - 1 . 4 . 2 - 1 . osg34 . el6  osg - system - profiler - viewer - 1 . 4 . 2 - 1 . osg34 . el6  osg - version - 3 . 4 . 6 - 1 . osg34 . el6  osg - wn - client - 3 . 4 - 3 . osg34 . el6  python2 - xrootd - 4 . 8 . 0 - 1 . osg34 . el6  python3 - xrootd - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - client - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - client - devel - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - client - libs - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - devel - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - doc - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - fuse - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - libs - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - private - devel - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - selinux - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - server - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - server - devel - 4 . 8 . 0 - 1 . osg34 . el6  xrootd - server - libs - 4 . 8 . 0 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 35 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 35 . bosco - 1 . osg34 . el7  cvmfs - 2 . 4 . 4 - 1 . osg34 . el7  cvmfs - devel - 2 . 4 . 4 - 1 . osg34 . el7  cvmfs - server - 2 . 4 . 4 - 1 . osg34 . el7  cvmfs - unittests - 2 . 4 . 4 - 1 . osg34 . el7  glideinwms - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - common - tools - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - condor - common - config - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - factory - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - factory - condor - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - libs - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - minimal - condor - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - usercollector - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - userschedd - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - vofrontend - 3 . 2 . 20 - 1 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 2 . 20 - 1 . osg34 . el7  htcondor - ce - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - client - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 0 . 4 - 1 . osg34 . el7  htcondor - ce - view - 3 . 0 . 4 - 1 . osg34 . el7  osg - ca - certs - updater - 1 . 7 - 1 . osg34 . el7  osg - ca - scripts - 1 . 2 . 2 - 1 . osg34 . el7  osg - configure - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - bosco - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - ce - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - condor - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - gateway - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - gip - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - gratia - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - lsf - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - misc - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - pbs - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - rsv - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - sge - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - siteinfo - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - slurm - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - squid - 2 . 2 . 3 - 1 . osg34 . el7  osg - configure - tests - 2 . 2 . 3 - 1 . osg34 . el7  osg - gridftp - 3 . 4 - 4 . osg34 . el7  osg - oasis - 8 - 5 . osg34 . el7  osg - pki - tools - 2 . 1 . 2 - 1 . osg34 . el7  osg - system - profiler - 1 . 4 . 2 - 1 . osg34 . el7  osg - system - profiler - viewer - 1 . 4 . 2 - 1 . osg34 . el7  osg - version - 3 . 4 . 6 - 1 . osg34 . el7  osg - wn - client - 3 . 4 - 3 . osg34 . el7  python2 - xrootd - 4 . 8 . 0 - 1 . osg34 . el7  python3 - xrootd - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - client - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - client - devel - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - client - libs - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - devel - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - doc - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - fuse - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - libs - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - private - devel - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - selinux - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - server - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - server - devel - 4 . 8 . 0 - 1 . osg34 . el7  xrootd - server - libs - 4 . 8 . 0 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-6_2", 
            "text": "blahp-1.18.35.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-7_2", 
            "text": "blahp-1.18.35.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 35 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 35 . bosco - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-6/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 35 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 35 . bosco - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.5-4\n\n\nRelease Date\n: 2017-12-20\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v77\n\n\nRemove voms1.egee.cesnet.cz VOMS server (auger VO)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension).\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-77-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-77-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\ngums\n-\nconfig\n-\n77\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n77\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n77\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n77\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\ngums\n-\nconfig\n-\n77\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n77\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n77\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n77\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.5-4"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#osg-software-stack-data-release-345-4", 
            "text": "Release Date : 2017-12-20", 
            "title": "OSG Software Stack -- Data Release -- 3.4.5-4"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#summary-of-changes", 
            "text": "This release contains:   VO Package v77  Remove voms1.egee.cesnet.cz VOMS server (auger VO)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension).", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#enterprise-linux-6", 
            "text": "vo-client-77-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#enterprise-linux-7", 
            "text": "vo-client-77-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#enterprise-linux-6_1", 
            "text": "osg - gums - config - 77 - 1 . osg34 . el6  vo - client - 77 - 1 . osg34 . el6  vo - client - edgmkgridmap - 77 - 1 . osg34 . el6  vo - client - lcmaps - voms - 77 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-4/#enterprise-linux-7_1", 
            "text": "osg - gums - config - 77 - 1 . osg34 . el7  vo - client - 77 - 1 . osg34 . el7  vo - client - edgmkgridmap - 77 - 1 . osg34 . el7  vo - client - lcmaps - voms - 77 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.5-3\n\n\nRelease Date\n: 2017-11-29\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.88\n\n\nupdated UKeScience 2B ICA based on a SHA-2 family digest (UK)\n\n\nadded new PKIUNAMgrid (2017) trust anchor for roll-over (MX)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension).\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.88-1.osg34.el6\n\n\nosg-ca-certs-1.68-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.88-1.osg34.el7\n\n\nosg-ca-certs-1.68-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n88\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n68\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n88\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n68\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.5-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#osg-software-stack-data-release-345-3", 
            "text": "Release Date : 2017-11-29", 
            "title": "OSG Software Stack -- Data Release -- 3.4.5-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.88  updated UKeScience 2B ICA based on a SHA-2 family digest (UK)  added new PKIUNAMgrid (2017) trust anchor for roll-over (MX)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension).", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.88-1.osg34.el6  osg-ca-certs-1.68-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.88-1.osg34.el7  osg-ca-certs-1.68-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 88 - 1 . osg34 . el6  osg - ca - certs - 1 . 68 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-3/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 88 - 1 . osg34 . el7  osg - ca - certs - 1 . 68 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.5-2\n\n\nRelease Date\n: 2017-11-20\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nVO Package v76\n\n\nDrop redundant geant4-lcgadmin objects\n\n\nAdd missing SNO+ VOMS servers\n\n\nAdd notice to voms-mapfile-default\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension).\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nvo-client-76-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nvo-client-76-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nosg\n-\ngums\n-\nconfig\n-\n76\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n76\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n76\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n76\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nosg\n-\ngums\n-\nconfig\n-\n76\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n76\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n76\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n76\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.5-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#osg-software-stack-data-release-345-2", 
            "text": "Release Date : 2017-11-20", 
            "title": "OSG Software Stack -- Data Release -- 3.4.5-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#summary-of-changes", 
            "text": "This release contains:   VO Package v76  Drop redundant geant4-lcgadmin objects  Add missing SNO+ VOMS servers  Add notice to voms-mapfile-default     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension).", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#enterprise-linux-6", 
            "text": "vo-client-76-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#enterprise-linux-7", 
            "text": "vo-client-76-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#enterprise-linux-6_1", 
            "text": "osg - gums - config - 76 - 1 . osg34 . el6  vo - client - 76 - 1 . osg34 . el6  vo - client - edgmkgridmap - 76 - 1 . osg34 . el6  vo - client - lcmaps - voms - 76 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5-2/#enterprise-linux-7_1", 
            "text": "osg - gums - config - 76 - 1 . osg34 . el7  vo - client - 76 - 1 . osg34 . el7  vo - client - edgmkgridmap - 76 - 1 . osg34 . el7  vo - client - lcmaps - voms - 76 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/", 
            "text": "OSG Software Release 3.4.5\n\n\nRelease Date\n: 2017-11-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG PKI: Host Certificate requests and retrievals are authenticated by default\n\n\nBLAHP 1.18.34\n\n\nFixed bug in Slurm memory-use parsing that caused jobs to be held\n\n\nFixed Unicode decode error when reading blah.config\n\n\n\n\n\n\nHTCondor: Update from version 8.6.6\n\n\nHTCondor 8.6.7\n\n\nHTCondor 8.6.8\n\n\n\n\n\n\nXRootD 4.7.1\n: Fixed occasional crash when LCMAPS callout to GUMS fails\n\n\nCVMFS 2.4.2\n: Server side bug fixes\n\n\nGridFTP-HDFS: Added support for CMVFS checksums\n\n\nGlobus GridFTP server: Fixed IPv6 redirection and IPv4 passive mode (EPSV) response\n\n\nLCMAPS VOMS Plugin: Documented how to map using all FQANs\n\n\nRSV: Fixed CRL freshness probe, removed unused probes\n\n\nosg-system-profiler: Dropped check for osg-version, stopped checking for deprecated software, updated instructions\n\n\nUpcoming\n\n\nHTCondor: Update from version 8.7.3\n\n\nHTCondor 8.7.4\n\n\nHTCondor 8.7.5\n\n\n\n\n\n\nBLAHP 1.18.34 (See above)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.34.bosco-1.osg34.el6\n\n\ncondor-8.6.8-1.osg34.el6\n\n\ncvmfs-2.4.2-1.osg34.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osg34.el6\n\n\nglobus-ftp-client-8.36-1.2.osg34.el6\n\n\nglobus-gridftp-server-12.2-1.2.osg34.el6\n\n\nglobus-gridftp-server-control-6.0-2.1.osg34.el6\n\n\ngratia-probe-1.18.2-2.osg34.el6\n\n\nlcmaps-1.6.6-1.8.osg34.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.5.osg34.el6\n\n\nosg-configure-2.2.2-1.osg34.el6\n\n\nosg-oasis-8-2.osg34.el6\n\n\nosg-pki-tools-2.0.0-1.osg34.el6\n\n\nosg-system-profiler-1.4.1-1.osg34.el6\n\n\nosg-test-2.0.0-1.osg34.el6\n\n\nosg-version-3.4.5-1.osg34.el6\n\n\nrsv-3.16.0-1.osg34.el6\n\n\nxrootd-4.7.1-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.34.bosco-1.osg34.el7\n\n\ncondor-8.6.8-1.osg34.el7\n\n\ncvmfs-2.4.2-1.osg34.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osg34.el7\n\n\nglobus-ftp-client-8.36-1.2.osg34.el7\n\n\nglobus-gridftp-server-12.2-1.2.osg34.el7\n\n\nglobus-gridftp-server-control-6.0-2.1.osg34.el7\n\n\ngratia-probe-1.18.2-2.osg34.el7\n\n\nlcmaps-1.6.6-1.8.osg34.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.5.osg34.el7\n\n\nosg-configure-2.2.2-1.osg34.el7\n\n\nosg-oasis-8-2.osg34.el7\n\n\nosg-pki-tools-2.0.0-1.osg34.el7\n\n\nosg-system-profiler-1.4.1-1.osg34.el7\n\n\nosg-test-2.0.0-1.osg34.el7\n\n\nosg-version-3.4.5-1.osg34.el7\n\n\nrsv-3.16.0-1.osg34.el7\n\n\nxrootd-4.7.1-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n \nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n \nglobus\n-\nftp\n-\nclient\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n-\ndoc\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglexec\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngram\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \nigtf\n-\nca\n-\ncerts\n \nlcmaps\n \nlcmaps\n-\ncommon\n-\ndevel\n \nlcmaps\n-\ndb\n-\ntemplates\n \nlcmaps\n-\ndebuginfo\n \nlcmaps\n-\ndevel\n \nlcmaps\n-\nplugins\n-\nvoms\n \nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n \nlcmaps\n-\nwithout\n-\ngsi\n \nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\noasis\n \nosg\n-\npki\n-\ntools\n \nosg\n-\nsystem\n-\nprofiler\n \nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\npython\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosg34\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ncommon\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndb\n-\ntemplates\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n8\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\npython\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosg34\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n36\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n6\n.\n0\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n18\n.\n2\n-\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ncommon\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndb\n-\ntemplates\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n8\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n8\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n2\n.\n0\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n16\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\npython\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n7\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.34.bosco-1.osgup.el6\n\n\ncondor-8.7.5-1.osgup.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.5.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.34.bosco-1.osgup.el7\n\n\ncondor-8.7.5-1.osgup.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.5.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n5\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n5\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n34\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n5\n-\n1\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n5\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n5\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.5"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#osg-software-release-345", 
            "text": "Release Date : 2017-11-14", 
            "title": "OSG Software Release 3.4.5"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#summary-of-changes", 
            "text": "This release contains:   OSG PKI: Host Certificate requests and retrievals are authenticated by default  BLAHP 1.18.34  Fixed bug in Slurm memory-use parsing that caused jobs to be held  Fixed Unicode decode error when reading blah.config    HTCondor: Update from version 8.6.6  HTCondor 8.6.7  HTCondor 8.6.8    XRootD 4.7.1 : Fixed occasional crash when LCMAPS callout to GUMS fails  CVMFS 2.4.2 : Server side bug fixes  GridFTP-HDFS: Added support for CMVFS checksums  Globus GridFTP server: Fixed IPv6 redirection and IPv4 passive mode (EPSV) response  LCMAPS VOMS Plugin: Documented how to map using all FQANs  RSV: Fixed CRL freshness probe, removed unused probes  osg-system-profiler: Dropped check for osg-version, stopped checking for deprecated software, updated instructions  Upcoming  HTCondor: Update from version 8.7.3  HTCondor 8.7.4  HTCondor 8.7.5    BLAHP 1.18.34 (See above)     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration. COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-6", 
            "text": "blahp-1.18.34.bosco-1.osg34.el6  condor-8.6.8-1.osg34.el6  cvmfs-2.4.2-1.osg34.el6  glite-ce-cream-client-api-c-1.15.4-2.4.osg34.el6  globus-ftp-client-8.36-1.2.osg34.el6  globus-gridftp-server-12.2-1.2.osg34.el6  globus-gridftp-server-control-6.0-2.1.osg34.el6  gratia-probe-1.18.2-2.osg34.el6  lcmaps-1.6.6-1.8.osg34.el6  lcmaps-plugins-voms-1.7.1-1.5.osg34.el6  osg-configure-2.2.2-1.osg34.el6  osg-oasis-8-2.osg34.el6  osg-pki-tools-2.0.0-1.osg34.el6  osg-system-profiler-1.4.1-1.osg34.el6  osg-test-2.0.0-1.osg34.el6  osg-version-3.4.5-1.osg34.el6  rsv-3.16.0-1.osg34.el6  xrootd-4.7.1-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-7", 
            "text": "blahp-1.18.34.bosco-1.osg34.el7  condor-8.6.8-1.osg34.el7  cvmfs-2.4.2-1.osg34.el7  glite-ce-cream-client-api-c-1.15.4-2.4.osg34.el7  globus-ftp-client-8.36-1.2.osg34.el7  globus-gridftp-server-12.2-1.2.osg34.el7  globus-gridftp-server-control-6.0-2.1.osg34.el7  gratia-probe-1.18.2-2.osg34.el7  lcmaps-1.6.6-1.8.osg34.el7  lcmaps-plugins-voms-1.7.1-1.5.osg34.el7  osg-configure-2.2.2-1.osg34.el7  osg-oasis-8-2.osg34.el7  osg-pki-tools-2.0.0-1.osg34.el7  osg-system-profiler-1.4.1-1.osg34.el7  osg-test-2.0.0-1.osg34.el7  osg-version-3.4.5-1.osg34.el7  rsv-3.16.0-1.osg34.el7  xrootd-4.7.1-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   globus - ftp - client   globus - ftp - client - debuginfo   globus - ftp - client - devel   globus - ftp - client - doc   globus - gridftp - server   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glexec   gratia - probe - glideinwms   gratia - probe - gram   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   igtf - ca - certs   lcmaps   lcmaps - common - devel   lcmaps - db - templates   lcmaps - debuginfo   lcmaps - devel   lcmaps - plugins - voms   lcmaps - plugins - voms - debuginfo   lcmaps - without - gsi   lcmaps - without - gsi - devel   osg - ca - certs   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - gums - config   osg - oasis   osg - pki - tools   osg - system - profiler   osg - system - profiler - viewer   osg - test   osg - test - log - viewer   osg - version   rsv   rsv - consumers   rsv - core   rsv - metrics   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - python   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 34 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 34 . bosco - 1 . osg34 . el6  condor - 8 . 6 . 8 - 1 . osg34 . el6  condor - all - 8 . 6 . 8 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 8 - 1 . osg34 . el6  condor - classads - 8 . 6 . 8 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 8 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 8 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 8 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 8 - 1 . osg34 . el6  condor - procd - 8 . 6 . 8 - 1 . osg34 . el6  condor - python - 8 . 6 . 8 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 8 - 1 . osg34 . el6  condor - test - 8 . 6 . 8 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 8 - 1 . osg34 . el6  cvmfs - 2 . 4 . 2 - 1 . osg34 . el6  cvmfs - devel - 2 . 4 . 2 - 1 . osg34 . el6  cvmfs - server - 2 . 4 . 2 - 1 . osg34 . el6  cvmfs - unittests - 2 . 4 . 2 - 1 . osg34 . el6  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 4 . osg34 . el6  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 4 . osg34 . el6  globus - ftp - client - 8 . 36 - 1 . 2 . osg34 . el6  globus - ftp - client - debuginfo - 8 . 36 - 1 . 2 . osg34 . el6  globus - ftp - client - devel - 8 . 36 - 1 . 2 . osg34 . el6  globus - ftp - client - doc - 8 . 36 - 1 . 2 . osg34 . el6  globus - gridftp - server - 12 . 2 - 1 . 2 . osg34 . el6  globus - gridftp - server - control - 6 . 0 - 2 . 1 . osg34 . el6  globus - gridftp - server - control - debuginfo - 6 . 0 - 2 . 1 . osg34 . el6  globus - gridftp - server - control - devel - 6 . 0 - 2 . 1 . osg34 . el6  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 2 . osg34 . el6  globus - gridftp - server - devel - 12 . 2 - 1 . 2 . osg34 . el6  globus - gridftp - server - progs - 12 . 2 - 1 . 2 . osg34 . el6  gratia - probe - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - common - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - condor - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - condor - events - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - dcache - storage - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - debuginfo - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - enstore - storage - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - glexec - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - glideinwms - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - gram - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - lsf - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - metric - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - onevm - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - services - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - sge - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - slurm - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 18 . 2 - 2 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 18 . 2 - 2 . osg34 . el6  lcmaps - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - common - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - db - templates - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - debuginfo - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 5 . osg34 . el6  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 5 . osg34 . el6  lcmaps - without - gsi - 1 . 6 . 6 - 1 . 8 . osg34 . el6  lcmaps - without - gsi - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el6  osg - configure - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - bosco - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - ce - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - condor - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - gateway - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - gip - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - gratia - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - lsf - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - managedfork - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - misc - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - network - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - pbs - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - rsv - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - sge - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - slurm - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - squid - 2 . 2 . 2 - 1 . osg34 . el6  osg - configure - tests - 2 . 2 . 2 - 1 . osg34 . el6  osg - oasis - 8 - 2 . osg34 . el6  osg - pki - tools - 2 . 0 . 0 - 1 . osg34 . el6  osg - system - profiler - 1 . 4 . 1 - 1 . osg34 . el6  osg - system - profiler - viewer - 1 . 4 . 1 - 1 . osg34 . el6  osg - test - 2 . 0 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 2 . 0 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 5 - 1 . osg34 . el6  rsv - 3 . 16 . 0 - 1 . osg34 . el6  rsv - consumers - 3 . 16 . 0 - 1 . osg34 . el6  rsv - core - 3 . 16 . 0 - 1 . osg34 . el6  rsv - metrics - 3 . 16 . 0 - 1 . osg34 . el6  xrootd - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - client - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - client - devel - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - client - libs - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - devel - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - doc - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - fuse - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - libs - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - private - devel - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - python - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - selinux - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - server - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - server - devel - 4 . 7 . 1 - 1 . osg34 . el6  xrootd - server - libs - 4 . 7 . 1 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 34 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 34 . bosco - 1 . osg34 . el7  condor - 8 . 6 . 8 - 1 . osg34 . el7  condor - all - 8 . 6 . 8 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 8 - 1 . osg34 . el7  condor - classads - 8 . 6 . 8 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 8 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 8 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 8 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 8 - 1 . osg34 . el7  condor - procd - 8 . 6 . 8 - 1 . osg34 . el7  condor - python - 8 . 6 . 8 - 1 . osg34 . el7  condor - test - 8 . 6 . 8 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 8 - 1 . osg34 . el7  cvmfs - 2 . 4 . 2 - 1 . osg34 . el7  cvmfs - devel - 2 . 4 . 2 - 1 . osg34 . el7  cvmfs - server - 2 . 4 . 2 - 1 . osg34 . el7  cvmfs - unittests - 2 . 4 . 2 - 1 . osg34 . el7  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 4 . osg34 . el7  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 4 . osg34 . el7  globus - ftp - client - 8 . 36 - 1 . 2 . osg34 . el7  globus - ftp - client - debuginfo - 8 . 36 - 1 . 2 . osg34 . el7  globus - ftp - client - devel - 8 . 36 - 1 . 2 . osg34 . el7  globus - ftp - client - doc - 8 . 36 - 1 . 2 . osg34 . el7  globus - gridftp - server - 12 . 2 - 1 . 2 . osg34 . el7  globus - gridftp - server - control - 6 . 0 - 2 . 1 . osg34 . el7  globus - gridftp - server - control - debuginfo - 6 . 0 - 2 . 1 . osg34 . el7  globus - gridftp - server - control - devel - 6 . 0 - 2 . 1 . osg34 . el7  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 2 . osg34 . el7  globus - gridftp - server - devel - 12 . 2 - 1 . 2 . osg34 . el7  globus - gridftp - server - progs - 12 . 2 - 1 . 2 . osg34 . el7  gratia - probe - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - common - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - condor - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - condor - events - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - dcache - storage - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - debuginfo - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - enstore - storage - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - glexec - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - glideinwms - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - gram - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - lsf - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - metric - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - onevm - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - services - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - sge - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - slurm - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 18 . 2 - 2 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 18 . 2 - 2 . osg34 . el7  lcmaps - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - common - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - db - templates - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - debuginfo - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 5 . osg34 . el7  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 5 . osg34 . el7  lcmaps - without - gsi - 1 . 6 . 6 - 1 . 8 . osg34 . el7  lcmaps - without - gsi - devel - 1 . 6 . 6 - 1 . 8 . osg34 . el7  osg - configure - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - bosco - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - ce - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - condor - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - gateway - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - gip - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - gratia - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - lsf - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - managedfork - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - misc - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - network - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - pbs - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - rsv - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - sge - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - slurm - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - squid - 2 . 2 . 2 - 1 . osg34 . el7  osg - configure - tests - 2 . 2 . 2 - 1 . osg34 . el7  osg - oasis - 8 - 2 . osg34 . el7  osg - pki - tools - 2 . 0 . 0 - 1 . osg34 . el7  osg - system - profiler - 1 . 4 . 1 - 1 . osg34 . el7  osg - system - profiler - viewer - 1 . 4 . 1 - 1 . osg34 . el7  osg - test - 2 . 0 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 2 . 0 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 5 - 1 . osg34 . el7  rsv - 3 . 16 . 0 - 1 . osg34 . el7  rsv - consumers - 3 . 16 . 0 - 1 . osg34 . el7  rsv - core - 3 . 16 . 0 - 1 . osg34 . el7  rsv - metrics - 3 . 16 . 0 - 1 . osg34 . el7  xrootd - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - client - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - client - devel - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - client - libs - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - devel - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - doc - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - fuse - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - libs - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - private - devel - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - python - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - selinux - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - server - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - server - devel - 4 . 7 . 1 - 1 . osg34 . el7  xrootd - server - libs - 4 . 7 . 1 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-6_2", 
            "text": "blahp-1.18.34.bosco-1.osgup.el6  condor-8.7.5-1.osgup.el6  glite-ce-cream-client-api-c-1.15.4-2.5.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-7_2", 
            "text": "blahp-1.18.34.bosco-1.osgup.el7  condor-8.7.5-1.osgup.el7  glite-ce-cream-client-api-c-1.15.4-2.5.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 34 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 34 . bosco - 1 . osgup . el6  condor - 8 . 7 . 5 - 1 . osgup . el6  condor - all - 8 . 7 . 5 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 5 - 1 . osgup . el6  condor - bosco - 8 . 7 . 5 - 1 . osgup . el6  condor - classads - 8 . 7 . 5 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 5 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 5 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 5 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 5 - 1 . osgup . el6  condor - procd - 8 . 7 . 5 - 1 . osgup . el6  condor - python - 8 . 7 . 5 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 5 - 1 . osgup . el6  condor - test - 8 . 7 . 5 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 5 - 1 . osgup . el6  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 5 . osgup . el6  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 5 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-5/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 34 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 34 . bosco - 1 . osgup . el7  condor - 8 . 7 . 5 - 1 . osgup . el7  condor - all - 8 . 7 . 5 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 5 - 1 . osgup . el7  condor - bosco - 8 . 7 . 5 - 1 . osgup . el7  condor - classads - 8 . 7 . 5 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 5 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 5 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 5 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 5 - 1 . osgup . el7  condor - procd - 8 . 7 . 5 - 1 . osgup . el7  condor - python - 8 . 7 . 5 - 1 . osgup . el7  condor - test - 8 . 7 . 5 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 5 - 1 . osgup . el7  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 5 . osgup . el7  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 5 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.4-3\n\n\nRelease Date\n: 2017-11-01\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.87\n\n\nadded new accredited classic DarkMatter Private Root G4 and ICA (AE)\n\n\nupdated PK-Grid-2007 trust anchor with extended validity period (PK)\n\n\nextended validity period for UNAMgrid-ca trust anchor (MX)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.87-1.osg34.el6\n\n\nosg-ca-certs-1.67-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.87-1.osg34.el7\n\n\nosg-ca-certs-1.67-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n87\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n67\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n87\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n67\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.4-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#osg-software-stack-data-release-344-3", 
            "text": "Release Date : 2017-11-01", 
            "title": "OSG Software Stack -- Data Release -- 3.4.4-3"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.87  added new accredited classic DarkMatter Private Root G4 and ICA (AE)  updated PK-Grid-2007 trust anchor with extended validity period (PK)  extended validity period for UNAMgrid-ca trust anchor (MX)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.87-1.osg34.el6  osg-ca-certs-1.67-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.87-1.osg34.el7  osg-ca-certs-1.67-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 87 - 1 . osg34 . el6  osg - ca - certs - 1 . 67 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-3/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 87 - 1 . osg34 . el7  osg - ca - certs - 1 . 67 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.4-2\n\n\nRelease Date\n: 2017-10-11\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.86\n\n\nupdated MaGrid CA with extended validity period (MA)\n\n\nremoved discontinued pkIRISGrid CA (ES)\n\n\n\n\n\n\nVO Package v75\n\n\nAdd CMS wildcard to default map file\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.86-1.osg34.el6\n\n\nosg-ca-certs-1.66-1.osg34.el6\n\n\nvo-client-75-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.86-1.osg34.el7\n\n\nosg-ca-certs-1.66-1.osg34.el7\n\n\nvo-client-75-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n86\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n66\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n75\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n75\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n75\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n75\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n86\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n66\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n75\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n75\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n75\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n75\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.4-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#osg-software-stack-data-release-344-2", 
            "text": "Release Date : 2017-10-11", 
            "title": "OSG Software Stack -- Data Release -- 3.4.4-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.86  updated MaGrid CA with extended validity period (MA)  removed discontinued pkIRISGrid CA (ES)    VO Package v75  Add CMS wildcard to default map file     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.86-1.osg34.el6  osg-ca-certs-1.66-1.osg34.el6  vo-client-75-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.86-1.osg34.el7  osg-ca-certs-1.66-1.osg34.el7  vo-client-75-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 86 - 1 . osg34 . el6  osg - ca - certs - 1 . 66 - 1 . osg34 . el6  osg - gums - config - 75 - 1 . osg34 . el6  vo - client - 75 - 1 . osg34 . el6  vo - client - edgmkgridmap - 75 - 1 . osg34 . el6  vo - client - lcmaps - voms - 75 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 86 - 1 . osg34 . el7  osg - ca - certs - 1 . 66 - 1 . osg34 . el7  osg - gums - config - 75 - 1 . osg34 . el7  vo - client - 75 - 1 . osg34 . el7  vo - client - edgmkgridmap - 75 - 1 . osg34 . el7  vo - client - lcmaps - voms - 75 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/", 
            "text": "OSG Software Release 3.4.4\n\n\nRelease Date\n: 2017-10-10\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated gsi-openssh-server to interoperate with clients using OpenSSL 1.1\n\n\nSingularity 2.3.2\n: Now works with Docker's updated registry RESTful API\n\n\nHTCondor 8.6.6\n: Bug fix release\n\n\nglobus-gridftp-server-control 5.2: Allow 400 responses to stat failures\n\n\nosg-ca-scripts now properly requires wget to be installed\n\n\nUpdated osg-configure\n\n\nto work properly when fetch-crl is missing\n\n\nto work properly with HTCondor 8.7.2+\n\n\n\n\n\n\nHTCondor 8.7.3\n in Upcoming\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the November release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.6.6-1.osg34.el6\n\n\nglobus-gridftp-server-control-5.2-1.1.osg34.el6\n\n\ngsi-openssh-7.3p1c-1.1.osg34.el6\n\n\nosg-build-1.10.2-1.osg34.el6\n\n\nosg-ca-scripts-1.1.7-2.osg34.el6\n\n\nosg-configure-2.2.1-1.osg34.el6\n\n\nosg-release-3.4-2.osg34.el6\n\n\nosg-release-itb-3.4-2.osg34.el6\n\n\nosg-tested-internal-3.4-5.osg34.el6\n\n\nosg-version-3.4.4-1.osg34.el6\n\n\nsingularity-2.3.2-0.1.1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.6.6-1.osg34.el7\n\n\nglobus-gridftp-server-control-5.2-1.1.osg34.el7\n\n\ngsi-openssh-7.3p1c-1.1.osg34.el7\n\n\nosg-build-1.10.2-1.osg34.el7\n\n\nosg-ca-scripts-1.1.7-2.osg34.el7\n\n\nosg-configure-2.2.1-1.osg34.el7\n\n\nosg-release-3.4-2.osg34.el7\n\n\nosg-release-itb-3.4-2.osg34.el7\n\n\nosg-tested-internal-3.4-5.osg34.el7\n\n\nosg-version-3.4.4-1.osg34.el7\n\n\nsingularity-2.3.2-0.1.1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \ngsi\n-\nopenssh\n \ngsi\n-\nopenssh\n-\nclients\n \ngsi\n-\nopenssh\n-\ndebuginfo\n \ngsi\n-\nopenssh\n-\nserver\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\nrelease\n \nosg\n-\nrelease\n-\nitb\n \nosg\n-\ntested\n-\ninternal\n \nosg\n-\nversion\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\nclients\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\ndebuginfo\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\nserver\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n7\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nrelease\n-\nitb\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n5\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nclients\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\ndebuginfo\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nserver\n-\n7\n.\n3\np1c\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n7\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nrelease\n-\nitb\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n5\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n3\n.\n2\n-\n0\n.\n1\n.\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\ncondor-8.7.3-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\ncondor-8.7.3-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\ncondor\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\ncondor\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n3\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#osg-software-release-344", 
            "text": "Release Date : 2017-10-10", 
            "title": "OSG Software Release 3.4.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#summary-of-changes", 
            "text": "This release contains:   Updated gsi-openssh-server to interoperate with clients using OpenSSL 1.1  Singularity 2.3.2 : Now works with Docker's updated registry RESTful API  HTCondor 8.6.6 : Bug fix release  globus-gridftp-server-control 5.2: Allow 400 responses to stat failures  osg-ca-scripts now properly requires wget to be installed  Updated osg-configure  to work properly when fetch-crl is missing  to work properly with HTCondor 8.7.2+    HTCondor 8.7.3  in Upcoming   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the November release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration. COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6", 
            "text": "condor-8.6.6-1.osg34.el6  globus-gridftp-server-control-5.2-1.1.osg34.el6  gsi-openssh-7.3p1c-1.1.osg34.el6  osg-build-1.10.2-1.osg34.el6  osg-ca-scripts-1.1.7-2.osg34.el6  osg-configure-2.2.1-1.osg34.el6  osg-release-3.4-2.osg34.el6  osg-release-itb-3.4-2.osg34.el6  osg-tested-internal-3.4-5.osg34.el6  osg-version-3.4.4-1.osg34.el6  singularity-2.3.2-0.1.1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7", 
            "text": "condor-8.6.6-1.osg34.el7  globus-gridftp-server-control-5.2-1.1.osg34.el7  gsi-openssh-7.3p1c-1.1.osg34.el7  osg-build-1.10.2-1.osg34.el7  osg-ca-scripts-1.1.7-2.osg34.el7  osg-configure-2.2.1-1.osg34.el7  osg-release-3.4-2.osg34.el7  osg-release-itb-3.4-2.osg34.el7  osg-tested-internal-3.4-5.osg34.el7  osg-version-3.4.4-1.osg34.el7  singularity-2.3.2-0.1.1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   gsi - openssh   gsi - openssh - clients   gsi - openssh - debuginfo   gsi - openssh - server   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - scripts   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - release   osg - release - itb   osg - tested - internal   osg - version   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_1", 
            "text": "condor - 8 . 6 . 6 - 1 . osg34 . el6  condor - all - 8 . 6 . 6 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 6 - 1 . osg34 . el6  condor - classads - 8 . 6 . 6 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 6 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 6 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 6 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 6 - 1 . osg34 . el6  condor - procd - 8 . 6 . 6 - 1 . osg34 . el6  condor - python - 8 . 6 . 6 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 6 - 1 . osg34 . el6  condor - test - 8 . 6 . 6 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 6 - 1 . osg34 . el6  globus - gridftp - server - control - 5 . 2 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - debuginfo - 5 . 2 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - devel - 5 . 2 - 1 . 1 . osg34 . el6  gsi - openssh - 7 . 3 p1c - 1 . 1 . osg34 . el6  gsi - openssh - clients - 7 . 3 p1c - 1 . 1 . osg34 . el6  gsi - openssh - debuginfo - 7 . 3 p1c - 1 . 1 . osg34 . el6  gsi - openssh - server - 7 . 3 p1c - 1 . 1 . osg34 . el6  osg - build - 1 . 10 . 2 - 1 . osg34 . el6  osg - build - base - 1 . 10 . 2 - 1 . osg34 . el6  osg - build - koji - 1 . 10 . 2 - 1 . osg34 . el6  osg - build - mock - 1 . 10 . 2 - 1 . osg34 . el6  osg - build - tests - 1 . 10 . 2 - 1 . osg34 . el6  osg - ca - scripts - 1 . 1 . 7 - 2 . osg34 . el6  osg - configure - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - bosco - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - ce - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - condor - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - gateway - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - gip - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - gratia - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - lsf - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - managedfork - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - misc - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - network - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - pbs - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - rsv - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - sge - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - slurm - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - squid - 2 . 2 . 1 - 1 . osg34 . el6  osg - configure - tests - 2 . 2 . 1 - 1 . osg34 . el6  osg - release - 3 . 4 - 2 . osg34 . el6  osg - release - itb - 3 . 4 - 2 . osg34 . el6  osg - tested - internal - 3 . 4 - 5 . osg34 . el6  osg - version - 3 . 4 . 4 - 1 . osg34 . el6  singularity - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el6  singularity - debuginfo - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el6  singularity - devel - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el6  singularity - runtime - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_1", 
            "text": "condor - 8 . 6 . 6 - 1 . osg34 . el7  condor - all - 8 . 6 . 6 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 6 - 1 . osg34 . el7  condor - classads - 8 . 6 . 6 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 6 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 6 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 6 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 6 - 1 . osg34 . el7  condor - procd - 8 . 6 . 6 - 1 . osg34 . el7  condor - python - 8 . 6 . 6 - 1 . osg34 . el7  condor - test - 8 . 6 . 6 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 6 - 1 . osg34 . el7  globus - gridftp - server - control - 5 . 2 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - debuginfo - 5 . 2 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - devel - 5 . 2 - 1 . 1 . osg34 . el7  gsi - openssh - 7 . 3 p1c - 1 . 1 . osg34 . el7  gsi - openssh - clients - 7 . 3 p1c - 1 . 1 . osg34 . el7  gsi - openssh - debuginfo - 7 . 3 p1c - 1 . 1 . osg34 . el7  gsi - openssh - server - 7 . 3 p1c - 1 . 1 . osg34 . el7  osg - build - 1 . 10 . 2 - 1 . osg34 . el7  osg - build - base - 1 . 10 . 2 - 1 . osg34 . el7  osg - build - koji - 1 . 10 . 2 - 1 . osg34 . el7  osg - build - mock - 1 . 10 . 2 - 1 . osg34 . el7  osg - build - tests - 1 . 10 . 2 - 1 . osg34 . el7  osg - ca - scripts - 1 . 1 . 7 - 2 . osg34 . el7  osg - configure - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - bosco - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - ce - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - condor - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - gateway - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - gip - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - gratia - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - lsf - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - managedfork - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - misc - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - network - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - pbs - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - rsv - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - sge - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - slurm - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - squid - 2 . 2 . 1 - 1 . osg34 . el7  osg - configure - tests - 2 . 2 . 1 - 1 . osg34 . el7  osg - release - 3 . 4 - 2 . osg34 . el7  osg - release - itb - 3 . 4 - 2 . osg34 . el7  osg - tested - internal - 3 . 4 - 5 . osg34 . el7  osg - version - 3 . 4 . 4 - 1 . osg34 . el7  singularity - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el7  singularity - debuginfo - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el7  singularity - devel - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el7  singularity - runtime - 2 . 3 . 2 - 0 . 1 . 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_2", 
            "text": "condor-8.7.3-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_2", 
            "text": "condor-8.7.3-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-6_3", 
            "text": "condor - 8 . 7 . 3 - 1 . osgup . el6  condor - all - 8 . 7 . 3 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 3 - 1 . osgup . el6  condor - bosco - 8 . 7 . 3 - 1 . osgup . el6  condor - classads - 8 . 7 . 3 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 3 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 3 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 3 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 3 - 1 . osgup . el6  condor - procd - 8 . 7 . 3 - 1 . osgup . el6  condor - python - 8 . 7 . 3 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 3 - 1 . osgup . el6  condor - test - 8 . 7 . 3 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 3 - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-4/#enterprise-linux-7_3", 
            "text": "condor - 8 . 7 . 3 - 1 . osgup . el7  condor - all - 8 . 7 . 3 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 3 - 1 . osgup . el7  condor - bosco - 8 . 7 . 3 - 1 . osgup . el7  condor - classads - 8 . 7 . 3 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 3 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 3 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 3 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 3 - 1 . osgup . el7  condor - procd - 8 . 7 . 3 - 1 . osgup . el7  condor - python - 8 . 7 . 3 - 1 . osgup . el7  condor - test - 8 . 7 . 3 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 3 - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/", 
            "text": "OSG Software Release 3.4.3\n\n\nRelease Date\n: 2017-09-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nUpdated to \nCVMFS 2.4.1\n\n\nUpdated to \nSingularity 2.3.1\n\n\nUpdated to BLAHP 1.18.33\n\n\nProperly parses times from Slurm's sacct command\n\n\npbs_pro does not need to be defined to query PBS for job status\n\n\n\n\n\n\nUpdated to \nXRootD 4.7.0\n\n\nUpdated to \nStashCache 0.8\n\n\nUpdated Globus packages to latest EPEL versions\n\n\nosg-ca-scripts now use HTTPS to download CA certificates\n\n\nAdded the ability to limit transfer load in the globus-gridftp-osg-extensions\n\n\nFixed a few memory management bugs in \nxrootd-lcmaps\n\n\nHTCondor CE 3.0.2\n reports an error if \nJOB_ROUTER_ENTRIES\n are not defined\n\n\nosg-configure 2.2.0 - remove last vestiges of GRAM\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n.\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the October release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.33.bosco-1.osg34.el6\n\n\ncvmfs-2.4.1-1.osg34.el6\n\n\nglobus-ftp-client-8.36-1.1.osg34.el6\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg34.el6\n\n\nglobus-gridftp-server-12.2-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-5.1-1.1.osg34.el6\n\n\nhtcondor-ce-3.0.2-1.osg34.el6\n\n\nmyproxy-6.1.28-1.1.osg34.el6\n\n\nosg-ca-scripts-1.1.7-1.osg34.el6\n\n\nosg-configure-2.2.0-1.osg34.el6\n\n\nosg-oasis-8-1.osg34.el6\n\n\nosg-test-1.11.2-1.osg34.el6\n\n\nosg-version-3.4.3-1.osg34.el6\n\n\nsingularity-2.3.1-0.1.4.osg34.el6\n\n\nxrootd-4.7.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.33.bosco-1.osg34.el7\n\n\ncvmfs-2.4.1-1.osg34.el7\n\n\nglobus-ftp-client-8.36-1.1.osg34.el7\n\n\nglobus-gridftp-osg-extensions-0.4-1.osg34.el7\n\n\nglobus-gridftp-server-12.2-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-5.1-1.1.osg34.el7\n\n\nhtcondor-ce-3.0.2-1.osg34.el7\n\n\nmyproxy-6.1.28-1.1.osg34.el7\n\n\nosg-ca-scripts-1.1.7-1.osg34.el7\n\n\nosg-configure-2.2.0-1.osg34.el7\n\n\nosg-oasis-8-1.osg34.el7\n\n\nosg-test-1.11.2-1.osg34.el7\n\n\nosg-version-3.4.3-1.osg34.el7\n\n\nsingularity-2.3.1-0.1.4.osg34.el7\n\n\nstashcache-0.8-1.osg34.el7\n\n\nxrootd-4.7.0-1.osg34.el7\n\n\nxrootd-lcmaps-1.3.4-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nglobus\n-\nftp\n-\nclient\n \nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n \nglobus\n-\nftp\n-\nclient\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n-\ndoc\n \nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n \nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nmyproxy\n \nmyproxy\n-\nadmin\n \nmyproxy\n-\ndebuginfo\n \nmyproxy\n-\ndevel\n \nmyproxy\n-\ndoc\n \nmyproxy\n-\nlibs\n \nmyproxy\n-\nserver\n \nmyproxy\n-\nvoms\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\noasis\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nsingularity\n \nsingularity\n-\ndebuginfo\n \nsingularity\n-\ndevel\n \nsingularity\n-\nruntime\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\npython\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n-\n0\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nserver\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n7\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n8\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\nsingularity\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nsingularity\n-\ndevel\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nsingularity\n-\nruntime\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\npython\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n36\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n-\n0\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n5\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n12\n.\n2\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nserver\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n1\n.\n28\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n7\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\nsingularity\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndebuginfo\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nsingularity\n-\ndevel\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nsingularity\n-\nruntime\n-\n2\n.\n3\n.\n1\n-\n0\n.\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nstashcache\n-\n0\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\n0\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ndaemon\n-\n0\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nstashcache\n-\norigin\n-\nserver\n-\n0\n.\n8\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n3\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n3\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\npython\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n7\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.33.bosco-1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.33.bosco-1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n33\n.\nbosco\n-\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#osg-software-release-343", 
            "text": "Release Date : 2017-09-12", 
            "title": "OSG Software Release 3.4.3"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#summary-of-changes", 
            "text": "This release contains:   Updated to  CVMFS 2.4.1  Updated to  Singularity 2.3.1  Updated to BLAHP 1.18.33  Properly parses times from Slurm's sacct command  pbs_pro does not need to be defined to query PBS for job status    Updated to  XRootD 4.7.0  Updated to  StashCache 0.8  Updated Globus packages to latest EPEL versions  osg-ca-scripts now use HTTPS to download CA certificates  Added the ability to limit transfer load in the globus-gridftp-osg-extensions  Fixed a few memory management bugs in  xrootd-lcmaps  HTCondor CE 3.0.2  reports an error if  JOB_ROUTER_ENTRIES  are not defined  osg-configure 2.2.0 - remove last vestiges of GRAM   These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here .", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the October release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration. COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#update-repositories", 
            "text": "To update to this series, you need to to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6", 
            "text": "blahp-1.18.33.bosco-1.osg34.el6  cvmfs-2.4.1-1.osg34.el6  globus-ftp-client-8.36-1.1.osg34.el6  globus-gridftp-osg-extensions-0.4-1.osg34.el6  globus-gridftp-server-12.2-1.1.osg34.el6  globus-gridftp-server-control-5.1-1.1.osg34.el6  htcondor-ce-3.0.2-1.osg34.el6  myproxy-6.1.28-1.1.osg34.el6  osg-ca-scripts-1.1.7-1.osg34.el6  osg-configure-2.2.0-1.osg34.el6  osg-oasis-8-1.osg34.el6  osg-test-1.11.2-1.osg34.el6  osg-version-3.4.3-1.osg34.el6  singularity-2.3.1-0.1.4.osg34.el6  xrootd-4.7.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7", 
            "text": "blahp-1.18.33.bosco-1.osg34.el7  cvmfs-2.4.1-1.osg34.el7  globus-ftp-client-8.36-1.1.osg34.el7  globus-gridftp-osg-extensions-0.4-1.osg34.el7  globus-gridftp-server-12.2-1.1.osg34.el7  globus-gridftp-server-control-5.1-1.1.osg34.el7  htcondor-ce-3.0.2-1.osg34.el7  myproxy-6.1.28-1.1.osg34.el7  osg-ca-scripts-1.1.7-1.osg34.el7  osg-configure-2.2.0-1.osg34.el7  osg-oasis-8-1.osg34.el7  osg-test-1.11.2-1.osg34.el7  osg-version-3.4.3-1.osg34.el7  singularity-2.3.1-0.1.4.osg34.el7  stashcache-0.8-1.osg34.el7  xrootd-4.7.0-1.osg34.el7  xrootd-lcmaps-1.3.4-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   globus - ftp - client   globus - ftp - client - debuginfo   globus - ftp - client - devel   globus - ftp - client - doc   globus - gridftp - osg - extensions   globus - gridftp - osg - extensions - debuginfo   globus - gridftp - server   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   myproxy   myproxy - admin   myproxy - debuginfo   myproxy - devel   myproxy - doc   myproxy - libs   myproxy - server   myproxy - voms   osg - ca - certs   osg - ca - scripts   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - oasis   osg - test   osg - test - log - viewer   osg - version   singularity   singularity - debuginfo   singularity - devel   singularity - runtime   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - fuse   xrootd - libs   xrootd - private - devel   xrootd - python   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 33 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 33 . bosco - 1 . osg34 . el6  cvmfs - 2 . 4 . 1 - 1 . osg34 . el6  cvmfs - devel - 2 . 4 . 1 - 1 . osg34 . el6  cvmfs - server - 2 . 4 . 1 - 1 . osg34 . el6  cvmfs - unittests - 2 . 4 . 1 - 1 . osg34 . el6  globus - ftp - client - 8 . 36 - 1 . 1 . osg34 . el6  globus - ftp - client - debuginfo - 8 . 36 - 1 . 1 . osg34 . el6  globus - ftp - client - devel - 8 . 36 - 1 . 1 . osg34 . el6  globus - ftp - client - doc - 8 . 36 - 1 . 1 . osg34 . el6  globus - gridftp - osg - extensions - 0 . 4 - 1 . osg34 . el6  globus - gridftp - osg - extensions - debuginfo - 0 . 4 - 1 . osg34 . el6  globus - gridftp - server - 12 . 2 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - 5 . 1 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - debuginfo - 5 . 1 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - devel - 5 . 1 - 1 . 1 . osg34 . el6  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 1 . osg34 . el6  globus - gridftp - server - devel - 12 . 2 - 1 . 1 . osg34 . el6  globus - gridftp - server - progs - 12 . 2 - 1 . 1 . osg34 . el6  htcondor - ce - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - client - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 0 . 2 - 1 . osg34 . el6  htcondor - ce - view - 3 . 0 . 2 - 1 . osg34 . el6  myproxy - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - admin - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - debuginfo - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - devel - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - doc - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - libs - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - server - 6 . 1 . 28 - 1 . 1 . osg34 . el6  myproxy - voms - 6 . 1 . 28 - 1 . 1 . osg34 . el6  osg - ca - scripts - 1 . 1 . 7 - 1 . osg34 . el6  osg - configure - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - bosco - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - ce - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - condor - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - gateway - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - gip - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - gratia - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - lsf - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - managedfork - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - misc - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - network - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - pbs - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - rsv - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - sge - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - slurm - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - squid - 2 . 2 . 0 - 1 . osg34 . el6  osg - configure - tests - 2 . 2 . 0 - 1 . osg34 . el6  osg - oasis - 8 - 1 . osg34 . el6  osg - test - 1 . 11 . 2 - 1 . osg34 . el6  osg - test - log - viewer - 1 . 11 . 2 - 1 . osg34 . el6  osg - version - 3 . 4 . 3 - 1 . osg34 . el6  singularity - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el6  singularity - debuginfo - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el6  singularity - devel - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el6  singularity - runtime - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el6  xrootd - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - client - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - client - devel - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - client - libs - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - devel - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - doc - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - fuse - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - libs - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - private - devel - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - python - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - selinux - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - server - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - server - devel - 4 . 7 . 0 - 1 . osg34 . el6  xrootd - server - libs - 4 . 7 . 0 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 33 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 33 . bosco - 1 . osg34 . el7  cvmfs - 2 . 4 . 1 - 1 . osg34 . el7  cvmfs - devel - 2 . 4 . 1 - 1 . osg34 . el7  cvmfs - server - 2 . 4 . 1 - 1 . osg34 . el7  cvmfs - unittests - 2 . 4 . 1 - 1 . osg34 . el7  globus - ftp - client - 8 . 36 - 1 . 1 . osg34 . el7  globus - ftp - client - debuginfo - 8 . 36 - 1 . 1 . osg34 . el7  globus - ftp - client - devel - 8 . 36 - 1 . 1 . osg34 . el7  globus - ftp - client - doc - 8 . 36 - 1 . 1 . osg34 . el7  globus - gridftp - osg - extensions - 0 . 4 - 1 . osg34 . el7  globus - gridftp - osg - extensions - debuginfo - 0 . 4 - 1 . osg34 . el7  globus - gridftp - server - 12 . 2 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - 5 . 1 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - debuginfo - 5 . 1 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - devel - 5 . 1 - 1 . 1 . osg34 . el7  globus - gridftp - server - debuginfo - 12 . 2 - 1 . 1 . osg34 . el7  globus - gridftp - server - devel - 12 . 2 - 1 . 1 . osg34 . el7  globus - gridftp - server - progs - 12 . 2 - 1 . 1 . osg34 . el7  htcondor - ce - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - client - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 0 . 2 - 1 . osg34 . el7  htcondor - ce - view - 3 . 0 . 2 - 1 . osg34 . el7  myproxy - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - admin - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - debuginfo - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - devel - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - doc - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - libs - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - server - 6 . 1 . 28 - 1 . 1 . osg34 . el7  myproxy - voms - 6 . 1 . 28 - 1 . 1 . osg34 . el7  osg - ca - scripts - 1 . 1 . 7 - 1 . osg34 . el7  osg - configure - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - bosco - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - ce - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - condor - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - gateway - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - gip - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - gratia - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - lsf - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - managedfork - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - misc - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - network - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - pbs - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - rsv - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - sge - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - slurm - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - squid - 2 . 2 . 0 - 1 . osg34 . el7  osg - configure - tests - 2 . 2 . 0 - 1 . osg34 . el7  osg - oasis - 8 - 1 . osg34 . el7  osg - test - 1 . 11 . 2 - 1 . osg34 . el7  osg - test - log - viewer - 1 . 11 . 2 - 1 . osg34 . el7  osg - version - 3 . 4 . 3 - 1 . osg34 . el7  singularity - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el7  singularity - debuginfo - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el7  singularity - devel - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el7  singularity - runtime - 2 . 3 . 1 - 0 . 1 . 4 . osg34 . el7  stashcache - 0 . 8 - 1 . osg34 . el7  stashcache - cache - server - 0 . 8 - 1 . osg34 . el7  stashcache - daemon - 0 . 8 - 1 . osg34 . el7  stashcache - origin - server - 0 . 8 - 1 . osg34 . el7  xrootd - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - client - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - client - devel - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - client - libs - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - devel - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - doc - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - fuse - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - lcmaps - 1 . 3 . 4 - 1 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 3 . 4 - 1 . osg34 . el7  xrootd - libs - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - private - devel - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - python - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - selinux - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - server - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - server - devel - 4 . 7 . 0 - 1 . osg34 . el7  xrootd - server - libs - 4 . 7 . 0 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_2", 
            "text": "blahp-1.18.33.bosco-1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_2", 
            "text": "blahp-1.18.33.bosco-1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 33 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 33 . bosco - 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-3/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 33 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 33 . bosco - 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.2-2\n\n\nRelease Date\n: 2017-08-14\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.85\n\n\nUpdated URL domain information for CyGrid (CY)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.85-1.osg34.el6\n\n\nosg-ca-certs-1.65-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.85-1.osg34.el7\n\n\nosg-ca-certs-1.65-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n85\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n65\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n85\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n65\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.2-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#osg-software-stack-data-release-342-2", 
            "text": "Release Date : 2017-08-14", 
            "title": "OSG Software Stack -- Data Release -- 3.4.2-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.85  Updated URL domain information for CyGrid (CY)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.85-1.osg34.el6  osg-ca-certs-1.65-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.85-1.osg34.el7  osg-ca-certs-1.65-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 85 - 1 . osg34 . el6  osg - ca - certs - 1 . 65 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 85 - 1 . osg34 . el7  osg - ca - certs - 1 . 65 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/", 
            "text": "OSG Software Release 3.4.2\n\n\nRelease Date\n: 2017-08-08\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nHTCondor 8.6.5\n\n\nUpdated SELinux profile which is required on Red Hat 7.4\n\n\nFixed a memory leak that would cause the condor_collector to slowly grow\n\n\nFixed several issues that occur when IPv6 is in use\n\n\nFixed a bug where condor_rm rarely removed another one of the user's jobs\n\n\nFixed a bug with parallel universe jobs starting on partitionable slots\n\n\n\n\n\n\nHTCondor-CE\n\n\nAdded pilot payload auditing\n\n\nDo not hold running jobs with an expired proxy\n\n\nInitialize \n$SPOOL/ceview/vos\n directory at installation time so that the VO tab functions in CEView before any pilots are received\n\n\nDon't warn about not running osg-configure, if osg-configure is not installed\n\n\n\n\n\n\nDefault configuration improvements for condor-cron\n\n\nClarified how to override the \nCONDOR_IDS\n\n\nDisable the unused GSI authentication that was producing spurious log messages.\n\n\n\n\n\n\nSeveral improvements to osg-configure\n\n\nEnsure that GUMS is configured before trying to query the GUMS server\n\n\nProgress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.\n\n\nosg-configure will issue same warnings and errors with -v option as it does with the -c option.\n\n\n\n\n\n\nAdded blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.\n\n\nReorganize the osg-ce packages \nSOFTWARE-2768\n\n\nUpcoming: patched HTCondor 8.7.2\n\n\nUpdated SELinux profile which is required on Red Hat 7.4\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the August release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.32.bosco-1.osg34.el6\n\n\ncondor-8.6.5-2.osg34.el6\n\n\ncondor-cron-1.1.3-1.osg34.el6\n\n\nhtcondor-ce-3.0.1-1.osg34.el6\n\n\nosg-ce-3.4-3.osg34.el6\n\n\nosg-configure-2.1.1-1.osg34.el6\n\n\nosg-test-1.11.1-1.osg34.el6\n\n\nosg-tested-internal-3.4-4.osg34.el6\n\n\nosg-version-3.4.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.32.bosco-1.osg34.el7\n\n\ncondor-8.6.5-2.osg34.el7\n\n\ncondor-cron-1.1.3-1.osg34.el7\n\n\nhtcondor-ce-3.0.1-1.osg34.el7\n\n\nosg-ce-3.4-3.osg34.el7\n\n\nosg-configure-2.1.1-1.osg34.el7\n\n\nosg-test-1.11.1-1.osg34.el7\n\n\nosg-tested-internal-3.4-4.osg34.el7\n\n\nosg-version-3.4.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ncron\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nce\n \nosg\n-\nce\n-\nbosco\n \nosg\n-\nce\n-\ncondor\n \nosg\n-\nce\n-\nlsf\n \nosg\n-\nce\n-\npbs\n \nosg\n-\nce\n-\nsge\n \nosg\n-\nce\n-\nslurm\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ntest\n \nosg\n-\ntested\n-\ninternal\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n5\n-\n2\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n3\n.\n0\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n4\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.32.bosco-1.osgup.el6\n\n\ncondor-8.7.2-2.1.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.32.bosco-1.osgup.el7\n\n\ncondor-8.7.2-2.1.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n32\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n2\n.\n1\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#osg-software-release-342", 
            "text": "Release Date : 2017-08-08", 
            "title": "OSG Software Release 3.4.2"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#summary-of-changes", 
            "text": "This release contains:   HTCondor 8.6.5  Updated SELinux profile which is required on Red Hat 7.4  Fixed a memory leak that would cause the condor_collector to slowly grow  Fixed several issues that occur when IPv6 is in use  Fixed a bug where condor_rm rarely removed another one of the user's jobs  Fixed a bug with parallel universe jobs starting on partitionable slots    HTCondor-CE  Added pilot payload auditing  Do not hold running jobs with an expired proxy  Initialize  $SPOOL/ceview/vos  directory at installation time so that the VO tab functions in CEView before any pilots are received  Don't warn about not running osg-configure, if osg-configure is not installed    Default configuration improvements for condor-cron  Clarified how to override the  CONDOR_IDS  Disable the unused GSI authentication that was producing spurious log messages.    Several improvements to osg-configure  Ensure that GUMS is configured before trying to query the GUMS server  Progress updates (such as informing when an operation is expected to take a while) during the user VO file validation are presented to the user rather than being sent to the log file.  osg-configure will issue same warnings and errors with -v option as it does with the -c option.    Added blahp configuration option for PBS Pro. This option must be used when the disambiguation code does not correctly determine which PBS is in use.  Reorganize the osg-ce packages  SOFTWARE-2768  Upcoming: patched HTCondor 8.7.2  Updated SELinux profile which is required on Red Hat 7.4     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the August release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#update-repositories", 
            "text": "To update to this series, you need to to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6", 
            "text": "blahp-1.18.32.bosco-1.osg34.el6  condor-8.6.5-2.osg34.el6  condor-cron-1.1.3-1.osg34.el6  htcondor-ce-3.0.1-1.osg34.el6  osg-ce-3.4-3.osg34.el6  osg-configure-2.1.1-1.osg34.el6  osg-test-1.11.1-1.osg34.el6  osg-tested-internal-3.4-4.osg34.el6  osg-version-3.4.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7", 
            "text": "blahp-1.18.32.bosco-1.osg34.el7  condor-8.6.5-2.osg34.el7  condor-cron-1.1.3-1.osg34.el7  htcondor-ce-3.0.1-1.osg34.el7  osg-ce-3.4-3.osg34.el7  osg-configure-2.1.1-1.osg34.el7  osg-test-1.11.1-1.osg34.el7  osg-tested-internal-3.4-4.osg34.el7  osg-version-3.4.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - cron   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   osg - ca - certs   osg - ce   osg - ce - bosco   osg - ce - condor   osg - ce - lsf   osg - ce - pbs   osg - ce - sge   osg - ce - slurm   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - test   osg - tested - internal   osg - test - log - viewer   osg - version   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 32 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 32 . bosco - 1 . osg34 . el6  condor - 8 . 6 . 5 - 2 . osg34 . el6  condor - all - 8 . 6 . 5 - 2 . osg34 . el6  condor - bosco - 8 . 6 . 5 - 2 . osg34 . el6  condor - classads - 8 . 6 . 5 - 2 . osg34 . el6  condor - classads - devel - 8 . 6 . 5 - 2 . osg34 . el6  condor - cream - gahp - 8 . 6 . 5 - 2 . osg34 . el6  condor - cron - 1 . 1 . 3 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 5 - 2 . osg34 . el6  condor - kbdd - 8 . 6 . 5 - 2 . osg34 . el6  condor - procd - 8 . 6 . 5 - 2 . osg34 . el6  condor - python - 8 . 6 . 5 - 2 . osg34 . el6  condor - std - universe - 8 . 6 . 5 - 2 . osg34 . el6  condor - test - 8 . 6 . 5 - 2 . osg34 . el6  condor - vm - gahp - 8 . 6 . 5 - 2 . osg34 . el6  htcondor - ce - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - bosco - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - client - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - collector - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - condor - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - lsf - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - pbs - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - sge - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - slurm - 3 . 0 . 1 - 1 . osg34 . el6  htcondor - ce - view - 3 . 0 . 1 - 1 . osg34 . el6  osg - ce - 3 . 4 - 3 . osg34 . el6  osg - ce - bosco - 3 . 4 - 3 . osg34 . el6  osg - ce - condor - 3 . 4 - 3 . osg34 . el6  osg - ce - lsf - 3 . 4 - 3 . osg34 . el6  osg - ce - pbs - 3 . 4 - 3 . osg34 . el6  osg - ce - sge - 3 . 4 - 3 . osg34 . el6  osg - ce - slurm - 3 . 4 - 3 . osg34 . el6  osg - configure - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - bosco - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - ce - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - condor - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - gateway - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - gip - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - gratia - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - infoservices - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - lsf - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - managedfork - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - misc - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - network - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - pbs - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - rsv - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - sge - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - slurm - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - squid - 2 . 1 . 1 - 1 . osg34 . el6  osg - configure - tests - 2 . 1 . 1 - 1 . osg34 . el6  osg - test - 1 . 11 . 1 - 1 . osg34 . el6  osg - tested - internal - 3 . 4 - 4 . osg34 . el6  osg - test - log - viewer - 1 . 11 . 1 - 1 . osg34 . el6  osg - version - 3 . 4 . 2 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 32 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 32 . bosco - 1 . osg34 . el7  condor - 8 . 6 . 5 - 2 . osg34 . el7  condor - all - 8 . 6 . 5 - 2 . osg34 . el7  condor - bosco - 8 . 6 . 5 - 2 . osg34 . el7  condor - classads - 8 . 6 . 5 - 2 . osg34 . el7  condor - classads - devel - 8 . 6 . 5 - 2 . osg34 . el7  condor - cream - gahp - 8 . 6 . 5 - 2 . osg34 . el7  condor - cron - 1 . 1 . 3 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 5 - 2 . osg34 . el7  condor - kbdd - 8 . 6 . 5 - 2 . osg34 . el7  condor - procd - 8 . 6 . 5 - 2 . osg34 . el7  condor - python - 8 . 6 . 5 - 2 . osg34 . el7  condor - test - 8 . 6 . 5 - 2 . osg34 . el7  condor - vm - gahp - 8 . 6 . 5 - 2 . osg34 . el7  htcondor - ce - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - bosco - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - client - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - collector - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - condor - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - lsf - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - pbs - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - sge - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - slurm - 3 . 0 . 1 - 1 . osg34 . el7  htcondor - ce - view - 3 . 0 . 1 - 1 . osg34 . el7  osg - ce - 3 . 4 - 3 . osg34 . el7  osg - ce - bosco - 3 . 4 - 3 . osg34 . el7  osg - ce - condor - 3 . 4 - 3 . osg34 . el7  osg - ce - lsf - 3 . 4 - 3 . osg34 . el7  osg - ce - pbs - 3 . 4 - 3 . osg34 . el7  osg - ce - sge - 3 . 4 - 3 . osg34 . el7  osg - ce - slurm - 3 . 4 - 3 . osg34 . el7  osg - configure - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - bosco - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - ce - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - condor - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - gateway - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - gip - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - gratia - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - infoservices - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - lsf - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - managedfork - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - misc - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - network - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - pbs - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - rsv - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - sge - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - slurm - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - squid - 2 . 1 . 1 - 1 . osg34 . el7  osg - configure - tests - 2 . 1 . 1 - 1 . osg34 . el7  osg - test - 1 . 11 . 1 - 1 . osg34 . el7  osg - tested - internal - 3 . 4 - 4 . osg34 . el7  osg - test - log - viewer - 1 . 11 . 1 - 1 . osg34 . el7  osg - version - 3 . 4 . 2 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_2", 
            "text": "blahp-1.18.32.bosco-1.osgup.el6  condor-8.7.2-2.1.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_2", 
            "text": "blahp-1.18.32.bosco-1.osgup.el7  condor-8.7.2-2.1.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 32 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 32 . bosco - 1 . osgup . el6  condor - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - all - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - bosco - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - classads - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - classads - devel - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - debuginfo - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - kbdd - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - procd - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - python - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - std - universe - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - test - 8 . 7 . 2 - 2 . 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 2 - 2 . 1 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-2/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 32 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 32 . bosco - 1 . osgup . el7  condor - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - all - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - bosco - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - classads - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - classads - devel - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - debuginfo - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - kbdd - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - procd - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - python - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - test - 8 . 7 . 2 - 2 . 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 2 - 2 . 1 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.1-2\n\n\nRelease Date\n: 2017-07-13\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.84\n\n\nUpdated ROSA root certificate with extended 20yr valitity (RO)\n\n\nUpdated contact details for CyGrid CA following transition to CYNET (CY)\n\n\nRemoved obsoleted KISTI-2007 trust anchor - replaced by KISTIv3 (KR)\n\n\nRemoved expiring LACGrid trust anchor a9082267 (BR)\n\n\nAdded UK Pathfinder AAAI CA 1 to unaccredited (misc) area (UK)\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.84-1.osg34.el6\n\n\nosg-ca-certs-1.64-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.84-1.osg34.el7\n\n\nosg-ca-certs-1.64-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n84\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n64\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n84\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n64\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.1-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#osg-software-stack-data-release-341-2", 
            "text": "Release Date : 2017-07-13", 
            "title": "OSG Software Stack -- Data Release -- 3.4.1-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.84  Updated ROSA root certificate with extended 20yr valitity (RO)  Updated contact details for CyGrid CA following transition to CYNET (CY)  Removed obsoleted KISTI-2007 trust anchor - replaced by KISTIv3 (KR)  Removed expiring LACGrid trust anchor a9082267 (BR)  Added UK Pathfinder AAAI CA 1 to unaccredited (misc) area (UK)     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.84-1.osg34.el6  osg-ca-certs-1.64-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.84-1.osg34.el7  osg-ca-certs-1.64-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 84 - 1 . osg34 . el6  osg - ca - certs - 1 . 64 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 84 - 1 . osg34 . el7  osg - ca - certs - 1 . 64 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/", 
            "text": "OSG Software Release 3.4.1\n\n\nRelease Date\n: 2017-07-12\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nBug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash\n\n\nosg-configure uses the new GUMS JSON interface\n\n\nThe BLAHP properly requests multi-core resources for Slurm batch systems\n\n\nHTCondor-CE 2.2.1\n\n\nFixed memory requirement requests to non-HTCondor batch systems\n\n\nCorrect CPU allocation for whole node jobs\n\n\n\n\n\n\nGratia probes\n\n\nsupport whole node jobs\n\n\ncan include arbitrary ClassAd attributes in Gratia usage records\n\n\n\n\n\n\nBug fix to CVMFS client to able to mount when large groups exist\n\n\nGridFTP server now uses correct configuration with a dsi plugin\n\n\ngridftp-dsi-posix replaces the xrootd-dsi plugin\n\n\nany local changes made to \n/etc/sysconfig/xrootd-dsi\n should be transferred over to \n/etc/sysconfig/gridftp-dsi-posix\n\n\n\n\n\n\nEnhanced gridftp-dsi-posix\n\n\nAdded MD5 checksum\n\n\nAdded GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens\n\n\n\n\n\n\nHTCondor 8.6.4\n: BOSCO now works without CA certificates on remote cluster\n\n\nHTCondor 8.7.2\n: introducing the 8.7 series in the upcoming repository\n\n\nRSV\n\n\nreplace software.grid.iu.edu with repo.grid.iu.edu\n\n\nparse condor_cron condor_q output properly\n\n\n\n\n\n\nosg-gridftp now pulls in osg-configure-misc\n\n\ncondor_cron: eliminate email on restart\n\n\nInternal tools\n\n\nosg-build update\n\n\nDrop unused tests from osg-test\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nKnown Issues\n\n\n\n\nBecause the Gratia system has been turned off, RSV emits the following warning: \nWARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details.\n. This warning can safely be ignored.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the August release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.30.bosco-1.osg34.el6\n\n\ncondor-8.6.4-1.osg34.el6\n\n\ncondor-cron-1.1.2-1.osg34.el6\n\n\ncvmfs-2.3.5-1.1.osg34.el6\n\n\nglobus-gridftp-server-11.8-1.3.osg34.el6\n\n\ngratia-probe-1.18.1-1.osg34.el6\n\n\ngridftp-dsi-posix-1.4-2.osg34.el6\n\n\nhtcondor-ce-2.2.1-1.osg34.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6\n\n\nosg-build-1.10.1-1.osg34.el6\n\n\nosg-configure-2.1.0-2.osg34.el6\n\n\nosg-gridftp-3.4-3.osg34.el6\n\n\nosg-test-1.11.0-1.osg34.el6\n\n\nosg-version-3.4.1-1.osg34.el6\n\n\nrsv-3.14.2-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.30.bosco-1.osg34.el7\n\n\ncondor-8.6.4-1.osg34.el7\n\n\ncondor-cron-1.1.2-1.osg34.el7\n\n\ncvmfs-2.3.5-1.1.osg34.el7\n\n\nglobus-gridftp-server-11.8-1.3.osg34.el7\n\n\ngratia-probe-1.18.1-1.osg34.el7\n\n\ngridftp-dsi-posix-1.4-2.osg34.el7\n\n\nhtcondor-ce-2.2.1-1.osg34.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7\n\n\nosg-build-1.10.1-1.osg34.el7\n\n\nosg-configure-2.1.0-2.osg34.el7\n\n\nosg-gridftp-3.4-3.osg34.el7\n\n\nosg-test-1.11.0-1.osg34.el7\n\n\nosg-version-3.4.1-1.osg34.el7\n\n\nrsv-3.14.2-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ncron\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglexec\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngram\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \ngridftp\n-\ndsi\n-\nposix\n \ngridftp\n-\ndsi\n-\nposix\n-\ndebuginfo\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ngridftp\n \nosg\n-\ntest\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nversion\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\nmetrics\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\ngridftp\n-\ndsi\n-\nposix\n-\n1\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\ngridftp\n-\ndsi\n-\nposix\n-\ndebuginfo\n-\n1\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n9\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n9\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n3\n.\n5\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n11\n.\n8\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n18\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\ngridftp\n-\ndsi\n-\nposix\n-\n1\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\ngridftp\n-\ndsi\n-\nposix\n-\ndebuginfo\n-\n1\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n2\n.\n2\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n9\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n9\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n1\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n1\n.\n11\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n11\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n14\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nblahp-1.18.30.bosco-1.osgup.el6\n\n\ncondor-8.7.2-1.osgup.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nblahp-1.18.30.bosco-1.osgup.el7\n\n\ncondor-8.7.2-1.osgup.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nblahp\n \nblahp\n-\ndebuginfo\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nannex\n-\nec2\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nblahp\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosgup\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nblahp\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n30\n.\nbosco\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nannex\n-\nec2\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n7\n.\n2\n-\n1\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosgup\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n4\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.1"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#osg-software-release-341", 
            "text": "Release Date : 2017-07-12", 
            "title": "OSG Software Release 3.4.1"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#summary-of-changes", 
            "text": "This release contains:   Bug fix in LCMAPS plugin that could cause the HTCondor-CE schedd to crash  osg-configure uses the new GUMS JSON interface  The BLAHP properly requests multi-core resources for Slurm batch systems  HTCondor-CE 2.2.1  Fixed memory requirement requests to non-HTCondor batch systems  Correct CPU allocation for whole node jobs    Gratia probes  support whole node jobs  can include arbitrary ClassAd attributes in Gratia usage records    Bug fix to CVMFS client to able to mount when large groups exist  GridFTP server now uses correct configuration with a dsi plugin  gridftp-dsi-posix replaces the xrootd-dsi plugin  any local changes made to  /etc/sysconfig/xrootd-dsi  should be transferred over to  /etc/sysconfig/gridftp-dsi-posix    Enhanced gridftp-dsi-posix  Added MD5 checksum  Added GRIDFTP_APPEND_XROOTD_CGI hook to support XRootD space tokens    HTCondor 8.6.4 : BOSCO now works without CA certificates on remote cluster  HTCondor 8.7.2 : introducing the 8.7 series in the upcoming repository  RSV  replace software.grid.iu.edu with repo.grid.iu.edu  parse condor_cron condor_q output properly    osg-gridftp now pulls in osg-configure-misc  condor_cron: eliminate email on restart  Internal tools  osg-build update  Drop unused tests from osg-test     These  JIRA tickets  were addressed in this release.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.    Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#known-issues", 
            "text": "Because the Gratia system has been turned off, RSV emits the following warning:  WARNING: Gratia server gratia-osg-prod.opensciencegrid.org:80 not responding to obtain last contact details. . This warning can safely be ignored.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the August release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#update-repositories", 
            "text": "To update to this series, you need to to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6", 
            "text": "blahp-1.18.30.bosco-1.osg34.el6  condor-8.6.4-1.osg34.el6  condor-cron-1.1.2-1.osg34.el6  cvmfs-2.3.5-1.1.osg34.el6  globus-gridftp-server-11.8-1.3.osg34.el6  gratia-probe-1.18.1-1.osg34.el6  gridftp-dsi-posix-1.4-2.osg34.el6  htcondor-ce-2.2.1-1.osg34.el6  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el6  osg-build-1.10.1-1.osg34.el6  osg-configure-2.1.0-2.osg34.el6  osg-gridftp-3.4-3.osg34.el6  osg-test-1.11.0-1.osg34.el6  osg-version-3.4.1-1.osg34.el6  rsv-3.14.2-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7", 
            "text": "blahp-1.18.30.bosco-1.osg34.el7  condor-8.6.4-1.osg34.el7  condor-cron-1.1.2-1.osg34.el7  cvmfs-2.3.5-1.1.osg34.el7  globus-gridftp-server-11.8-1.3.osg34.el7  gratia-probe-1.18.1-1.osg34.el7  gridftp-dsi-posix-1.4-2.osg34.el7  htcondor-ce-2.2.1-1.osg34.el7  lcmaps-plugins-verify-proxy-1.5.9-1.2.osg34.el7  osg-build-1.10.1-1.osg34.el7  osg-configure-2.1.0-2.osg34.el7  osg-gridftp-3.4-3.osg34.el7  osg-test-1.11.0-1.osg34.el7  osg-version-3.4.1-1.osg34.el7  rsv-3.14.2-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - cron   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs   cvmfs - devel   cvmfs - server   cvmfs - unittests   globus - gridftp - server   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   gratia - probe - bdii - status   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glexec   gratia - probe - glideinwms   gratia - probe - gram   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   gridftp - dsi - posix   gridftp - dsi - posix - debuginfo   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   lcmaps - plugins - verify - proxy   lcmaps - plugins - verify - proxy - debuginfo   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - certs   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - gridftp   osg - test   osg - test - log - viewer   osg - version   rsv   rsv - consumers   rsv - core   rsv - metrics   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_1", 
            "text": "blahp - 1 . 18 . 30 . bosco - 1 . osg34 . el6  blahp - debuginfo - 1 . 18 . 30 . bosco - 1 . osg34 . el6  condor - 8 . 6 . 4 - 1 . osg34 . el6  condor - all - 8 . 6 . 4 - 1 . osg34 . el6  condor - bosco - 8 . 6 . 4 - 1 . osg34 . el6  condor - classads - 8 . 6 . 4 - 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 4 - 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 4 - 1 . osg34 . el6  condor - cron - 1 . 1 . 2 - 1 . osg34 . el6  condor - debuginfo - 8 . 6 . 4 - 1 . osg34 . el6  condor - kbdd - 8 . 6 . 4 - 1 . osg34 . el6  condor - procd - 8 . 6 . 4 - 1 . osg34 . el6  condor - python - 8 . 6 . 4 - 1 . osg34 . el6  condor - std - universe - 8 . 6 . 4 - 1 . osg34 . el6  condor - test - 8 . 6 . 4 - 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 4 - 1 . osg34 . el6  cvmfs - 2 . 3 . 5 - 1 . 1 . osg34 . el6  cvmfs - devel - 2 . 3 . 5 - 1 . 1 . osg34 . el6  cvmfs - server - 2 . 3 . 5 - 1 . 1 . osg34 . el6  cvmfs - unittests - 2 . 3 . 5 - 1 . 1 . osg34 . el6  globus - gridftp - server - 11 . 8 - 1 . 3 . osg34 . el6  globus - gridftp - server - debuginfo - 11 . 8 - 1 . 3 . osg34 . el6  globus - gridftp - server - devel - 11 . 8 - 1 . 3 . osg34 . el6  globus - gridftp - server - progs - 11 . 8 - 1 . 3 . osg34 . el6  gratia - probe - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - bdii - status - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - common - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - condor - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - glexec - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - gram - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - metric - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - services - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - sge - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 18 . 1 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 18 . 1 - 1 . osg34 . el6  gridftp - dsi - posix - 1 . 4 - 2 . osg34 . el6  gridftp - dsi - posix - debuginfo - 1 . 4 - 2 . osg34 . el6  htcondor - ce - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - bosco - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - client - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - collector - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - condor - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - lsf - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - pbs - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - sge - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - slurm - 2 . 2 . 1 - 1 . osg34 . el6  htcondor - ce - view - 2 . 2 . 1 - 1 . osg34 . el6  lcmaps - plugins - verify - proxy - 1 . 5 . 9 - 1 . 2 . osg34 . el6  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 9 - 1 . 2 . osg34 . el6  osg - build - 1 . 10 . 1 - 1 . osg34 . el6  osg - build - base - 1 . 10 . 1 - 1 . osg34 . el6  osg - build - koji - 1 . 10 . 1 - 1 . osg34 . el6  osg - build - mock - 1 . 10 . 1 - 1 . osg34 . el6  osg - build - tests - 1 . 10 . 1 - 1 . osg34 . el6  osg - configure - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - bosco - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - ce - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - condor - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - gateway - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - gip - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - gratia - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - infoservices - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - lsf - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - managedfork - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - misc - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - network - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - pbs - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - rsv - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - sge - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - slurm - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - squid - 2 . 1 . 0 - 2 . osg34 . el6  osg - configure - tests - 2 . 1 . 0 - 2 . osg34 . el6  osg - gridftp - 3 . 4 - 3 . osg34 . el6  osg - test - 1 . 11 . 0 - 1 . osg34 . el6  osg - test - log - viewer - 1 . 11 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 1 - 1 . osg34 . el6  rsv - 3 . 14 . 2 - 1 . osg34 . el6  rsv - consumers - 3 . 14 . 2 - 1 . osg34 . el6  rsv - core - 3 . 14 . 2 - 1 . osg34 . el6  rsv - metrics - 3 . 14 . 2 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_1", 
            "text": "blahp - 1 . 18 . 30 . bosco - 1 . osg34 . el7  blahp - debuginfo - 1 . 18 . 30 . bosco - 1 . osg34 . el7  condor - 8 . 6 . 4 - 1 . osg34 . el7  condor - all - 8 . 6 . 4 - 1 . osg34 . el7  condor - bosco - 8 . 6 . 4 - 1 . osg34 . el7  condor - classads - 8 . 6 . 4 - 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 4 - 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 4 - 1 . osg34 . el7  condor - cron - 1 . 1 . 2 - 1 . osg34 . el7  condor - debuginfo - 8 . 6 . 4 - 1 . osg34 . el7  condor - kbdd - 8 . 6 . 4 - 1 . osg34 . el7  condor - procd - 8 . 6 . 4 - 1 . osg34 . el7  condor - python - 8 . 6 . 4 - 1 . osg34 . el7  condor - test - 8 . 6 . 4 - 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 4 - 1 . osg34 . el7  cvmfs - 2 . 3 . 5 - 1 . 1 . osg34 . el7  cvmfs - devel - 2 . 3 . 5 - 1 . 1 . osg34 . el7  cvmfs - server - 2 . 3 . 5 - 1 . 1 . osg34 . el7  cvmfs - unittests - 2 . 3 . 5 - 1 . 1 . osg34 . el7  globus - gridftp - server - 11 . 8 - 1 . 3 . osg34 . el7  globus - gridftp - server - debuginfo - 11 . 8 - 1 . 3 . osg34 . el7  globus - gridftp - server - devel - 11 . 8 - 1 . 3 . osg34 . el7  globus - gridftp - server - progs - 11 . 8 - 1 . 3 . osg34 . el7  gratia - probe - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - bdii - status - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - common - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - condor - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - glexec - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - gram - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - metric - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - services - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - sge - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 18 . 1 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 18 . 1 - 1 . osg34 . el7  gridftp - dsi - posix - 1 . 4 - 2 . osg34 . el7  gridftp - dsi - posix - debuginfo - 1 . 4 - 2 . osg34 . el7  htcondor - ce - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - bosco - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - client - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - collector - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - condor - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - lsf - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - pbs - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - sge - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - slurm - 2 . 2 . 1 - 1 . osg34 . el7  htcondor - ce - view - 2 . 2 . 1 - 1 . osg34 . el7  lcmaps - plugins - verify - proxy - 1 . 5 . 9 - 1 . 2 . osg34 . el7  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 9 - 1 . 2 . osg34 . el7  osg - build - 1 . 10 . 1 - 1 . osg34 . el7  osg - build - base - 1 . 10 . 1 - 1 . osg34 . el7  osg - build - koji - 1 . 10 . 1 - 1 . osg34 . el7  osg - build - mock - 1 . 10 . 1 - 1 . osg34 . el7  osg - build - tests - 1 . 10 . 1 - 1 . osg34 . el7  osg - configure - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - bosco - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - ce - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - condor - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - gateway - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - gip - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - gratia - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - infoservices - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - lsf - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - managedfork - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - misc - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - network - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - pbs - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - rsv - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - sge - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - slurm - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - squid - 2 . 1 . 0 - 2 . osg34 . el7  osg - configure - tests - 2 . 1 . 0 - 2 . osg34 . el7  osg - gridftp - 3 . 4 - 3 . osg34 . el7  osg - test - 1 . 11 . 0 - 1 . osg34 . el7  osg - test - log - viewer - 1 . 11 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 1 - 1 . osg34 . el7  rsv - 3 . 14 . 2 - 1 . osg34 . el7  rsv - consumers - 3 . 14 . 2 - 1 . osg34 . el7  rsv - core - 3 . 14 . 2 - 1 . osg34 . el7  rsv - metrics - 3 . 14 . 2 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_2", 
            "text": "blahp-1.18.30.bosco-1.osgup.el6  condor-8.7.2-1.osgup.el6  glite-ce-cream-client-api-c-1.15.4-2.4.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_2", 
            "text": "blahp-1.18.30.bosco-1.osgup.el7  condor-8.7.2-1.osgup.el7  glite-ce-cream-client-api-c-1.15.4-2.4.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  blahp   blahp - debuginfo   condor   condor - all   condor - annex - ec2   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-6_3", 
            "text": "blahp - 1 . 18 . 30 . bosco - 1 . osgup . el6  blahp - debuginfo - 1 . 18 . 30 . bosco - 1 . osgup . el6  condor - 8 . 7 . 2 - 1 . osgup . el6  condor - all - 8 . 7 . 2 - 1 . osgup . el6  condor - annex - ec2 - 8 . 7 . 2 - 1 . osgup . el6  condor - bosco - 8 . 7 . 2 - 1 . osgup . el6  condor - classads - 8 . 7 . 2 - 1 . osgup . el6  condor - classads - devel - 8 . 7 . 2 - 1 . osgup . el6  condor - cream - gahp - 8 . 7 . 2 - 1 . osgup . el6  condor - debuginfo - 8 . 7 . 2 - 1 . osgup . el6  condor - kbdd - 8 . 7 . 2 - 1 . osgup . el6  condor - procd - 8 . 7 . 2 - 1 . osgup . el6  condor - python - 8 . 7 . 2 - 1 . osgup . el6  condor - std - universe - 8 . 7 . 2 - 1 . osgup . el6  condor - test - 8 . 7 . 2 - 1 . osgup . el6  condor - vm - gahp - 8 . 7 . 2 - 1 . osgup . el6  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 4 . osgup . el6  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 4 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-1/#enterprise-linux-7_3", 
            "text": "blahp - 1 . 18 . 30 . bosco - 1 . osgup . el7  blahp - debuginfo - 1 . 18 . 30 . bosco - 1 . osgup . el7  condor - 8 . 7 . 2 - 1 . osgup . el7  condor - all - 8 . 7 . 2 - 1 . osgup . el7  condor - annex - ec2 - 8 . 7 . 2 - 1 . osgup . el7  condor - bosco - 8 . 7 . 2 - 1 . osgup . el7  condor - classads - 8 . 7 . 2 - 1 . osgup . el7  condor - classads - devel - 8 . 7 . 2 - 1 . osgup . el7  condor - cream - gahp - 8 . 7 . 2 - 1 . osgup . el7  condor - debuginfo - 8 . 7 . 2 - 1 . osgup . el7  condor - kbdd - 8 . 7 . 2 - 1 . osgup . el7  condor - procd - 8 . 7 . 2 - 1 . osgup . el7  condor - python - 8 . 7 . 2 - 1 . osgup . el7  condor - test - 8 . 7 . 2 - 1 . osgup . el7  condor - vm - gahp - 8 . 7 . 2 - 1 . osgup . el7  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 4 . osgup . el7  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 4 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/", 
            "text": "OSG Software Stack -- Data Release -- 3.4.0-2\n\n\nRelease Date\n: 2017-06-15\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nCA Certificates based on \nIGTF 1.83\n\n\nAdded new trust anchor for accredited KISTI CA v3 (KR)\n\n\nRemoved obsolete GEANT TCS G1 and G2 (old Comodo-backed) trust anchors:\n\n\nUTN-USERFirst-Hardware\n\n\nTERENA-eScience-SSL-CA\n\n\nAAACertificateServices\n\n\nUTNAAAClient\n\n\nTERENAeSciencePersonalCA\n\n\nUTN-USERTrust-RSA-CA\n\n\nTERENA-eScience-SSL-CA-2\n\n\nTERENAeSciencePersonalCA2 (EU)\n\n\n\n\n\n\n\n\n\n\nVO Package v74\n\n\nFix the edg-mkgridmap entries for project8 and miniclean\n\n\nAdd new VOMS entry for CIGI\n\n\nAdd LIGO entry to GUMS template\n\n\nFix vo-client ATLAS mappings\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nUpdating to the new release\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nigtf-ca-certs-1.83-1.osg34.el6\n\n\nosg-ca-certs-1.63-1.osg34.el6\n\n\nvo-client-74-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nigtf-ca-certs-1.83-1.osg34.el7\n\n\nosg-ca-certs-1.63-1.osg34.el7\n\n\nvo-client-74-1.osg34.el7\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nigtf\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\ngums\n-\nconfig\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n83\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n63\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n74\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n74\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n74\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n74\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n83\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n63\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n74\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n74\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n74\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n74\n-\n1\n.\nosg34\n.\nel7", 
            "title": "OSG Release 3.4.0-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#osg-software-stack-data-release-340-2", 
            "text": "Release Date : 2017-06-15", 
            "title": "OSG Software Stack -- Data Release -- 3.4.0-2"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#summary-of-changes", 
            "text": "This release contains:   CA Certificates based on  IGTF 1.83  Added new trust anchor for accredited KISTI CA v3 (KR)  Removed obsolete GEANT TCS G1 and G2 (old Comodo-backed) trust anchors:  UTN-USERFirst-Hardware  TERENA-eScience-SSL-CA  AAACertificateServices  UTNAAAClient  TERENAeSciencePersonalCA  UTN-USERTrust-RSA-CA  TERENA-eScience-SSL-CA-2  TERENAeSciencePersonalCA2 (EU)      VO Package v74  Fix the edg-mkgridmap entries for project8 and miniclean  Add new VOMS entry for CIGI  Add LIGO entry to GUMS template  Fix vo-client ATLAS mappings     These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#updating-to-the-new-release", 
            "text": "", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#update-repositories", 
            "text": "To update to this series, you need to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#update-software", 
            "text": "Once the repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6", 
            "text": "igtf-ca-certs-1.83-1.osg34.el6  osg-ca-certs-1.63-1.osg34.el6  vo-client-74-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7", 
            "text": "igtf-ca-certs-1.83-1.osg34.el7  osg-ca-certs-1.63-1.osg34.el7  vo-client-74-1.osg34.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  igtf - ca - certs   osg - ca - certs   osg - gums - config   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-6_1", 
            "text": "igtf - ca - certs - 1 . 83 - 1 . osg34 . el6  osg - ca - certs - 1 . 63 - 1 . osg34 . el6  osg - gums - config - 74 - 1 . osg34 . el6  vo - client - 74 - 1 . osg34 . el6  vo - client - edgmkgridmap - 74 - 1 . osg34 . el6  vo - client - lcmaps - voms - 74 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0-2/#enterprise-linux-7_1", 
            "text": "igtf - ca - certs - 1 . 83 - 1 . osg34 . el7  osg - ca - certs - 1 . 63 - 1 . osg34 . el7  osg - gums - config - 74 - 1 . osg34 . el7  vo - client - 74 - 1 . osg34 . el7  vo - client - edgmkgridmap - 74 - 1 . osg34 . el7  vo - client - lcmaps - voms - 74 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/", 
            "text": "OSG Software Release 3.4.0\n\n\nRelease Date\n: 2017-06-14\n\n\nWhat's New in OSG 3.4\n\n\nThe OSG 3.4.0 software stack features a more streamlined and consolidated package list. Specifically, the varied authentication solutions proved to be good candidates for consolidation and a new piece of software, the LCMAPS VOMS plugin, has been designed to replace both edg-mkgridmap and GUMS.\n\n\nSee \ninstall the LCMAPS VOMS plugin\n to replace GUMS + edg-mkgridmap.\n\n\nSee \nmigrating from edg-mkgridmap to lcmaps VOMS plugin\n to transistion from edg-mkgridmap.\n\n\nIn 3.4.0, we dropped HDFS 2.x with the intention of adding HDFS 3.x in a subsequent OSG 3.4 release when it becomes available upstream.\n\n\nIn addition to GUMS, edg-mkgridmap, and HDFS 2.x, we dropped packages related to the following software:\n\n\n\n\nVOMS Admin Server - \nRetirement Policy\n\n\nBeStMan - replaced by \nLoad Balanced GridFTP\n\n\nGLExec - replaced by \nSingularty\n\n\nGlobus GRAM available from EPEL\n\n\nGIP and OSG Info Services BDII servers retired\n\n\n\n\nThe aforementioned packages are still be available in OSG 3.3 and will receive regular support until December 2017 and security updates until June 2018 per our \nrelease policy\n. See \nthis section\n for the complete list of packages removed from OSG 3.4.\n\n\n\n\nNotes\n\n\n\n\nOSG 3.4 contains only 64-bit components.\n\n\nStashCache is supported on EL7 only.\n\n\nxrootd-lcmaps will remain at 1.2.1-2 on EL6.\n\n\n\n\n\n\nSummary of changes\n\n\nThis release contains:\n\n\n\n\nOSG 3.4.0\n\n\nHTCondor 8.6.3\n: See \nUpgrading from 8.4\n for additional information\n\n\nFrontier squid 3.5.24-3.1\n: See \nUpgrading from 2.x to 3.x\n for additional information\n\n\nUpdate to \nXRootD 4.6.1\n\n\nUpdate to xrootd-lcmaps 1.3.3 for EL7\n\n\n\n\n\n\nUpdate StashCache meta-packages to require XRootD 4.6.1\n\n\nUpdate to \nGlideinWMS 3.2.19\n\n\nMake the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS\n\n\nHTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system\n\n\nHTCondor 8.6.3\n: See \nUpgrading from 8.4\n for additional information\n\n\nOSG CE 3.4\n\n\nAdd vo-client-lcmaps-voms dependency\n\n\nRemove gridftp dependency\n\n\nDrop client tools\n\n\n\n\n\n\nAdd vo-client-lcmaps-voms dependency to osg-gridftp\n\n\nFix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM\n\n\nosg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)\n\n\nosg-configure 2.0.0\n\n\nreject empty \nallowed_vos\n in subclusters\n\n\nget default \nallowed_vos\n from LCMAPS VOMS plugin\n\n\nissue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present\n\n\ndrop 'RSV is not installed' warning\n\n\nremove configure-osg alias\n\n\ndeprecate GUMS support\n\n\ndisable GRAM configuration\n\n\ndrop managedfork and network modules\n\n\ndrop glexec support\n\n\nremove nonfunctional osg-cleanup\n\n\n\n\n\n\nDrop glexec and java from osg-wn-client\n\n\nBeSTMan 2 is no longer part of the OSG Software Stack\n\n\nGUMS is no longer part of the OSG Software Stack\n\n\nedg-mkgridmap in no longer part of the OSG Software Stack\n\n\nDrop bestman2 and globus*run RSV metrics\n\n\nosg-build 1.10.0\n\n\ndrop vdt-build alias\n\n\ndrop ~/.osg-build.ini configuration file\n\n\n\n\n\n\n\n\n\n\n\n\nThese \nJIRA tickets\n were addressed in this release.\n\n\nDetailed changes are below. All of the documentation can be found \nhere\n\n\nKnown Issues\n\n\n\n\nCurrently, OSG 3.4 CEs cannot be configured to authenticate via GUMS (\nSOFTWARE-2482\n). This issue is expected to be fixed in the July release.\n\n\nUsing the LCMAPS VOMS will result in a failing \"supported VO\" RSV test (\nSOFTWARE-2763\n). This can be ignored and a fix is targeted for the July release.\n\n\nIn GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.\n\n\n\n\nCOLLECTOR\n.\nUSE_SHARED_PORT\n=\nFalse\n\n\n\n\n\n\nUpdating to the new release\n\n\nTo update to the OSG 3.4 series, please consult the page on \nupdating between release series\n.\n\n\nUpdate Repositories\n\n\nTo update to this series, you need to to \ninstall the current OSG repositories\n.\n\n\nUpdate Software\n\n\nOnce the new repositories are installed, you can update to this new release with:\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNotes\n\n\n\n\nPlease be aware that running \nyum update\n may also update other RPMs. You can exclude packages from being updated using the \n--exclude=[package-name or glob]\n option for the \nyum\n command.\n\n\nWatch the yum update carefully for any messages about a \n.rpmnew\n file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a \n.rpmnew\n extension. You will need to merge any edits that have made into the \n.rpmnew\n file and then move the merged version into place (that is, without the \n.rpmnew\n extension). Watch especially for \n/etc/lcmaps.db\n, which every site is expected to edit.\n\n\n\n\n\n\nNeed help?\n\n\nDo you need help with this release? \nContact us for help\n.\n\n\nDetailed changes in this release\n\n\nPackages\n\n\nWe added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nautopyfactory-2.4.6-4.osg34.el6\n\n\nblahp-1.18.29.bosco-3.osg34.el6\n\n\nbwctl-1.4-7.osg34.el6\n\n\ncctools-4.4.3-1.osg34.el6\n\n\ncondor-8.6.3-1.1.osg34.el6\n\n\ncondor-cron-1.1.1-2.osg34.el6\n\n\ncvmfs-2.3.5-1.osg34.el6\n\n\ncvmfs-config-osg-2.0-2.osg34.el6\n\n\ncvmfs-x509-helper-1.0-1.osg34.el6\n\n\nfrontier-squid-3.5.24-3.1.osg34.el6\n\n\nglideinwms-3.2.19-2.osg34.el6\n\n\nglite-build-common-cpp-3.3.0.2-1.osg34.el6\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6\n\n\nglite-ce-wsdl-1.15.1-1.1.osg34.el6\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6\n\n\nglobus-ftp-client-8.29-1.1.osg34.el6\n\n\nglobus-gridftp-osg-extensions-0.3-2.osg34.el6\n\n\nglobus-gridftp-server-11.8-1.1.osg34.el6\n\n\nglobus-gridftp-server-control-4.1-1.3.osg34.el6\n\n\ngratia-probe-1.17.5-1.osg34.el6\n\n\ngsi-openssh-7.1p2f-1.2.osg34.el6\n\n\nhtcondor-ce-2.2.0-1.osg34.el6\n\n\nigtf-ca-certs-1.82-1.osg34.el6\n\n\njavascriptrrd-1.1.1-1.osg34.el6\n\n\nkoji-1.11.0-1.5.osg34.el6\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6\n\n\nlcmaps-1.6.6-1.6.osg34.el6\n\n\nlcmaps-plugins-basic-1.7.0-2.osg34.el6\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el6\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el6\n\n\nllrun-0.1.3-1.3.osg34.el6\n\n\nmash-0.5.22-3.osg34.el6\n\n\nmyproxy-6.1.18-1.4.osg34.el6\n\n\nnuttcp-6.1.2-1.osg34.el6\n\n\nosg-build-1.10.0-1.osg34.el6\n\n\nosg-ca-certs-1.62-1.osg34.el6\n\n\nosg-ca-certs-updater-1.4-1.osg34.el6\n\n\nosg-ca-generator-1.2.0-1.osg34.el6\n\n\nosg-ca-scripts-1.1.6-1.osg34.el6\n\n\nosg-ce-3.4-2.osg34.el6\n\n\nosg-configure-2.0.0-3.osg34.el6\n\n\nosg-control-1.1.0-1.osg34.el6\n\n\nosg-gridftp-3.4-2.osg34.el6\n\n\nosg-gridftp-xrootd-3.4-1.osg34.el6\n\n\nosg-oasis-7-9.osg34.el6\n\n\nosg-pki-tools-1.2.20-1.osg34.el6\n\n\nosg-system-profiler-1.4.0-1.osg34.el6\n\n\nosg-test-1.10.1-1.osg34.el6\n\n\nosg-tested-internal-3.4-2.osg34.el6\n\n\nosg-update-vos-1.4.0-1.osg34.el6\n\n\nosg-version-3.4.0-1.osg34.el6\n\n\nosg-vo-map-0.0.2-1.osg34.el6\n\n\nosg-wn-client-3.4-1.osg34.el6\n\n\nowamp-3.2rc4-2.osg34.el6\n\n\npegasus-4.7.4-1.1.osg34.el6\n\n\nrsv-3.14.0-2.osg34.el6\n\n\nrsv-gwms-tester-1.1.2-1.osg34.el6\n\n\nuberftp-2.8-2.1.osg34.el6\n\n\nvo-client-73-1.osg34.el6\n\n\nvoms-2.0.14-1.3.osg34.el6\n\n\nxacml-1.5.0-1.osg34.el6\n\n\nxrootd-4.6.1-1.osg34.el6\n\n\nxrootd-dsi-3.0.4-22.osg34.el6\n\n\nxrootd-lcmaps-1.2.1-2.osg34.el6\n\n\nxrootd-voms-plugin-0.4.0-1.osg34.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nautopyfactory-2.4.6-4.osg34.el7\n\n\nblahp-1.18.29.bosco-3.osg34.el7\n\n\nbwctl-1.4-7.osg34.el7\n\n\ncctools-4.4.3-1.osg34.el7\n\n\ncondor-8.6.3-1.1.osg34.el7\n\n\ncondor-cron-1.1.1-2.osg34.el7\n\n\ncvmfs-2.3.5-1.osg34.el7\n\n\ncvmfs-config-osg-2.0-2.osg34.el7\n\n\ncvmfs-x509-helper-1.0-1.osg34.el7\n\n\nfrontier-squid-3.5.24-3.1.osg34.el7\n\n\nglideinwms-3.2.19-2.osg34.el7\n\n\nglite-build-common-cpp-3.3.0.2-1.osg34.el7\n\n\nglite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7\n\n\nglite-ce-wsdl-1.15.1-1.1.osg34.el7\n\n\nglite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7\n\n\nglobus-ftp-client-8.29-1.1.osg34.el7\n\n\nglobus-gridftp-osg-extensions-0.3-2.osg34.el7\n\n\nglobus-gridftp-server-11.8-1.1.osg34.el7\n\n\nglobus-gridftp-server-control-4.1-1.3.osg34.el7\n\n\ngratia-probe-1.17.5-1.osg34.el7\n\n\ngsi-openssh-7.1p2f-1.2.osg34.el7\n\n\nhtcondor-ce-2.2.0-1.osg34.el7\n\n\nigtf-ca-certs-1.82-1.osg34.el7\n\n\njavascriptrrd-1.1.1-1.osg34.el7\n\n\nkoji-1.11.0-1.5.osg34.el7\n\n\nlcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7\n\n\nlcmaps-1.6.6-1.6.osg34.el7\n\n\nlcmaps-plugins-basic-1.7.0-2.osg34.el7\n\n\nlcmaps-plugins-scas-client-0.5.6-1.osg34.el7\n\n\nlcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7\n\n\nlcmaps-plugins-voms-1.7.1-1.4.osg34.el7\n\n\nllrun-0.1.3-1.3.osg34.el7\n\n\nmash-0.5.22-3.osg34.el7\n\n\nmyproxy-6.1.18-1.4.osg34.el7\n\n\nnuttcp-6.1.2-1.osg34.el7\n\n\nosg-build-1.10.0-1.osg34.el7\n\n\nosg-ca-certs-1.62-1.osg34.el7\n\n\nosg-ca-certs-updater-1.4-1.osg34.el7\n\n\nosg-ca-generator-1.2.0-1.osg34.el7\n\n\nosg-ca-scripts-1.1.6-1.osg34.el7\n\n\nosg-ce-3.4-2.osg34.el7\n\n\nosg-configure-2.0.0-3.osg34.el7\n\n\nosg-control-1.1.0-1.osg34.el7\n\n\nosg-gridftp-3.4-2.osg34.el7\n\n\nosg-gridftp-xrootd-3.4-1.osg34.el7\n\n\nosg-oasis-7-9.osg34.el7\n\n\nosg-pki-tools-1.2.20-1.osg34.el7\n\n\nosg-system-profiler-1.4.0-1.osg34.el7\n\n\nosg-test-1.10.1-1.osg34.el7\n\n\nosg-tested-internal-3.4-2.osg34.el7\n\n\nosg-update-vos-1.4.0-1.osg34.el7\n\n\nosg-version-3.4.0-1.osg34.el7\n\n\nosg-vo-map-0.0.2-1.osg34.el7\n\n\nosg-wn-client-3.4-1.osg34.el7\n\n\nowamp-3.2rc4-2.osg34.el7\n\n\npegasus-4.7.4-1.1.osg34.el7\n\n\nrsv-3.14.0-2.osg34.el7\n\n\nrsv-gwms-tester-1.1.2-1.osg34.el7\n\n\nstashcache-0.7-2.osg34.el7\n\n\nuberftp-2.8-2.1.osg34.el7\n\n\nvo-client-73-1.osg34.el7\n\n\nvoms-2.0.14-1.3.osg34.el7\n\n\nxacml-1.5.0-1.osg34.el7\n\n\nxrootd-4.6.1-1.osg34.el7\n\n\nxrootd-dsi-3.0.4-22.osg34.el7\n\n\nxrootd-lcmaps-1.3.3-3.osg34.el7\n\n\nxrootd-voms-plugin-0.4.0-1.osg34.el7\n\n\n\n\n#PackagesRemoved\n\n\nPackages Removed from OSG 3.4\n\n\nMany packages that were available in OSG 3.3 will not be included in the OSG 3.4 software stack. The following packages are not in the 3.4 repositories.\n\n\nEnterprise Linux 6\n\n\nbestman2\n\n\nbigtop\n-\njsvc\n\n\nbigtop\n-\nutils\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n\n\ncilogon\n-\nosg\n-\nca\n-\ncert\n\n\ncog\n-\njglobus\n-\naxis\n\n\nedg\n-\nmkgridmap\n\n\nemi\n-\ntrustmanager\n\n\nemi\n-\ntrustmanager\n-\naxis\n\n\nemi\n-\ntrustmanager\n-\ntomcat\n\n\ngip\n\n\nglexec\n\n\nglexec\n-\nwrapper\n-\nscripts\n\n\nglite\n-\nce\n-\ncream\n-\nutils\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngss\n\n\nglobus\n-\nauthz\n\n\nglobus\n-\nauthz\n-\ncallout\n-\nerror\n\n\nglobus\n-\ncallout\n\n\nglobus\n-\ncommon\n\n\nglobus\n-\nftp\n-\ncontrol\n\n\nglobus\n-\ngass\n-\ncache\n\n\nglobus\n-\ngass\n-\ncache\n-\nprogram\n\n\nglobus\n-\ngass\n-\ncopy\n\n\nglobus\n-\ngass\n-\nserver\n-\nez\n\n\nglobus\n-\ngass\n-\ntransfer\n\n\nglobus\n-\ngatekeeper\n\n\nglobus\n-\ngfork\n\n\nglobus\n-\ngram\n-\naudit\n\n\nglobus\n-\ngram\n-\nclient\n\n\nglobus\n-\ngram\n-\nclient\n-\ntools\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\ncallout\n-\nerror\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\ncondor\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nfork\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nlsf\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nmanagedfork\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\npbs\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nscripts\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nsge\n\n\nglobus\n-\ngram\n-\nprotocol\n\n\nglobus\n-\ngridmap\n-\ncallout\n-\nerror\n\n\nglobus\n-\ngsi\n-\ncallback\n\n\nglobus\n-\ngsi\n-\ncert\n-\nutils\n\n\nglobus\n-\ngsi\n-\ncredential\n\n\nglobus\n-\ngsi\n-\nopenssl\n-\nerror\n\n\nglobus\n-\ngsi\n-\nproxy\n-\ncore\n\n\nglobus\n-\ngsi\n-\nproxy\n-\nssl\n\n\nglobus\n-\ngsi\n-\nsysconfig\n\n\nglobus\n-\ngssapi\n-\nerror\n\n\nglobus\n-\ngssapi\n-\ngsi\n\n\nglobus\n-\ngss\n-\nassist\n\n\nglobus\n-\nio\n\n\nglobus\n-\nopenssl\n-\nmodule\n\n\nglobus\n-\nproxy\n-\nutils\n\n\nglobus\n-\nrsl\n\n\nglobus\n-\nscheduler\n-\nevent\n-\ngenerator\n\n\nglobus\n-\nsimple\n-\nca\n\n\nglobus\n-\nusage\n\n\nglobus\n-\nxio\n\n\nglobus\n-\nxio\n-\ngsi\n-\ndriver\n\n\nglobus\n-\nxioperf\n\n\nglobus\n-\nxio\n-\npipe\n-\ndriver\n\n\nglobus\n-\nxio\n-\npopen\n-\ndriver\n\n\nglobus\n-\nxio\n-\nudt\n-\ndriver\n\n\ngratia\n\n\ngratia\n-\nreporting\n-\nemail\n\n\ngridftp\n-\nhdfs\n\n\ngums\n\n\nhadoop\n\n\nI2util\n\n\njetty\n\n\njglobus\n\n\njoda\n-\ntime\n\n\nlcmaps\n-\nplugins\n-\nglexec\n-\ntracking\n\n\nlcmaps\n-\nplugins\n-\ngums\n-\nclient\n\n\nlcmaps\n-\nplugins\n-\nmount\n-\nunder\n-\nscratch\n\n\nlcmaps\n-\nplugins\n-\nprocess\n-\ntracking\n\n\nmkgltempdir\n\n\nndt\n\n\nnetlogger\n\n\nosg\n-\ncert\n-\nscripts\n\n\nosg\n-\ncleanup\n\n\nosg\n-\ngridftp\n-\nhdfs\n\n\nosg\n-\ngums\n\n\nosg\n-\ninfo\n-\nservices\n\n\nosg\n-\njava7\n-\ncompat\n\n\nosg\n-\nrelease\n\n\nosg\n-\nrelease\n-\nitb\n\n\nosg\n-\nse\n-\nbestman\n\n\nosg\n-\nse\n-\nbestman\n-\nxrootd\n\n\nosg\n-\nse\n-\nhadoop\n\n\nosg\n-\nvoms\n\n\nosg\n-\nwebapp\n-\ncommon\n\n\nprivilege\n-\nxacml\n\n\nrsv\n-\nvo\n-\ngwms\n\n\nstashcache\n\n\nstashcache\n-\ndaemon\n\n\nvoms\n-\nadmin\n-\nclient\n\n\nvoms\n-\nadmin\n-\nserver\n\n\nvoms\n-\napi\n-\njava\n\n\nvoms\n-\nmysql\n-\nplugin\n\n\nweb100_userland\n\n\nxrootd\n-\nhdfs\n\n\nxrootd\n-\nstatus\n-\nprobe\n\n\nzookeeper\n\n\n\n\n\n\nEnterprise Linux 7\n\n\naxis\n\n\nbestman2\n\n\nbigtop\n-\njsvc\n\n\nbigtop\n-\nutils\n\n\ncilogon\n-\nopenid\n-\nca\n-\ncert\n\n\ncilogon\n-\nosg\n-\nca\n-\ncert\n\n\ncog\n-\njglobus\n-\naxis\n\n\nedg\n-\nmkgridmap\n\n\nemi\n-\ntrustmanager\n\n\nemi\n-\ntrustmanager\n-\naxis\n\n\nemi\n-\ntrustmanager\n-\ntomcat\n\n\ngip\n\n\nglexec\n\n\nglexec\n-\nwrapper\n-\nscripts\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngss\n\n\nglobus\n-\nauthz\n\n\nglobus\n-\nauthz\n-\ncallout\n-\nerror\n\n\nglobus\n-\ncallout\n\n\nglobus\n-\ncommon\n\n\nglobus\n-\nftp\n-\ncontrol\n\n\nglobus\n-\ngass\n-\ncache\n\n\nglobus\n-\ngass\n-\ncache\n-\nprogram\n\n\nglobus\n-\ngass\n-\ncopy\n\n\nglobus\n-\ngass\n-\nserver\n-\nez\n\n\nglobus\n-\ngass\n-\ntransfer\n\n\nglobus\n-\ngatekeeper\n\n\nglobus\n-\ngfork\n\n\nglobus\n-\ngram\n-\naudit\n\n\nglobus\n-\ngram\n-\nclient\n\n\nglobus\n-\ngram\n-\nclient\n-\ntools\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\ncallout\n-\nerror\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\ncondor\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nfork\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nlsf\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nmanagedfork\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\npbs\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nscripts\n\n\nglobus\n-\ngram\n-\njob\n-\nmanager\n-\nsge\n\n\nglobus\n-\ngram\n-\nprotocol\n\n\nglobus\n-\ngridmap\n-\ncallout\n-\nerror\n\n\nglobus\n-\ngsi\n-\ncallback\n\n\nglobus\n-\ngsi\n-\ncert\n-\nutils\n\n\nglobus\n-\ngsi\n-\ncredential\n\n\nglobus\n-\ngsi\n-\nopenssl\n-\nerror\n\n\nglobus\n-\ngsi\n-\nproxy\n-\ncore\n\n\nglobus\n-\ngsi\n-\nproxy\n-\nssl\n\n\nglobus\n-\ngsi\n-\nsysconfig\n\n\nglobus\n-\ngssapi\n-\nerror\n\n\nglobus\n-\ngssapi\n-\ngsi\n\n\nglobus\n-\ngss\n-\nassist\n\n\nglobus\n-\nio\n\n\nglobus\n-\nopenssl\n-\nmodule\n\n\nglobus\n-\nproxy\n-\nutils\n\n\nglobus\n-\nrsl\n\n\nglobus\n-\nscheduler\n-\nevent\n-\ngenerator\n\n\nglobus\n-\nsimple\n-\nca\n\n\nglobus\n-\nusage\n\n\nglobus\n-\nxio\n\n\nglobus\n-\nxio\n-\ngsi\n-\ndriver\n\n\nglobus\n-\nxioperf\n\n\nglobus\n-\nxio\n-\npipe\n-\ndriver\n\n\nglobus\n-\nxio\n-\npopen\n-\ndriver\n\n\nglobus\n-\nxio\n-\nudt\n-\ndriver\n\n\ngratia\n\n\ngratia\n-\nreporting\n-\nemail\n\n\ngridftp\n-\nhdfs\n\n\ngums\n\n\nhadoop\n\n\nI2util\n\n\njavamail\n\n\njetty\n\n\njglobus\n\n\njoda\n-\ntime\n\n\nlcmaps\n-\nplugins\n-\nglexec\n-\ntracking\n\n\nlcmaps\n-\nplugins\n-\ngums\n-\nclient\n\n\nlcmaps\n-\nplugins\n-\nmount\n-\nunder\n-\nscratch\n\n\nlcmaps\n-\nplugins\n-\nprocess\n-\ntracking\n\n\nmkgltempdir\n\n\nndt\n\n\nnetlogger\n\n\nosg\n-\ncert\n-\nscripts\n\n\nosg\n-\ncleanup\n\n\nosg\n-\ngridftp\n-\nhdfs\n\n\nosg\n-\ngums\n\n\nosg\n-\ninfo\n-\nservices\n\n\nosg\n-\njava7\n-\ncompat\n\n\nosg\n-\nrelease\n\n\nosg\n-\nrelease\n-\nitb\n\n\nosg\n-\nse\n-\nbestman\n\n\nosg\n-\nse\n-\nbestman\n-\nxrootd\n\n\nosg\n-\nse\n-\nhadoop\n\n\nosg\n-\nvoms\n\n\nosg\n-\nwebapp\n-\ncommon\n\n\nprivilege\n-\nxacml\n\n\npython\n-\nZSI\n\n\nPyXML\n\n\nrsv\n-\nvo\n-\ngwms\n\n\nstashcache\n-\ndaemon\n\n\nvoms\n-\nadmin\n-\nclient\n\n\nvoms\n-\nmysql\n-\nplugin\n\n\nweb100_userland\n\n\nwsdl4j\n\n\nxrootd\n-\nhdfs\n\n\nxrootd\n-\nstatus\n-\nprobe\n\n\nzookeeper\n\n\n\n\n\n\nRPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nautopyfactory\n-\ncloud\n \nautopyfactory\n-\ncommon\n \nautopyfactory\n-\npanda\n \nautopyfactory\n-\nplugins\n-\ncloud\n \nautopyfactory\n-\nplugins\n-\nlocal\n \nautopyfactory\n-\nplugins\n-\nmonitor\n \nautopyfactory\n-\nplugins\n-\npanda\n \nautopyfactory\n-\nplugins\n-\nremote\n \nautopyfactory\n-\nplugins\n-\nscheds\n \nautopyfactory\n-\nproxymanager\n \nautopyfactory\n-\nremote\n \nautopyfactory\n-\nwms\n \nblahp\n \nblahp\n-\ndebuginfo\n \nbwctl\n \nbwctl\n-\nclient\n \nbwctl\n-\ndebuginfo\n \nbwctl\n-\ndevel\n \nbwctl\n-\nserver\n \ncctools\n-\nchirp\n \ncctools\n-\ndebuginfo\n \ncctools\n-\ndoc\n \ncctools\n-\ndttools\n \ncctools\n-\nmakeflow\n \ncctools\n-\nparrot\n \ncctools\n-\nresource_monitor\n \ncctools\n-\nsand\n \ncctools\n-\nwavefront\n \ncctools\n-\nweaver\n \ncctools\n-\nwork_queue\n \ncondor\n \ncondor\n-\nall\n \ncondor\n-\nbosco\n \ncondor\n-\nclassads\n \ncondor\n-\nclassads\n-\ndevel\n \ncondor\n-\ncream\n-\ngahp\n \ncondor\n-\ncron\n \ncondor\n-\ndebuginfo\n \ncondor\n-\nkbdd\n \ncondor\n-\nprocd\n \ncondor\n-\npython\n \ncondor\n-\nstd\n-\nuniverse\n \ncondor\n-\ntest\n \ncondor\n-\nvm\n-\ngahp\n \ncvmfs\n \ncvmfs\n-\nconfig\n-\nosg\n \ncvmfs\n-\ndevel\n \ncvmfs\n-\nserver\n \ncvmfs\n-\nunittests\n \ncvmfs\n-\nx509\n-\nhelper\n \ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n \nfrontier\n-\nsquid\n \nfrontier\n-\nsquid\n-\ndebuginfo\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n \nglite\n-\nbuild\n-\ncommon\n-\ncpp\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n \nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n \nglite\n-\nce\n-\nwsdl\n \nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n \nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndebuginfo\n \nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n \nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n \nglobus\n-\nftp\n-\nclient\n-\ndevel\n \nglobus\n-\nftp\n-\nclient\n-\ndoc\n \nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n \nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n \nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n \nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n \ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n \ngratia\n-\nprobe\n-\ncommon\n \ngratia\n-\nprobe\n-\ncondor\n \ngratia\n-\nprobe\n-\ncondor\n-\nevents\n \ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n \ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n \ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n \ngratia\n-\nprobe\n-\ndebuginfo\n \ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n \ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n \ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n \ngratia\n-\nprobe\n-\nglexec\n \ngratia\n-\nprobe\n-\nglideinwms\n \ngratia\n-\nprobe\n-\ngram\n \ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n \ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n \ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n \ngratia\n-\nprobe\n-\nlsf\n \ngratia\n-\nprobe\n-\nmetric\n \ngratia\n-\nprobe\n-\nonevm\n \ngratia\n-\nprobe\n-\npbs\n-\nlsf\n \ngratia\n-\nprobe\n-\nservices\n \ngratia\n-\nprobe\n-\nsge\n \ngratia\n-\nprobe\n-\nslurm\n \ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n \ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n \ngsi\n-\nopenssh\n \ngsi\n-\nopenssh\n-\nclients\n \ngsi\n-\nopenssh\n-\ndebuginfo\n \ngsi\n-\nopenssh\n-\nserver\n \nhtcondor\n-\nce\n \nhtcondor\n-\nce\n-\nbosco\n \nhtcondor\n-\nce\n-\nclient\n \nhtcondor\n-\nce\n-\ncollector\n \nhtcondor\n-\nce\n-\ncondor\n \nhtcondor\n-\nce\n-\nlsf\n \nhtcondor\n-\nce\n-\npbs\n \nhtcondor\n-\nce\n-\nsge\n \nhtcondor\n-\nce\n-\nslurm\n \nhtcondor\n-\nce\n-\nview\n \nigtf\n-\nca\n-\ncerts\n \njavascriptrrd\n \nkoji\n \nkoji\n-\nbuilder\n \nkoji\n-\nhub\n \nkoji\n-\nhub\n-\nplugins\n \nkoji\n-\nutils\n \nkoji\n-\nvm\n \nkoji\n-\nweb\n \nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n \nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n-\ndebuginfo\n \nlcmaps\n \nlcmaps\n-\ncommon\n-\ndevel\n \nlcmaps\n-\ndb\n-\ntemplates\n \nlcmaps\n-\ndebuginfo\n \nlcmaps\n-\ndevel\n \nlcmaps\n-\nplugins\n-\nbasic\n \nlcmaps\n-\nplugins\n-\nbasic\n-\ndebuginfo\n \nlcmaps\n-\nplugins\n-\nbasic\n-\nldap\n \nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n \nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n-\ndebuginfo\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n \nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n \nlcmaps\n-\nplugins\n-\nvoms\n \nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n \nlcmaps\n-\nwithout\n-\ngsi\n \nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n \nllrun\n \nllrun\n-\ndebuginfo\n \nmash\n \nmyproxy\n \nmyproxy\n-\nadmin\n \nmyproxy\n-\ndebuginfo\n \nmyproxy\n-\ndevel\n \nmyproxy\n-\ndoc\n \nmyproxy\n-\nlibs\n \nmyproxy\n-\nserver\n \nmyproxy\n-\nvoms\n \nnuttcp\n \nnuttcp\n-\ndebuginfo\n \nosg\n-\nbase\n-\nce\n \nosg\n-\nbase\n-\nce\n-\nbosco\n \nosg\n-\nbase\n-\nce\n-\ncondor\n \nosg\n-\nbase\n-\nce\n-\nlsf\n \nosg\n-\nbase\n-\nce\n-\npbs\n \nosg\n-\nbase\n-\nce\n-\nsge\n \nosg\n-\nbase\n-\nce\n-\nslurm\n \nosg\n-\nbuild\n \nosg\n-\nbuild\n-\nbase\n \nosg\n-\nbuild\n-\nkoji\n \nosg\n-\nbuild\n-\nmock\n \nosg\n-\nbuild\n-\ntests\n \nosg\n-\nca\n-\ncerts\n \nosg\n-\nca\n-\ncerts\n-\nupdater\n \nosg\n-\nca\n-\ngenerator\n \nosg\n-\nca\n-\nscripts\n \nosg\n-\nce\n \nosg\n-\nce\n-\nbosco\n \nosg\n-\nce\n-\ncondor\n \nosg\n-\nce\n-\nlsf\n \nosg\n-\nce\n-\npbs\n \nosg\n-\nce\n-\nsge\n \nosg\n-\nce\n-\nslurm\n \nosg\n-\nconfigure\n \nosg\n-\nconfigure\n-\nbosco\n \nosg\n-\nconfigure\n-\nce\n \nosg\n-\nconfigure\n-\ncondor\n \nosg\n-\nconfigure\n-\ngateway\n \nosg\n-\nconfigure\n-\ngip\n \nosg\n-\nconfigure\n-\ngratia\n \nosg\n-\nconfigure\n-\ninfoservices\n \nosg\n-\nconfigure\n-\nlsf\n \nosg\n-\nconfigure\n-\nmanagedfork\n \nosg\n-\nconfigure\n-\nmisc\n \nosg\n-\nconfigure\n-\nnetwork\n \nosg\n-\nconfigure\n-\npbs\n \nosg\n-\nconfigure\n-\nrsv\n \nosg\n-\nconfigure\n-\nsge\n \nosg\n-\nconfigure\n-\nslurm\n \nosg\n-\nconfigure\n-\nsquid\n \nosg\n-\nconfigure\n-\ntests\n \nosg\n-\ncontrol\n \nosg\n-\ngridftp\n \nosg\n-\ngridftp\n-\nxrootd\n \nosg\n-\ngums\n-\nconfig\n \nosg\n-\nhtcondor\n-\nce\n \nosg\n-\nhtcondor\n-\nce\n-\nbosco\n \nosg\n-\nhtcondor\n-\nce\n-\ncondor\n \nosg\n-\nhtcondor\n-\nce\n-\nlsf\n \nosg\n-\nhtcondor\n-\nce\n-\npbs\n \nosg\n-\nhtcondor\n-\nce\n-\nsge\n \nosg\n-\nhtcondor\n-\nce\n-\nslurm\n \nosg\n-\noasis\n \nosg\n-\npki\n-\ntools\n \nosg\n-\npki\n-\ntools\n-\ntests\n \nosg\n-\nrelease\n \nosg\n-\nrelease\n-\nitb\n \nosg\n-\nsystem\n-\nprofiler\n \nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n \nosg\n-\ntest\n \nosg\n-\ntested\n-\ninternal\n \nosg\n-\ntest\n-\nlog\n-\nviewer\n \nosg\n-\nupdate\n-\ndata\n \nosg\n-\nupdate\n-\nvos\n \nosg\n-\nversion\n \nosg\n-\nvo\n-\nmap\n \nosg\n-\nwn\n-\nclient\n \nowamp\n \nowamp\n-\nclient\n \nowamp\n-\ndebuginfo\n \nowamp\n-\nserver\n \npegasus\n \npegasus\n-\ndebuginfo\n \nrsv\n \nrsv\n-\nconsumers\n \nrsv\n-\ncore\n \nrsv\n-\ngwms\n-\ntester\n \nrsv\n-\nmetrics\n \nuberftp\n \nuberftp\n-\ndebuginfo\n \nvo\n-\nclient\n \nvo\n-\nclient\n-\nedgmkgridmap\n \nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n \nvoms\n \nvoms\n-\nclients\n-\ncpp\n \nvoms\n-\ndebuginfo\n \nvoms\n-\ndevel\n \nvoms\n-\ndoc\n \nvoms\n-\nserver\n \nxacml\n \nxacml\n-\ndebuginfo\n \nxacml\n-\ndevel\n \nxrootd\n \nxrootd\n-\nclient\n \nxrootd\n-\nclient\n-\ndevel\n \nxrootd\n-\nclient\n-\nlibs\n \nxrootd\n-\ndebuginfo\n \nxrootd\n-\ndevel\n \nxrootd\n-\ndoc\n \nxrootd\n-\ndsi\n \nxrootd\n-\ndsi\n-\ndebuginfo\n \nxrootd\n-\nfuse\n \nxrootd\n-\nlcmaps\n \nxrootd\n-\nlcmaps\n-\ndebuginfo\n \nxrootd\n-\nlibs\n \nxrootd\n-\nprivate\n-\ndevel\n \nxrootd\n-\npython\n \nxrootd\n-\nselinux\n \nxrootd\n-\nserver\n \nxrootd\n-\nserver\n-\ndevel\n \nxrootd\n-\nserver\n-\nlibs\n \nxrootd\n-\nvoms\n-\nplugin\n \nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n \nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nautopyfactory\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\ncloud\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\ncommon\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\npanda\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\ncloud\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nlocal\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nmonitor\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\npanda\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nremote\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nplugins\n-\nscheds\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nproxymanager\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nremote\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nautopyfactory\n-\nwms\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel6\n\n\nblahp\n-\n1\n.\n18\n.\n29\n.\nbosco\n-\n3\n.\nosg34\n.\nel6\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n29\n.\nbosco\n-\n3\n.\nosg34\n.\nel6\n\n\nbwctl\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nclient\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nbwctl\n-\ndevel\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\nbwctl\n-\nserver\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel6\n\n\ncctools\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nchirp\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndebuginfo\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndoc\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\ndttools\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nmakeflow\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nparrot\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nresource_monitor\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nsand\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nwavefront\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nweaver\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncctools\n-\nwork_queue\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n1\n-\n2\n.\nosg34\n.\nel6\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nstd\n-\nuniverse\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nconfig\n-\nosg\n-\n2\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nserver\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n24\n-\n3\n.\n1\n.\nosg34\n.\nel6\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n24\n-\n3\n.\n1\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel6\n\n\nglite\n-\nbuild\n-\ncommon\n-\ncpp\n-\n3\n.\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n3\n.\nosg34\n.\nel6\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n3\n.\nosg34\n.\nel6\n\n\nglite\n-\nce\n-\nwsdl\n-\n1\n.\n15\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndebuginfo\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndevel\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\n0\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n-\n0\n.\n3\n-\n2\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\nclients\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\ndebuginfo\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\ngsi\n-\nopenssh\n-\nserver\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nclient\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\npbs\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nsge\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nhtcondor\n-\nce\n-\nview\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n82\n-\n1\n.\nosg34\n.\nel6\n\n\njavascriptrrd\n-\n1\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nkoji\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel6\n\n\nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n-\n0\n.\n3\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n-\ndebuginfo\n-\n0\n.\n3\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ncommon\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndb\n-\ntemplates\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\nldap\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n-\n0\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n-\ndebuginfo\n-\n0\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel6\n\n\nllrun\n-\n0\n.\n1\n.\n3\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nllrun\n-\ndebuginfo\n-\n0\n.\n1\n.\n3\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nmash\n-\n0\n.\n5\n.\n22\n-\n3\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nserver\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel6\n\n\nnuttcp\n-\n6\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nnuttcp\n-\ndebuginfo\n-\n6\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbase\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n62\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\ngenerator\n-\n1\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n6\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel6\n\n\nosg\n-\ncontrol\n-\n1\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ngums\n-\nconfig\n-\n73\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\noasis\n-\n7\n-\n9\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\n1\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\npki\n-\ntools\n-\ntests\n-\n1\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel6\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nupdate\n-\ndata\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nupdate\n-\nvos\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nvo\n-\nmap\n-\n0\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n1\n.\nosg34\n.\nel6\n\n\nowamp\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel6\n\n\nowamp\n-\nclient\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel6\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel6\n\n\nowamp\n-\nserver\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel6\n\n\npegasus\n-\n4\n.\n7\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n7\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nrsv\n-\nconsumers\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nrsv\n-\ncore\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nrsv\n-\ngwms\n-\ntester\n-\n1\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel6\n\n\nrsv\n-\nmetrics\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel6\n\n\nuberftp\n-\n2\n.\n8\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nuberftp\n-\ndebuginfo\n-\n2\n.\n8\n-\n2\n.\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\n73\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n73\n-\n1\n.\nosg34\n.\nel6\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n73\n-\n1\n.\nosg34\n.\nel6\n\n\nvoms\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nvoms\n-\nclients\n-\ncpp\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndebuginfo\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndevel\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nvoms\n-\ndoc\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nvoms\n-\nserver\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel6\n\n\nxacml\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxacml\n-\ndebuginfo\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxacml\n-\ndevel\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndoc\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndsi\n-\n3\n.\n0\n.\n4\n-\n22\n.\nosg34\n.\nel6\n\n\nxrootd\n-\ndsi\n-\ndebuginfo\n-\n3\n.\n0\n.\n4\n-\n22\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nfuse\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n2\n.\n1\n-\n2\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n2\n.\n1\n-\n2\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\npython\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nselinux\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nautopyfactory\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\ncloud\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\ncommon\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\npanda\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\ncloud\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nlocal\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nmonitor\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\npanda\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nremote\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nplugins\n-\nscheds\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nproxymanager\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nremote\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nautopyfactory\n-\nwms\n-\n2\n.\n4\n.\n6\n-\n4\n.\nosg34\n.\nel7\n\n\nblahp\n-\n1\n.\n18\n.\n29\n.\nbosco\n-\n3\n.\nosg34\n.\nel7\n\n\nblahp\n-\ndebuginfo\n-\n1\n.\n18\n.\n29\n.\nbosco\n-\n3\n.\nosg34\n.\nel7\n\n\nbwctl\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nclient\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndebuginfo\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nbwctl\n-\ndevel\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\nbwctl\n-\nserver\n-\n1\n.\n4\n-\n7\n.\nosg34\n.\nel7\n\n\ncctools\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nchirp\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndebuginfo\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndoc\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\ndttools\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nmakeflow\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nparrot\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nresource_monitor\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nsand\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nwavefront\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nweaver\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncctools\n-\nwork_queue\n-\n4\n.\n4\n.\n3\n-\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nall\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nbosco\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nclassads\n-\ndevel\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncream\n-\ngahp\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ncron\n-\n1\n.\n1\n.\n1\n-\n2\n.\nosg34\n.\nel7\n\n\ncondor\n-\ndebuginfo\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nkbdd\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nprocd\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\npython\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\ntest\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncondor\n-\nvm\n-\ngahp\n-\n8\n.\n6\n.\n3\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nconfig\n-\nosg\n-\n2\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\ndevel\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nserver\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nunittests\n-\n2\n.\n3\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\ncvmfs\n-\nx509\n-\nhelper\n-\ndebuginfo\n-\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\n3\n.\n5\n.\n24\n-\n3\n.\n1\n.\nosg34\n.\nel7\n\n\nfrontier\n-\nsquid\n-\ndebuginfo\n-\n3\n.\n5\n.\n24\n-\n3\n.\n1\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n2\n.\n19\n-\n2\n.\nosg34\n.\nel7\n\n\nglite\n-\nbuild\n-\ncommon\n-\ncpp\n-\n3\n.\n3\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\napi\n-\nc\n-\n1\n.\n15\n.\n4\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\nglite\n-\nce\n-\ncream\n-\nclient\n-\ndevel\n-\n1\n.\n15\n.\n4\n-\n2\n.\n3\n.\nosg34\n.\nel7\n\n\nglite\n-\nce\n-\nwsdl\n-\n1\n.\n15\n.\n1\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndebuginfo\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglite\n-\nlbjp\n-\ncommon\n-\ngsoap\n-\nplugin\n-\ndevel\n-\n3\n.\n2\n.\n12\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndebuginfo\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndevel\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\nftp\n-\nclient\n-\ndoc\n-\n8\n.\n29\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\n0\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nosg\n-\nextensions\n-\ndebuginfo\n-\n0\n.\n3\n-\n2\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndebuginfo\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ncontrol\n-\ndevel\n-\n4\n.\n1\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndebuginfo\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\ndevel\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nglobus\n-\ngridftp\n-\nserver\n-\nprogs\n-\n11\n.\n8\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nbdii\n-\nstatus\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncommon\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ncondor\n-\nevents\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\nstoragegroup\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndcache\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ndebuginfo\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntapedrive\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nenstore\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglexec\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nglideinwms\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngram\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\ngridftp\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhadoop\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nhtcondor\n-\nce\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nlsf\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nmetric\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nonevm\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\npbs\n-\nlsf\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nservices\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nsge\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nslurm\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\nstorage\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngratia\n-\nprobe\n-\nxrootd\n-\ntransfer\n-\n1\n.\n17\n.\n5\n-\n1\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nclients\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\ndebuginfo\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\ngsi\n-\nopenssh\n-\nserver\n-\n7\n.\n1\np2f\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nbosco\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nclient\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncollector\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\ncondor\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nlsf\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\npbs\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nsge\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nslurm\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nhtcondor\n-\nce\n-\nview\n-\n2\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nigtf\n-\nca\n-\ncerts\n-\n1\n.\n82\n-\n1\n.\nosg34\n.\nel7\n\n\njavascriptrrd\n-\n1\n.\n1\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nkoji\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nbuilder\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nhub\n-\nplugins\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nutils\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nvm\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nkoji\n-\nweb\n-\n1\n.\n11\n.\n0\n-\n1\n.\n5\n.\nosg34\n.\nel7\n\n\nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n-\n0\n.\n3\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nlcas\n-\nlcmaps\n-\ngt4\n-\ninterface\n-\ndebuginfo\n-\n0\n.\n3\n.\n1\n-\n1\n.\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ncommon\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndb\n-\ntemplates\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndebuginfo\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\ndebuginfo\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nbasic\n-\nldap\n-\n1\n.\n7\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n-\n0\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nscas\n-\nclient\n-\ndebuginfo\n-\n0\n.\n5\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\n1\n.\n5\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nverify\n-\nproxy\n-\ndebuginfo\n-\n1\n.\n5\n.\n9\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\n1\n.\n7\n.\n1\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nplugins\n-\nvoms\n-\ndebuginfo\n-\n1\n.\n7\n.\n1\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nlcmaps\n-\nwithout\n-\ngsi\n-\ndevel\n-\n1\n.\n6\n.\n6\n-\n1\n.\n6\n.\nosg34\n.\nel7\n\n\nllrun\n-\n0\n.\n1\n.\n3\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nllrun\n-\ndebuginfo\n-\n0\n.\n1\n.\n3\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nmash\n-\n0\n.\n5\n.\n22\n-\n3\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nadmin\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndebuginfo\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndevel\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\ndoc\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nlibs\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nserver\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nmyproxy\n-\nvoms\n-\n6\n.\n1\n.\n18\n-\n1\n.\n4\n.\nosg34\n.\nel7\n\n\nnuttcp\n-\n6\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nnuttcp\n-\ndebuginfo\n-\n6\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbase\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nbase\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nkoji\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\nmock\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nbuild\n-\ntests\n-\n1\n.\n10\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\n1\n.\n62\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ncerts\n-\nupdater\n-\n1\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\ngenerator\n-\n1\n.\n2\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nca\n-\nscripts\n-\n1\n.\n1\n.\n6\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nbosco\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nce\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ncondor\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngateway\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngip\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ngratia\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ninfoservices\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nlsf\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmanagedfork\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nmisc\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nnetwork\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\npbs\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nrsv\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsge\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nslurm\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\nsquid\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\nconfigure\n-\ntests\n-\n2\n.\n0\n.\n0\n-\n3\n.\nosg34\n.\nel7\n\n\nosg\n-\ncontrol\n-\n1\n.\n1\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\ngridftp\n-\nxrootd\n-\n3\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ngums\n-\nconfig\n-\n73\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\nbosco\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\ncondor\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\nlsf\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\npbs\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\nsge\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\nhtcondor\n-\nce\n-\nslurm\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\noasis\n-\n7\n-\n9\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\n1\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\npki\n-\ntools\n-\ntests\n-\n1\n.\n2\n.\n20\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nsystem\n-\nprofiler\n-\nviewer\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\ntested\n-\ninternal\n-\n3\n.\n4\n-\n2\n.\nosg34\n.\nel7\n\n\nosg\n-\ntest\n-\nlog\n-\nviewer\n-\n1\n.\n10\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nupdate\n-\ndata\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nupdate\n-\nvos\n-\n1\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nversion\n-\n3\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nvo\n-\nmap\n-\n0\n.\n0\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nosg\n-\nwn\n-\nclient\n-\n3\n.\n4\n-\n1\n.\nosg34\n.\nel7\n\n\nowamp\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel7\n\n\nowamp\n-\nclient\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel7\n\n\nowamp\n-\ndebuginfo\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel7\n\n\nowamp\n-\nserver\n-\n3\n.\n2\nrc4\n-\n2\n.\nosg34\n.\nel7\n\n\npegasus\n-\n4\n.\n7\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\npegasus\n-\ndebuginfo\n-\n4\n.\n7\n.\n4\n-\n1\n.\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nrsv\n-\nconsumers\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nrsv\n-\ncore\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nrsv\n-\ngwms\n-\ntester\n-\n1\n.\n1\n.\n2\n-\n1\n.\nosg34\n.\nel7\n\n\nrsv\n-\nmetrics\n-\n3\n.\n14\n.\n0\n-\n2\n.\nosg34\n.\nel7\n\n\nstashcache\n-\n0\n.\n7\n-\n2\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ncache\n-\nserver\n-\n0\n.\n7\n-\n2\n.\nosg34\n.\nel7\n\n\nstashcache\n-\ndaemon\n-\n0\n.\n7\n-\n2\n.\nosg34\n.\nel7\n\n\nstashcache\n-\norigin\n-\nserver\n-\n0\n.\n7\n-\n2\n.\nosg34\n.\nel7\n\n\nuberftp\n-\n2\n.\n8\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nuberftp\n-\ndebuginfo\n-\n2\n.\n8\n-\n2\n.\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\n73\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nedgmkgridmap\n-\n73\n-\n1\n.\nosg34\n.\nel7\n\n\nvo\n-\nclient\n-\nlcmaps\n-\nvoms\n-\n73\n-\n1\n.\nosg34\n.\nel7\n\n\nvoms\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nvoms\n-\nclients\n-\ncpp\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndebuginfo\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndevel\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nvoms\n-\ndoc\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nvoms\n-\nserver\n-\n2\n.\n0\n.\n14\n-\n1\n.\n3\n.\nosg34\n.\nel7\n\n\nxacml\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxacml\n-\ndebuginfo\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxacml\n-\ndevel\n-\n1\n.\n5\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nclient\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndebuginfo\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndoc\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndsi\n-\n3\n.\n0\n.\n4\n-\n22\n.\nosg34\n.\nel7\n\n\nxrootd\n-\ndsi\n-\ndebuginfo\n-\n3\n.\n0\n.\n4\n-\n22\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nfuse\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\n1\n.\n3\n.\n3\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlcmaps\n-\ndebuginfo\n-\n1\n.\n3\n.\n3\n-\n3\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nprivate\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\npython\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nselinux\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\ndevel\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nserver\n-\nlibs\n-\n4\n.\n6\n.\n1\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndebuginfo\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\nxrootd\n-\nvoms\n-\nplugin\n-\ndevel\n-\n0\n.\n4\n.\n0\n-\n1\n.\nosg34\n.\nel7\n\n\n\n\n\n\nUpcoming Packages\n\n\nWe added or updated the following packages to the \nupcoming\n OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.\n\n\nEnterprise Linux 6\n\n\n\n\nglideinwms-3.3.2-2.osgup.el6\n\n\n\n\nEnterprise Linux 7\n\n\n\n\nglideinwms-3.3.2-2.osgup.el7\n\n\n\n\nUpcoming RPMs\n\n\nIf you wish to manually update your system, you can run yum update against the following packages:\n\n\nglideinwms\n \nglideinwms\n-\ncommon\n-\ntools\n \nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n \nglideinwms\n-\nfactory\n \nglideinwms\n-\nfactory\n-\ncondor\n \nglideinwms\n-\nglidecondor\n-\ntools\n \nglideinwms\n-\nlibs\n \nglideinwms\n-\nminimal\n-\ncondor\n \nglideinwms\n-\nusercollector\n \nglideinwms\n-\nuserschedd\n \nglideinwms\n-\nvofrontend\n \nglideinwms\n-\nvofrontend\n-\nstandalone\n\n\n\n\n\n\nIf you wish to only update the RPMs that changed, the set of RPMs is:\n\n\nEnterprise Linux 6\n\n\nglideinwms\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel6\n\n\n\n\n\n\nEnterprise Linux 7\n\n\nglideinwms\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncommon\n-\ntools\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\ncondor\n-\ncommon\n-\nconfig\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nfactory\n-\ncondor\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nglidecondor\n-\ntools\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nlibs\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nminimal\n-\ncondor\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nusercollector\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nuserschedd\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7\n\n\nglideinwms\n-\nvofrontend\n-\nstandalone\n-\n3\n.\n3\n.\n2\n-\n2\n.\nosgup\n.\nel7", 
            "title": "OSG Release 3.4.0"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#osg-software-release-340", 
            "text": "Release Date : 2017-06-14", 
            "title": "OSG Software Release 3.4.0"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#whats-new-in-osg-34", 
            "text": "The OSG 3.4.0 software stack features a more streamlined and consolidated package list. Specifically, the varied authentication solutions proved to be good candidates for consolidation and a new piece of software, the LCMAPS VOMS plugin, has been designed to replace both edg-mkgridmap and GUMS.  See  install the LCMAPS VOMS plugin  to replace GUMS + edg-mkgridmap.  See  migrating from edg-mkgridmap to lcmaps VOMS plugin  to transistion from edg-mkgridmap.  In 3.4.0, we dropped HDFS 2.x with the intention of adding HDFS 3.x in a subsequent OSG 3.4 release when it becomes available upstream.  In addition to GUMS, edg-mkgridmap, and HDFS 2.x, we dropped packages related to the following software:   VOMS Admin Server -  Retirement Policy  BeStMan - replaced by  Load Balanced GridFTP  GLExec - replaced by  Singularty  Globus GRAM available from EPEL  GIP and OSG Info Services BDII servers retired   The aforementioned packages are still be available in OSG 3.3 and will receive regular support until December 2017 and security updates until June 2018 per our  release policy . See  this section  for the complete list of packages removed from OSG 3.4.   Notes   OSG 3.4 contains only 64-bit components.  StashCache is supported on EL7 only.  xrootd-lcmaps will remain at 1.2.1-2 on EL6.", 
            "title": "What's New in OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#summary-of-changes", 
            "text": "This release contains:   OSG 3.4.0  HTCondor 8.6.3 : See  Upgrading from 8.4  for additional information  Frontier squid 3.5.24-3.1 : See  Upgrading from 2.x to 3.x  for additional information  Update to  XRootD 4.6.1  Update to xrootd-lcmaps 1.3.3 for EL7    Update StashCache meta-packages to require XRootD 4.6.1  Update to  GlideinWMS 3.2.19  Make the LCMAPS VOMS plugin consider only the first FQAN to be consistent with GUMS  HTCondor-CE: Add WholeNodeWanted ClassAd expression so jobs can request a whole node from the batch system  HTCondor 8.6.3 : See  Upgrading from 8.4  for additional information  OSG CE 3.4  Add vo-client-lcmaps-voms dependency  Remove gridftp dependency  Drop client tools    Add vo-client-lcmaps-voms dependency to osg-gridftp  Fix osg-update-vos script to clean yum cache in order pick up the latest vo-client RPM  osg-ca-scripts now refers to repo.grid.iu.edu (rather than the retired software.grid.iu.edu)  osg-configure 2.0.0  reject empty  allowed_vos  in subclusters  get default  allowed_vos  from LCMAPS VOMS plugin  issue warning (rather than error out) if OSG_APP or OSG_DATA directories are not present  drop 'RSV is not installed' warning  remove configure-osg alias  deprecate GUMS support  disable GRAM configuration  drop managedfork and network modules  drop glexec support  remove nonfunctional osg-cleanup    Drop glexec and java from osg-wn-client  BeSTMan 2 is no longer part of the OSG Software Stack  GUMS is no longer part of the OSG Software Stack  edg-mkgridmap in no longer part of the OSG Software Stack  Drop bestman2 and globus*run RSV metrics  osg-build 1.10.0  drop vdt-build alias  drop ~/.osg-build.ini configuration file       These  JIRA tickets  were addressed in this release.  Detailed changes are below. All of the documentation can be found  here", 
            "title": "Summary of changes"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#known-issues", 
            "text": "Currently, OSG 3.4 CEs cannot be configured to authenticate via GUMS ( SOFTWARE-2482 ). This issue is expected to be fixed in the July release.  Using the LCMAPS VOMS will result in a failing \"supported VO\" RSV test ( SOFTWARE-2763 ). This can be ignored and a fix is targeted for the July release.  In GlideinWMS, a small configuration change must be added to account for changes in HTCondor 8.6. Add the following line to the HTCondor configuration.   COLLECTOR . USE_SHARED_PORT = False", 
            "title": "Known Issues"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#updating-to-the-new-release", 
            "text": "To update to the OSG 3.4 series, please consult the page on  updating between release series .", 
            "title": "Updating to the new release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#update-repositories", 
            "text": "To update to this series, you need to to  install the current OSG repositories .", 
            "title": "Update Repositories"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#update-software", 
            "text": "Once the new repositories are installed, you can update to this new release with:  root@host #  yum update   Notes   Please be aware that running  yum update  may also update other RPMs. You can exclude packages from being updated using the  --exclude=[package-name or glob]  option for the  yum  command.  Watch the yum update carefully for any messages about a  .rpmnew  file being created. That means that a configuration file had been edited, and a new default version was to be installed. In that case, RPM does not overwrite the edited configuration file but instead installs the new version with a  .rpmnew  extension. You will need to merge any edits that have made into the  .rpmnew  file and then move the merged version into place (that is, without the  .rpmnew  extension). Watch especially for  /etc/lcmaps.db , which every site is expected to edit.", 
            "title": "Update Software"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#need-help", 
            "text": "Do you need help with this release?  Contact us for help .", 
            "title": "Need help?"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#detailed-changes-in-this-release", 
            "text": "", 
            "title": "Detailed changes in this release"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#packages", 
            "text": "We added or updated the following packages to the production OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6", 
            "text": "autopyfactory-2.4.6-4.osg34.el6  blahp-1.18.29.bosco-3.osg34.el6  bwctl-1.4-7.osg34.el6  cctools-4.4.3-1.osg34.el6  condor-8.6.3-1.1.osg34.el6  condor-cron-1.1.1-2.osg34.el6  cvmfs-2.3.5-1.osg34.el6  cvmfs-config-osg-2.0-2.osg34.el6  cvmfs-x509-helper-1.0-1.osg34.el6  frontier-squid-3.5.24-3.1.osg34.el6  glideinwms-3.2.19-2.osg34.el6  glite-build-common-cpp-3.3.0.2-1.osg34.el6  glite-ce-cream-client-api-c-1.15.4-2.3.osg34.el6  glite-ce-wsdl-1.15.1-1.1.osg34.el6  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el6  globus-ftp-client-8.29-1.1.osg34.el6  globus-gridftp-osg-extensions-0.3-2.osg34.el6  globus-gridftp-server-11.8-1.1.osg34.el6  globus-gridftp-server-control-4.1-1.3.osg34.el6  gratia-probe-1.17.5-1.osg34.el6  gsi-openssh-7.1p2f-1.2.osg34.el6  htcondor-ce-2.2.0-1.osg34.el6  igtf-ca-certs-1.82-1.osg34.el6  javascriptrrd-1.1.1-1.osg34.el6  koji-1.11.0-1.5.osg34.el6  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el6  lcmaps-1.6.6-1.6.osg34.el6  lcmaps-plugins-basic-1.7.0-2.osg34.el6  lcmaps-plugins-scas-client-0.5.6-1.osg34.el6  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el6  lcmaps-plugins-voms-1.7.1-1.4.osg34.el6  llrun-0.1.3-1.3.osg34.el6  mash-0.5.22-3.osg34.el6  myproxy-6.1.18-1.4.osg34.el6  nuttcp-6.1.2-1.osg34.el6  osg-build-1.10.0-1.osg34.el6  osg-ca-certs-1.62-1.osg34.el6  osg-ca-certs-updater-1.4-1.osg34.el6  osg-ca-generator-1.2.0-1.osg34.el6  osg-ca-scripts-1.1.6-1.osg34.el6  osg-ce-3.4-2.osg34.el6  osg-configure-2.0.0-3.osg34.el6  osg-control-1.1.0-1.osg34.el6  osg-gridftp-3.4-2.osg34.el6  osg-gridftp-xrootd-3.4-1.osg34.el6  osg-oasis-7-9.osg34.el6  osg-pki-tools-1.2.20-1.osg34.el6  osg-system-profiler-1.4.0-1.osg34.el6  osg-test-1.10.1-1.osg34.el6  osg-tested-internal-3.4-2.osg34.el6  osg-update-vos-1.4.0-1.osg34.el6  osg-version-3.4.0-1.osg34.el6  osg-vo-map-0.0.2-1.osg34.el6  osg-wn-client-3.4-1.osg34.el6  owamp-3.2rc4-2.osg34.el6  pegasus-4.7.4-1.1.osg34.el6  rsv-3.14.0-2.osg34.el6  rsv-gwms-tester-1.1.2-1.osg34.el6  uberftp-2.8-2.1.osg34.el6  vo-client-73-1.osg34.el6  voms-2.0.14-1.3.osg34.el6  xacml-1.5.0-1.osg34.el6  xrootd-4.6.1-1.osg34.el6  xrootd-dsi-3.0.4-22.osg34.el6  xrootd-lcmaps-1.2.1-2.osg34.el6  xrootd-voms-plugin-0.4.0-1.osg34.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7", 
            "text": "autopyfactory-2.4.6-4.osg34.el7  blahp-1.18.29.bosco-3.osg34.el7  bwctl-1.4-7.osg34.el7  cctools-4.4.3-1.osg34.el7  condor-8.6.3-1.1.osg34.el7  condor-cron-1.1.1-2.osg34.el7  cvmfs-2.3.5-1.osg34.el7  cvmfs-config-osg-2.0-2.osg34.el7  cvmfs-x509-helper-1.0-1.osg34.el7  frontier-squid-3.5.24-3.1.osg34.el7  glideinwms-3.2.19-2.osg34.el7  glite-build-common-cpp-3.3.0.2-1.osg34.el7  glite-ce-cream-client-api-c-1.15.4-2.3.osg34.el7  glite-ce-wsdl-1.15.1-1.1.osg34.el7  glite-lbjp-common-gsoap-plugin-3.2.12-1.1.osg34.el7  globus-ftp-client-8.29-1.1.osg34.el7  globus-gridftp-osg-extensions-0.3-2.osg34.el7  globus-gridftp-server-11.8-1.1.osg34.el7  globus-gridftp-server-control-4.1-1.3.osg34.el7  gratia-probe-1.17.5-1.osg34.el7  gsi-openssh-7.1p2f-1.2.osg34.el7  htcondor-ce-2.2.0-1.osg34.el7  igtf-ca-certs-1.82-1.osg34.el7  javascriptrrd-1.1.1-1.osg34.el7  koji-1.11.0-1.5.osg34.el7  lcas-lcmaps-gt4-interface-0.3.1-1.2.osg34.el7  lcmaps-1.6.6-1.6.osg34.el7  lcmaps-plugins-basic-1.7.0-2.osg34.el7  lcmaps-plugins-scas-client-0.5.6-1.osg34.el7  lcmaps-plugins-verify-proxy-1.5.9-1.1.osg34.el7  lcmaps-plugins-voms-1.7.1-1.4.osg34.el7  llrun-0.1.3-1.3.osg34.el7  mash-0.5.22-3.osg34.el7  myproxy-6.1.18-1.4.osg34.el7  nuttcp-6.1.2-1.osg34.el7  osg-build-1.10.0-1.osg34.el7  osg-ca-certs-1.62-1.osg34.el7  osg-ca-certs-updater-1.4-1.osg34.el7  osg-ca-generator-1.2.0-1.osg34.el7  osg-ca-scripts-1.1.6-1.osg34.el7  osg-ce-3.4-2.osg34.el7  osg-configure-2.0.0-3.osg34.el7  osg-control-1.1.0-1.osg34.el7  osg-gridftp-3.4-2.osg34.el7  osg-gridftp-xrootd-3.4-1.osg34.el7  osg-oasis-7-9.osg34.el7  osg-pki-tools-1.2.20-1.osg34.el7  osg-system-profiler-1.4.0-1.osg34.el7  osg-test-1.10.1-1.osg34.el7  osg-tested-internal-3.4-2.osg34.el7  osg-update-vos-1.4.0-1.osg34.el7  osg-version-3.4.0-1.osg34.el7  osg-vo-map-0.0.2-1.osg34.el7  osg-wn-client-3.4-1.osg34.el7  owamp-3.2rc4-2.osg34.el7  pegasus-4.7.4-1.1.osg34.el7  rsv-3.14.0-2.osg34.el7  rsv-gwms-tester-1.1.2-1.osg34.el7  stashcache-0.7-2.osg34.el7  uberftp-2.8-2.1.osg34.el7  vo-client-73-1.osg34.el7  voms-2.0.14-1.3.osg34.el7  xacml-1.5.0-1.osg34.el7  xrootd-4.6.1-1.osg34.el7  xrootd-dsi-3.0.4-22.osg34.el7  xrootd-lcmaps-1.3.3-3.osg34.el7  xrootd-voms-plugin-0.4.0-1.osg34.el7   #PackagesRemoved", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#packages-removed-from-osg-34", 
            "text": "Many packages that were available in OSG 3.3 will not be included in the OSG 3.4 software stack. The following packages are not in the 3.4 repositories.", 
            "title": "Packages Removed from OSG 3.4"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_1", 
            "text": "bestman2  bigtop - jsvc  bigtop - utils  cilogon - openid - ca - cert  cilogon - osg - ca - cert  cog - jglobus - axis  edg - mkgridmap  emi - trustmanager  emi - trustmanager - axis  emi - trustmanager - tomcat  gip  glexec  glexec - wrapper - scripts  glite - ce - cream - utils  glite - lbjp - common - gss  globus - authz  globus - authz - callout - error  globus - callout  globus - common  globus - ftp - control  globus - gass - cache  globus - gass - cache - program  globus - gass - copy  globus - gass - server - ez  globus - gass - transfer  globus - gatekeeper  globus - gfork  globus - gram - audit  globus - gram - client  globus - gram - client - tools  globus - gram - job - manager  globus - gram - job - manager - callout - error  globus - gram - job - manager - condor  globus - gram - job - manager - fork  globus - gram - job - manager - lsf  globus - gram - job - manager - managedfork  globus - gram - job - manager - pbs  globus - gram - job - manager - scripts  globus - gram - job - manager - sge  globus - gram - protocol  globus - gridmap - callout - error  globus - gsi - callback  globus - gsi - cert - utils  globus - gsi - credential  globus - gsi - openssl - error  globus - gsi - proxy - core  globus - gsi - proxy - ssl  globus - gsi - sysconfig  globus - gssapi - error  globus - gssapi - gsi  globus - gss - assist  globus - io  globus - openssl - module  globus - proxy - utils  globus - rsl  globus - scheduler - event - generator  globus - simple - ca  globus - usage  globus - xio  globus - xio - gsi - driver  globus - xioperf  globus - xio - pipe - driver  globus - xio - popen - driver  globus - xio - udt - driver  gratia  gratia - reporting - email  gridftp - hdfs  gums  hadoop  I2util  jetty  jglobus  joda - time  lcmaps - plugins - glexec - tracking  lcmaps - plugins - gums - client  lcmaps - plugins - mount - under - scratch  lcmaps - plugins - process - tracking  mkgltempdir  ndt  netlogger  osg - cert - scripts  osg - cleanup  osg - gridftp - hdfs  osg - gums  osg - info - services  osg - java7 - compat  osg - release  osg - release - itb  osg - se - bestman  osg - se - bestman - xrootd  osg - se - hadoop  osg - voms  osg - webapp - common  privilege - xacml  rsv - vo - gwms  stashcache  stashcache - daemon  voms - admin - client  voms - admin - server  voms - api - java  voms - mysql - plugin  web100_userland  xrootd - hdfs  xrootd - status - probe  zookeeper", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_1", 
            "text": "axis  bestman2  bigtop - jsvc  bigtop - utils  cilogon - openid - ca - cert  cilogon - osg - ca - cert  cog - jglobus - axis  edg - mkgridmap  emi - trustmanager  emi - trustmanager - axis  emi - trustmanager - tomcat  gip  glexec  glexec - wrapper - scripts  glite - lbjp - common - gss  globus - authz  globus - authz - callout - error  globus - callout  globus - common  globus - ftp - control  globus - gass - cache  globus - gass - cache - program  globus - gass - copy  globus - gass - server - ez  globus - gass - transfer  globus - gatekeeper  globus - gfork  globus - gram - audit  globus - gram - client  globus - gram - client - tools  globus - gram - job - manager  globus - gram - job - manager - callout - error  globus - gram - job - manager - condor  globus - gram - job - manager - fork  globus - gram - job - manager - lsf  globus - gram - job - manager - managedfork  globus - gram - job - manager - pbs  globus - gram - job - manager - scripts  globus - gram - job - manager - sge  globus - gram - protocol  globus - gridmap - callout - error  globus - gsi - callback  globus - gsi - cert - utils  globus - gsi - credential  globus - gsi - openssl - error  globus - gsi - proxy - core  globus - gsi - proxy - ssl  globus - gsi - sysconfig  globus - gssapi - error  globus - gssapi - gsi  globus - gss - assist  globus - io  globus - openssl - module  globus - proxy - utils  globus - rsl  globus - scheduler - event - generator  globus - simple - ca  globus - usage  globus - xio  globus - xio - gsi - driver  globus - xioperf  globus - xio - pipe - driver  globus - xio - popen - driver  globus - xio - udt - driver  gratia  gratia - reporting - email  gridftp - hdfs  gums  hadoop  I2util  javamail  jetty  jglobus  joda - time  lcmaps - plugins - glexec - tracking  lcmaps - plugins - gums - client  lcmaps - plugins - mount - under - scratch  lcmaps - plugins - process - tracking  mkgltempdir  ndt  netlogger  osg - cert - scripts  osg - cleanup  osg - gridftp - hdfs  osg - gums  osg - info - services  osg - java7 - compat  osg - release  osg - release - itb  osg - se - bestman  osg - se - bestman - xrootd  osg - se - hadoop  osg - voms  osg - webapp - common  privilege - xacml  python - ZSI  PyXML  rsv - vo - gwms  stashcache - daemon  voms - admin - client  voms - mysql - plugin  web100_userland  wsdl4j  xrootd - hdfs  xrootd - status - probe  zookeeper", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  autopyfactory - cloud   autopyfactory - common   autopyfactory - panda   autopyfactory - plugins - cloud   autopyfactory - plugins - local   autopyfactory - plugins - monitor   autopyfactory - plugins - panda   autopyfactory - plugins - remote   autopyfactory - plugins - scheds   autopyfactory - proxymanager   autopyfactory - remote   autopyfactory - wms   blahp   blahp - debuginfo   bwctl   bwctl - client   bwctl - debuginfo   bwctl - devel   bwctl - server   cctools - chirp   cctools - debuginfo   cctools - doc   cctools - dttools   cctools - makeflow   cctools - parrot   cctools - resource_monitor   cctools - sand   cctools - wavefront   cctools - weaver   cctools - work_queue   condor   condor - all   condor - bosco   condor - classads   condor - classads - devel   condor - cream - gahp   condor - cron   condor - debuginfo   condor - kbdd   condor - procd   condor - python   condor - std - universe   condor - test   condor - vm - gahp   cvmfs   cvmfs - config - osg   cvmfs - devel   cvmfs - server   cvmfs - unittests   cvmfs - x509 - helper   cvmfs - x509 - helper - debuginfo   frontier - squid   frontier - squid - debuginfo   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   glite - build - common - cpp   glite - ce - cream - client - api - c   glite - ce - cream - client - devel   glite - ce - wsdl   glite - lbjp - common - gsoap - plugin   glite - lbjp - common - gsoap - plugin - debuginfo   glite - lbjp - common - gsoap - plugin - devel   globus - ftp - client   globus - ftp - client - debuginfo   globus - ftp - client - devel   globus - ftp - client - doc   globus - gridftp - osg - extensions   globus - gridftp - osg - extensions - debuginfo   globus - gridftp - server   globus - gridftp - server - control   globus - gridftp - server - control - debuginfo   globus - gridftp - server - control - devel   globus - gridftp - server - debuginfo   globus - gridftp - server - devel   globus - gridftp - server - progs   gratia - probe - bdii - status   gratia - probe - common   gratia - probe - condor   gratia - probe - condor - events   gratia - probe - dcache - storage   gratia - probe - dcache - storagegroup   gratia - probe - dcache - transfer   gratia - probe - debuginfo   gratia - probe - enstore - storage   gratia - probe - enstore - tapedrive   gratia - probe - enstore - transfer   gratia - probe - glexec   gratia - probe - glideinwms   gratia - probe - gram   gratia - probe - gridftp - transfer   gratia - probe - hadoop - storage   gratia - probe - htcondor - ce   gratia - probe - lsf   gratia - probe - metric   gratia - probe - onevm   gratia - probe - pbs - lsf   gratia - probe - services   gratia - probe - sge   gratia - probe - slurm   gratia - probe - xrootd - storage   gratia - probe - xrootd - transfer   gsi - openssh   gsi - openssh - clients   gsi - openssh - debuginfo   gsi - openssh - server   htcondor - ce   htcondor - ce - bosco   htcondor - ce - client   htcondor - ce - collector   htcondor - ce - condor   htcondor - ce - lsf   htcondor - ce - pbs   htcondor - ce - sge   htcondor - ce - slurm   htcondor - ce - view   igtf - ca - certs   javascriptrrd   koji   koji - builder   koji - hub   koji - hub - plugins   koji - utils   koji - vm   koji - web   lcas - lcmaps - gt4 - interface   lcas - lcmaps - gt4 - interface - debuginfo   lcmaps   lcmaps - common - devel   lcmaps - db - templates   lcmaps - debuginfo   lcmaps - devel   lcmaps - plugins - basic   lcmaps - plugins - basic - debuginfo   lcmaps - plugins - basic - ldap   lcmaps - plugins - scas - client   lcmaps - plugins - scas - client - debuginfo   lcmaps - plugins - verify - proxy   lcmaps - plugins - verify - proxy - debuginfo   lcmaps - plugins - voms   lcmaps - plugins - voms - debuginfo   lcmaps - without - gsi   lcmaps - without - gsi - devel   llrun   llrun - debuginfo   mash   myproxy   myproxy - admin   myproxy - debuginfo   myproxy - devel   myproxy - doc   myproxy - libs   myproxy - server   myproxy - voms   nuttcp   nuttcp - debuginfo   osg - base - ce   osg - base - ce - bosco   osg - base - ce - condor   osg - base - ce - lsf   osg - base - ce - pbs   osg - base - ce - sge   osg - base - ce - slurm   osg - build   osg - build - base   osg - build - koji   osg - build - mock   osg - build - tests   osg - ca - certs   osg - ca - certs - updater   osg - ca - generator   osg - ca - scripts   osg - ce   osg - ce - bosco   osg - ce - condor   osg - ce - lsf   osg - ce - pbs   osg - ce - sge   osg - ce - slurm   osg - configure   osg - configure - bosco   osg - configure - ce   osg - configure - condor   osg - configure - gateway   osg - configure - gip   osg - configure - gratia   osg - configure - infoservices   osg - configure - lsf   osg - configure - managedfork   osg - configure - misc   osg - configure - network   osg - configure - pbs   osg - configure - rsv   osg - configure - sge   osg - configure - slurm   osg - configure - squid   osg - configure - tests   osg - control   osg - gridftp   osg - gridftp - xrootd   osg - gums - config   osg - htcondor - ce   osg - htcondor - ce - bosco   osg - htcondor - ce - condor   osg - htcondor - ce - lsf   osg - htcondor - ce - pbs   osg - htcondor - ce - sge   osg - htcondor - ce - slurm   osg - oasis   osg - pki - tools   osg - pki - tools - tests   osg - release   osg - release - itb   osg - system - profiler   osg - system - profiler - viewer   osg - test   osg - tested - internal   osg - test - log - viewer   osg - update - data   osg - update - vos   osg - version   osg - vo - map   osg - wn - client   owamp   owamp - client   owamp - debuginfo   owamp - server   pegasus   pegasus - debuginfo   rsv   rsv - consumers   rsv - core   rsv - gwms - tester   rsv - metrics   uberftp   uberftp - debuginfo   vo - client   vo - client - edgmkgridmap   vo - client - lcmaps - voms   voms   voms - clients - cpp   voms - debuginfo   voms - devel   voms - doc   voms - server   xacml   xacml - debuginfo   xacml - devel   xrootd   xrootd - client   xrootd - client - devel   xrootd - client - libs   xrootd - debuginfo   xrootd - devel   xrootd - doc   xrootd - dsi   xrootd - dsi - debuginfo   xrootd - fuse   xrootd - lcmaps   xrootd - lcmaps - debuginfo   xrootd - libs   xrootd - private - devel   xrootd - python   xrootd - selinux   xrootd - server   xrootd - server - devel   xrootd - server - libs   xrootd - voms - plugin   xrootd - voms - plugin - debuginfo   xrootd - voms - plugin - devel   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_2", 
            "text": "autopyfactory - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - cloud - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - common - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - panda - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - cloud - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - local - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - monitor - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - panda - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - remote - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - plugins - scheds - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - proxymanager - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - remote - 2 . 4 . 6 - 4 . osg34 . el6  autopyfactory - wms - 2 . 4 . 6 - 4 . osg34 . el6  blahp - 1 . 18 . 29 . bosco - 3 . osg34 . el6  blahp - debuginfo - 1 . 18 . 29 . bosco - 3 . osg34 . el6  bwctl - 1 . 4 - 7 . osg34 . el6  bwctl - client - 1 . 4 - 7 . osg34 . el6  bwctl - debuginfo - 1 . 4 - 7 . osg34 . el6  bwctl - devel - 1 . 4 - 7 . osg34 . el6  bwctl - server - 1 . 4 - 7 . osg34 . el6  cctools - 4 . 4 . 3 - 1 . osg34 . el6  cctools - chirp - 4 . 4 . 3 - 1 . osg34 . el6  cctools - debuginfo - 4 . 4 . 3 - 1 . osg34 . el6  cctools - doc - 4 . 4 . 3 - 1 . osg34 . el6  cctools - dttools - 4 . 4 . 3 - 1 . osg34 . el6  cctools - makeflow - 4 . 4 . 3 - 1 . osg34 . el6  cctools - parrot - 4 . 4 . 3 - 1 . osg34 . el6  cctools - resource_monitor - 4 . 4 . 3 - 1 . osg34 . el6  cctools - sand - 4 . 4 . 3 - 1 . osg34 . el6  cctools - wavefront - 4 . 4 . 3 - 1 . osg34 . el6  cctools - weaver - 4 . 4 . 3 - 1 . osg34 . el6  cctools - work_queue - 4 . 4 . 3 - 1 . osg34 . el6  condor - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - all - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - bosco - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - classads - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - classads - devel - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - cream - gahp - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - cron - 1 . 1 . 1 - 2 . osg34 . el6  condor - debuginfo - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - kbdd - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - procd - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - python - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - std - universe - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - test - 8 . 6 . 3 - 1 . 1 . osg34 . el6  condor - vm - gahp - 8 . 6 . 3 - 1 . 1 . osg34 . el6  cvmfs - 2 . 3 . 5 - 1 . osg34 . el6  cvmfs - config - osg - 2 . 0 - 2 . osg34 . el6  cvmfs - devel - 2 . 3 . 5 - 1 . osg34 . el6  cvmfs - server - 2 . 3 . 5 - 1 . osg34 . el6  cvmfs - unittests - 2 . 3 . 5 - 1 . osg34 . el6  cvmfs - x509 - helper - 1 . 0 - 1 . osg34 . el6  cvmfs - x509 - helper - debuginfo - 1 . 0 - 1 . osg34 . el6  frontier - squid - 3 . 5 . 24 - 3 . 1 . osg34 . el6  frontier - squid - debuginfo - 3 . 5 . 24 - 3 . 1 . osg34 . el6  glideinwms - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - common - tools - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - condor - common - config - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - factory - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - factory - condor - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - glidecondor - tools - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - libs - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - minimal - condor - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - usercollector - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - userschedd - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - vofrontend - 3 . 2 . 19 - 2 . osg34 . el6  glideinwms - vofrontend - standalone - 3 . 2 . 19 - 2 . osg34 . el6  glite - build - common - cpp - 3 . 3 . 0 . 2 - 1 . osg34 . el6  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 3 . osg34 . el6  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 3 . osg34 . el6  glite - ce - wsdl - 1 . 15 . 1 - 1 . 1 . osg34 . el6  glite - lbjp - common - gsoap - plugin - 3 . 2 . 12 - 1 . 1 . osg34 . el6  glite - lbjp - common - gsoap - plugin - debuginfo - 3 . 2 . 12 - 1 . 1 . osg34 . el6  glite - lbjp - common - gsoap - plugin - devel - 3 . 2 . 12 - 1 . 1 . osg34 . el6  globus - ftp - client - 8 . 29 - 1 . 1 . osg34 . el6  globus - ftp - client - debuginfo - 8 . 29 - 1 . 1 . osg34 . el6  globus - ftp - client - devel - 8 . 29 - 1 . 1 . osg34 . el6  globus - ftp - client - doc - 8 . 29 - 1 . 1 . osg34 . el6  globus - gridftp - osg - extensions - 0 . 3 - 2 . osg34 . el6  globus - gridftp - osg - extensions - debuginfo - 0 . 3 - 2 . osg34 . el6  globus - gridftp - server - 11 . 8 - 1 . 1 . osg34 . el6  globus - gridftp - server - control - 4 . 1 - 1 . 3 . osg34 . el6  globus - gridftp - server - control - debuginfo - 4 . 1 - 1 . 3 . osg34 . el6  globus - gridftp - server - control - devel - 4 . 1 - 1 . 3 . osg34 . el6  globus - gridftp - server - debuginfo - 11 . 8 - 1 . 1 . osg34 . el6  globus - gridftp - server - devel - 11 . 8 - 1 . 1 . osg34 . el6  globus - gridftp - server - progs - 11 . 8 - 1 . 1 . osg34 . el6  gratia - probe - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - bdii - status - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - common - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - condor - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - condor - events - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - dcache - storage - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - dcache - storagegroup - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - dcache - transfer - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - debuginfo - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - enstore - storage - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - enstore - tapedrive - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - enstore - transfer - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - glexec - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - glideinwms - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - gram - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - gridftp - transfer - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - hadoop - storage - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - htcondor - ce - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - lsf - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - metric - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - onevm - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - pbs - lsf - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - services - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - sge - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - slurm - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - xrootd - storage - 1 . 17 . 5 - 1 . osg34 . el6  gratia - probe - xrootd - transfer - 1 . 17 . 5 - 1 . osg34 . el6  gsi - openssh - 7 . 1 p2f - 1 . 2 . osg34 . el6  gsi - openssh - clients - 7 . 1 p2f - 1 . 2 . osg34 . el6  gsi - openssh - debuginfo - 7 . 1 p2f - 1 . 2 . osg34 . el6  gsi - openssh - server - 7 . 1 p2f - 1 . 2 . osg34 . el6  htcondor - ce - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - bosco - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - client - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - collector - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - condor - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - lsf - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - pbs - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - sge - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - slurm - 2 . 2 . 0 - 1 . osg34 . el6  htcondor - ce - view - 2 . 2 . 0 - 1 . osg34 . el6  igtf - ca - certs - 1 . 82 - 1 . osg34 . el6  javascriptrrd - 1 . 1 . 1 - 1 . osg34 . el6  koji - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - builder - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - hub - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - hub - plugins - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - utils - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - vm - 1 . 11 . 0 - 1 . 5 . osg34 . el6  koji - web - 1 . 11 . 0 - 1 . 5 . osg34 . el6  lcas - lcmaps - gt4 - interface - 0 . 3 . 1 - 1 . 2 . osg34 . el6  lcas - lcmaps - gt4 - interface - debuginfo - 0 . 3 . 1 - 1 . 2 . osg34 . el6  lcmaps - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - common - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - db - templates - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - debuginfo - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - plugins - basic - 1 . 7 . 0 - 2 . osg34 . el6  lcmaps - plugins - basic - debuginfo - 1 . 7 . 0 - 2 . osg34 . el6  lcmaps - plugins - basic - ldap - 1 . 7 . 0 - 2 . osg34 . el6  lcmaps - plugins - scas - client - 0 . 5 . 6 - 1 . osg34 . el6  lcmaps - plugins - scas - client - debuginfo - 0 . 5 . 6 - 1 . osg34 . el6  lcmaps - plugins - verify - proxy - 1 . 5 . 9 - 1 . 1 . osg34 . el6  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 9 - 1 . 1 . osg34 . el6  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 4 . osg34 . el6  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 4 . osg34 . el6  lcmaps - without - gsi - 1 . 6 . 6 - 1 . 6 . osg34 . el6  lcmaps - without - gsi - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el6  llrun - 0 . 1 . 3 - 1 . 3 . osg34 . el6  llrun - debuginfo - 0 . 1 . 3 - 1 . 3 . osg34 . el6  mash - 0 . 5 . 22 - 3 . osg34 . el6  myproxy - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - admin - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - debuginfo - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - devel - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - doc - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - libs - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - server - 6 . 1 . 18 - 1 . 4 . osg34 . el6  myproxy - voms - 6 . 1 . 18 - 1 . 4 . osg34 . el6  nuttcp - 6 . 1 . 2 - 1 . osg34 . el6  nuttcp - debuginfo - 6 . 1 . 2 - 1 . osg34 . el6  osg - base - ce - 3 . 4 - 2 . osg34 . el6  osg - base - ce - bosco - 3 . 4 - 2 . osg34 . el6  osg - base - ce - condor - 3 . 4 - 2 . osg34 . el6  osg - base - ce - lsf - 3 . 4 - 2 . osg34 . el6  osg - base - ce - pbs - 3 . 4 - 2 . osg34 . el6  osg - base - ce - sge - 3 . 4 - 2 . osg34 . el6  osg - base - ce - slurm - 3 . 4 - 2 . osg34 . el6  osg - build - 1 . 10 . 0 - 1 . osg34 . el6  osg - build - base - 1 . 10 . 0 - 1 . osg34 . el6  osg - build - koji - 1 . 10 . 0 - 1 . osg34 . el6  osg - build - mock - 1 . 10 . 0 - 1 . osg34 . el6  osg - build - tests - 1 . 10 . 0 - 1 . osg34 . el6  osg - ca - certs - 1 . 62 - 1 . osg34 . el6  osg - ca - certs - updater - 1 . 4 - 1 . osg34 . el6  osg - ca - generator - 1 . 2 . 0 - 1 . osg34 . el6  osg - ca - scripts - 1 . 1 . 6 - 1 . osg34 . el6  osg - ce - 3 . 4 - 2 . osg34 . el6  osg - ce - bosco - 3 . 4 - 2 . osg34 . el6  osg - ce - condor - 3 . 4 - 2 . osg34 . el6  osg - ce - lsf - 3 . 4 - 2 . osg34 . el6  osg - ce - pbs - 3 . 4 - 2 . osg34 . el6  osg - ce - sge - 3 . 4 - 2 . osg34 . el6  osg - ce - slurm - 3 . 4 - 2 . osg34 . el6  osg - configure - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - bosco - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - ce - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - condor - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - gateway - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - gip - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - gratia - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - infoservices - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - lsf - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - managedfork - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - misc - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - network - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - pbs - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - rsv - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - sge - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - slurm - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - squid - 2 . 0 . 0 - 3 . osg34 . el6  osg - configure - tests - 2 . 0 . 0 - 3 . osg34 . el6  osg - control - 1 . 1 . 0 - 1 . osg34 . el6  osg - gridftp - 3 . 4 - 2 . osg34 . el6  osg - gridftp - xrootd - 3 . 4 - 1 . osg34 . el6  osg - gums - config - 73 - 1 . osg34 . el6  osg - htcondor - ce - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - bosco - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - condor - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - lsf - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - pbs - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - sge - 3 . 4 - 2 . osg34 . el6  osg - htcondor - ce - slurm - 3 . 4 - 2 . osg34 . el6  osg - oasis - 7 - 9 . osg34 . el6  osg - pki - tools - 1 . 2 . 20 - 1 . osg34 . el6  osg - pki - tools - tests - 1 . 2 . 20 - 1 . osg34 . el6  osg - system - profiler - 1 . 4 . 0 - 1 . osg34 . el6  osg - system - profiler - viewer - 1 . 4 . 0 - 1 . osg34 . el6  osg - test - 1 . 10 . 1 - 1 . osg34 . el6  osg - tested - internal - 3 . 4 - 2 . osg34 . el6  osg - test - log - viewer - 1 . 10 . 1 - 1 . osg34 . el6  osg - update - data - 1 . 4 . 0 - 1 . osg34 . el6  osg - update - vos - 1 . 4 . 0 - 1 . osg34 . el6  osg - version - 3 . 4 . 0 - 1 . osg34 . el6  osg - vo - map - 0 . 0 . 2 - 1 . osg34 . el6  osg - wn - client - 3 . 4 - 1 . osg34 . el6  owamp - 3 . 2 rc4 - 2 . osg34 . el6  owamp - client - 3 . 2 rc4 - 2 . osg34 . el6  owamp - debuginfo - 3 . 2 rc4 - 2 . osg34 . el6  owamp - server - 3 . 2 rc4 - 2 . osg34 . el6  pegasus - 4 . 7 . 4 - 1 . 1 . osg34 . el6  pegasus - debuginfo - 4 . 7 . 4 - 1 . 1 . osg34 . el6  rsv - 3 . 14 . 0 - 2 . osg34 . el6  rsv - consumers - 3 . 14 . 0 - 2 . osg34 . el6  rsv - core - 3 . 14 . 0 - 2 . osg34 . el6  rsv - gwms - tester - 1 . 1 . 2 - 1 . osg34 . el6  rsv - metrics - 3 . 14 . 0 - 2 . osg34 . el6  uberftp - 2 . 8 - 2 . 1 . osg34 . el6  uberftp - debuginfo - 2 . 8 - 2 . 1 . osg34 . el6  vo - client - 73 - 1 . osg34 . el6  vo - client - edgmkgridmap - 73 - 1 . osg34 . el6  vo - client - lcmaps - voms - 73 - 1 . osg34 . el6  voms - 2 . 0 . 14 - 1 . 3 . osg34 . el6  voms - clients - cpp - 2 . 0 . 14 - 1 . 3 . osg34 . el6  voms - debuginfo - 2 . 0 . 14 - 1 . 3 . osg34 . el6  voms - devel - 2 . 0 . 14 - 1 . 3 . osg34 . el6  voms - doc - 2 . 0 . 14 - 1 . 3 . osg34 . el6  voms - server - 2 . 0 . 14 - 1 . 3 . osg34 . el6  xacml - 1 . 5 . 0 - 1 . osg34 . el6  xacml - debuginfo - 1 . 5 . 0 - 1 . osg34 . el6  xacml - devel - 1 . 5 . 0 - 1 . osg34 . el6  xrootd - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - client - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - client - devel - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - client - libs - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - debuginfo - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - devel - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - doc - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - dsi - 3 . 0 . 4 - 22 . osg34 . el6  xrootd - dsi - debuginfo - 3 . 0 . 4 - 22 . osg34 . el6  xrootd - fuse - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - lcmaps - 1 . 2 . 1 - 2 . osg34 . el6  xrootd - lcmaps - debuginfo - 1 . 2 . 1 - 2 . osg34 . el6  xrootd - libs - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - private - devel - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - python - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - selinux - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - server - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - server - devel - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - server - libs - 4 . 6 . 1 - 1 . osg34 . el6  xrootd - voms - plugin - 0 . 4 . 0 - 1 . osg34 . el6  xrootd - voms - plugin - debuginfo - 0 . 4 . 0 - 1 . osg34 . el6  xrootd - voms - plugin - devel - 0 . 4 . 0 - 1 . osg34 . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_2", 
            "text": "autopyfactory - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - cloud - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - common - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - panda - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - cloud - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - local - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - monitor - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - panda - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - remote - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - plugins - scheds - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - proxymanager - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - remote - 2 . 4 . 6 - 4 . osg34 . el7  autopyfactory - wms - 2 . 4 . 6 - 4 . osg34 . el7  blahp - 1 . 18 . 29 . bosco - 3 . osg34 . el7  blahp - debuginfo - 1 . 18 . 29 . bosco - 3 . osg34 . el7  bwctl - 1 . 4 - 7 . osg34 . el7  bwctl - client - 1 . 4 - 7 . osg34 . el7  bwctl - debuginfo - 1 . 4 - 7 . osg34 . el7  bwctl - devel - 1 . 4 - 7 . osg34 . el7  bwctl - server - 1 . 4 - 7 . osg34 . el7  cctools - 4 . 4 . 3 - 1 . osg34 . el7  cctools - chirp - 4 . 4 . 3 - 1 . osg34 . el7  cctools - debuginfo - 4 . 4 . 3 - 1 . osg34 . el7  cctools - doc - 4 . 4 . 3 - 1 . osg34 . el7  cctools - dttools - 4 . 4 . 3 - 1 . osg34 . el7  cctools - makeflow - 4 . 4 . 3 - 1 . osg34 . el7  cctools - parrot - 4 . 4 . 3 - 1 . osg34 . el7  cctools - resource_monitor - 4 . 4 . 3 - 1 . osg34 . el7  cctools - sand - 4 . 4 . 3 - 1 . osg34 . el7  cctools - wavefront - 4 . 4 . 3 - 1 . osg34 . el7  cctools - weaver - 4 . 4 . 3 - 1 . osg34 . el7  cctools - work_queue - 4 . 4 . 3 - 1 . osg34 . el7  condor - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - all - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - bosco - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - classads - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - classads - devel - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - cream - gahp - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - cron - 1 . 1 . 1 - 2 . osg34 . el7  condor - debuginfo - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - kbdd - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - procd - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - python - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - test - 8 . 6 . 3 - 1 . 1 . osg34 . el7  condor - vm - gahp - 8 . 6 . 3 - 1 . 1 . osg34 . el7  cvmfs - 2 . 3 . 5 - 1 . osg34 . el7  cvmfs - config - osg - 2 . 0 - 2 . osg34 . el7  cvmfs - devel - 2 . 3 . 5 - 1 . osg34 . el7  cvmfs - server - 2 . 3 . 5 - 1 . osg34 . el7  cvmfs - unittests - 2 . 3 . 5 - 1 . osg34 . el7  cvmfs - x509 - helper - 1 . 0 - 1 . osg34 . el7  cvmfs - x509 - helper - debuginfo - 1 . 0 - 1 . osg34 . el7  frontier - squid - 3 . 5 . 24 - 3 . 1 . osg34 . el7  frontier - squid - debuginfo - 3 . 5 . 24 - 3 . 1 . osg34 . el7  glideinwms - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - common - tools - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - condor - common - config - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - factory - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - factory - condor - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - glidecondor - tools - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - libs - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - minimal - condor - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - usercollector - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - userschedd - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - vofrontend - 3 . 2 . 19 - 2 . osg34 . el7  glideinwms - vofrontend - standalone - 3 . 2 . 19 - 2 . osg34 . el7  glite - build - common - cpp - 3 . 3 . 0 . 2 - 1 . osg34 . el7  glite - ce - cream - client - api - c - 1 . 15 . 4 - 2 . 3 . osg34 . el7  glite - ce - cream - client - devel - 1 . 15 . 4 - 2 . 3 . osg34 . el7  glite - ce - wsdl - 1 . 15 . 1 - 1 . 1 . osg34 . el7  glite - lbjp - common - gsoap - plugin - 3 . 2 . 12 - 1 . 1 . osg34 . el7  glite - lbjp - common - gsoap - plugin - debuginfo - 3 . 2 . 12 - 1 . 1 . osg34 . el7  glite - lbjp - common - gsoap - plugin - devel - 3 . 2 . 12 - 1 . 1 . osg34 . el7  globus - ftp - client - 8 . 29 - 1 . 1 . osg34 . el7  globus - ftp - client - debuginfo - 8 . 29 - 1 . 1 . osg34 . el7  globus - ftp - client - devel - 8 . 29 - 1 . 1 . osg34 . el7  globus - ftp - client - doc - 8 . 29 - 1 . 1 . osg34 . el7  globus - gridftp - osg - extensions - 0 . 3 - 2 . osg34 . el7  globus - gridftp - osg - extensions - debuginfo - 0 . 3 - 2 . osg34 . el7  globus - gridftp - server - 11 . 8 - 1 . 1 . osg34 . el7  globus - gridftp - server - control - 4 . 1 - 1 . 3 . osg34 . el7  globus - gridftp - server - control - debuginfo - 4 . 1 - 1 . 3 . osg34 . el7  globus - gridftp - server - control - devel - 4 . 1 - 1 . 3 . osg34 . el7  globus - gridftp - server - debuginfo - 11 . 8 - 1 . 1 . osg34 . el7  globus - gridftp - server - devel - 11 . 8 - 1 . 1 . osg34 . el7  globus - gridftp - server - progs - 11 . 8 - 1 . 1 . osg34 . el7  gratia - probe - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - bdii - status - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - common - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - condor - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - condor - events - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - dcache - storage - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - dcache - storagegroup - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - dcache - transfer - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - debuginfo - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - enstore - storage - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - enstore - tapedrive - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - enstore - transfer - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - glexec - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - glideinwms - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - gram - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - gridftp - transfer - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - hadoop - storage - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - htcondor - ce - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - lsf - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - metric - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - onevm - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - pbs - lsf - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - services - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - sge - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - slurm - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - xrootd - storage - 1 . 17 . 5 - 1 . osg34 . el7  gratia - probe - xrootd - transfer - 1 . 17 . 5 - 1 . osg34 . el7  gsi - openssh - 7 . 1 p2f - 1 . 2 . osg34 . el7  gsi - openssh - clients - 7 . 1 p2f - 1 . 2 . osg34 . el7  gsi - openssh - debuginfo - 7 . 1 p2f - 1 . 2 . osg34 . el7  gsi - openssh - server - 7 . 1 p2f - 1 . 2 . osg34 . el7  htcondor - ce - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - bosco - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - client - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - collector - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - condor - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - lsf - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - pbs - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - sge - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - slurm - 2 . 2 . 0 - 1 . osg34 . el7  htcondor - ce - view - 2 . 2 . 0 - 1 . osg34 . el7  igtf - ca - certs - 1 . 82 - 1 . osg34 . el7  javascriptrrd - 1 . 1 . 1 - 1 . osg34 . el7  koji - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - builder - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - hub - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - hub - plugins - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - utils - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - vm - 1 . 11 . 0 - 1 . 5 . osg34 . el7  koji - web - 1 . 11 . 0 - 1 . 5 . osg34 . el7  lcas - lcmaps - gt4 - interface - 0 . 3 . 1 - 1 . 2 . osg34 . el7  lcas - lcmaps - gt4 - interface - debuginfo - 0 . 3 . 1 - 1 . 2 . osg34 . el7  lcmaps - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - common - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - db - templates - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - debuginfo - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - plugins - basic - 1 . 7 . 0 - 2 . osg34 . el7  lcmaps - plugins - basic - debuginfo - 1 . 7 . 0 - 2 . osg34 . el7  lcmaps - plugins - basic - ldap - 1 . 7 . 0 - 2 . osg34 . el7  lcmaps - plugins - scas - client - 0 . 5 . 6 - 1 . osg34 . el7  lcmaps - plugins - scas - client - debuginfo - 0 . 5 . 6 - 1 . osg34 . el7  lcmaps - plugins - verify - proxy - 1 . 5 . 9 - 1 . 1 . osg34 . el7  lcmaps - plugins - verify - proxy - debuginfo - 1 . 5 . 9 - 1 . 1 . osg34 . el7  lcmaps - plugins - voms - 1 . 7 . 1 - 1 . 4 . osg34 . el7  lcmaps - plugins - voms - debuginfo - 1 . 7 . 1 - 1 . 4 . osg34 . el7  lcmaps - without - gsi - 1 . 6 . 6 - 1 . 6 . osg34 . el7  lcmaps - without - gsi - devel - 1 . 6 . 6 - 1 . 6 . osg34 . el7  llrun - 0 . 1 . 3 - 1 . 3 . osg34 . el7  llrun - debuginfo - 0 . 1 . 3 - 1 . 3 . osg34 . el7  mash - 0 . 5 . 22 - 3 . osg34 . el7  myproxy - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - admin - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - debuginfo - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - devel - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - doc - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - libs - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - server - 6 . 1 . 18 - 1 . 4 . osg34 . el7  myproxy - voms - 6 . 1 . 18 - 1 . 4 . osg34 . el7  nuttcp - 6 . 1 . 2 - 1 . osg34 . el7  nuttcp - debuginfo - 6 . 1 . 2 - 1 . osg34 . el7  osg - base - ce - 3 . 4 - 2 . osg34 . el7  osg - base - ce - bosco - 3 . 4 - 2 . osg34 . el7  osg - base - ce - condor - 3 . 4 - 2 . osg34 . el7  osg - base - ce - lsf - 3 . 4 - 2 . osg34 . el7  osg - base - ce - pbs - 3 . 4 - 2 . osg34 . el7  osg - base - ce - sge - 3 . 4 - 2 . osg34 . el7  osg - base - ce - slurm - 3 . 4 - 2 . osg34 . el7  osg - build - 1 . 10 . 0 - 1 . osg34 . el7  osg - build - base - 1 . 10 . 0 - 1 . osg34 . el7  osg - build - koji - 1 . 10 . 0 - 1 . osg34 . el7  osg - build - mock - 1 . 10 . 0 - 1 . osg34 . el7  osg - build - tests - 1 . 10 . 0 - 1 . osg34 . el7  osg - ca - certs - 1 . 62 - 1 . osg34 . el7  osg - ca - certs - updater - 1 . 4 - 1 . osg34 . el7  osg - ca - generator - 1 . 2 . 0 - 1 . osg34 . el7  osg - ca - scripts - 1 . 1 . 6 - 1 . osg34 . el7  osg - ce - 3 . 4 - 2 . osg34 . el7  osg - ce - bosco - 3 . 4 - 2 . osg34 . el7  osg - ce - condor - 3 . 4 - 2 . osg34 . el7  osg - ce - lsf - 3 . 4 - 2 . osg34 . el7  osg - ce - pbs - 3 . 4 - 2 . osg34 . el7  osg - ce - sge - 3 . 4 - 2 . osg34 . el7  osg - ce - slurm - 3 . 4 - 2 . osg34 . el7  osg - configure - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - bosco - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - ce - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - condor - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - gateway - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - gip - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - gratia - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - infoservices - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - lsf - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - managedfork - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - misc - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - network - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - pbs - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - rsv - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - sge - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - slurm - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - squid - 2 . 0 . 0 - 3 . osg34 . el7  osg - configure - tests - 2 . 0 . 0 - 3 . osg34 . el7  osg - control - 1 . 1 . 0 - 1 . osg34 . el7  osg - gridftp - 3 . 4 - 2 . osg34 . el7  osg - gridftp - xrootd - 3 . 4 - 1 . osg34 . el7  osg - gums - config - 73 - 1 . osg34 . el7  osg - htcondor - ce - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - bosco - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - condor - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - lsf - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - pbs - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - sge - 3 . 4 - 2 . osg34 . el7  osg - htcondor - ce - slurm - 3 . 4 - 2 . osg34 . el7  osg - oasis - 7 - 9 . osg34 . el7  osg - pki - tools - 1 . 2 . 20 - 1 . osg34 . el7  osg - pki - tools - tests - 1 . 2 . 20 - 1 . osg34 . el7  osg - system - profiler - 1 . 4 . 0 - 1 . osg34 . el7  osg - system - profiler - viewer - 1 . 4 . 0 - 1 . osg34 . el7  osg - test - 1 . 10 . 1 - 1 . osg34 . el7  osg - tested - internal - 3 . 4 - 2 . osg34 . el7  osg - test - log - viewer - 1 . 10 . 1 - 1 . osg34 . el7  osg - update - data - 1 . 4 . 0 - 1 . osg34 . el7  osg - update - vos - 1 . 4 . 0 - 1 . osg34 . el7  osg - version - 3 . 4 . 0 - 1 . osg34 . el7  osg - vo - map - 0 . 0 . 2 - 1 . osg34 . el7  osg - wn - client - 3 . 4 - 1 . osg34 . el7  owamp - 3 . 2 rc4 - 2 . osg34 . el7  owamp - client - 3 . 2 rc4 - 2 . osg34 . el7  owamp - debuginfo - 3 . 2 rc4 - 2 . osg34 . el7  owamp - server - 3 . 2 rc4 - 2 . osg34 . el7  pegasus - 4 . 7 . 4 - 1 . 1 . osg34 . el7  pegasus - debuginfo - 4 . 7 . 4 - 1 . 1 . osg34 . el7  rsv - 3 . 14 . 0 - 2 . osg34 . el7  rsv - consumers - 3 . 14 . 0 - 2 . osg34 . el7  rsv - core - 3 . 14 . 0 - 2 . osg34 . el7  rsv - gwms - tester - 1 . 1 . 2 - 1 . osg34 . el7  rsv - metrics - 3 . 14 . 0 - 2 . osg34 . el7  stashcache - 0 . 7 - 2 . osg34 . el7  stashcache - cache - server - 0 . 7 - 2 . osg34 . el7  stashcache - daemon - 0 . 7 - 2 . osg34 . el7  stashcache - origin - server - 0 . 7 - 2 . osg34 . el7  uberftp - 2 . 8 - 2 . 1 . osg34 . el7  uberftp - debuginfo - 2 . 8 - 2 . 1 . osg34 . el7  vo - client - 73 - 1 . osg34 . el7  vo - client - edgmkgridmap - 73 - 1 . osg34 . el7  vo - client - lcmaps - voms - 73 - 1 . osg34 . el7  voms - 2 . 0 . 14 - 1 . 3 . osg34 . el7  voms - clients - cpp - 2 . 0 . 14 - 1 . 3 . osg34 . el7  voms - debuginfo - 2 . 0 . 14 - 1 . 3 . osg34 . el7  voms - devel - 2 . 0 . 14 - 1 . 3 . osg34 . el7  voms - doc - 2 . 0 . 14 - 1 . 3 . osg34 . el7  voms - server - 2 . 0 . 14 - 1 . 3 . osg34 . el7  xacml - 1 . 5 . 0 - 1 . osg34 . el7  xacml - debuginfo - 1 . 5 . 0 - 1 . osg34 . el7  xacml - devel - 1 . 5 . 0 - 1 . osg34 . el7  xrootd - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - client - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - client - devel - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - client - libs - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - debuginfo - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - devel - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - doc - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - dsi - 3 . 0 . 4 - 22 . osg34 . el7  xrootd - dsi - debuginfo - 3 . 0 . 4 - 22 . osg34 . el7  xrootd - fuse - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - lcmaps - 1 . 3 . 3 - 3 . osg34 . el7  xrootd - lcmaps - debuginfo - 1 . 3 . 3 - 3 . osg34 . el7  xrootd - libs - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - private - devel - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - python - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - selinux - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - server - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - server - devel - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - server - libs - 4 . 6 . 1 - 1 . osg34 . el7  xrootd - voms - plugin - 0 . 4 . 0 - 1 . osg34 . el7  xrootd - voms - plugin - debuginfo - 0 . 4 . 0 - 1 . osg34 . el7  xrootd - voms - plugin - devel - 0 . 4 . 0 - 1 . osg34 . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#upcoming-packages", 
            "text": "We added or updated the following packages to the  upcoming  OSG yum repository. Note that in some cases, there are multiple RPMs for each package. You can click on any given package to see the set of RPMs or see the complete list below.", 
            "title": "Upcoming Packages"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_3", 
            "text": "glideinwms-3.3.2-2.osgup.el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_3", 
            "text": "glideinwms-3.3.2-2.osgup.el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#upcoming-rpms", 
            "text": "If you wish to manually update your system, you can run yum update against the following packages:  glideinwms   glideinwms - common - tools   glideinwms - condor - common - config   glideinwms - factory   glideinwms - factory - condor   glideinwms - glidecondor - tools   glideinwms - libs   glideinwms - minimal - condor   glideinwms - usercollector   glideinwms - userschedd   glideinwms - vofrontend   glideinwms - vofrontend - standalone   If you wish to only update the RPMs that changed, the set of RPMs is:", 
            "title": "Upcoming RPMs"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-6_4", 
            "text": "glideinwms - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - common - tools - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - condor - common - config - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - factory - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - factory - condor - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - glidecondor - tools - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - libs - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - minimal - condor - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - usercollector - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - userschedd - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - vofrontend - 3 . 3 . 2 - 2 . osgup . el6  glideinwms - vofrontend - standalone - 3 . 3 . 2 - 2 . osgup . el6", 
            "title": "Enterprise Linux 6"
        }, 
        {
            "location": "/release/3.4/release-3-4-0/#enterprise-linux-7_4", 
            "text": "glideinwms - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - common - tools - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - condor - common - config - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - factory - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - factory - condor - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - glidecondor - tools - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - libs - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - minimal - condor - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - usercollector - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - userschedd - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - vofrontend - 3 . 3 . 2 - 2 . osgup . el7  glideinwms - vofrontend - standalone - 3 . 3 . 2 - 2 . osgup . el7", 
            "title": "Enterprise Linux 7"
        }, 
        {
            "location": "/release/release_series/", 
            "text": "OSG Release Series\n\n\nAn OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.\n\n\nWhen a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.\n\n\nOur goal is, within a series, that one may upgrade their OSG services via \nyum update\n cleanly and without any necessary config file changes or excessive downtime.\n\n\nOSG Release Series\n\n\nSince the start of the RPM-based OSG software stack, we have offered the following release series:\n\n\n\n\n\n\nOSG 3.1\n started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.\n\n\n\n\n\n\nOSG 3.2\n started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.\n\n\n\n\n\n\nOSG 3.3\n started in August 2015 and was end-of-lifed in May 2018. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.\n\n\n\n\n\n\nOSG 3.4\n started June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.\n\n\n\n\n\n\nOSG Upcoming\n\n\nThere is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming (\nosg-upcoming\n and \nosg-upcoming-testing\n) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing, \nosg-upcoming\n is meant to work with OSG 3.4.\n\n\nInstalling an OSG Release Series\n\n\nSee the \nyum repositories document\n for instructions on installing the OSG repositories.\n\n\n\n\nUpdating from OSG 3.1, 3.2, 3.3 to 3.4\n\n\n\n\n\n\nIf you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the \nold series\n), and want to upgrade to 3.4 (the \nnew series\n), we recommend the following procedure:\n\n\nFirst, remove the old series yum repositories:\n\n\nroot@host #\n rpm -e osg-release\n\n\n\n\n\nThis step ensures that any local modifications to \n*.repo\n files will not prevent installing the new series repos. Any modified \n*.repo\n files should appear under \n/etc/yum.repos.d/\n with the \n*.rpmsave\n extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the \n*.rpmsave\n files to the new \n*.repo\n files.\n\n\n\n\n\n\nInstall the OSG repositories:\n\n\nroot@host #\n rpm -Uvh \nURL\n\n\n\n\n\n\nwhere \nURL\n is one of the following:\n\n\n\n\n\n\n\n\nSeries\n\n\nEL6 URL (for RHEL6, CentOS6, or SL6)\n\n\nEL7 URL (for RHEL7, CentOS7, or SL7)\n\n\n\n\n\n\n\n\n\n\nOSG 3.4\n\n\nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm\n\n\nhttp://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm\n\n\n\n\n\n\n\n\n\n\n\n\nClean yum cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\n\n\n\n\nUpdate software:\n\n\nroot@host #\n yum update\n\n\n\n\n\nThis command will update \nall\n packages on your system.\n\n\n\n\n\n\nTroubleshooting\n If you are not having the expected result or having problems with Yum please see the \nYum troubleshooting guide\n\n\nRetiring Your VOMS Server\n\n\nDue to the \nend of OSG 3.3 support\n, VOMS Admin server is\nno longer supported in the OSG as of May 2018.\nPlease see \nthis policy document\n for details on the\nVOMS Admin retirement.\n\n\nThis section describes how to gracefully retire your VOMS host.\n\n\nDisabling the VOMS Admin service\n\n\nThe VOMS Admin service provides a web interface to manage membership of your virtual organization (VO).\nIt also handles queries for the list of VO members, which may have\n\nGeneral Data Protection Regulation\n implications.\nTherefore, the VOMS Admin service should be disabled immediately.\n\n\n\n\n\n\nChoose the service name based on your OS version for subsequent steps:\n\n\n\n\n\n\n\n\nIf your host OS is...\n\n\nThen the service name is...\n\n\n\n\n\n\n\n\n\n\nEL6\n\n\ntomcat6\n\n\n\n\n\n\nEL7\n\n\ntomcat\n\n\n\n\n\n\n\n\n\n\n\n\nTurn off and disable the VOMS Admin server webapp:\n\n\nroot@voms #\n service \nSERVICE NAME\n stop\n\nroot@voms #\n chkconfig disable \nSERVICE NAME\n\n\n\n\n\n\n\n\n\n\nDisabling the VOMS service\n\n\nThe VOMS service accepts requests from clients to sign the VOMS attributes of their proxies.\nThese VOMS attributes can then be used to authorize job submissions and file transfers in the OSG.\n\n\nBefore turning off the VOMS service, it is important to migrate any clients to avoid disruption of your VO's workflows.\n\n\nIdentifying VOMS clients\n\n\nTo find all clients requesting VOMS attribute signatures, run the following command:\n\n\nroot@voms #\n awk -F \n:\n \n/Received request from/ {print $11}\n \n\\\n\n    /var/log/voms/voms.\nVO\n \n\\\n\n    /var/log/voms/voms.\nVO\n.\n[\n0\n-9\n]\n \n|\n sort \n|\n uniq -c\n\n\n\n\n\n\n\n\n\nIf there are any GlideinWMS frontends requesting VOMS attribute signatures:\n\n\n\n\nSecurely copy the VOMS certificate and key to the frontend host(s)\n\n\nConfigure the GlideinWMS frontend to directly sign the VOMS attributes of the pilot proxies using the\ninstructions in \nthis section\n.\n\n\n\n\n\n\n\n\nIf there are any VO users requesting VOMS attribute signatures: contact them to tell them that they will need to\n  generate proxies differently after the VOMS server retirement using one of the following commands:\n\n\n\n\nuser@host $ voms-proxy-init\n\n\nuser@host$ grid-proxy-init\n\n\n\n\n\n\n\n\nFinalizing the VOMS retirement\n\n\nWhen you have migrated all clients off of your VOMS server, it is safe to turn off and disable the VOMS service:\n\n\nroot@voms #\n service voms stop\n\nroot@voms #\n chkconfig disable voms\n\n\n\n\n\nMigrating from edg-mkgridmap to LCMAPS VOMS Plugin\n\n\nAfter following the update instructions above, perform the migration process documented \nhere\n.\n\n\nUpdating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)\n\n\nThe program \nfrontier-squid\n received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the \nupstream upgrade documentation\n when transitioning your squid server to OSG 3.4.\n\n\nUninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)\n\n\nThe program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by \nload-balanced GridFTP\n. To update your storage element to OSG 3.4, you must perform the following procedure:\n\n\n\n\n\n\nEnsure that OSG BeStMan packages are installed:\n\n\nroot@host #\n rpm -q osg-se-bestman\n\n\n\n\n\n\n\n\n\nStop the \nbestman2\n service:\n\n\nroot@host #\n service bestman2 stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\nroot@host #\n yum erase bestman2-tester-libs bestman2-common-libs \n\\\n\n                            bestman2-server-libs bestman2-server-dep-libs \n\\\n\n                            bestman2-client-libs bestman2-tester bestman2-client \n\\\n\n                            bestman2-server osg-se-bestman\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.\n\n\n\n\nAfter successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.\n\n\nUninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)\n\n\nThe program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.\n\n\nTo cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\nroot@host #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.4-1 (OSG 3.4).  If not, update them:\n\n\nroot@host #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nStop the \nosg-info-services\n service:\n\n\nroot@host #\n service osg-info-services stop\n\n\n\n\n\n\n\n\n\nUninstall the software:\n\n\nroot@host #\n yum erase gip osg-info-services\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.4 versions, and try again.\n\n\n\n\nUninstalling CEMon from the Compute Element (upgrading from OSG 3.1)\n\n\nThe program CEMon (found in the package \nglite-ce-monitor\n) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.\n\n\nTo cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):\n\n\n\n\n\n\nEnsure that you are using a sufficiently new version of the \nosg-ce\n metapackages:\n\n\nroot@host #\n rpm -q osg-ce\n\n\n\n\n\nshould be at least 3.4-1 (OSG 3.4). If not, update them:\n\n\nroot@host #\n yum update osg-ce\n\n\n\n\n\n\n\n\n\nIf there is a CEMon configuration file at \n/etc/osg/config.d/30-cemon.ini\n, remove it.\n\n\n\n\nRemove CEMon and related packages:\nroot@host #\n yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nIn the output from this command, yum should \nnot\n list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.4 versions (they should have \n.osg34\n in their versions), and try again.\n\n\n\n\nReferences\n\n\n\n\nYum repositories\n\n\nBasic use of Yum", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series", 
            "text": "An OSG release series is a sequence of OSG software releases that are intended to provide a painless upgrade path. For example, the 3.2 release series contains OSG software 3.2.0, 3.2.1, 3.2.2, and so forth. A release series corresponds to a set of Yum software repositories, including ones for development, testing, and production use. The Yum repositories for one release series are completely distinct from the repositories for a different release series, even though they share many common packages.  A particular release within a series is a snapshot of packages and their exact versions at one point in time. When you install software from a release series, say 3.2, you get the most current versions of software packages within that series, regardless of the current release version.  When a new series is released, it is an opportunity for the OSG Technology area to add major new software packages, make substantial updates to existing packages, and remove obsolete packages. When a new series is initially released, most packages are identical to the previous release, but two adjacent series will diverge over time.  Our goal is, within a series, that one may upgrade their OSG services via  yum update  cleanly and without any necessary config file changes or excessive downtime.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-release-series_1", 
            "text": "Since the start of the RPM-based OSG software stack, we have offered the following release series:    OSG 3.1  started in April 2012, and was end-of-lifed in April 2015. While the files have not been removed, it is strongly recommended that it not be installed anymore. Historically, there were 3.0.x releases as well, but there was no separate release series for 3.0 and 3.1; we simply went from 3.0.10 to 3.1.0 in the same repositories.    OSG 3.2  started in November 2013, and was end-of-lifed in August 2016. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between it and 3.1 were the introduction of glideinWMS 3.2, HTCondor 8.0, and Hadoop/HDFS 2.0; also the gLite CE Monitor system was dropped in favor of osg-info-services.    OSG 3.3  started in August 2015 and was end-of-lifed in May 2018. While the files have not been removed, it is strongly recommended that it not be installed anymore. The main differences between 3.3 and 3.2 are the dropping of EL5 support, the addition of EL7 support, and the dropping of Globus GRAM support.    OSG 3.4  started June 2017. The main differences between it and 3.3 are the removal of edg-mkgridmap, GUMS, BeStMan, and VOMS Admin Server packages.", 
            "title": "OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#osg-upcoming", 
            "text": "There is one more OSG Series called \"upcoming\" which contains major updates planned for a future release series. The yum repositories for upcoming ( osg-upcoming  and  osg-upcoming-testing ) are available from all OSG 3.x series, and individual packages can be installed from Upcoming without needing to update entirely to a new series. Note, however, that packages in the \"upcoming\" repositories are tested against the most recent OSG series.  As of the time of writing,  osg-upcoming  is meant to work with OSG 3.4.", 
            "title": "OSG Upcoming"
        }, 
        {
            "location": "/release/release_series/#installing-an-osg-release-series", 
            "text": "See the  yum repositories document  for instructions on installing the OSG repositories.", 
            "title": "Installing an OSG Release Series"
        }, 
        {
            "location": "/release/release_series/#updating-from-osg-31-32-33-to-34", 
            "text": "If you have an existing installation based on OSG 3.1, 3.2, or 3.3 (which will be referred to as the  old series ), and want to upgrade to 3.4 (the  new series ), we recommend the following procedure:  First, remove the old series yum repositories:  root@host #  rpm -e osg-release  This step ensures that any local modifications to  *.repo  files will not prevent installing the new series repos. Any modified  *.repo  files should appear under  /etc/yum.repos.d/  with the  *.rpmsave  extension. After installing the new OSG repositories (the next step) you may want to apply any changes made in the  *.rpmsave  files to the new  *.repo  files.    Install the OSG repositories:  root@host #  rpm -Uvh  URL   where  URL  is one of the following:     Series  EL6 URL (for RHEL6, CentOS6, or SL6)  EL7 URL (for RHEL7, CentOS7, or SL7)      OSG 3.4  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el6-release-latest.rpm  http://repo.opensciencegrid.org/osg/3.4/osg-3.4-el7-release-latest.rpm       Clean yum cache:  root@host #  yum clean all --enablerepo = *    Update software:  root@host #  yum update  This command will update  all  packages on your system.    Troubleshooting  If you are not having the expected result or having problems with Yum please see the  Yum troubleshooting guide", 
            "title": "Updating from OSG 3.1, 3.2, 3.3 to 3.4"
        }, 
        {
            "location": "/release/release_series/#retiring-your-voms-server", 
            "text": "Due to the  end of OSG 3.3 support , VOMS Admin server is\nno longer supported in the OSG as of May 2018.\nPlease see  this policy document  for details on the\nVOMS Admin retirement.  This section describes how to gracefully retire your VOMS host.", 
            "title": "Retiring Your VOMS Server"
        }, 
        {
            "location": "/release/release_series/#disabling-the-voms-admin-service", 
            "text": "The VOMS Admin service provides a web interface to manage membership of your virtual organization (VO).\nIt also handles queries for the list of VO members, which may have General Data Protection Regulation  implications.\nTherefore, the VOMS Admin service should be disabled immediately.    Choose the service name based on your OS version for subsequent steps:     If your host OS is...  Then the service name is...      EL6  tomcat6    EL7  tomcat       Turn off and disable the VOMS Admin server webapp:  root@voms #  service  SERVICE NAME  stop root@voms #  chkconfig disable  SERVICE NAME", 
            "title": "Disabling the VOMS Admin service"
        }, 
        {
            "location": "/release/release_series/#disabling-the-voms-service", 
            "text": "The VOMS service accepts requests from clients to sign the VOMS attributes of their proxies.\nThese VOMS attributes can then be used to authorize job submissions and file transfers in the OSG.  Before turning off the VOMS service, it is important to migrate any clients to avoid disruption of your VO's workflows.", 
            "title": "Disabling the VOMS service"
        }, 
        {
            "location": "/release/release_series/#identifying-voms-clients", 
            "text": "To find all clients requesting VOMS attribute signatures, run the following command:  root@voms #  awk -F  :   /Received request from/ {print $11}   \\ \n    /var/log/voms/voms. VO   \\ \n    /var/log/voms/voms. VO . [ 0 -9 ]   |  sort  |  uniq -c    If there are any GlideinWMS frontends requesting VOMS attribute signatures:   Securely copy the VOMS certificate and key to the frontend host(s)  Configure the GlideinWMS frontend to directly sign the VOMS attributes of the pilot proxies using the\ninstructions in  this section .     If there are any VO users requesting VOMS attribute signatures: contact them to tell them that they will need to\n  generate proxies differently after the VOMS server retirement using one of the following commands:   user@host $ voms-proxy-init  user@host$ grid-proxy-init", 
            "title": "Identifying VOMS clients"
        }, 
        {
            "location": "/release/release_series/#finalizing-the-voms-retirement", 
            "text": "When you have migrated all clients off of your VOMS server, it is safe to turn off and disable the VOMS service:  root@voms #  service voms stop root@voms #  chkconfig disable voms", 
            "title": "Finalizing the VOMS retirement"
        }, 
        {
            "location": "/release/release_series/#migrating-from-edg-mkgridmap-to-lcmaps-voms-plugin", 
            "text": "After following the update instructions above, perform the migration process documented  here .", 
            "title": "Migrating from edg-mkgridmap to LCMAPS VOMS Plugin"
        }, 
        {
            "location": "/release/release_series/#updating-from-frontier-squid-27-to-frontier-squid-35-upgrading-from-osg-33", 
            "text": "The program  frontier-squid  received a major version upgrade (versions 2.7 to 3.5) between OSG 3.3 and OSG 3.4. Follow the  upstream upgrade documentation  when transitioning your squid server to OSG 3.4.", 
            "title": "Updating from Frontier Squid 2.7 to Frontier Squid 3.5 (upgrading from OSG 3.3)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-bestman2-from-the-storage-element-upgrading-to-osg-34", 
            "text": "The program BeStMan2 is no longer available in OSG 3.4 and its functionality has been replaced by  load-balanced GridFTP . To update your storage element to OSG 3.4, you must perform the following procedure:    Ensure that OSG BeStMan packages are installed:  root@host #  rpm -q osg-se-bestman    Stop the  bestman2  service:  root@host #  service bestman2 stop    Uninstall the software:  root@host #  yum erase bestman2-tester-libs bestman2-common-libs  \\ \n                            bestman2-server-libs bestman2-server-dep-libs  \\ \n                            bestman2-client-libs bestman2-tester bestman2-client  \\ \n                            bestman2-server osg-se-bestman     Note  In the output from this command, yum should  not  list other packages than those nine. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.3 versions, and try again.   After successfully removing BeStMan2, continue updating your host to OSG 3.4 by following the instructions above.", 
            "title": "Uninstalling BeStMan2 from the Storage Element (upgrading to OSG 3.4)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-osg-info-services-from-the-compute-element-upgrading-from-osg-33-or-32", 
            "text": "The program OSG Info Services is no longer required on OSG 3.3, and is no longer available starting in OSG 3.4. This is because the service that OSG Info Services reported to, named BDII, has been retired and is no longer functional.  To cleanly uninstall OSG Info Services from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  root@host #  rpm -q osg-ce  should be at least 3.4-1 (OSG 3.4).  If not, update them:  root@host #  yum update osg-ce    Stop the  osg-info-services  service:  root@host #  service osg-info-services stop    Uninstall the software:  root@host #  yum erase gip osg-info-services     Note  In the output from this command, yum should  not  list other packages than those two. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their latest OSG 3.4 versions, and try again.", 
            "title": "Uninstalling OSG Info Services from the Compute Element (upgrading from OSG 3.3 or 3.2)"
        }, 
        {
            "location": "/release/release_series/#uninstalling-cemon-from-the-compute-element-upgrading-from-osg-31", 
            "text": "The program CEMon (found in the package  glite-ce-monitor ) is no longer available starting in OSG 3.2. Its functionality is no longer required because the service that CEMon reported to has been retired and is no longer functional.  To cleanly uninstall CEMon from your CE, perform the following procedure (after following the main update instructions above):    Ensure that you are using a sufficiently new version of the  osg-ce  metapackages:  root@host #  rpm -q osg-ce  should be at least 3.4-1 (OSG 3.4). If not, update them:  root@host #  yum update osg-ce    If there is a CEMon configuration file at  /etc/osg/config.d/30-cemon.ini , remove it.   Remove CEMon and related packages: root@host #  yum erase glite-ce-monitor glite-ce-osg-ce-plugin osg-configure-cemon     Note  In the output from this command, yum should  not  list other packages than those three. If it lists other packages, cancel the erase operation, make sure the other packages are updated to their OSG 3.4 versions (they should have  .osg34  in their versions), and try again.", 
            "title": "Uninstalling CEMon from the Compute Element (upgrading from OSG 3.1)"
        }, 
        {
            "location": "/release/release_series/#references", 
            "text": "Yum repositories  Basic use of Yum", 
            "title": "References"
        }, 
        {
            "location": "/release/supported_platforms/", 
            "text": "OSG Software Supported Operating Systems\n\n\nThe OSG Software 3.4 release series is supported on Red Hat Enterprise 6 and 7 and compatible platforms,\nfor 64-bit Intel architectures.\n\n\nOSG also supports select rebuilds of RHEL.  Specifically:\n\n\n\n\nCentOS 6\n\n\nCentOS 7\n\n\nRed Hat Enterprise Linux 6\n\n\nRed Hat Enterprise Linux 7\n\n\nScientific Linux 6\n\n\nScientific Linux 7\n\n\n\n\nOSG builds and tests its RPMs on the latest releases of the relevant platforms (e.g., in 2018, the RHEL 7 builds were based on RHEL 7.5).\nOlder releases of the OS may not receive thorough testing and have subtle bugs; we may ask sites to update to the latest OS packages\nas part of the support process.\n\n\nThe OSG Software 3.3 release series was supported on Red Hat Enterprise 6 (32-bit and 64-bit) and 7 (64-bit only) and compatible\nplatforms.  Support for this release series (including security patches) ended in May 2018.  If you need help migrating from OSG 3.3,\nplease contact support@opensciencegrid.org.", 
            "title": "Supported Platforms"
        }, 
        {
            "location": "/release/supported_platforms/#osg-software-supported-operating-systems", 
            "text": "The OSG Software 3.4 release series is supported on Red Hat Enterprise 6 and 7 and compatible platforms,\nfor 64-bit Intel architectures.  OSG also supports select rebuilds of RHEL.  Specifically:   CentOS 6  CentOS 7  Red Hat Enterprise Linux 6  Red Hat Enterprise Linux 7  Scientific Linux 6  Scientific Linux 7   OSG builds and tests its RPMs on the latest releases of the relevant platforms (e.g., in 2018, the RHEL 7 builds were based on RHEL 7.5).\nOlder releases of the OS may not receive thorough testing and have subtle bugs; we may ask sites to update to the latest OS packages\nas part of the support process.  The OSG Software 3.3 release series was supported on Red Hat Enterprise 6 (32-bit and 64-bit) and 7 (64-bit only) and compatible\nplatforms.  Support for this release series (including security patches) ended in May 2018.  If you need help migrating from OSG 3.3,\nplease contact support@opensciencegrid.org.", 
            "title": "OSG Software Supported Operating Systems"
        }, 
        {
            "location": "/release/yum-basics/", 
            "text": "Basics of using yum and RPM\n\n\nAbout This Document\n\n\nThis document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format, \nyum\n is the command you will use to do the installation. For example, \nyum\n will resolve and download the dependencies for the package you want to install; \nrpm\n will simply complain if you want to install a package that does not have all its dependencies installed.\n\n\nInstallation\n\n\nInstallation is done with the \nyum install\n command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.\n\n\nroot@host #\n sudo yum install osg-ca-certs\n\nLoaded plugins: kernel-module, priorities\n\n\nepel                                                                                         | 3.7 kB     00:00     \n\n\nepel/primary_db                                                                              | 3.8 MB     00:00     \n\n\nfermi-base                                                                                   | 2.1 kB     00:00     \n\n\nfermi-base/primary_db                                                                        |  48 kB     00:00     \n\n\nfermi-security                                                                               | 1.9 kB     00:00     \n\n\nfermi-security/primary_db                                                                    | 1.7 MB     00:00     \n\n\nosg                                                                                          | 1.9 kB     00:00     \n\n\nosg/primary_db                                                                               |  65 kB     00:00     \n\n\nsl-base                                                                                      | 2.1 kB     00:00     \n\n\nsl-base/primary_db                                                                           | 2.0 MB     00:00     \n\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Install Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package osg-ca-certs.noarch 0:1.23-1 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                         Arch                      Version                     Repository              Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n osg-ca-certs                    noarch                    1.23-1                      osg                    450 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 450 k\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nosg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603\n\n\nosg/gpgkey                                                                                   | 1.7 kB     00:00     \n\n\nImporting GPG key 0x824B8603 \nOSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : osg-ca-certs                                                                                 1/1 \n\n\n\nInstalled:\n\n\n  osg-ca-certs.noarch 0:1.23-1                                                                                      \n\n\n\nComplete!\n\n\n\n\n\n\nPlease Note\n: When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the \ninformation on our signed RPMs\n. If it doesn't match, there is a problem somewhere and you should report it to the OSG via help@opensciencegrid.org.\n\n\nVerifying Packages and Installations\n\n\nYou can check if an RPM has been modified. For instance, to check to see if any files have been modified in the \nosg-ca-certs\n RPM you just installed:\n\n\nuser@host $\n rpm --verify osg-ca-certs\n\n\n\n\n\nThe lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:\n\n\nuser@host $\n rpm --verify -v osg-ca-certs\n\n........    /etc/grid-security/certificates\n\n\n........    /etc/grid-security/certificates/0119347c.0\n\n\n........    /etc/grid-security/certificates/0119347c.namespaces\n\n\n........    /etc/grid-security/certificates/0119347c.signing_policy\n\n\n... etc ...\n\n\n\n\n\n\nEach dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:\n\n\nuser@host $\n rpm --verify osg-ca-certs\n\n..5....T    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nThis means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the \nrpm\n man page) are:\n\n\n\n\n\n\n\n\nLetter\n\n\nMeaning\n\n\n\n\n\n\n\n\n\n\nS\n\n\nfile Size differs\n\n\n\n\n\n\nM\n\n\nMode differs (includes permissions and file type)\n\n\n\n\n\n\n5\n\n\nMD5 sum differs\n\n\n\n\n\n\nD\n\n\nDevice major/minor number mismatch\n\n\n\n\n\n\nL\n\n\nreadLink(2) path mismatch\n\n\n\n\n\n\nU\n\n\nUser ownership differs\n\n\n\n\n\n\nG\n\n\nGroup ownership differs\n\n\n\n\n\n\nT\n\n\nmTime differs\n\n\n\n\n\n\n\n\nIf you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:\n\n\nuser@host $\n rpm --verify --nomtime osg-ca-certs\n\n..5.....    /etc/grid-security/certificates/ffc3d59b.0\n\n\n\n\n\n\nUnderstanding a package\n\n\nIf you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the \nsrm-ls\n command, you can do:\n\n\n#\n \n1\n. Find the exact path\n\nuser@host $\n which srm-ls\n\n/usr/bin/srm-ls\n\n\n\n#\n \n2\n. Ask rpm what package it is part of:\n\nuser@host $\n rpm -q --file /usr/bin/srm-ls\n\nbestman2-client-2.2.0-14.osg.el5.noarch\n\n\n\n\n\n\nIf you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:\n\n\nuser@host $\n rpm -ql bestman2-client\n\n/etc/bestman2/conf/bestman2.rc\n\n\n/etc/bestman2/conf/bestman2.rc.samples\n\n\n/etc/bestman2/conf/srmclient.conf\n\n\n/etc/sysconfig/bestman2\n\n\n/usr/bin/srm-copy\n\n\n/usr/bin/srm-copy-status\n\n\n/usr/bin/srm-extendfilelifetime\n\n\n/usr/bin/srm-ls\n\n\n/usr/bin/srm-ls-status\n\n\n... output trimmed ...\n\n\n\n\n\n\nWhat else does a package install?\n\n\nSometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding \nmeta-packages\n, which are packages such as the \nosg-wn-client\n (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the \n--requires\n option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install \ncurl\n, \nuberftp\n, \nlcg-utils\n, and a dozen or so other packages.\n\n\nuser@host $\n rpm -q --requires osg-wn-client\n\n/usr/bin/curl  \n\n\n/usr/bin/dccp  \n\n\n/usr/bin/ldapsearch  \n\n\n/usr/bin/uberftp  \n\n\n/usr/bin/wget  \n\n\nbestman2-client  \n\n\nconfig(osg-wn-client) = 3.0.0-16.osg.el5\n\n\ndcache-srmclient  \n\n\ndcap-tunnel-gsi  \n\n\nedg-gridftp-client  \n\n\nfetch-crl  \n\n\nglite-fts-client  \n\n\nglobus-gass-copy-progs  \n\n\ngrid-certificates  \n\n\njava-1.6.0-sun-compat  \n\n\nlcg-utils  \n\n\nlfc-client  \n\n\nlfc-python  \n\n\nmyproxy  \n\n\nosg-system-profiler  \n\n\nosg-version  \n\n\nrpmlib(CompressedFileNames) \n= 3.0.4-1\n\n\nrpmlib(PayloadFilesHavePrefix) \n= 4.0-1\n\n\nvo-client\n\n\n\n\n\n\nFinding RPM Packages\n\n\nIt is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":\n\n\nuser@host $\n yum list \nvoms*\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nAvailable Packages\n\n\nvoms.i386                                                    2.0.6-3.osg                                        osg \n\n\nvoms.x86_64                                                  2.0.6-3.osg                                        osg \n\n\nvoms-admin-client.x86_64                                     2.0.16-1                                           osg \n\n\nvoms-admin-server.noarch                                     2.6.1-9                                            osg \n\n\nvoms-clients.x86_64                                          2.0.6-3.osg                                        osg \n\n\nvoms-compat.i386                                             1.9.19.2-6.osg                                     osg \n\n\nvoms-compat.x86_64                                           1.9.19.2-6.osg                                     osg \n\n\nvoms-devel.i386                                              2.0.6-3.osg                                        osg \n\n\nvoms-devel.x86_64                                            2.0.6-3.osg                                        osg \n\n\nvoms-doc.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvoms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel\n\n\nvoms-server.x86_64                                           2.0.6-3.osg                                        osg \n\n\nvomsjapi.x86_64                                              2.0.6-3.osg                                        osg \n\n\nvomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg\n\n\n\n\n\n\nIf you want to search for packages that contain VOMS anywhere in the name or description, you can use \nyum search\n:\n\n\nuser@host $\n yum search voms\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n================================================== Matched: voms ===================================================\n\n\nosg-voms.noarch : OSG VOMS\n\n\nperl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nperl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation\n\n\nphp-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP\n\n\nvoms.i386 : Virtual Organization Membership Service\n\n\nvoms.x86_64 : Virtual Organization Membership Service\n\n\n... etc ...\n\n\n\n\n\n\nOne last example, if you want to know what RPM would give you the \nvoms-proxy-init\n command, you can ask \nyum\n. The \n*\n indicates that you don't know the full pathname of \nvoms-proxy-init\n.\n\n\nuser@host $\n yum whatprovides \n*voms-proxy-init\n\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\nvoms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients\n\n\nRepo        : osg\n\n\nMatched from:\n\n\nFilename    : /usr/bin/voms-proxy-init\n\n\n\n\n\n\nRemoving Packages\n\n\nTo remove a single RPM, you can use \nyum remove\n. Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the \nvoms-clients\n package, I also installed another package it depends on called \nvoms\n. If I remove \nvoms\n, yum will also remove \nvoms-clients\n:\n\n\nuser@host $\n sudo yum remove voms\n\nLoaded plugins: kernel-module, priorities\n\n\nSetting up Remove Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package voms.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients\n\n\n--\n Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients\n\n\n--\n Running transaction check\n\n\n---\n Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                      Arch                   Version                        Repository                 Size\n\n\n====================================================================================================================\n\n\nRemoving:\n\n\n voms                         x86_64                 2.0.6-3.osg                    installed                 407 k\n\n\nRemoving for dependencies:\n\n\n voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nRemove        2 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Erasing        : voms                                                                                         1/2 \n\n\n  Erasing        : voms-clients                                                                                 2/2 \n\n\n\nRemoved:\n\n\n  voms.x86_64 0:2.0.6-3.osg                                                                                         \n\n\n\nDependency Removed:\n\n\n  voms-clients.x86_64 0:2.0.6-3.osg                                                                                 \n\n\n\nComplete!\n\n\n\n\n\n\nUpgrading Packages\n\n\nYou can check for updates with \nyum check-update\n. For example:\n\n\nroot@host #\n yum check-update\n\nLoaded plugins: kernel-module, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n\n\n\n\nYou can do the update with \nyum update\n. Note that in this case we got more than was listed due to dependencies that needed to be resolved:\n\n\nroot@host #\n yum update\n\n957 packages excluded due to repository priority protections\n\n\nSetting up Update Process\n\n\nResolving Dependencies\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed\n\n\n---\n Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated\n\n\n--\n Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: monitor-edid for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::IP) for package: ocsinventory-agent\n\n\n--\n Processing Dependency: nmap for package: ocsinventory-agent\n\n\n--\n Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent\n\n\n--\n Running transaction check\n\n\n---\n Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated\n\n\n---\n Package nmap.x86_64 2:4.11-1.1 set to be updated\n\n\n---\n Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated\n\n\n---\n Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated\n\n\n---\n Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated\n\n\n---\n Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated\n\n\n--\n Finished Dependency Resolution\n\n\nBeginning Kernel Module Plugin\n\n\nFinished Kernel Module Plugin\n\n\n--\n Running transaction check\n\n\n---\n Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased\n\n\n--\n Finished Dependency Resolution\n\n\n\nDependencies Resolved\n\n\n\n====================================================================================================================\n\n\n Package                        Arch               Version                         Repository                  Size\n\n\n====================================================================================================================\n\n\nInstalling:\n\n\n kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M\n\n\n ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k\n\n\n     replacing  ocsinventory-client.noarch 0.9.9-10\n\n\n\nRemoving:\n\n\n kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M\n\n\nInstalling for dependencies:\n\n\n monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k\n\n\n nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k\n\n\n perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k\n\n\n perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k\n\n\n perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k\n\n\n perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k\n\n\n\nTransaction Summary\n\n\n====================================================================================================================\n\n\nInstall       8 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\nRemove        1 Package(s)\n\n\nReinstall     0 Package(s)\n\n\nDowngrade     0 Package(s)\n\n\n\nTotal download size: 22 M\n\n\nIs this ok [y/N]: y\n\n\nDownloading Packages:\n\n\n(1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00     \n\n\n(2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00     \n\n\n(3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00     \n\n\n(4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00     \n\n\n(5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00     \n\n\n(6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00     \n\n\n(7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00     \n\n\n(8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00     \n\n\n--------------------------------------------------------------------------------------------------------------------\n\n\nTotal                                                                               3.5 MB/s |  22 MB     00:06     \n\n\nwarning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6\n\n\nepel/gpgkey                                                                                  | 1.7 kB     00:00     \n\n\nImporting GPG key 0x217521F6 \nFedora EPEL \nepel@fedoraproject.org\n from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL\n\n\nIs this ok [y/N]: y\n\n\nRunning rpm_check_debug\n\n\nRunning Transaction Test\n\n\nFinished Transaction Test\n\n\nTransaction Test Succeeded\n\n\nRunning Transaction\n\n\n  Installing     : perl-Net-SSLeay                                                                             1/10 \n\n\n  Installing     : nmap                                                                                        2/10 \n\n\n  Installing     : monitor-edid                                                                                3/10 \n\n\n  Installing     : perl-Crypt-SSLeay                                                                           4/10 \n\n\n  Installing     : perl-Net-IP                                                                                 5/10 \n\n\n  Installing     : perl-Proc-Daemon                                                                            6/10 \n\n\n  Installing     : kernel                                                                                      7/10 \n\n\n  Installing     : ocsinventory-agent                                                                          8/10 \n\n\nule, priorities\n\n\n957 packages excluded due to repository priority protections\n\n\n\nkernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security\n\n\nObsoleting Packages\n\n\nocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel          \n\n\n    ocsinventory-client.noarch                           0.9.9-10                                     installed     \n\n\n  Erasing        : ocsinventory-client                                                                         9/10 \n\n\nwarning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave\n\n\n  Cleanup        : kernel                                                                                     10/10 \n\n\n\nRemoved:\n\n\n  kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                \n\n\n\nInstalled:\n\n\n  kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                   \n\n\n\nDependency Installed:\n\n\n  monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5  \n\n\n  perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5    \n\n\n\nReplaced:\n\n\n  ocsinventory-client.noarch 0:0.9.9-10                                                                             \n\n\n\nComplete!\n\n\n\n\n\n\nAdvanced topic: Only geting OSG updates\n\n\nIf you only want to get updates from the OSG repository and \nno other\n repositories, you can tell yum to do that with the following command:\n\n\nroot@host #\n yum --disablerepo\n=\n* --enablerepo\n=\nosg update\n\n\n\n\n\nAdvanced topic: Getting debugging information for installed software\n\n\nIf you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run \ngdb\n or \npstack\n to get information about a program.\n\n\nInstalling the debuginfo package requires three steps.\n\n\n\n\n\n\nEnable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually \n/etc/yum.repos.d/osg.repo\n to enable the separate debuginfo repository. Near the bottom of the file, you'll see the \nosg-debug\n repo: \n\n\n[osg-debug]\n\n\n\nname\n=\nOSG Software for Enterprise Linux 5 - $basearch - Debug\n\n\nbaseurl\n=\nhttp://repo.opensciencegrid.org/osg-release/$basearch/debu\n\n\nfailovermethod\n=\npriority \n\n\npriority\n=\n98 \n\n\nenabled\n=\n1\n\n\ngpgcheck\n=\n1 \n\n\ngpgkey\n=\nfile:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nMake sure that \"enabled\" is set to 1.\n\n\n\n\n\n\nFigure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:\n\n\nuser@host $\n rpm -qf \n`\nwhich grid-proxy-init\n`\n\n\nglobus-proxy-utils-5.0-5.osg.x86_64\n\n\n\n\n\n\n\n\n\n\nInstall the debugging information for that package. Continuing this example: \n\n\nroot@host #\n debuginfo-install globus-proxy-utils\n\n...\n\n\n=================================================================================================================================\n\n\n Package                                      Arch                   Version                     Repository                 Size\n\n\n=================================================================================================================================\n\n\nInstalling:\n\n\n globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k\n\n\n\nTransaction Summary\n\n\n=================================================================================================================================\n\n\nInstall       1 Package(s)\n\n\nUpgrade       0 Package(s)\n\n\n\nTotal download size: 61 k\n\n\nIs this ok [y/N]: y\n\n\n...\n\n\nInstalled:\n\n\n  globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg\n\n\n\n\n\n\nThis last step will select the right package name, then use \nyum\n to install it.\n\n\n\n\n\n\nTroubleshooting\n\n\nYum not finding packages\n\n\nIf you is not finding some packages, e.g.:\n\n\nError Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.\n\n\n\n\n\nthen you can try cleaning up Yum's cache:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n*\n\n\n\n\n\nYum complaining about missing keys\n\n\nIf yum is complaining you can re-import the keys in your distribution:\n\n\nroot@host #\n rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*\n\n\n\n\n\nReferences\n\n\n\n\nThe main Yum web site\n\n\nA good description of the commands for RPM and Yum can be found at \nLearn Linux 101: RPM and Yum Package Management\n.", 
            "title": "Yum Basics"
        }, 
        {
            "location": "/release/yum-basics/#basics-of-using-yum-and-rpm", 
            "text": "", 
            "title": "Basics of using yum and RPM"
        }, 
        {
            "location": "/release/yum-basics/#about-this-document", 
            "text": "This document introduces package management tools that help you install, update, and remove packages. OSG uses RPMs (the Red Hat Packaging Manager) to package its software. While RPM is the packaging format,  yum  is the command you will use to do the installation. For example,  yum  will resolve and download the dependencies for the package you want to install;  rpm  will simply complain if you want to install a package that does not have all its dependencies installed.", 
            "title": "About This Document"
        }, 
        {
            "location": "/release/yum-basics/#installation", 
            "text": "Installation is done with the  yum install  command. Each of the individual installation guide shows you the correct command to use to do an installation. Here is an example installation with all of the output from yum.  root@host #  sudo yum install osg-ca-certs Loaded plugins: kernel-module, priorities  epel                                                                                         | 3.7 kB     00:00       epel/primary_db                                                                              | 3.8 MB     00:00       fermi-base                                                                                   | 2.1 kB     00:00       fermi-base/primary_db                                                                        |  48 kB     00:00       fermi-security                                                                               | 1.9 kB     00:00       fermi-security/primary_db                                                                    | 1.7 MB     00:00       osg                                                                                          | 1.9 kB     00:00       osg/primary_db                                                                               |  65 kB     00:00       sl-base                                                                                      | 2.1 kB     00:00       sl-base/primary_db                                                                           | 2.0 MB     00:00       957 packages excluded due to repository priority protections  Setting up Install Process  Resolving Dependencies  --  Running transaction check  ---  Package osg-ca-certs.noarch 0:1.23-1 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                         Arch                      Version                     Repository              Size  ====================================================================================================================  Installing:   osg-ca-certs                    noarch                    1.23-1                      osg                    450 k  Transaction Summary  ====================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 450 k  Is this ok [y/N]: y  Downloading Packages:  osg-ca-certs-1.23-1.noarch.rpm                                                               | 450 kB     00:00       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 824b8603  osg/gpgkey                                                                                   | 1.7 kB     00:00       Importing GPG key 0x824B8603  OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : osg-ca-certs                                                                                 1/1   Installed:    osg-ca-certs.noarch 0:1.23-1                                                                                        Complete!   Please Note : When you first install a package from the OSG repository, you will be prompted to import the GPG key. We use this key to sign our RPMs as a security measure. You should double-check the key id (above it is 824B8603) with the  information on our signed RPMs . If it doesn't match, there is a problem somewhere and you should report it to the OSG via help@opensciencegrid.org.", 
            "title": "Installation"
        }, 
        {
            "location": "/release/yum-basics/#verifying-packages-and-installations", 
            "text": "You can check if an RPM has been modified. For instance, to check to see if any files have been modified in the  osg-ca-certs  RPM you just installed:  user@host $  rpm --verify osg-ca-certs  The lack of any output means there were no problems. If you would like to see all the files for which there are no problems, you can do:  user@host $  rpm --verify -v osg-ca-certs ........    /etc/grid-security/certificates  ........    /etc/grid-security/certificates/0119347c.0  ........    /etc/grid-security/certificates/0119347c.namespaces  ........    /etc/grid-security/certificates/0119347c.signing_policy  ... etc ...   Each dot indicates a specific check that was made and passed. If someone had modified a file, you might see this:  user@host $  rpm --verify osg-ca-certs ..5....T    /etc/grid-security/certificates/ffc3d59b.0   This means the files MD5 checksum has changed (so the contents have changed) and the timestamp is different. The complete set of changes you might see (copied from the  rpm  man page) are:     Letter  Meaning      S  file Size differs    M  Mode differs (includes permissions and file type)    5  MD5 sum differs    D  Device major/minor number mismatch    L  readLink(2) path mismatch    U  User ownership differs    G  Group ownership differs    T  mTime differs     If you don't care about some of those changes, you can tell rpm to ignore them. For instance, to ignore changes in the modification time:  user@host $  rpm --verify --nomtime osg-ca-certs ..5.....    /etc/grid-security/certificates/ffc3d59b.0", 
            "title": "Verifying Packages and Installations"
        }, 
        {
            "location": "/release/yum-basics/#understanding-a-package", 
            "text": "If you want to know what package a file belongs to, you can ask rpm. For instance, if you're curious what package contains the  srm-ls  command, you can do:  #   1 . Find the exact path user@host $  which srm-ls /usr/bin/srm-ls  #   2 . Ask rpm what package it is part of: user@host $  rpm -q --file /usr/bin/srm-ls bestman2-client-2.2.0-14.osg.el5.noarch   If you want to know what other things are in a package--perhaps the other available tools or configuration files--you can do that as well:  user@host $  rpm -ql bestman2-client /etc/bestman2/conf/bestman2.rc  /etc/bestman2/conf/bestman2.rc.samples  /etc/bestman2/conf/srmclient.conf  /etc/sysconfig/bestman2  /usr/bin/srm-copy  /usr/bin/srm-copy-status  /usr/bin/srm-extendfilelifetime  /usr/bin/srm-ls  /usr/bin/srm-ls-status  ... output trimmed ...", 
            "title": "Understanding a package"
        }, 
        {
            "location": "/release/yum-basics/#what-else-does-a-package-install", 
            "text": "Sometimes you need to understand what other software is installed by a package. This can be particularly useful for understanding  meta-packages , which are packages such as the  osg-wn-client  (worker node client) that contain nothing by themselves but only depend on other RPMs. To do this, use the  --requires  option to rpm. For example, you can see that the worker node client (as of OSG 3.1.8 in early September, 2012) will install  curl ,  uberftp ,  lcg-utils , and a dozen or so other packages.  user@host $  rpm -q --requires osg-wn-client /usr/bin/curl    /usr/bin/dccp    /usr/bin/ldapsearch    /usr/bin/uberftp    /usr/bin/wget    bestman2-client    config(osg-wn-client) = 3.0.0-16.osg.el5  dcache-srmclient    dcap-tunnel-gsi    edg-gridftp-client    fetch-crl    glite-fts-client    globus-gass-copy-progs    grid-certificates    java-1.6.0-sun-compat    lcg-utils    lfc-client    lfc-python    myproxy    osg-system-profiler    osg-version    rpmlib(CompressedFileNames)  = 3.0.4-1  rpmlib(PayloadFilesHavePrefix)  = 4.0-1  vo-client", 
            "title": "What else does a package install?"
        }, 
        {
            "location": "/release/yum-basics/#finding-rpm-packages", 
            "text": "It is normally best to read the OSG documentation to decide which packages to install because it may not be obvious what the correct packages to install are. That said, you can use yum to find out all sort of things. For instance, you can list packages that begin with \"voms\":  user@host $  yum list  voms*  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  Available Packages  voms.i386                                                    2.0.6-3.osg                                        osg   voms.x86_64                                                  2.0.6-3.osg                                        osg   voms-admin-client.x86_64                                     2.0.16-1                                           osg   voms-admin-server.noarch                                     2.6.1-9                                            osg   voms-clients.x86_64                                          2.0.6-3.osg                                        osg   voms-compat.i386                                             1.9.19.2-6.osg                                     osg   voms-compat.x86_64                                           1.9.19.2-6.osg                                     osg   voms-devel.i386                                              2.0.6-3.osg                                        osg   voms-devel.x86_64                                            2.0.6-3.osg                                        osg   voms-doc.x86_64                                              2.0.6-3.osg                                        osg   voms-mysql-plugin.x86_64                                     3.1.5.1-1.el5                                      epel  voms-server.x86_64                                           2.0.6-3.osg                                        osg   vomsjapi.x86_64                                              2.0.6-3.osg                                        osg   vomsjapi-javadoc.x86_64                                      2.0.6-3.osg                                        osg   If you want to search for packages that contain VOMS anywhere in the name or description, you can use  yum search :  user@host $  yum search voms Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  ================================================== Matched: voms ===================================================  osg-voms.noarch : OSG VOMS  perl-VOMS-Lite.noarch : Perl extension for VOMS Attribute certificate creation  perl-voms-server.noarch : Perl extension for VOMS Attribute certificate creation  php-voms-admin.noarch : Web based interface to control VOMS parameters written in PHP  voms.i386 : Virtual Organization Membership Service  voms.x86_64 : Virtual Organization Membership Service  ... etc ...   One last example, if you want to know what RPM would give you the  voms-proxy-init  command, you can ask  yum . The  *  indicates that you don't know the full pathname of  voms-proxy-init .  user@host $  yum whatprovides  *voms-proxy-init  Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  voms-clients-2.0.6-3.osg.x86_64 : Virtual Organization Membership Service Clients  Repo        : osg  Matched from:  Filename    : /usr/bin/voms-proxy-init", 
            "title": "Finding RPM Packages"
        }, 
        {
            "location": "/release/yum-basics/#removing-packages", 
            "text": "To remove a single RPM, you can use  yum remove . Not only will it uninstall the RPM you requested, but it will uninstall anything that depends on it. For example, if I previously installed the  voms-clients  package, I also installed another package it depends on called  voms . If I remove  voms , yum will also remove  voms-clients :  user@host $  sudo yum remove voms Loaded plugins: kernel-module, priorities  Setting up Remove Process  Resolving Dependencies  --  Running transaction check  ---  Package voms.x86_64 0:2.0.6-3.osg set to be erased  --  Processing Dependency: libvomsapi.so.1()(64bit) for package: voms-clients  --  Processing Dependency: voms = 2.0.6-3.osg for package: voms-clients  --  Running transaction check  ---  Package voms-clients.x86_64 0:2.0.6-3.osg set to be erased  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  Dependencies Resolved  ====================================================================================================================   Package                      Arch                   Version                        Repository                 Size  ====================================================================================================================  Removing:   voms                         x86_64                 2.0.6-3.osg                    installed                 407 k  Removing for dependencies:   voms-clients                 x86_64                 2.0.6-3.osg                    installed                 373 k  Transaction Summary  ====================================================================================================================  Remove        2 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Is this ok [y/N]: y  Downloading Packages:  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Erasing        : voms                                                                                         1/2     Erasing        : voms-clients                                                                                 2/2   Removed:    voms.x86_64 0:2.0.6-3.osg                                                                                           Dependency Removed:    voms-clients.x86_64 0:2.0.6-3.osg                                                                                   Complete!", 
            "title": "Removing Packages"
        }, 
        {
            "location": "/release/yum-basics/#upgrading-packages", 
            "text": "You can check for updates with  yum check-update . For example:  root@host #  yum check-update Loaded plugins: kernel-module, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed        You can do the update with  yum update . Note that in this case we got more than was listed due to dependencies that needed to be resolved:  root@host #  yum update 957 packages excluded due to repository priority protections  Setting up Update Process  Resolving Dependencies  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-274.3.1.el5 set to be installed  ---  Package ocsinventory-agent.noarch 0:1.1.2.1-1.el5 set to be updated  --  Processing Dependency: perl(Crypt::SSLeay) for package: ocsinventory-agent  --  Processing Dependency: perl(Proc::Daemon) for package: ocsinventory-agent  --  Processing Dependency: monitor-edid for package: ocsinventory-agent  --  Processing Dependency: perl(Net::IP) for package: ocsinventory-agent  --  Processing Dependency: nmap for package: ocsinventory-agent  --  Processing Dependency: perl(Net::SSLeay) for package: ocsinventory-agent  --  Running transaction check  ---  Package monitor-edid.x86_64 0:2.5-1.el5.1 set to be updated  ---  Package nmap.x86_64 2:4.11-1.1 set to be updated  ---  Package perl-Crypt-SSLeay.x86_64 0:0.51-11.el5 set to be updated  ---  Package perl-Net-IP.noarch 0:1.25-2.fc6 set to be updated  ---  Package perl-Net-SSLeay.x86_64 0:1.30-4.fc6 set to be updated  ---  Package perl-Proc-Daemon.noarch 0:0.03-1.el5 set to be updated  --  Finished Dependency Resolution  Beginning Kernel Module Plugin  Finished Kernel Module Plugin  --  Running transaction check  ---  Package kernel.x86_64 0:2.6.18-238.1.1.el5 set to be erased  --  Finished Dependency Resolution  Dependencies Resolved  ====================================================================================================================   Package                        Arch               Version                         Repository                  Size  ====================================================================================================================  Installing:   kernel                         x86_64             2.6.18-274.3.1.el5              fermi-security              21 M   ocsinventory-agent             noarch             1.1.2.1-1.el5                   epel                       156 k       replacing  ocsinventory-client.noarch 0.9.9-10  Removing:   kernel                         x86_64             2.6.18-238.1.1.el5              installed                   93 M  Installing for dependencies:   monitor-edid                   x86_64             2.5-1.el5.1                     epel                        82 k   nmap                           x86_64             2:4.11-1.1                      sl-base                    680 k   perl-Crypt-SSLeay              x86_64             0.51-11.el5                     sl-base                     45 k   perl-Net-IP                    noarch             1.25-2.fc6                      sl-base                     31 k   perl-Net-SSLeay                x86_64             1.30-4.fc6                      sl-base                    192 k   perl-Proc-Daemon               noarch             0.03-1.el5                      epel                       9.4 k  Transaction Summary  ====================================================================================================================  Install       8 Package(s)  Upgrade       0 Package(s)  Remove        1 Package(s)  Reinstall     0 Package(s)  Downgrade     0 Package(s)  Total download size: 22 M  Is this ok [y/N]: y  Downloading Packages:  (1/8): perl-Proc-Daemon-0.03-1.el5.noarch.rpm                                                | 9.4 kB     00:00       (2/8): perl-Net-IP-1.25-2.fc6.noarch.rpm                                                     |  31 kB     00:00       (3/8): perl-Crypt-SSLeay-0.51-11.el5.x86_64.rpm                                              |  45 kB     00:00       (4/8): monitor-edid-2.5-1.el5.1.x86_64.rpm                                                   |  82 kB     00:00       (5/8): ocsinventory-agent-1.1.2.1-1.el5.noarch.rpm                                           | 156 kB     00:00       (6/8): perl-Net-SSLeay-1.30-4.fc6.x86_64.rpm                                                 | 192 kB     00:00       (7/8): nmap-4.11-1.1.x86_64.rpm                                                              | 680 kB     00:00       (8/8): kernel-2.6.18-274.3.1.el5.x86_64.rpm                                                  |  21 MB     00:00       --------------------------------------------------------------------------------------------------------------------  Total                                                                               3.5 MB/s |  22 MB     00:06       warning: rpmts_HdrFromFdno: Header V3 DSA signature: NOKEY, key ID 217521f6  epel/gpgkey                                                                                  | 1.7 kB     00:00       Importing GPG key 0x217521F6  Fedora EPEL  epel@fedoraproject.org  from /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL  Is this ok [y/N]: y  Running rpm_check_debug  Running Transaction Test  Finished Transaction Test  Transaction Test Succeeded  Running Transaction    Installing     : perl-Net-SSLeay                                                                             1/10     Installing     : nmap                                                                                        2/10     Installing     : monitor-edid                                                                                3/10     Installing     : perl-Crypt-SSLeay                                                                           4/10     Installing     : perl-Net-IP                                                                                 5/10     Installing     : perl-Proc-Daemon                                                                            6/10     Installing     : kernel                                                                                      7/10     Installing     : ocsinventory-agent                                                                          8/10   ule, priorities  957 packages excluded due to repository priority protections  kernel.x86_64                                            2.6.18-274.3.1.el5                           fermi-security  Obsoleting Packages  ocsinventory-agent.noarch                                1.1.2.1-1.el5                                epel                ocsinventory-client.noarch                           0.9.9-10                                     installed         Erasing        : ocsinventory-client                                                                         9/10   warning: /etc/ocsinventory-client/ocsinv.conf saved as /etc/ocsinventory-client/ocsinv.conf.rpmsave    Cleanup        : kernel                                                                                     10/10   Removed:    kernel.x86_64 0:2.6.18-238.1.1.el5                                                                                  Installed:    kernel.x86_64 0:2.6.18-274.3.1.el5                    ocsinventory-agent.noarch 0:1.1.2.1-1.el5                     Dependency Installed:    monitor-edid.x86_64 0:2.5-1.el5.1   nmap.x86_64 2:4.11-1.1                perl-Crypt-SSLeay.x86_64 0:0.51-11.el5      perl-Net-IP.noarch 0:1.25-2.fc6     perl-Net-SSLeay.x86_64 0:1.30-4.fc6   perl-Proc-Daemon.noarch 0:0.03-1.el5      Replaced:    ocsinventory-client.noarch 0:0.9.9-10                                                                               Complete!", 
            "title": "Upgrading Packages"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-only-geting-osg-updates", 
            "text": "If you only want to get updates from the OSG repository and  no other  repositories, you can tell yum to do that with the following command:  root@host #  yum --disablerepo = * --enablerepo = osg update", 
            "title": "Advanced topic: Only geting OSG updates"
        }, 
        {
            "location": "/release/yum-basics/#advanced-topic-getting-debugging-information-for-installed-software", 
            "text": "If you run into a problem with our software and have a hankering to debug it directly (or perhaps we need to ask you for some help), you can install so-called \"debuginfo\" packages. These packages will provide debugging symbols and source code so that you can do things like run  gdb  or  pstack  to get information about a program.  Installing the debuginfo package requires three steps.    Enable the installation of debuginfo packages. This only needs to be done once. Edit the yum repo file, usually  /etc/yum.repos.d/osg.repo  to enable the separate debuginfo repository. Near the bottom of the file, you'll see the  osg-debug  repo:   [osg-debug]  name = OSG Software for Enterprise Linux 5 - $basearch - Debug  baseurl = http://repo.opensciencegrid.org/osg-release/$basearch/debu  failovermethod = priority   priority = 98   enabled = 1  gpgcheck = 1   gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-OSG   Make sure that \"enabled\" is set to 1.    Figure out which package installed the program you want to debug. One way to figure it out is to ask RPM. For example, if you want to debug grid-proxy-init:  user@host $  rpm -qf  ` which grid-proxy-init `  globus-proxy-utils-5.0-5.osg.x86_64     Install the debugging information for that package. Continuing this example:   root@host #  debuginfo-install globus-proxy-utils ...  =================================================================================================================================   Package                                      Arch                   Version                     Repository                 Size  =================================================================================================================================  Installing:   globus-proxy-utils-debuginfo                 x86_64                 5.0-5.osg                   osg-debug                  61 k  Transaction Summary  =================================================================================================================================  Install       1 Package(s)  Upgrade       0 Package(s)  Total download size: 61 k  Is this ok [y/N]: y  ...  Installed:    globus-proxy-utils-debuginfo.x86_64 0:5.0-5.osg   This last step will select the right package name, then use  yum  to install it.", 
            "title": "Advanced topic: Getting debugging information for installed software"
        }, 
        {
            "location": "/release/yum-basics/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/release/yum-basics/#yum-not-finding-packages", 
            "text": "If you is not finding some packages, e.g.:  Error Downloading Packages:\n  packageXYZ: failure: packageXYZ.rpm from osg: [Errno 256] No more mirrors to try.  then you can try cleaning up Yum's cache:  root@host #  yum clean all --enablerepo = *", 
            "title": "Yum not finding packages"
        }, 
        {
            "location": "/release/yum-basics/#yum-complaining-about-missing-keys", 
            "text": "If yum is complaining you can re-import the keys in your distribution:  root@host #  rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY*", 
            "title": "Yum complaining about missing keys"
        }, 
        {
            "location": "/release/yum-basics/#references", 
            "text": "The main Yum web site  A good description of the commands for RPM and Yum can be found at  Learn Linux 101: RPM and Yum Package Management .", 
            "title": "References"
        }, 
        {
            "location": "/release/signing/", 
            "text": "OSG Release Signing Information\n\n\nVerifying OSG's RPMs\n\n\nWe use a GPG key to sign our software packages. Normally \nyum\n and \nrpm\n transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the \nosg-release\n RPM.\n\n\nIf you wish to verify one of our RPMs manually, you can run:\n\n\n$\n rpm --checksig -v \nNAME.RPM\n\n\n\n\n\n\nFor example:\n\n\n$\n rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm \n\nglobus-core-8.0-2.osg.x86_64.rpm:\n\n\n    Header V3 DSA signature: OK, key ID 824b8603\n\n\n    Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)\n\n\n    MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)\n\n\n    V3 DSA signature: OK, key ID 824b8603\n\n\n\n\n\n\nThe OSG Packaging Signing Key\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\n/etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\n\n\n\n\n\nDownload\n\n\nUW-Madison\n, \nGitHub\n\n\n\n\n\n\nFingerprint\n\n\n6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603\n\n\n\n\n\n\nKey ID\n\n\n824b8603\n\n\n\n\n\n\n\n\nYou can see the fingerprint for yourself:\n\n\n$\n gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG\n\npub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages) \nvdt-support@opensciencegrid.org\n\n\n      Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603\n\n\nsub  2048g/28E5857C 2011-09-15", 
            "title": "RPM Signing"
        }, 
        {
            "location": "/release/signing/#osg-release-signing-information", 
            "text": "", 
            "title": "OSG Release Signing Information"
        }, 
        {
            "location": "/release/signing/#verifying-osgs-rpms", 
            "text": "We use a GPG key to sign our software packages. Normally  yum  and  rpm  transparently use the GPG signatures to verify the packages have not been corrupted and were created by us. You get our GPG public key when you install the  osg-release  RPM.  If you wish to verify one of our RPMs manually, you can run:  $  rpm --checksig -v  NAME.RPM   For example:  $  rpm --checksig -v globus-core-8.0-2.osg.x86_64.rpm  globus-core-8.0-2.osg.x86_64.rpm:      Header V3 DSA signature: OK, key ID 824b8603      Header SHA1 digest: OK (2b5af4348c548c27f10e2e47e1ec80500c4f85d7)      MD5 digest: OK (d11503a229a1a0e02262034efe0f7e46)      V3 DSA signature: OK, key ID 824b8603", 
            "title": "Verifying OSG's RPMs"
        }, 
        {
            "location": "/release/signing/#the-osg-packaging-signing-key", 
            "text": "Location  /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG    Download  UW-Madison ,  GitHub    Fingerprint  6459 !D9D2 AAA9 AB67 A251  FB44 2110 !B1C8 824B 8603    Key ID  824b8603     You can see the fingerprint for yourself:  $  gpg --with-fingerprint /etc/pki/rpm-gpg/RPM-GPG-KEY-OSG pub  1024D/824B8603 2011-09-15 OSG Software Team (RPM Signing Key for Koji Packages)  vdt-support@opensciencegrid.org        Key fingerprint = 6459 D9D2 AAA9 AB67 A251  FB44 2110 B1C8 824B 8603  sub  2048g/28E5857C 2011-09-15", 
            "title": "The OSG Packaging Signing Key"
        }, 
        {
            "location": "/monitoring/install-rsv/", 
            "text": "Installing, Configuring, Using, and Troubleshooting RSV\n\n\nAbout This Guide\n\n\nThe Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called \nprobes\n) to run.\n\n\nUse this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the \nReference section\n.\n\n\nIntroduction to RSV\n\n\nThe Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain\nresource and service monitoring infrastructure.\nIt consists of client tools that allow a site administrator to run tests against their site by providing a set of tests\n(which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and\nstoring the results (using Gratia).\nThe client package is not installed by default and may be installed on a CE or other host.\nGenerally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on\na local website. Also, the client can upload test results to a central collector (see next item).\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nUser IDs:\n If they do not exist already, the installation will create the Linux user IDs \nrsv\n and \ncndrcron\n\n\nService certificate:\n The RSV service requires a service certificate (\n/etc/grid-security/rsv/rsvcert.pem\n) and matching key (\n/etc/grid-security/rsv/rsvkey.pem\n)\n\n\nNetwork ports:\n To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too\n\n\nHost choice:\n Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host\n\n\n\n\nAs with all OSG software installations, there are some one-time (per host) steps to prepare in advance:\n\n\n\n\nEnsure the RSV host has \na supported operating system\n\n\nObtain root access to the host\n\n\nPrepare \nthe required Yum repositories\n\n\nInstall \nCA certificates\n\n\n\n\nInstalling RSV\n\n\nAn installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.\n\n\n\n\n\n\nConsider updating your local cache of Yum repository data and your existing RPM packages:\n\n\nroot@host #\n yum clean all --enablerepo\n=\n\\*\n\n\nroot@host #\n yum update\n\n\n\n\n\n\n\nNote\n\n\nThe \nupdate\n command will update \nall\n packages on your system.\n\n\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:\n\n\nroot@host #\n yum install empty-condor --enablerepo\n=\nosg-empty\n\n\n\n\n\n\n\n\n\nInstall RSV and related software:\n\n\nroot@host #\n yum install rsv\n\n\n\n\n\n\n\n\n\nConfiguring RSV\n\n\nAfter installation, there are some one-time configuration steps to tell RSV how to operate at your site.\n\n\n\n\n\n\nEdit \n/etc/osg/config.d/30-rsv.ini\n and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the \nhtcondor_ce_hosts\n variable to the fully qualified hostname of your CE.\n\n\n\n\n\n\nIf you have installed HTCondor already but not by RPM, specify the location of the Condor installation in \n30-rsv.ini\n in the \ncondor_location\n setting. If an HTCondor RPM is installed, you do not need to set \ncondor_location\n.\n\n\n\n\n\n\nComplete the configuration using the \nosg-configure\n tool:\n\n\nroot@host #\n osg-configure -v\n\nroot@host #\n osg-configure -c\n\n\n\n\n\n\n\n\n\nOptional configuration\n\n\nThe following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to \nthe section on using RSV\n.\n\n\nGenerally speaking, read the \nConfigureRsv\n page for more advanced configuration options.\n\n\nConfiguring RSV to run probes using a remote server\n\n\nRSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.\n\n\nIn this case, remember to:\n\n\n\n\nAdd the RSV user \nrsv\n on all the systems where the probes may run, and\n\n\nMap the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. \nThis can be done in the configuration of the \nLCMAPS VOMS plugin\n on your CE.\n\n\n\n\nConfiguring the RSV web server to use HTTPS instead of HTTP\n\n\nIf you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in \n/etc/grid-security/http/\n. If not, omit the \nSSLCertificate*\n modifications below, and your web server will start with its own, self-signed certificate.\n\n\n\n\n\n\nInstall \nmod_ssl\n:\n\n\nroot@host #\n yum install mod_ssl\n\n\n\n\n\n\n\n\n\nMake an alternate set of HTTP service certificate files:\n\n\nroot@host #\n cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem\n\nroot@host #\n cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem\n\nroot@host #\n chown apache:apache /etc/grid-security/http/http*2.pem\n\n\n\n\n\n\n\n\n\nBack up existing Apache configuration files:\n\n\nroot@host #\n cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig\n\nroot@host #\n cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig\n\n\n\n\n\n\n\n\n\nChange the default port for HTTP connections to 8000 by editing \n/etc/httpd/conf/httpd.conf\n\n\nListen\n \n8000\n\n\n\n\n\n\n\n\n\n\nSet up HTTPS access by editing \n/etc/httpd/conf.d/ssl.conf\n:\n\n\nListen\n \n8443\n\n\nVirtualHost\n \n_default_\n:\n8443\n\n\nSSLCertificateFile\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nhttp\n/\nhttpcert2\n.\npem\n\n\nSSLCertificateKeyFile\n \n/\netc\n/\ngrid\n-\nsecurity\n/\nhttp\n/\nhttpkey2\n.\npem\n\n\n\n\n\n\nAfter these changes, when you start the Apache service, it will listening on ports \n8000\n (for HTTP) and \n8443\n (for HTTPS), rather than the default port \n80\n (for HTTP only).\n\n\n\n\nWarning\n\n\nif you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.\n\n\n\n\n\n\n\n\nUsing RSV\n\n\nManaging RSV and associated services\n\n\nIn addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nFetch CRL\n\n\nfetch-crl-boot\n and \nfetch-crl-cron\n\n\nSee \nCA documentation\n\n\n\n\n\n\nApache\n\n\nhttpd\n\n\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\n\n\n\n\n\n\n\n\nStart the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as \nroot\n):\n\n\n\n\n\n\n\n\nTo \u2026\n\n\nRun the command \u2026\n\n\n\n\n\n\n\n\n\n\nStart a service\n\n\nservice \nSERVICE-NAME\n start\n\n\n\n\n\n\nStop a service\n\n\nservice \nSERVICE-NAME\n stop\n\n\n\n\n\n\nEnable a service to start during boot\n\n\nchkconfig \nSERVICE-NAME\n on\n\n\n\n\n\n\nDisable a service from starting during boot\n\n\nchkconfig \nSERVICE-NAME\n off\n\n\n\n\n\n\n\n\nRunning RSV manually\n\n\nNormally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:\n\n\nroot@host #\n rsv-control --run --all-enabled\n\n\n\n\n\nIf successful, results will be available from your local RSV web server (e.g., \nhttp://localhost/rsv\n) and, if enabled (which is the default) on \nMyOSG\n.\n\n\nYou can also run the metrics individually or pass special parameters as explained in the \nrsv-control document\n.\n\n\nTroubleshooting RSV\n\n\nTo get assistance, use the \nhelp procedure\n.\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\nYou can find more information on troubleshooting RSV in the \nrsv-control documentation\n.\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded:\n\n\n\n\nFailed to send via Gratia\n\n\nIf you see \nFailed to send record Failed to send via Gratia: Server unable to receive data:\n in \n/var/log/rsv/consumers/gratia-consumer.output\n you should process to disable the gratia consumer using the following commands\n\n\nroot@host#\n  rsv-control --disable --host \nYOUR RSV HOST\n gratia-consumer\n\nroot@host#\n  rsv-control --off --host \nYOUR RSV HOST\n gratia-consumer\n\n\n\n\n\nImportant file locations\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nHTML files\n\n\n/usr/share/rsv/www/\n\n\nAvailable at \nhttp://your.host.example.com/rsv\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv\n\n\nGenerally files in this directory should not be edited directly. Use \nosg-configure\n instead.\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/HOSTNAME/METRICNAME.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run rsv-control with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nFor example, here is the output when running a metric with -v2.\n\n\n\n  \nShow detailed ouput\n\n\n   [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu\n\n\n   INFO: Reading configuration file /etc/rsv/rsv.conf\n\n\n   INFO: Reading configuration file /etc/rsv/consumers.conf\n\n\n   INFO: Validating configuration:\n\n\n   INFO: Validating user:\n\n\nINFO:     Invoked as root.  Switching to \nrsv\n user (uid: 100 - gid: 102)\n\n\nINFO: Registered consumers: html-consumer, gratia-consumer\n\n\nINFO: Loading config file \n/etc/rsv/meta/metrics/org.osg.general.osg-version.meta\n\n\nINFO: Loading config file \n/etc/rsv/metrics/org.osg.general.osg-version.conf\n\n\nINFO: Optional config file \n/etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf\n does not exist\n\n\nINFO: Checking proxy:\n\n\nINFO:     Using service certificate proxy\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Service certificate valid for at least 6 hours.\n\n\nINFO: Pinging host osg-edu.cs.wisc.edu:\n\n\nINFO: Running command with timeout (1200 seconds):\n\n\n        /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu\n\n\nINFO: Exit code of job: 0\n\n\nINFO:     Ping successful\n\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nINFO: Executing job remotely using Condor-G\n\n\nINFO: Setting up job environment:\n\n\nINFO:     No environment setup declared\n\n\nINFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF\n\n\nINFO: Forming arguments:\n\n\nINFO:     Arguments: \n\n\nINFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm\n\n\nINFO: Condor submission: Submitting job(s).\n\n\n1 job(s) submitted to cluster 2.\n\n\nINFO: Trimming data to 10000 bytes because details-data-trim-length is set\n\n\nINFO: Creating record for html-consumer consumer at \n/var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn\n\n\nINFO: Creating record for gratia-consumer consumer at \n/var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL\n\n\nINFO: Result:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2012-01-25 16:12:40 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: fermicloud016.fnal.gov\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.26\n\n\n\nEOT\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use \nthis page\n and attach the output of \nrsv-control --profile\n:\n\n\nroot@host #\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\nUsers\n\n\nThe RSV installation will create two users unless they are already created. The users are created when the \nrsv\n and \ncondor-cron\n packages are installed.\n\n\n\n\n\n\n\n\nUser\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nrsv\n\n\nRuns the RSV tests; the RSV certificate (below) will need to be owned by this user\n\n\n\n\n\n\ncndrcron\n\n\nRuns the Condor Cron processes to schedule the running of the tests\n\n\n\n\n\n\n\n\n\n\nNote\n\n\nif you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of \n/sbin/nologin\n.\n\n\n\n\n\n\nWarning\n\n\nIf you manage your \n/etc/passwd\n file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in \n/etc/condor-cron/config.d/condor_ids\n matches the UID and GID of the \ncndrcron\n user and group in \n/etc/passwd\n. If it does not, create a file named \n/etc/condor-cron/config.d/condor_ids_override\n with the contents:\n\n\n\n\nCONDOR_IDS\n=\nUID\n.\nGID\n\n\n\n\n\n\nwhere \nUID\n and \nGID\n are the UID and GID of the \ncndrcron\n user and group.\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nRSV service certificate\n\n\nrsv\n\n\n/etc/grid-security/rsv/rsvcert.pem\n\n\n\n\n\n\n\n\n\n\n/etc/grid-security/rsv/rsvkey.pem\n\n\n\n\n\n\n\n\nEnsure an RSV service certificate is installed in \n/etc/grid-security/rsv/\n and the certificate files are owned by the \nrsv\n user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).\n\n\nYou may need another certificate owned by \napache\n if you'd like an authenticated web server; see \nConfiguring the RSV web server to use HTTPS instead of HTTP\n above.\n\n\nSee \ninstructions\n to request a service certificate.\n\n\nNetworking\n\n\n\n\n\n\n\n\nService Name\n\n\nProtocol\n\n\nPort Number\n\n\nInbound\n\n\nOutbound\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\nYES\n\n\n\n\nRSV runs an HTTP server (Apache) that publishes a page with the RSV testing results\n\n\n\n\n\n\nHTTP\n\n\ntcp\n\n\n80\n\n\n\n\nYES\n\n\nRSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org\n\n\n\n\n\n\nvarious\n\n\nvarious\n\n\nvarious\n\n\n\n\nYES\n\n\nAllow outbound network connection to all services that you want to test\n\n\n\n\n\n\n\n\nOr, if you'd rather have your RSV web page appear as \nhttps\n://...:8443/rsv/\n like it used to in OSG 1.2, the first column above would be \nHTTPS\n / \ntcp\n / \n8443\n. See \nabove\n for how to configure this.", 
            "title": "Install RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-configuring-using-and-troubleshooting-rsv", 
            "text": "", 
            "title": "Installing, Configuring, Using, and Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#about-this-guide", 
            "text": "The Resource and Service Validation (RSV) software helps a site administrator verify that certain site resources and services are working as expected. OSG recommends that sites install and run RSV, but it is optional; further, each site selects which specific tests (called  probes ) to run.  Use this page to learn more about RSV in general, and how to install, configure, run, test, and troubleshoot RSV from the OSG software repositories. For documentation on specific probes or on how to write your own probes, please check the  Reference section .", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv/#introduction-to-rsv", 
            "text": "The Resource and Service Validation (RSV) software provides OSG site administrators a scalable and easy-to-maintain\nresource and service monitoring infrastructure.\nIt consists of client tools that allow a site administrator to run tests against their site by providing a set of tests\n(which can run on the same or other hosts within a site), HTCondor-Cron for scheduling, and tools for collecting and\nstoring the results (using Gratia).\nThe client package is not installed by default and may be installed on a CE or other host.\nGenerally, you configure the RSV client to run tests at scheduled time intervals and then it makes results available on\na local website. Also, the client can upload test results to a central collector (see next item).", 
            "title": "Introduction to RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   User IDs:  If they do not exist already, the installation will create the Linux user IDs  rsv  and  cndrcron  Service certificate:  The RSV service requires a service certificate ( /etc/grid-security/rsv/rsvcert.pem ) and matching key ( /etc/grid-security/rsv/rsvkey.pem )  Network ports:  To view results, port 80 must accept incoming requests; outbound connectivity to tested services must work, too  Host choice:  Install RSV on your site CE unless you have specific reasons (e.g., performance) for installing on a separate host   As with all OSG software installations, there are some one-time (per host) steps to prepare in advance:   Ensure the RSV host has  a supported operating system  Obtain root access to the host  Prepare  the required Yum repositories  Install  CA certificates", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv/#installing-rsv", 
            "text": "An installation of RSV at a site consists of the RSV client software, the Apache web server, parts of HTCondor (for its cron-like scheduling capabilities), and various other small tools. To simplify installation, OSG provides a convenience RPM that installs all required software with a single command.    Consider updating your local cache of Yum repository data and your existing RPM packages:  root@host #  yum clean all --enablerepo = \\*  root@host #  yum update   Note  The  update  command will update  all  packages on your system.     If you have installed HTCondor already but not by RPM, install a special empty RPM to make RSV happy:  root@host #  yum install empty-condor --enablerepo = osg-empty    Install RSV and related software:  root@host #  yum install rsv", 
            "title": "Installing RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv", 
            "text": "After installation, there are some one-time configuration steps to tell RSV how to operate at your site.    Edit  /etc/osg/config.d/30-rsv.ini  and follow the instructions in the file. There are detailed comments for each setting. In the simplest case \u2014 to monitor only your CE \u2014 set the  htcondor_ce_hosts  variable to the fully qualified hostname of your CE.    If you have installed HTCondor already but not by RPM, specify the location of the Condor installation in  30-rsv.ini  in the  condor_location  setting. If an HTCondor RPM is installed, you do not need to set  condor_location .    Complete the configuration using the  osg-configure  tool:  root@host #  osg-configure -v root@host #  osg-configure -c", 
            "title": "Configuring RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#optional-configuration", 
            "text": "The following configuration steps are optional and will likely not be required for setting up a small or typical site. If you do not need any of the following special configurations, skip to  the section on using RSV .  Generally speaking, read the  ConfigureRsv  page for more advanced configuration options.", 
            "title": "Optional configuration"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-rsv-to-run-probes-using-a-remote-server", 
            "text": "RSV monitors systems by running probes, which can run on the RSV host itself (the default case), via a separate batch system like HTCondor, or via a remote batch system using a Globus gatekeeper and its job manager. The last two options both can count those jobs and report them to, for example, Gratia.  In this case, remember to:   Add the RSV user  rsv  on all the systems where the probes may run, and  Map the RSV service certificate to the user you intend to use for RSV. This should be a local user used exclusively for RSV and not belonging to an institutional VO to avoid for the RSV probes to be accounted as regular VO jobs in Gratia. \nThis can be done in the configuration of the  LCMAPS VOMS plugin  on your CE.", 
            "title": "Configuring RSV to run probes using a remote server"
        }, 
        {
            "location": "/monitoring/install-rsv/#configuring-the-rsv-web-server-to-use-https-instead-of-http", 
            "text": "If you would like your local RSV web server to use HTTPS instead of the default HTTP (for compatibility or security reasons), complete the steps below. This procedure assumes that you already have an HTTP service certificate (or a copy of the host certificate) in  /etc/grid-security/http/ . If not, omit the  SSLCertificate*  modifications below, and your web server will start with its own, self-signed certificate.    Install  mod_ssl :  root@host #  yum install mod_ssl    Make an alternate set of HTTP service certificate files:  root@host #  cp -p /etc/grid-security/http/httpcert.pem /etc/grid-security/http/httpcert2.pem root@host #  cp -p /etc/grid-security/http/httpkey.pem /etc/grid-security/http/httpkey2.pem root@host #  chown apache:apache /etc/grid-security/http/http*2.pem    Back up existing Apache configuration files:  root@host #  cp -p /etc/httpd/conf/httpd.conf /etc/httpd/conf/httpd.conf.orig root@host #  cp -p /etc/httpd/conf.d/ssl.conf /etc/httpd/conf.d/ssl.conf.orig    Change the default port for HTTP connections to 8000 by editing  /etc/httpd/conf/httpd.conf  Listen   8000     Set up HTTPS access by editing  /etc/httpd/conf.d/ssl.conf :  Listen   8443  VirtualHost   _default_ : 8443  SSLCertificateFile   / etc / grid - security / http / httpcert2 . pem  SSLCertificateKeyFile   / etc / grid - security / http / httpkey2 . pem   After these changes, when you start the Apache service, it will listening on ports  8000  (for HTTP) and  8443  (for HTTPS), rather than the default port  80  (for HTTP only).   Warning  if you make the changes above, you must restart the Apache server after each CA certificate update to pick up the changes.", 
            "title": "Configuring the RSV web server to use HTTPS instead of HTTP"
        }, 
        {
            "location": "/monitoring/install-rsv/#using-rsv", 
            "text": "", 
            "title": "Using RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#managing-rsv-and-associated-services", 
            "text": "In addition to the RSV service itself, there are a number of supporting services in your installation. The specific services are:     Software  Service name  Notes      Fetch CRL  fetch-crl-boot  and  fetch-crl-cron  See  CA documentation    Apache  httpd     HTCondor-Cron  condor-cron     RSV  rsv      Start the services in the order listed and stop them in reverse order. As a reminder, here are common service commands (all run as  root ):     To \u2026  Run the command \u2026      Start a service  service  SERVICE-NAME  start    Stop a service  service  SERVICE-NAME  stop    Enable a service to start during boot  chkconfig  SERVICE-NAME  on    Disable a service from starting during boot  chkconfig  SERVICE-NAME  off", 
            "title": "Managing RSV and associated services"
        }, 
        {
            "location": "/monitoring/install-rsv/#running-rsv-manually", 
            "text": "Normally, the HTCondor-Cron scheduler runs RSV periodically. However, you can run RSV probes manually at any time:  root@host #  rsv-control --run --all-enabled  If successful, results will be available from your local RSV web server (e.g.,  http://localhost/rsv ) and, if enabled (which is the default) on  MyOSG .  You can also run the metrics individually or pass special parameters as explained in the  rsv-control document .", 
            "title": "Running RSV manually"
        }, 
        {
            "location": "/monitoring/install-rsv/#troubleshooting-rsv", 
            "text": "To get assistance, use the  help procedure .  RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)   You can find more information on troubleshooting RSV in the  rsv-control documentation .   Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded:", 
            "title": "Troubleshooting RSV"
        }, 
        {
            "location": "/monitoring/install-rsv/#failed-to-send-via-gratia", 
            "text": "If you see  Failed to send record Failed to send via Gratia: Server unable to receive data:  in  /var/log/rsv/consumers/gratia-consumer.output  you should process to disable the gratia consumer using the following commands  root@host#   rsv-control --disable --host  YOUR RSV HOST  gratia-consumer root@host#   rsv-control --off --host  YOUR RSV HOST  gratia-consumer", 
            "title": "Failed to send via Gratia"
        }, 
        {
            "location": "/monitoring/install-rsv/#important-file-locations", 
            "text": "Logs and configuration:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     HTML files  /usr/share/rsv/www/  Available at  http://your.host.example.com/rsv        File Description  Location  Comment      Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    RSV configuration  /etc/rsv  Generally files in this directory should not be edited directly. Use  osg-configure  instead.    Metric configuration  /etc/rsv/metrics/HOSTNAME/METRICNAME.conf  To change arguments and environment     To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important file locations"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run rsv-control with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output   For example, here is the output when running a metric with -v2.  \n   Show detailed ouput     [root@fermicloud016 condor]# rsv-control -r org.osg.general.osg-version -v 2 -u osg-edu.cs.wisc.edu     INFO: Reading configuration file /etc/rsv/rsv.conf     INFO: Reading configuration file /etc/rsv/consumers.conf     INFO: Validating configuration:     INFO: Validating user:  INFO:     Invoked as root.  Switching to  rsv  user (uid: 100 - gid: 102)  INFO: Registered consumers: html-consumer, gratia-consumer  INFO: Loading config file  /etc/rsv/meta/metrics/org.osg.general.osg-version.meta  INFO: Loading config file  /etc/rsv/metrics/org.osg.general.osg-version.conf  INFO: Optional config file  /etc/rsv/metrics/osg-edu.cs.wisc.edu/org.osg.general.osg-version.conf  does not exist  INFO: Checking proxy:  INFO:     Using service certificate proxy  INFO: Running command with timeout (1200 seconds):          /usr/bin/openssl x509 -in /tmp/rsvproxy -noout -enddate -checkend 21600  INFO: Exit code of job: 0  INFO:     Service certificate valid for at least 6 hours.  INFO: Pinging host osg-edu.cs.wisc.edu:  INFO: Running command with timeout (1200 seconds):          /bin/ping -W 3 -c 1 osg-edu.cs.wisc.edu  INFO: Exit code of job: 0  INFO:     Ping successful  Running metric org.osg.general.osg-version:  INFO: Executing job remotely using Condor-G  INFO: Setting up job environment:  INFO:     No environment setup declared  INFO: Condor-G working directory: /var/tmp/rsv/condor_g-JiQthF  INFO: Forming arguments:  INFO:     Arguments:   INFO: List of files to transfer: /usr/libexec/rsv/probes/RSVMetric.pm  INFO: Condor submission: Submitting job(s).  1 job(s) submitted to cluster 2.  INFO: Trimming data to 10000 bytes because details-data-trim-length is set  INFO: Creating record for html-consumer consumer at  /var/spool/rsv/html-consumer/org.osg.general.osg-version.7rgLfn  INFO: Creating record for gratia-consumer consumer at  /var/spool/rsv/gratia-consumer/org.osg.general.osg-version.-qelnL  INFO: Result:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2012-01-25 16:12:40 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: fermicloud016.fnal.gov  summaryData: OK  detailsData: OSG 1.2.26  EOT", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/install-rsv/#getting-help", 
            "text": "To get assistance, please use  this page  and attach the output of  rsv-control --profile :  root@host #  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv/#users", 
            "text": "The RSV installation will create two users unless they are already created. The users are created when the  rsv  and  condor-cron  packages are installed.     User  Comment      rsv  Runs the RSV tests; the RSV certificate (below) will need to be owned by this user    cndrcron  Runs the Condor Cron processes to schedule the running of the tests      Note  if you pre-create the RSV user, it should have a working shell. That is, it shouldn't have a default shell of  /sbin/nologin .    Warning  If you manage your  /etc/passwd  file with configuration management software such as Puppet, CFEngine or 411, make sure the UID and GID in  /etc/condor-cron/config.d/condor_ids  matches the UID and GID of the  cndrcron  user and group in  /etc/passwd . If it does not, create a file named  /etc/condor-cron/config.d/condor_ids_override  with the contents:   CONDOR_IDS = UID . GID   where  UID  and  GID  are the UID and GID of the  cndrcron  user and group.", 
            "title": "Users"
        }, 
        {
            "location": "/monitoring/install-rsv/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      RSV service certificate  rsv  /etc/grid-security/rsv/rsvcert.pem      /etc/grid-security/rsv/rsvkey.pem     Ensure an RSV service certificate is installed in  /etc/grid-security/rsv/  and the certificate files are owned by the  rsv  user. Adjust the permissions if necessary (cert needs to be readable by all, key needs to be readable by nobody but owner).  You may need another certificate owned by  apache  if you'd like an authenticated web server; see  Configuring the RSV web server to use HTTPS instead of HTTP  above.  See  instructions  to request a service certificate.", 
            "title": "Certificates"
        }, 
        {
            "location": "/monitoring/install-rsv/#networking", 
            "text": "Service Name  Protocol  Port Number  Inbound  Outbound  Comment      HTTP  tcp  80  YES   RSV runs an HTTP server (Apache) that publishes a page with the RSV testing results    HTTP  tcp  80   YES  RSV pushes testing results to the OSG Gratia Collectors at opensciencegrid.org    various  various  various   YES  Allow outbound network connection to all services that you want to test     Or, if you'd rather have your RSV web page appear as  https ://...:8443/rsv/  like it used to in OSG 1.2, the first column above would be  HTTPS  /  tcp  /  8443 . See  above  for how to configure this.", 
            "title": "Networking"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/", 
            "text": "Advanced RSV Configuration\n\n\nAbout This Document\n\n\nMost site administrators will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running osg-configure as described in \nthe RSV installation document\n.  This document provides instructions for configuration beyond what osg-configure is able to do.\n\n\nConfiguring metrics\n\n\nIf you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.\n\n\n\n\n/etc/rsv/metrics\n - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts\n\n\n/etc/rsv/metrics/\nHOST\n - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host\n\n\n\n\nThe configuration files are in INI format and have two sections:\n\n\n\n\na first one named after the metric with execution options\n\n\na second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation\n\n\n\n\nChanging the times a metric runs\n\n\nTo change the time a metric runs set the \ncron-interval\n setting in the metric's conf file. Use \nman 5 crontab\n for a description of the format. For example, to change the \norg.osg.general.ping-host\n to run at a different time:\n\n\n[org.osg.general.ping-host]\n\n\ncron-interval\n \n=\n \n45 * * * *\n\n\n\n[org.osg.general.ping-host args]\n\n\n#ping-count =\n\n\n#ping-timeout =\n\n\n\n\n\n\n\n\nNote\n\n\nBe sure to put the \ncron-interval\n setting in the \n[org.osg.general.ping-host]\n section, and not the \n[org.osg.general.ping-host args]\n section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.\n\n\n\n\nAfter modifying the cron time of a metric you must restart RSV for the change to take effect.\n\n\nTo see what times each of the metrics is running you can use \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control -l --cron-times\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times\n\n\n----------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                         | 28 * * * *\n\n\norg.osg.srm.srmping                                 | 13,33,53 * * * *\n\n\n...\n\n\n\n\n\n\nPassing extra parameters to a metric\n\n\nAny \nkey=value\n pairs in the \"args\" section of the metric's \nconf\n file will be turned into command line parameters to the probe. For example, for this file:\n\n\n[org.osg.certificates.cacert-expiry args]\n\n\nwarning-hours\n \n=\n \n6\n\n\nerror-hours\n \n=\n \n12\n\n\n\n\n\n\nThis would lead to the probe getting called with the command-line parameters \n--warning-hours 6 --error-hours 12\n.\n\n\nConfigure consumers\n\n\nThere is a configuration file common to all consumers: \n/etc/rsv/consumers.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nenabled\n\n\nconsumers\n\n\nComma-separated list of consumers to be enabled\n\n\n\n\n\n\ntimestamp\n\n\nlocal\n\n\nIf this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.\n\n\n\n\n\n\n\n\nEach consumer has a configuration file in \n/etc/rsv/consumers\n named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in \n/etc/rsv/\n. Below is an example for the Nagios consumer.\n\n\nSending RSV records to Nagios\n\n\n\n\nEdit your \n/etc/rsv/rsv-nagios.conf\n file and fill in the appropriate information. The path of the configuration file is specified in \n/etc/rsv/consumers/nagios-consumer.conf\n.\n\n\n\n\nIf your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace \nrsvuser\n with the group of your RSV user (\nrsvuser\n by default)):\n\n\nroot@host#\n chown root:\nrsvuser\n /etc/rsv/rsv-nagios.conf\n\nroot@host#\n chmod \n0440\n /etc/rsv/rsv-nagios.conf\n\n\n\n\n\n\n\n\n\nIn the configuration file at \n/etc/rsv/consumers/nagios-consumer.conf\n, check the following two settings:\n\n\n\n\nMake sure that the path to your config file is correct. It may be referencing a directory \nconfig\n instead of \netc\n\n\nIf you want to use \nrsv2nsca\n add the string \"--send-nsca\" to the \nargs\n line.\n\n\n\n\n\n\n\n\nEnable and start the Nagios consumer by editing \nconsumers.conf\n or by using \nrsv-control\n as follows:\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\n\n\n\n\nThe Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:\n\n\nroot@host#\n rsv-control --on nagios-consumer\n\n\n\n\n\n\n\n\n\nTo verify that the Nagios consumer is running you can run \nrsv-control -j\n.\n\n\n\n\nThe log information for the Nagios consumer can be found in these files:\n\n\n/var/log/rsv/consumers/nagios-consumer.log\n\n\n/var/log/rsv/consumers/nagios-consumer.out\n\n\n/var/log/rsv/consumers/nagios-consumer.err\n\n\n\n\n\n\n\n\nGeneral RSV configuration options\n\n\nYou can configure the RSV framework using \n/etc/rsv/rsv.conf\n. It is a file in INI format and possible entries are:\n\n\n\n\n\n\n\n\nSetting\n\n\nValues\n\n\nDetails\n\n\n\n\n\n\n\n\n\n\nuser\n\n\nusername\n\n\nThe UNIX username that owns RSV. This is mandatory\n\n\n\n\n\n\nservice-cert\n\n\npath\n\n\nAbsolute path to the service certificate file. If this is set service-key and service-proxy must also be set.\n\n\n\n\n\n\nservice-key\n\n\npath\n\n\nAbsolute path to the service key file. This must be used with service-cert.\n\n\n\n\n\n\nservice-proxy\n\n\npath\n\n\nAbsolute path where the service proxy will be generated. This must be used with service-cert.\n\n\n\n\n\n\nproxy-file\n\n\npath\n\n\nAlternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.\n\n\n\n\n\n\ndetails-data-trim-length\n\n\ninteger\n\n\nThe number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.\n\n\n\n\n\n\njob-timeout\n\n\ninteger\n\n\nTime in seconds before a metric is killed. A metric that times out will return a CRITICAL status.\n\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\nImportant files locations\n\n\nConfiguration files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nRSV configuration directory\n\n\n/etc/rsv\n\n\n\n\n\n\n\n\nRSV configuration\n\n\n/etc/rsv/rsv.conf\n\n\nRSV framework configuration\n\n\n\n\n\n\nConsumers configuration in RSV\n\n\n/etc/rsv/consumers.conf\n\n\nSelect the consumers and change generic options\n\n\n\n\n\n\nConsumers configuration\n\n\n/etc/rsv/consumers/\nCONSUMERNAME\n\n\nTo change arguments and environment\n\n\n\n\n\n\nGeneric metrics configuration\n\n\n/etc/rsv/metrics/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\nHost specific metrics configuration\n\n\n/etc/rsv/metrics/\nHOSTNAME\n/\nMETRICNAME\n.conf\n\n\nTo change arguments and environment when running on HOSTNAME\n\n\n\n\n\n\n\n\nOther files:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric log files\n\n\n/var/log/rsv/metrics\n\n\n\n\n\n\n\n\nConsumer log files\n\n\n/var/log/rsv/consumers\n\n\n\n\n\n\n\n\nInitial configuration\n\n\n/etc/osg/config.d/30-rsv.ini\n\n\nRead by \nosg-configure\n\n\n\n\n\n\nWeb files output\n\n\n/usr/share/rsv/www/\n\n\n\n\n\n\n\n\n\n\nTo find the metrics and the other files in RSV you can use also the RPM commands: \nrpm -ql rsv-metrics\n and \nrpm -ql rsv\n.", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#advanced-rsv-configuration", 
            "text": "", 
            "title": "Advanced RSV Configuration"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#about-this-document", 
            "text": "Most site administrators will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running osg-configure as described in  the RSV installation document .  This document provides instructions for configuration beyond what osg-configure is able to do.", 
            "title": "About This Document"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configuring-metrics", 
            "text": "If you need to change the behavior of a metric you can edit the metric configuration files. These replace the spec files from previous versions of RSV.   /etc/rsv/metrics  - changes made to conf file in this directory named after a metric will affect the metric when run against all hosts  /etc/rsv/metrics/ HOST  - changes made to conf files in this directory (named as the host FQDN) will affect the metric when run against the specific host   The configuration files are in INI format and have two sections:   a first one named after the metric with execution options  a second one with the name including the \"args\" keyword, including parameters sent to the probe at invokation", 
            "title": "Configuring metrics"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#changing-the-times-a-metric-runs", 
            "text": "To change the time a metric runs set the  cron-interval  setting in the metric's conf file. Use  man 5 crontab  for a description of the format. For example, to change the  org.osg.general.ping-host  to run at a different time:  [org.osg.general.ping-host]  cron-interval   =   45 * * * *  [org.osg.general.ping-host args]  #ping-count =  #ping-timeout =    Note  Be sure to put the  cron-interval  setting in the  [org.osg.general.ping-host]  section, and not the  [org.osg.general.ping-host args]  section! The purpose of the \"args\" section is described in the \"passing extra parameters to a metric\" section below.   After modifying the cron time of a metric you must restart RSV for the change to take effect.  To see what times each of the metrics is running you can use  rsv-control  as follows:  root@host#  rsv-control -l --cron-times Metrics enabled for host: osg-edu.cs.wisc.edu:10443 | Cron times  ----------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                         | 28 * * * *  org.osg.srm.srmping                                 | 13,33,53 * * * *  ...", 
            "title": "Changing the times a metric runs"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#passing-extra-parameters-to-a-metric", 
            "text": "Any  key=value  pairs in the \"args\" section of the metric's  conf  file will be turned into command line parameters to the probe. For example, for this file:  [org.osg.certificates.cacert-expiry args]  warning-hours   =   6  error-hours   =   12   This would lead to the probe getting called with the command-line parameters  --warning-hours 6 --error-hours 12 .", 
            "title": "Passing extra parameters to a metric"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#configure-consumers", 
            "text": "There is a configuration file common to all consumers:  /etc/rsv/consumers.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      enabled  consumers  Comma-separated list of consumers to be enabled    timestamp  local  If this is set to local, a record with a local timestamp will be supplied to the consumer. If this is set to any other value, or is not set, a record with the GMT will be created.     Each consumer has a configuration file in  /etc/rsv/consumers  named after it. This allows to specify command lines and environment for the consumers. Some consumers may have also their own configuration file, usually in  /etc/rsv/ . Below is an example for the Nagios consumer.", 
            "title": "Configure consumers"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#sending-rsv-records-to-nagios", 
            "text": "Edit your  /etc/rsv/rsv-nagios.conf  file and fill in the appropriate information. The path of the configuration file is specified in  /etc/rsv/consumers/nagios-consumer.conf .   If your Nagios config file contains password information you will want to lock down the permissions. Here is a suggested way to do this (replace  rsvuser  with the group of your RSV user ( rsvuser  by default)):  root@host#  chown root: rsvuser  /etc/rsv/rsv-nagios.conf root@host#  chmod  0440  /etc/rsv/rsv-nagios.conf    In the configuration file at  /etc/rsv/consumers/nagios-consumer.conf , check the following two settings:   Make sure that the path to your config file is correct. It may be referencing a directory  config  instead of  etc  If you want to use  rsv2nsca  add the string \"--send-nsca\" to the  args  line.     Enable and start the Nagios consumer by editing  consumers.conf  or by using  rsv-control  as follows:  root@host#  rsv-control --enable nagios-consumer  The Nagios consumer will be started the next time that you start RSV. If you are already running RSV you can turn on the Nagios consumer immediately by running:  root@host#  rsv-control --on nagios-consumer    To verify that the Nagios consumer is running you can run  rsv-control -j .   The log information for the Nagios consumer can be found in these files:  /var/log/rsv/consumers/nagios-consumer.log  /var/log/rsv/consumers/nagios-consumer.out  /var/log/rsv/consumers/nagios-consumer.err", 
            "title": "Sending RSV records to Nagios"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#general-rsv-configuration-options", 
            "text": "You can configure the RSV framework using  /etc/rsv/rsv.conf . It is a file in INI format and possible entries are:     Setting  Values  Details      user  username  The UNIX username that owns RSV. This is mandatory    service-cert  path  Absolute path to the service certificate file. If this is set service-key and service-proxy must also be set.    service-key  path  Absolute path to the service key file. This must be used with service-cert.    service-proxy  path  Absolute path where the service proxy will be generated. This must be used with service-cert.    proxy-file  path  Alternative to service-cert. The absolute path where the user proxy file is located. This will not be auto-regenerated.    details-data-trim-length  integer  The number of bytes to trim the detailsData section to. If set to 0 no trimming will occur.    job-timeout  integer  Time in seconds before a metric is killed. A metric that times out will return a CRITICAL status.", 
            "title": "General RSV configuration options"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/advanced-rsv-configuration/#important-files-locations", 
            "text": "Configuration files:     File Description  Location  Comment      RSV configuration directory  /etc/rsv     RSV configuration  /etc/rsv/rsv.conf  RSV framework configuration    Consumers configuration in RSV  /etc/rsv/consumers.conf  Select the consumers and change generic options    Consumers configuration  /etc/rsv/consumers/ CONSUMERNAME  To change arguments and environment    Generic metrics configuration  /etc/rsv/metrics/ METRICNAME .conf  To change arguments and environment    Host specific metrics configuration  /etc/rsv/metrics/ HOSTNAME / METRICNAME .conf  To change arguments and environment when running on HOSTNAME     Other files:     File Description  Location  Comment      Metric log files  /var/log/rsv/metrics     Consumer log files  /var/log/rsv/consumers     Initial configuration  /etc/osg/config.d/30-rsv.ini  Read by  osg-configure    Web files output  /usr/share/rsv/www/      To find the metrics and the other files in RSV you can use also the RPM commands:  rpm -ql rsv-metrics  and  rpm -ql rsv .", 
            "title": "Important files locations"
        }, 
        {
            "location": "/monitoring/rsv-control/", 
            "text": "Using rsv-control\n\n\nOverview\n\n\nThis document is for System Administrators. It details the usage of the \nrsv-control\n command for enabling, disabling, testing and running RSV probes.\n\n\nrsv-control\n provides an interface to many RSV tasks. \nrsv-control\n can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.\n\n\n\n\nWarning\n\n\nrsv-control\n can be used to configure RSV as described here and in \nthe advanced configuration document\n. Most site admins will be able to configure RSV by editing \n/etc/osg/config.d/30-rsv.ini\n and running \nosg-configure\n as described in the \ninstallation doc\n.\n\n\n\n\nUsing \nrsv-control\n to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use \nrsv-control\n for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user (\nrsv\n by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.\n\n\nViewing RSV jobs\n\n\nrsv-control provides two different views: viewing the \ndesired\n state and viewing the current \nactual\n state.\n\n\n\n\nDesired = what metrics and consumers will start the next time RSV is started\n\n\nActual = what metrics and consumers are currently running\n\n\n\n\nDesired state\n\n\nTo view the desired state, use the \n--list\n (\n-l\n for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.\n\n\nroot@host#\n rsv-control --list\n\n\nMetrics enabled for host: osgitb1.nhn.ou.edu              | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.batch.jobmanager-default-status                   | OSG-CE\n\n\norg.osg.batch.jobmanagers-available                       | OSG-CE\n\n\norg.osg.certificates.cacert-expiry                        | OSG-CE\n\n\norg.osg.certificates.crl-expiry                           | OSG-CE\n\n\norg.osg.general.osg-directories-CE-permissions            | OSG-CE\n\n\norg.osg.general.osg-version                               | OSG-CE\n\n\norg.osg.general.ping-host                                 | OSG-CE\n\n\norg.osg.general.vdt-version                               | OSG-CE\n\n\norg.osg.general.vo-supported                              | OSG-CE\n\n\norg.osg.globus.gram-authentication                        | OSG-CE\n\n\norg.osg.globus.gridftp-simple                             | OSG-GridFTP\n\n\norg.osg.gratia.condor                                     | OSG-CE\n\n\norg.osg.gratia.metric                                     | OSG-CE\n\n\n\n\nMetrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service\n\n\n----------------------------------------------------------+--------------------\n\n\norg.osg.srm.srmcp-readwrite                               | OSG-SRM\n\n\norg.osg.srm.srmping                                       | OSG-SRM\n\n\n\n\n\n\nOther options:\n\n\n\n\nTo view all installed metrics use the \n--all\n (\n-a\n) flag along with \n--list\n. This will print an extra table showing metrics that are disabled on all hosts.\n\n\nIf you are having problems with the output being truncated, try the \n--wide\n (\n-w\n) flag.\n\n\n\n\nActual state\n\n\nTo view the current, running state of RSV jobs, use the \n--job-list\n flag (\n-j\n for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).\n\n\nroot@host#\n rsv-control --job-list\n\n\nHostname: osg-edu.cs.wisc.edu\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry\n\n\n  155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric\n\n\n  156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version\n\n\n  157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry\n\n\n  158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication\n\n\n  159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version\n\n\n  160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status\n\n\n  161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available\n\n\n  162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions\n\n\n  163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple\n\n\n  164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor\n\n\n  165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host\n\n\n  166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported\n\n\n\nHostname: osg-edu.cs.wisc.edu:10443\n\n\n     ID OWNER      ST NEXT RUN TIME   METRIC\n\n\n  113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping\n\n\n  114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite\n\n\n\n     ID OWNER      ST CONSUMER\n\n\n  198.0 rsvuser    R  html-consumer\n\n\n  199.0 rsvuser    R  gratia-consumer\n\n\n\n\n\n\nThe ST field indicates the current job status:\n\n\n\n\nR = the metric is currently running\n\n\nI = the metric is idle and will be run at the next scheduled interval\n\n\nAny other letter may indicate a problem\n\n\nConsumers will always appear to be running even though they will only run once every five minutes.\n\n\n\n\nRunning a metric\n\n\nrsv-control\n can be used to run metrics one time against a host. This can be useful for:\n\n\n\n\nupdating the status of a metric that had a problem instead of waiting until the next scheduled run time\n\n\ntesting a metric against a host before deciding whether to enable it\n\n\n\n\nNote that \nthe record for each run will be published to all active consumers\n. That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.\n\n\nSimplest test\n\n\nUse the \n--run\n (\n-r\n) flag. You must also provide the \n--host\n flag. The syntax is:\n\n\nrsv-control --run --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nwhere \nMETRIC\n is the full metric name (e.g. \norg.osg.general.osg-version\n). You can get the metric names from the \n--list\n output.\n\n\nroot@host#\n rsv-control --run \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version\n\n\nRunning metric org.osg.general.osg-version:\n\n\n\nmetricName: org.osg.general.osg-version\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 11:40:19 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: OSG 1.2.15\n\n\nEOT\n\n\n\n\n\n\nNote the \nmetricStatus\n in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.\n\n\nYou may run multiple metrics against a single host by specifying multiple metrics to \nrsv-control\n.\n\n\nIn order to run metrics against multiple hosts you must run \nrsv-control\n multiple times, once for each host.\n\n\nRunning all enabled metrics\n\n\nWhen RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the \n--all-enabled\n flag along with \n--run\n. With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).\n\n\nroot@host#\n rsv-control -r --all-enabled\n\n\nRunning metric org.osg.certificates.cacert-expiry (1 of 15)\n\n\n\nmetricName: org.osg.certificates.cacert-expiry\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 13:44:08 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-CE\n\n\nserviceURI: osg-edu.cs.wisc.edu\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: Security Probe Version: 1.1\n\n\nOK: CAs are in sync with OSG distribution\n\n\nEOT\n\n\n\n\n...\n\n\n\n\n\n\nPassing extra configuration\n\n\nIf you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the \norg.osg.srm.srmclient-ping\n metric (tmp-srm.ini):\n\n\n[org.osg.srm.srmclient-ping args]\n\n\nsrm-destination-dir\n=\n/srmcache/~\n\n\nsrm-webservice-path\n=\nsrm/v2/server\n\n\n\n\n\n\nThen use the \n--extra-config-file\n parameter and pass the path to the INI file:\n\n\nroot@host#\n rsv-control -r --extra-config-file tmp-srm.ini \n\\\n\n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping\n\n\nRunning metric org.osg.srm.srmclient-ping:\n\n\n\nmetricName: org.osg.srm.srmclient-ping\n\n\nmetricType: status\n\n\ntimestamp: 2010-11-19 14:12:35 CST\n\n\nmetricStatus: OK\n\n\nserviceType: OSG-SRM\n\n\nserviceURI: osg-edu.cs.wisc.edu:10443\n\n\ngatheredAt: vdt-itb.cs.wisc.edu\n\n\nsummaryData: OK\n\n\ndetailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.\n\n\n.  Details: Storage Resource Manager (SRM) Client version 2.1.5-16\n\n\nCopyright (c) 2002-2009 Fermi National Accelerator Laboratory\n\n\n\n...\n\n\n\n\n\n\nEnabling and disabling metrics and consumers\n\n\nMetrics and consumers can be enabled or disabled by \nrsv-control\n using the \n--enable\n and \n--disable\n flags. Note that \"enable\" and \"disable\" are desired states (this is similar to \nosg-control\n). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).\n\n\nEnabling\n\n\nThe syntax for enabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --enable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --enable \n\\\n\n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency\n\nEnabling metric \norg.osg.gip.consistency\n for host \nosg-edu.cs.wisc.edu\n\n\n\nOne or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run \nrsv-control --on\n.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --enable nagios-consumer\n\nEnabling consumer nagios-consumer\n\n\n\n\n\n\nDisabling\n\n\nThe syntax for disabling metrics looks similar to the syntax for running metrics:\n\n\nrsv-control --disable --host \nHOST\n \nMETRIC\n [ \nMETRIC2\n ...]\n\n\nYou must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run \nrsv-control\n once per host).\n\n\nroot@host#\n rsv-control --disable \n\\\n\n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry\n\nDisabling metric \norg.osg.local.containercert-expiry\n for host \nvdt-itb.cs.wisc.edu\n\n\n\nOne or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.\n\n\n\n\n\n\nConsumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).\n\n\nroot@host#\n rsv-control --disable html-consumer gratia-consumer\n\nDisabling consumer html-consumer\n\n\nDisabling consumer gratia-consumer\n\n\n   Consumer already disabled\n\n\n\n\n\n\nMetrics and consumers can both be listed in the same disable command.\n\n\nTroubleshooting\n\n\nGetting more information from rsv-control\n\n\nThe first step to getting more information is to run \nrsv-control\n with more verbosity. Use the \n--verbose\n (\n-v\n) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:\n\n\n\n\n0 = print nothing\n\n\n1 = print warnings and errors along with usual output of command being run (1 is the default level)\n\n\n2 = adds informational messages\n\n\n3 = full debugging output\n\n\n\n\nUsing the RSV verify tool\n\n\nThe \n--verify\n flag will run some basic checks for your RSV installation:\n\n\nroot@host#\n rsv-control --verify\n\nTesting if Condor-Cron is running...\n\n\nOK\n\n\n\nTesting if metrics are running...\n\n\nOK (98 running metrics)\n\n\n\nTesting if consumers are running...\n\n\nOK (1 running consumers)\n\n\n\nChecking which consumers are configured...\n\n\nThe following consumers are enabled: html-consumer\n\n\nWARNING: The gratia-consumer is not enabled.  This indicates that your\n\n\n         resource is not reporting to OSG.\n\n\n\n\n\n\nThis tool is still under development and it does only basic checks, but it is a good first step when debugging issues.\n\n\nRunning the RSV profiler\n\n\nRSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:\n\n\nroot@host#\n rsv-control --profile\n\nRunning the rsv-profiler...\n\n\nOSG-RSV Profiler\n\n\nAnalyzing...\n\n\nMaking tarball (rsv-profiler.tar.gz)\n\n\n\n\n\n\n\n\nNote\n\n\nIf you are getting assistance via the trouble ticket system, you must add a \n.txt\n extension to the tarball so it can be uploaded.\n\n\nroot@host#\n mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Manage RSV via rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-rsv-control", 
            "text": "", 
            "title": "Using rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#overview", 
            "text": "This document is for System Administrators. It details the usage of the  rsv-control  command for enabling, disabling, testing and running RSV probes.  rsv-control  provides an interface to many RSV tasks.  rsv-control  can view RSV jobs, run metrics, enable or disable metrics and consumers, and allow advanced configuration.   Warning  rsv-control  can be used to configure RSV as described here and in  the advanced configuration document . Most site admins will be able to configure RSV by editing  /etc/osg/config.d/30-rsv.ini  and running  osg-configure  as described in the  installation doc .   Using  rsv-control  to configure is for advanced RSV use including enabling non-default metrics. Admins who don't use  rsv-control  for configuration can still use it to view their RSV jobs, run RSV tests, and help debug RSV problems. Anyone can view the jobs, but you must be root or the RSV user ( rsv  by default) to execute other commands, e.g. run, enable and disable probes, or to turn RSV on and off.", 
            "title": "Overview"
        }, 
        {
            "location": "/monitoring/rsv-control/#viewing-rsv-jobs", 
            "text": "rsv-control provides two different views: viewing the  desired  state and viewing the current  actual  state.   Desired = what metrics and consumers will start the next time RSV is started  Actual = what metrics and consumers are currently running", 
            "title": "Viewing RSV jobs"
        }, 
        {
            "location": "/monitoring/rsv-control/#desired-state", 
            "text": "To view the desired state, use the  --list  ( -l  for short) flag. This will create one table for each host showing the metrics that are enabled to run against that host.  root@host#  rsv-control --list Metrics enabled for host: osgitb1.nhn.ou.edu              | Service  ----------------------------------------------------------+--------------------  org.osg.batch.jobmanager-default-status                   | OSG-CE  org.osg.batch.jobmanagers-available                       | OSG-CE  org.osg.certificates.cacert-expiry                        | OSG-CE  org.osg.certificates.crl-expiry                           | OSG-CE  org.osg.general.osg-directories-CE-permissions            | OSG-CE  org.osg.general.osg-version                               | OSG-CE  org.osg.general.ping-host                                 | OSG-CE  org.osg.general.vdt-version                               | OSG-CE  org.osg.general.vo-supported                              | OSG-CE  org.osg.globus.gram-authentication                        | OSG-CE  org.osg.globus.gridftp-simple                             | OSG-GridFTP  org.osg.gratia.condor                                     | OSG-CE  org.osg.gratia.metric                                     | OSG-CE  Metrics enabled for host: osg-edu.cs.wisc.edu:10443       | Service  ----------------------------------------------------------+--------------------  org.osg.srm.srmcp-readwrite                               | OSG-SRM  org.osg.srm.srmping                                       | OSG-SRM   Other options:   To view all installed metrics use the  --all  ( -a ) flag along with  --list . This will print an extra table showing metrics that are disabled on all hosts.  If you are having problems with the output being truncated, try the  --wide  ( -w ) flag.", 
            "title": "Desired state"
        }, 
        {
            "location": "/monitoring/rsv-control/#actual-state", 
            "text": "To view the current, running state of RSV jobs, use the  --job-list  flag ( -j  for short). This will show all metrics and consumers running in RSV. (It queries the underlying Condor Cron system that we use to run the metrics).  root@host#  rsv-control --job-list Hostname: osg-edu.cs.wisc.edu       ID OWNER      ST NEXT RUN TIME   METRIC    154.0 rsvuser    I  11-19 12:15     org.osg.certificates.cacert-expiry    155.0 rsvuser    R  11-19 11:23     org.osg.gratia.metric    156.0 rsvuser    I  11-19 18:47     org.osg.general.vdt-version    157.0 rsvuser    I  11-19 12:30     org.osg.certificates.crl-expiry    158.0 rsvuser    I  11-19 11:31     org.osg.globus.gram-authentication    159.0 rsvuser    I  11-19 11:41     org.osg.general.osg-version    160.0 rsvuser    R  11-19 11:25     org.osg.batch.jobmanager-default-status    161.0 rsvuser    I  11-20 04:59     org.osg.batch.jobmanagers-available    162.0 rsvuser    I  11-19 11:37     org.osg.general.osg-directories-CE-permissions    163.0 rsvuser    I  11-19 12:08     org.osg.globus.gridftp-simple    164.0 rsvuser    I  11-19 12:09     org.osg.gratia.condor    165.0 rsvuser    R  11-19 11:27     org.osg.general.ping-host    166.0 rsvuser    I  11-19 18:47     org.osg.general.vo-supported  Hostname: osg-edu.cs.wisc.edu:10443       ID OWNER      ST NEXT RUN TIME   METRIC    113.0 rsvuser    I  11-19 11:33     org.osg.srm.srmping    114.0 rsvuser    R  11-19 11:28     org.osg.srm.srmcp-readwrite       ID OWNER      ST CONSUMER    198.0 rsvuser    R  html-consumer    199.0 rsvuser    R  gratia-consumer   The ST field indicates the current job status:   R = the metric is currently running  I = the metric is idle and will be run at the next scheduled interval  Any other letter may indicate a problem  Consumers will always appear to be running even though they will only run once every five minutes.", 
            "title": "Actual state"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-a-metric", 
            "text": "rsv-control  can be used to run metrics one time against a host. This can be useful for:   updating the status of a metric that had a problem instead of waiting until the next scheduled run time  testing a metric against a host before deciding whether to enable it   Note that  the record for each run will be published to all active consumers . That is, it will be published to Gratia or will show up on your local web page, if you have those enabled.", 
            "title": "Running a metric"
        }, 
        {
            "location": "/monitoring/rsv-control/#simplest-test", 
            "text": "Use the  --run  ( -r ) flag. You must also provide the  --host  flag. The syntax is:  rsv-control --run --host  HOST   METRIC  [  METRIC2  ...]  where  METRIC  is the full metric name (e.g.  org.osg.general.osg-version ). You can get the metric names from the  --list  output.  root@host#  rsv-control --run  \\ \n    --host osg-edu.cs.wisc.edu org.osg.general.osg-version Running metric org.osg.general.osg-version:  metricName: org.osg.general.osg-version  metricType: status  timestamp: 2010-11-19 11:40:19 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: OSG 1.2.15  EOT   Note the  metricStatus  in the example above: that's where you can see if it was successful or not. In this case, it was successful, because it printed OK.  You may run multiple metrics against a single host by specifying multiple metrics to  rsv-control .  In order to run metrics against multiple hosts you must run  rsv-control  multiple times, once for each host.", 
            "title": "Simplest test"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-all-enabled-metrics", 
            "text": "When RSV is first installed it can take up to a day for each enabled metric to run once. A new option is provided to force each metric to run immediately, for all hosts. Use the  --all-enabled  flag along with  --run . With this option it is not necessary to specify a host - all enabled metrics for all configured hosts will be run (in fact, if you do specify a host it will be ignored).  root@host#  rsv-control -r --all-enabled Running metric org.osg.certificates.cacert-expiry (1 of 15)  metricName: org.osg.certificates.cacert-expiry  metricType: status  timestamp: 2010-11-19 13:44:08 CST  metricStatus: OK  serviceType: OSG-CE  serviceURI: osg-edu.cs.wisc.edu  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: Security Probe Version: 1.1  OK: CAs are in sync with OSG distribution  EOT  ...", 
            "title": "Running all enabled metrics"
        }, 
        {
            "location": "/monitoring/rsv-control/#passing-extra-configuration", 
            "text": "If you want to pass extra configuration when running a metric without editing its configuration file you can make an INI-formatted file and pass it on the command line. For example, you can make a file like this for the  org.osg.srm.srmclient-ping  metric (tmp-srm.ini):  [org.osg.srm.srmclient-ping args]  srm-destination-dir = /srmcache/~  srm-webservice-path = srm/v2/server   Then use the  --extra-config-file  parameter and pass the path to the INI file:  root@host#  rsv-control -r --extra-config-file tmp-srm.ini  \\ \n    --host osg-edu.cs.wisc.edu:10443 org.osg.srm.srmclient-ping Running metric org.osg.srm.srmclient-ping:  metricName: org.osg.srm.srmclient-ping  metricType: status  timestamp: 2010-11-19 14:12:35 CST  metricStatus: OK  serviceType: OSG-SRM  serviceURI: osg-edu.cs.wisc.edu:10443  gatheredAt: vdt-itb.cs.wisc.edu  summaryData: OK  detailsData: SRM server running on osg-edu.cs.wisc.edu is alive and responding to the srmping command.  .  Details: Storage Resource Manager (SRM) Client version 2.1.5-16  Copyright (c) 2002-2009 Fermi National Accelerator Laboratory  ...", 
            "title": "Passing extra configuration"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling-and-disabling-metrics-and-consumers", 
            "text": "Metrics and consumers can be enabled or disabled by  rsv-control  using the  --enable  and  --disable  flags. Note that \"enable\" and \"disable\" are desired states (this is similar to  osg-control ). After enabling a metric you should turn it on if you want it to be running immediately. After disabling a metric that is running, you should still turn it off (a message will print after each of these actions to remind you of this behavior).", 
            "title": "Enabling and disabling metrics and consumers"
        }, 
        {
            "location": "/monitoring/rsv-control/#enabling", 
            "text": "The syntax for enabling metrics looks similar to the syntax for running metrics:  rsv-control --enable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to enable the metric against (in order to enable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --enable  \\ \n    --host osg-edu.cs.wisc.edu org.osg.gip.consistency Enabling metric  org.osg.gip.consistency  for host  osg-edu.cs.wisc.edu  One or more metrics have been enabled and will be started the next time RSV is started.  To turn them on immediately run  rsv-control --on .   Consumers do not run against a specific host, they process records for all hosts. When enabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --enable nagios-consumer Enabling consumer nagios-consumer", 
            "title": "Enabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#disabling", 
            "text": "The syntax for disabling metrics looks similar to the syntax for running metrics:  rsv-control --disable --host  HOST   METRIC  [  METRIC2  ...]  You must provide a host to disable the metric against (in order to disable a metric on multiple hosts you must run  rsv-control  once per host).  root@host#  rsv-control --disable  \\ \n    --host vdt-itb.cs.wisc.edu org.osg.local.containercert-expiry Disabling metric  org.osg.local.containercert-expiry  for host  vdt-itb.cs.wisc.edu  One or more metrics have been disabled and will not start the next time RSV is started.  You may still need to turn them off if they are currently running.   Consumers do not run against a specific host, they process records for all hosts. When disabling consumers a host is not required (if a host is passed it will be ignored).  root@host#  rsv-control --disable html-consumer gratia-consumer Disabling consumer html-consumer  Disabling consumer gratia-consumer     Consumer already disabled   Metrics and consumers can both be listed in the same disable command.", 
            "title": "Disabling"
        }, 
        {
            "location": "/monitoring/rsv-control/#troubleshooting", 
            "text": "", 
            "title": "Troubleshooting"
        }, 
        {
            "location": "/monitoring/rsv-control/#getting-more-information-from-rsv-control", 
            "text": "The first step to getting more information is to run  rsv-control  with more verbosity. Use the  --verbose  ( -v ) flag. This flag can be used with any of rsv-control's abilities (run, enable, list, etc). The verbosity levels are:   0 = print nothing  1 = print warnings and errors along with usual output of command being run (1 is the default level)  2 = adds informational messages  3 = full debugging output", 
            "title": "Getting more information from rsv-control"
        }, 
        {
            "location": "/monitoring/rsv-control/#using-the-rsv-verify-tool", 
            "text": "The  --verify  flag will run some basic checks for your RSV installation:  root@host#  rsv-control --verify Testing if Condor-Cron is running...  OK  Testing if metrics are running...  OK (98 running metrics)  Testing if consumers are running...  OK (1 running consumers)  Checking which consumers are configured...  The following consumers are enabled: html-consumer  WARNING: The gratia-consumer is not enabled.  This indicates that your           resource is not reporting to OSG.   This tool is still under development and it does only basic checks, but it is a good first step when debugging issues.", 
            "title": "Using the RSV verify tool"
        }, 
        {
            "location": "/monitoring/rsv-control/#running-the-rsv-profiler", 
            "text": "RSV has a tool to collect information useful for troubleshooting into a tarball that can be shared with the developers and support staff.\nTo use it:  root@host#  rsv-control --profile Running the rsv-profiler...  OSG-RSV Profiler  Analyzing...  Making tarball (rsv-profiler.tar.gz)    Note  If you are getting assistance via the trouble ticket system, you must add a  .txt  extension to the tarball so it can be uploaded.  root@host#  mv rsv-profiler.tar.gz rsv-profiler.tar.gz.txt", 
            "title": "Running the RSV profiler"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/", 
            "text": "Installing and Using the RSV GlideinWMS Tester\n\n\nAbout This Guide\n\n\nThe RSV GlideinWMS Tester (or \nTester\n, in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.\n\n\nUse this page to learn how to install, configure, and use the Tester for your VO front-end.\n\n\nBefore Starting\n\n\nBefore starting the installation process, consider the following points (consulting \nthe Reference section below\n as needed):\n\n\n\n\nSoftware:\n You must have \na GlideinWMS Front-end\n installed\n\n\nConfiguration:\n The GlideinWMS Front-end must be configured (a) \nto have at least one group that matches pilots to sites using DESIRED_SITES\n, and (b) \nto support the is_itb user job attribute\n\n\nHost choice:\n The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal\n\n\nService certificate:\n The Tester requires a host certificate at \n/etc/grid-security/hostcert.pem\n and an accompanying key at \n/etc/grid-security/hostkey.pem\n\n\nNetwork ports:\n Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.\n\n\n\n\nInstalling the Tester\n\n\nThe Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.\n\n\nroot@host #\n yum install rsv-gwms-tester\n\n\n\n\n\nConfiguring the Tester\n\n\nBefore you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.\n\n\nConfiguring the GlideinWMS Front-end Central Manager\n\n\nComplete these steps \non your GlideinWMS Front-end Central Manager host\n:\n\n\n\n\n\n\nAuthorize the Tester host to connect to your Central Manager:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nTESTER_DN\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nRSV GWMS Tester on Fermicloud\n \n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n condor\n\n\n\n\n\n\n\n\n\nRestart HTCondor to apply the changes\n\n\nOn \nEL\u00a06\n systems:\n\n\nroot@host #\n service condor restart\n\n\n\n\n\nOn \nEL\u00a07\n systems:\n\n\nroot@host #\n systemctl restart condor\n\n\n\n\n\n\n\n\n\nAdd the new Tester to your GlideinWMS front-end configuration.\n   Edit the file \n/etc/gwms-frontend/frontend.xml\n and add a line as follows within the \nschedds\n element\n\n\nschedd\n \nDN\n=\nTESTER_DN\n \nfullname\n=\nTESTER_HOSTNAME\n\n\n\n\n\n\nWhere \nTESTER_DN\n is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and \nTESTER_HOSTNAME\n is the fully qualified hostname of the Tester host. For example:\n\n\nschedd\n \nDN\n=\n/DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov\n \nfullname\n=\nfermicloud357.fnal.gov\n\n\n\n\n\n\nReconfigure your GlideinWMS front-end to apply the changes:\n\n\nroot@host #\n service gwms-frontend reconfig\n\n\n\n\n\n\n\n\n\nConfiguring the Tester host\n\n\nComplete the following steps \non your Tester host\n:\n\n\n\n\n\n\nConfigure the Tester for the VOs that your Front-end supports\n\n\nEdit the file \n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n. The \nconstraint\n line is an HTCondor ClassAd expression containing one \nstringListMember\n function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator, \n||\n. Edit the \nconstraint\n line for your Front-end.\n\n\nFor example, for a single VO named \nFoo\n, the line would be:\n\n\nconstraint\n \n=\n \nstringListMember\n(\nFoo\n,\n \nGLIDEIN_Supported_VOs\n)\n\n\n\n\n\n\nFor two VOs named \nFoo\n and \nBar\n, the line would be:\n\n\nconstraint\n \n=\n \nstringListMember\n(\nFoo\n,\n \nGLIDEIN_Supported_VOs\n)\n \n||\n \nstringListMember\n(\nBar\n,\n \nGLIDEIN_Supported_VOs\n)\n\n\n\n\n\n\nDo not change the other settings in this file, unless you have clear and specific reasons to do so.\n\n\n\n\n\n\nAuthorize the central manager of your Front-end to connect to the tester host:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nCOMMENT\n \nCENTRAL_MGR\n condor\n\n\n\n\n\nWhere \nCOMMENT\n is a human-readable identifier for the Central Manager, and \nCENTRAL_MGR\n is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:\n\n\nroot@host #\n glidecondor_addDN -allow-others -daemon \nUCSD central manager DN\n \n/DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu\n condor\n\n\n\n\n\n\n\n\n\nConfigure the special HTCondor-RSV instance with your host IP address.\n\n\nCreate the file \n/etc/condor/config.d/98_public_interface.config\n with this content:\n\n\nNETWORK_INTERFACE\n \n=\n \n%\nRED\n%\nADDRESS\n%\nENDCOLOR\n%\n\n\nCONDOR_HOST\n \n=\n \n%\nRED\n%\nCENTRAL_MGR\n%\nENDCOLOR\n%\n\n\n\n\n\n\nWhere \nADDRESS\n is the IP address of your Tester host, and \nCENTRAL_MGR\n is the hostname of your GlideinWMS Front-end Central Manager.\n\n\n\n\n\n\nEnable the Tester\u2019s RSV probe:\n\n\nroot@host #\n rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\n\n\n\n\nUsing the Tester\n\n\nThere are at least two aspects of using the Tester:\n\n\n\n\nManaging the services that are associated with the Tester software\n\n\nViewing results from the Tester\n\n\n\n\nManaging Tester services\n\n\nBecause the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:\n\n\n\n\n\n\n\n\nSoftware\n\n\nService name\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\nApache HTTP Server\n\n\nhttpd\n\n\nWeb server for results\n\n\n\n\n\n\nHTCondor-Cron\n\n\ncondor-cron\n\n\ncron-like jobs in HTCondor\n\n\n\n\n\n\nRSV\n\n\nrsv\n\n\nOSG site validator\n\n\n\n\n\n\n\n\nViewing Tester results\n\n\nOnce the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:\n\n\n\n\nOne probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes\n\n\nOne probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes\n\n\n\n\nYou can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.\n\n\nViewing RSV results online\n\n\nTo see the latest results, access \nhttps://\nHOSTNAME\n/rsv/\n (where \nHOSTNAME\n is the name of your Tester host).\n\n\n\n\nThere should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka \nmetric\n)\n\n\nThere should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka \nmetric\n)\n\n\nThere is a legend for the background colors at the end of the page\n\n\n\n\nIdeally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations (\n) about the failing site, including a link to your Tester RSV results page.\n\n\nTo see detailed information from each probe, click on the probe name in the Metric column.\n\n\nTo see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.\n\n\nListing supported sites manually\n\n\nTo manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:\n\n\nroot@host #\n rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost\n\n\n\n\n\nThe probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:\n\n\nMSG: Updating configuration for host \nUCSD\n\n\n\n\n\n\nThe highlighted name is the site name, and there should be one such line per site supported by your VO(s).\n\n\nTroubleshooting RSV-GWMS-Tester\n\n\nYou can find more information on troubleshooting in the \nRSV troubleshooting section\n\n\nLogs and configuration:\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nCondor Cron log files\n\n\n/var/log/condor-cron\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Description\n\n\nLocation\n\n\nComment\n\n\n\n\n\n\n\n\n\n\nMetric configuration\n\n\n/etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf\n\n\nTo change arguments and environment\n\n\n\n\n\n\n\n\nGetting Help\n\n\nTo get assistance, please use the \nthis page\n.\n\n\nReference\n\n\nCertificates\n\n\n\n\n\n\n\n\nCertificate\n\n\nUser that owns certificate\n\n\nPath to certificate\n\n\n\n\n\n\n\n\n\n\nHost certificate\n\n\nroot\n\n\n/etc/grid-security/hostcert.pem\n\n\n\n\n\n\nHost key\n\n\nroot\n\n\n/etc/grid-security/hostkey.pem\n\n\n\n\n\n\n\n\nFind instructions to request a host certificate \nhere\n.", 
            "title": "RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-and-using-the-rsv-glideinwms-tester", 
            "text": "", 
            "title": "Installing and Using the RSV GlideinWMS Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#about-this-guide", 
            "text": "The RSV GlideinWMS Tester (or  Tester , in this document) is a tool that a VO front-end administrator can use to test remote sites for the ability to run the VO\u2019s jobs. It is particularly useful when setting up a VO for the first time or when changing the sites at which a VO\u2019s jobs can run. For a site to pass the test, it must successfully run a simple test job via the normal GlideinWMS mechanisms, in much the same way as a real VO job.  Use this page to learn how to install, configure, and use the Tester for your VO front-end.", 
            "title": "About This Guide"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#before-starting", 
            "text": "Before starting the installation process, consider the following points (consulting  the Reference section below  as needed):   Software:  You must have  a GlideinWMS Front-end  installed  Configuration:  The GlideinWMS Front-end must be configured (a)  to have at least one group that matches pilots to sites using DESIRED_SITES , and (b)  to support the is_itb user job attribute  Host choice:  The Tester should be installed on its own host; a small Virtual Machine (VM) is ideal  Service certificate:  The Tester requires a host certificate at  /etc/grid-security/hostcert.pem  and an accompanying key at  /etc/grid-security/hostkey.pem  Network ports:  Test jobs must be able to contact the tester using the HTCondor Shared Port on port 9615 (TCP), and you must be able to contact a web server on port 80 (TCP) to view test results.", 
            "title": "Before Starting"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#installing-the-tester", 
            "text": "The Tester software takes advantage of several other OSG software components, so the installation will also include OSG\u2019s site validation system (RSV), HTCondor, and the GlideinWMS pilot submission software.  root@host #  yum install rsv-gwms-tester", 
            "title": "Installing the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester", 
            "text": "Before you use the Tester, there are some one-time configuration steps to complete, one set on your GlideinWMS Front-end Central Manager host and one set on the Tester host.", 
            "title": "Configuring the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-glideinwms-front-end-central-manager", 
            "text": "Complete these steps  on your GlideinWMS Front-end Central Manager host :    Authorize the Tester host to connect to your Central Manager:  root@host #  glidecondor_addDN -allow-others -daemon  COMMENT   TESTER_DN  condor  Where  COMMENT  is a human-readable label for the Tester host (e.g., \u201cRSV GWMS Tester at myhost\u201d), and  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  root@host #  glidecondor_addDN -allow-others -daemon  RSV GWMS Tester on Fermicloud   /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov  condor    Restart HTCondor to apply the changes  On  EL\u00a06  systems:  root@host #  service condor restart  On  EL\u00a07  systems:  root@host #  systemctl restart condor    Add the new Tester to your GlideinWMS front-end configuration.\n   Edit the file  /etc/gwms-frontend/frontend.xml  and add a line as follows within the  schedds  element  schedd   DN = TESTER_DN   fullname = TESTER_HOSTNAME   Where  TESTER_DN  is the Distinguished Name (DN) of the host certificate of your Tester host (as above), and  TESTER_HOSTNAME  is the fully qualified hostname of the Tester host. For example:  schedd   DN = /DC=com/DC=DigiCert-Grid/O=Open Science Grid/OU=Services/CN=fermicloud357.fnal.gov   fullname = fermicloud357.fnal.gov   Reconfigure your GlideinWMS front-end to apply the changes:  root@host #  service gwms-frontend reconfig", 
            "title": "Configuring the GlideinWMS Front-end Central Manager"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#configuring-the-tester-host", 
            "text": "Complete the following steps  on your Tester host :    Configure the Tester for the VOs that your Front-end supports  Edit the file  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf . The  constraint  line is an HTCondor ClassAd expression containing one  stringListMember  function per VO that your Front-end supports. If there is more than one VO, the function invocations are joined by the \u201clogical or\u201d operator,  || . Edit the  constraint  line for your Front-end.  For example, for a single VO named  Foo , the line would be:  constraint   =   stringListMember ( Foo ,   GLIDEIN_Supported_VOs )   For two VOs named  Foo  and  Bar , the line would be:  constraint   =   stringListMember ( Foo ,   GLIDEIN_Supported_VOs )   ||   stringListMember ( Bar ,   GLIDEIN_Supported_VOs )   Do not change the other settings in this file, unless you have clear and specific reasons to do so.    Authorize the central manager of your Front-end to connect to the tester host:  root@host #  glidecondor_addDN -allow-others -daemon  COMMENT   CENTRAL_MGR  condor  Where  COMMENT  is a human-readable identifier for the Central Manager, and  CENTRAL_MGR  is the Distinguished Name (DN) of the host certificate of your GlideinWMS Front-end\u2019s Central Manager host. Most likely, you will need to quote both of these values to protect them from the shell. For example:  root@host #  glidecondor_addDN -allow-others -daemon  UCSD central manager DN   /DC=org/DC=opensciencegrid/O=Open Science Grid/OU=Services/CN=osg-ligo-1.t2.ucsd.edu  condor    Configure the special HTCondor-RSV instance with your host IP address.  Create the file  /etc/condor/config.d/98_public_interface.config  with this content:  NETWORK_INTERFACE   =   % RED % ADDRESS % ENDCOLOR %  CONDOR_HOST   =   % RED % CENTRAL_MGR % ENDCOLOR %   Where  ADDRESS  is the IP address of your Tester host, and  CENTRAL_MGR  is the hostname of your GlideinWMS Front-end Central Manager.    Enable the Tester\u2019s RSV probe:  root@host #  rsv-control --enable org.osg.local-gfactory-site-querying-local --host localhost", 
            "title": "Configuring the Tester host"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#using-the-tester", 
            "text": "There are at least two aspects of using the Tester:   Managing the services that are associated with the Tester software  Viewing results from the Tester", 
            "title": "Using the Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#managing-tester-services", 
            "text": "Because the Tester is built on other OSG software, there are a number of services in your installation. The specific services are:     Software  Service name  Notes      Apache HTTP Server  httpd  Web server for results    HTCondor-Cron  condor-cron  cron-like jobs in HTCondor    RSV  rsv  OSG site validator", 
            "title": "Managing Tester services"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-tester-results", 
            "text": "Once the Tester RSV probe is enabled and active, and the services listed above have been started, there are two kinds of RSV probes that run periodically:   One probe asks the GlideinWMS factory for the up-to-date list of sites supported by your VO(s)\u00a0\u2014 runs every 30 minutes  One probe submits and monitors one test job to each site supported by your VO(s)\u00a0\u2014 run every 60 minutes   You can view the latest results of both probe types on an RSV results web page, or you can manually run the first probe to see the full list of sites.", 
            "title": "Viewing Tester results"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#viewing-rsv-results-online", 
            "text": "To see the latest results, access  https:// HOSTNAME /rsv/  (where  HOSTNAME  is the name of your Tester host).   There should be one result row per site supported by your VO(s), using the \u201corg.osg.general.dummy-vanilla-probe\u201d probe (aka  metric )  There should be exactly one result row for the probe that fetches the list of sites, which is the \u201corg.osg.local-gfactory-site-querying-local\u201d probe (aka  metric )  There is a legend for the background colors at the end of the page   Ideally, each site supported by your VO(s) should be shown with a green background, which indicates that a Tester job ran at that site recently and successfully. There may be transient failures but if you notice a site in the failed state over multiple days, contact OSG Factory Operations ( ) about the failing site, including a link to your Tester RSV results page.  To see detailed information from each probe, click on the probe name in the Metric column.  To see the list of sites that are supported by your VO(s) and are being tested, click the \u201corg.osg.local-gfactory-site-querying-local\u201d link at the bottom of the list of probes. You can also run the probe manually, as described next.", 
            "title": "Viewing RSV results online"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#listing-supported-sites-manually", 
            "text": "To manually run the probe that fetches the list of sites supported by your VO(s), run the following command on your Tester host:  root@host #  rsv-control --run org.osg.local-gfactory-site-querying-local --host localhost  The probe produces many lines of output, some of which are just about the probe execution itself. But look for lines like this:  MSG: Updating configuration for host  UCSD   The highlighted name is the site name, and there should be one such line per site supported by your VO(s).", 
            "title": "Listing supported sites manually"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#troubleshooting-rsv-gwms-tester", 
            "text": "You can find more information on troubleshooting in the  RSV troubleshooting section  Logs and configuration:     File Description  Location  Comment      Condor Cron log files  /var/log/condor-cron         File Description  Location  Comment      Metric configuration  /etc/rsv/metrics/org.osg.local-gfactory-site-querying-local.conf  To change arguments and environment", 
            "title": "Troubleshooting RSV-GWMS-Tester"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#getting-help", 
            "text": "To get assistance, please use the  this page .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#reference", 
            "text": "", 
            "title": "Reference"
        }, 
        {
            "location": "/monitoring/install-rsv-gwms-tester/#certificates", 
            "text": "Certificate  User that owns certificate  Path to certificate      Host certificate  root  /etc/grid-security/hostcert.pem    Host key  root  /etc/grid-security/hostkey.pem     Find instructions to request a host certificate  here .", 
            "title": "Certificates"
        }, 
        {
            "location": "/common/help/", 
            "text": "How to Get Help\n\n\nThis page is aimed at OSG site administrators looking for support.\nHelp for OSG users can be found at our \nsupport desk\n.\n\n\nSecurity Incidents\n\n\nSecurity incidents can be reported by following the instructions on the\n\nIncident Discovery and Reporting\n page.\n\n\nSoftware or Service Support\n\n\nIf you are experiencing issues with OSG software or services, please consult the following resources before opening a\nsupport inquiry:\n\n\n\n\nTroubleshooting sections or pages for the problematic software\n\n\nRecent OSG Software \nrelease notes\n\n\nOutage\n information for OSG services\n\n\n\n\nSubmitting support inquiries\n\n\nIf your problem still hasn't been resolved by consulting the resources above, please submit a support inquiry with\nthe information noted below:\n\n\n\n\n\n\nIf you came to this page from an installation guide, please provide the following information:\n\n\n\n\nCommands and output from any \nTroubleshooting\n sections or pages\n\n\nThe OSG system profile (\nosg-profile.txt\n), generated by running the following command:\nroot@host #\n osg-system-profiler\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubmit a support inquiry to the system based on the VOs that you are associated with:\n\n\n\n\n\n\n\n\nIf you are primarily associated with...\n\n\nSubmit new tickets to...\n\n\n\n\n\n\n\n\n\n\nLHC VOs\n\n\nGGUS\n\n\n\n\n\n\nAnyone else\n\n\nhelp@opensciencegrid.org\n\n\n\n\n\n\n\n\n\n\n\n\nCommunity-specific support\n\n\nSome OSG VOs have dedicated forums or mechanisms for community-specific support.\nIf your VO provides user support, that should be a user's first line of support because the VO is most familiar with\nyour applications and requirements.\n\n\n\n\nThe list of support centers for OSG VOs can be found in the\n\nhere\n.\n\n\nResources for \nCMS\n sites:\n\n\nhttp://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml\n\n\nCMS Hyper News: \nhttps://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html\n\n\nCMS Twiki: \nhttps://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Get Help"
        }, 
        {
            "location": "/common/help/#how-to-get-help", 
            "text": "This page is aimed at OSG site administrators looking for support.\nHelp for OSG users can be found at our  support desk .", 
            "title": "How to Get Help"
        }, 
        {
            "location": "/common/help/#security-incidents", 
            "text": "Security incidents can be reported by following the instructions on the Incident Discovery and Reporting  page.", 
            "title": "Security Incidents"
        }, 
        {
            "location": "/common/help/#software-or-service-support", 
            "text": "If you are experiencing issues with OSG software or services, please consult the following resources before opening a\nsupport inquiry:   Troubleshooting sections or pages for the problematic software  Recent OSG Software  release notes  Outage  information for OSG services", 
            "title": "Software or Service Support"
        }, 
        {
            "location": "/common/help/#submitting-support-inquiries", 
            "text": "If your problem still hasn't been resolved by consulting the resources above, please submit a support inquiry with\nthe information noted below:    If you came to this page from an installation guide, please provide the following information:   Commands and output from any  Troubleshooting  sections or pages  The OSG system profile ( osg-profile.txt ), generated by running the following command: root@host #  osg-system-profiler      Submit a support inquiry to the system based on the VOs that you are associated with:     If you are primarily associated with...  Submit new tickets to...      LHC VOs  GGUS    Anyone else  help@opensciencegrid.org", 
            "title": "Submitting support inquiries"
        }, 
        {
            "location": "/common/help/#community-specific-support", 
            "text": "Some OSG VOs have dedicated forums or mechanisms for community-specific support.\nIf your VO provides user support, that should be a user's first line of support because the VO is most familiar with\nyour applications and requirements.   The list of support centers for OSG VOs can be found in the here .  Resources for  CMS  sites:  http://www.uscms.org/uscms_at_work/physics/computing/grid/index.shtml  CMS Hyper News:  https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3.html  CMS Twiki:  https://twiki.cern.ch/twiki/bin/viewauth/CMS/USTier3Computing", 
            "title": "Community-specific support"
        }
    ]
}