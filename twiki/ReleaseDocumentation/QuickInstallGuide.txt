%META:TOPICINFO{author="KyleGross" date="1481047990" format="1.1" version="1.21"}%
%META:TOPICPARENT{name="WebHome"}%
%SHOW_DOC_STATUS_TABLE%

---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---++ Preliminary
Before you start installing the CE, the following software and requirements must be available on your server:

   * Valid host, http, container certs (optional: service cert for rsv)
   * This guide assumes that software will be installation in /opt
   * A working pacman-%PACMAN_VERSION% installation is also assumed

   
---++ Preparing the server
   * Create the =/etc/grid-security= directory to hold the server certificates
   * Copy hostcert.pem & hostkey.pem to =/etc/grid-security/=
   * Copy httpcert.pem & httpkey.pem to =/etc/grid-security/http=, make sure these are owned by the daemon user
   * Optional: Add a user account for the RSV service if needed
   * Optional: Copy the rsv service cert to =/etc/grid-security/= and make sure that the rsv user account owns the cert and key

---++ Data directories
For an OSG ce installation, you'll need the following locations

   * OSG_APP - to hold vo installed software
      * Must be readable and writable on CE and readable on worker nodes (should have approximately 10GB per VO, some VOs may need significantly more space) 
   * OSG_GRID - location where wn-client software is located
      * Can be shared or local on worker nodes
   * OSG_DATA - transient storage data storage between nodes 
      * Optional - condor nfslite allows this to be omitted
   * OSG_READ / OSG_WRITE 
      * Transient storage visible on all worker nodes
      * Can be shared directory or URI (for srm locations)
   * OSG_WN_TMP
      * Scratch space on worker nodes about 10GB per job slot should be sufficient

---++ Installation


---+++ Preliminaries

   * Create /opt/osg-1.2
   * cd /opt/osg-1.2
   * Source the setup.sh from the pacman installation

---+++ Installing the CE software
   * Run =export OLD_VDT_LOCATION= if you're doing an upgrade and want to preserve your settings
   * Run =export PER_JOB_HISTORY_DIR=/opt/osg-1.2/gratia/var/data= for gratia
   * If using condor, run =export VDTSETUP_CONDOR_LOCATION=/your/condor/location=
   * Run <tt>pacman -allow trust-all-caches  -get %CACHE%:ce</tt>
   * Run <tt>pacman -get %CACHE%:Globus-JOBMANAGER-Setup</tt> where JOBMANAGER is Condor, PBS, LSF, or SGE
   * Optional: Run <tt>pacman -get %CACHE%:ManagedFork</tt>


---+++ Configuring the CE software
The hardest part of the configuration - editing the config.ini - is given in the "Minimal Configuration File Settings" section below.

   * Run =source setup.sh= 
   * Run =vdt-post-install=
   * Run =vdt-ca-manage setupCA --url SOURCE --location LOCATION=, where SOURCE is probably osg and LOCATION is the appopriate location for your certificates
   * If upgrading an installation
      * Backup =$VDT_LOCATION/osg/etc/config.ini=
      * Run =extract-config=
      * Edit =./extracted-config.ini= to fill in any missing values
      * Copy =extracted-config.ini= to =$VDT_LOCATION/osg/etc/=
   * Edit =osg/etc/config.ini= and set options appropriately.  A quick run through of the minimal changes needed for config.ini is given below.
   * Run =configure-osg -v= to verify your settings
   * Run =configure-osg -c= to configure the system
   * Run =visudo= to add entries for ws-gram

---+++ Enabling systems
   * Run =vdt-control --on=

---+++ Verify the install
   * Switch to your personal account, create a grid proxy and run the site verify script from =$VDT_LOCATION/verify/site_verify.pl=

---++ Minimal Configuration File Settings

This section gives the steps need to configure for a basic CE (no SE, minimal services, etc.) using the config.ini template provided.

   * Edit =$VDT_LOCATION/osg/etc/config.ini=  and do the following:
      * Fill in the =admin_email= and =localhost= settings in the =[Default]= section
      * Go to the =[Site Information]= section and fill in the settings that are listed as needing to be changed
      * Go to the section for your batch manager (e.g. PBS, LSF, Condor, etc.):
         * Set =enabled= to =True= or =%(enable)s=
         * Set =home= to your home directory location
         * Set =wsgram= to  =True= or =%(enable)s= if you are planning on providing WS-GRAM services
         * For SGE sites, *do not* set the =home= option in OSG 1.2.2 and above; this has been deprecated.  Instead, set =sge_root= to the value of $SGE_ROOT and =sge_cell= to $SGE_CELL.  The OSG CE will assume that SGE can be added to its environment by sourcing $SGE_ROOT/$SGE_CELL/common/settings.sh.
      * In =[Managed Fork]=, set =enabled= to =True= or =%(enable)s= if you are using !ManagedFork and set the =condor_location= and =condor_config= if you haven't set =VDT_CONDOR_LOCATION= or =CONDOR_LOCATION= in your environment
      * In =[Misc Services]=, set the =glexec_location= if using !GLExec and set =authorization_method= to =prima=, =xacml=, or =gridmap= 
      * In =[Storage]=, you will need to set the settings that are indicated as needing to be changed
      * In =[GIP]=, you will need to change the following settings:
         * =batch= should be set to the name of your batch manager in lower case (e.g. =pbs=, =sge=, =condor=)
         * =gsiftp_path= should be set to the =osg_data= directory that you specified in the =[Storage]= section
      * In the =[Subcluster CHANGEME= the settings for the subcluster should be filled in and =CHANGEME= should be changed to the subcluster's name
      * If you are using RSV, you will need to do the following the in =[RSV]= section:
         * Set =enabled= to =True= or =%(enable)s=
         * Fill in the =rsv_user= if rsv should not run using the =rsvuser= unix account
         * Enable the appropriate probes on your system
         * If you are using a service certificate:
            * Set =use_service_cert= to =True= or =%(enabled)s= 
            * Set =rsv_cert_file=, =rsv_key_file=, =rsv_proxy_file= if the defaults aren't appropriate
         * Otherwise set =proxy_file= to point to the proxy that RSV should use to submit jobs (*This file must exist*)

---++  Testing your installation
We will test your installation by checking to see whether it accepts grid jobs, gridftp requests, ws-gram job requests and that the rsv system checks are up and running.


----++++ Testing grid jobs
Here we will test the grid job submission system.  You can do this using the following steps, where your.ce.hostname is replaced with the hostname of your install host:
<pre class="screen">
$ globus-job-run your.ce.hostname:2119/jobmanager /usr/bin/id
</pre>

You should get the uid and gids for the user account that your DN is mapped to.  In addition, you should be able to do this using a different job manager by specifying jobmanager-condor instead of jobmanager.

---++++ Testing gridftp
Gridftp is a system for transfering files between various grid aware systems.  You can test your system by following the steps below:
 <pre class="screen">   
$ echo "test file" > test.file
$ globus-url-copy file:///$PWD/test.file gsiftp://your.ce.hostname/tmp/
</pre>
   * Once this is done, you should see a test.file in the /tmp on your ce
<pre class="screen">
$ globus-url-copy gsiftp://your.ce.hostname/tmp/test.file file:///$PWD/new.file
</pre>
   * Once this is done, you should have a file called new.file in your current directory

---++++ Testing WS-GRAM
Testing ws-gram is similar to testing the job grid submission:
<pre class="screen">
$ globusrun-ws -submit -F your.ce.hostname:9443 -s -c /usr/bin/whoami=
</pre>

You should get the uid and gids for the user account that your DN is mapped to.  In addition, you should be able to do this using a different job manager by specifying Condor instead of Fork.

---++++ Checking RSV
RSV consists of a series of probes that run at varying intervals on your system to test your system's integrity and functionality. You can see the results of these probe tests by going to [[https://your.ce.hostname:8443/rsv/][https://your.ce.hostname:8443/rsv/]].

----++++
At this point your installation should be complete and operational. 





%BR%



---++ *Comments*
| PM2RPM_TASK = CE | Main.RobertEngel | 28 Aug 2011 - 06:09 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = SuchandraThapa

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %NO%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = 
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %NO%


############################################################################################################
-->