%META:TOPICINFO{author="MarcoMambelli" date="1266528650" format="1.1" version="1.1"}%
%META:TOPICPARENT{name="ClusterCondorInstall"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%

---++ Introduction

We will use the latest stable release of Condor 7.2.4, released June 25, 2009.  Our approach will be to install Condor in  =/opt/condor= on the management node (*gc1-ce*) and share it with the other nodes in the cluster.  We will also prepare the host-specific configuration files for all nodes participating in the Condor pool and store them in a standard location =/opt/condor/etc=.  This will make it easy to make cluster-wide configuration changes.

Some useful links:
   * Condor download: http://www.cs.wisc.edu/condor/downloads-v2/download.pl
   * Installation manual: http://www.cs.wisc.edu/condor/manual/v7.2/3_2Installation.html

%BR%
*A note about the directory structure:*
This Condor installation was structured to facilitate upgrades to Condor with minimal effort.  The Condor release directory on each host is =/opt/condor= which is a soft link to the NFS exported =/nfs/condor/condor= which in turn is a link to the actual Condor installation directory which includes the version number: =/nfs/condor/condor-2.7.4=.  The motivation for this choice will hopefully become clear in the Upgrades section below.


---++ Management node installation

Install Condor on the machine that will be the central manager.  In this tutorial use *gc1-ce*, the same host as the OSG compute element.  

   * Go to the [[http://www.cs.wisc.edu/condor/downloads-v2/download.pl][download page]] and choose the Condor version (e.g. 7.2.4, current production version).
   * Choose the correct distribution for your platform (the one below condor 7.2.4 for RHEL 5 dynamically linked ):
      *  =condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz=  (for  x86 OS)
      *  =condor-7.2.4-linux-x86_64-rhel5-dynamic.tar.gz=  (for x86_64 OS)
   * Save the downloaded file to your home directory.
   * Create the machine-specific directories that Condor uses: <pre>
mkdir /scratch
mkdir /scratch/condor
</pre>
   * Make sure the *condor* user exists with a shared home directory (=/home/condor=).
   * Make sure  that =/opt/condor= is a soft link to =/nfs/condor/condor=. 
   * Create a temporary directory and extract the source from the downloaded file in your home directory<pre>
mkdir /tmp/condor-src
cd /tmp/condor-src
tar xvzf ~/condor-7.2.4-linux-x86-rhel5-dynamic.tar.gz 
</pre>

   * Prepare the target installation directory: <pre>
mkdir /nfs/condor/condor-7.2.4
ln -s /nfs/condor/condor-7.2.4 /nfs/condor/condor
</pre>

   * Check that =/opt/condor= exists and is linked to =/nfs/condor/condor=
   * Run the Condor installation script:<pre>
./condor_install --prefix=/opt/condor --local-dir=/scratch/condor --type=manager</pre>   
   * The installation script will place the machine-specific configuration file in the the directory specified by =local-dir=.  We want to manage these files in the shared area, so: <pre>
cp /scratch/condor/condor_config.local /opt/condor/etc/
cp /opt/condor/etc/condor_config.local /opt/condor/etc/condor_config.gc1-ce
</pre>
   * Open the global Condor configuration file =/opt/condor/etc/condor_config= in an editor.
   * Change the value of these variables:<pre>
LOCAL_CONFIG_FILE = /opt/condor/etc/condor_config.$(HOSTNAME)
UID_DOMAIN = yourdomain.org
FILESYSTEM_DOMAIN = yourdomain.org
HOSTALLOW_WRITE = *.yourdomain.org
</pre>
   * Make sure that you have <pre>CONDOR_HOST = gc1-ce</pre>
      * Note: =CONDOR_HOST= can be set with or without the domain name: =gc1-ce= or =gc1-ce.yourdomain.org=
      
   * Link =condor_config= in the *condor* user's home directory to the location of the global configuration file:<pre>
ln -s /opt/condor/etc/condor_config ~condor/condor_config
</pre>
   * Start up the Condor master and check for running processes <pre>
/opt/condor/sbin/condor_master
ps -ef |grep condor
condor    4404     1  0 06:42 ?        00:00:00 /opt/condor/sbin/condor_master
condor    4405  4404  0 06:42 ?        00:00:00 condor_collector -f
condor    4406  4404  1 06:43 ?        00:00:00 condor_negotiator -f
root      4410  4348  0 06:43 pts/2    00:00:00 grep condor
</pre>
   * To shutdown <pre>/opt/condor/sbin/condor_off -master
</pre>

   * Now create configuration files for each of the hosts for editing.   All hosts must have a corresponding condor_config.${HOST} file.  In addition all compute nodes must be configured the same way, so we create links to a single compute node configuration file (=condor_config.gc1-cXX=): <pre>
cd /opt/condor/etc/
cp condor_config.gc1-ce condor_config.gc1-cXXX
ln -s condor_config.gc1-cXXX condor_config.gc1-c001
ln -s condor_config.gc1-cXXX condor_config.gc1-c002
ln -s condor_config.gc1-cXXX condor_config.gc1-c003
cp condor_config.gc1-ce condor_config.gc1-ui
</pre>
   * For the user interface machine, where one submits jobs, edit =condor_config.gc1-ui= and enable the master and schedd processes by setting =DAEMON_LIST = MASTER, SCHEDD=.
   * For compute nodes, which run jobs, edit =condor_config.gc1-cXXX=. Enable master and startd processes by setting =DAEMON_LIST = MASTER, STARTD= and make sure that job start also when the worker node is used by adding the line  =START=TRUE=. By default Condor waits that a host is not used interactively before starting a job. Summarizing, the lines to add/modify:<pre>
DAEMON_LIST = MASTER, STARTD
START=TRUE
</pre>
   * In all local config files (=condor_config.gc1-ce=, =condor_config.gc1-ui=, =condor_config.gc1-cXXX=) make sure that lines containing the definition of =UID_DOMAIN=  and =FILESYSTEM_DOMAIN= are commented. In other words there should be no line starting with  =UID_DOMAIN == or =FILESYSTEM_DOMAIN == else these would override the value set in =condor_config=.
   * Remove the unused configuration file in the =/scratch/condor/= directory:<pre>
rm /scratch/condor/condor_config.local 
</pre>
   * Enable automatic startup at boot by setting the correct path of the =condor_master= executable (=MASTER=/opt/condor/sbin/condor_master=) in the boot file =/opt/condor/etc/examples/condor.boot=. Then:<pre>
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor on
</pre>
   * Add Condor commands to the user environment (if you like to have condor commands in the path): <pre>
cp /opt/condor/condor.sh /opt/condor/consor.csh /etc/profile.d/
</pre> 

---++ Setting up the other nodes
This should be executed on all other nodes where Condor should be running (worker nodes, submit host)
   * Create the local Condor directories in =/scratch/condor= (making sure that =execute= directory has the right permissions =a+rwx +t=) <pre>
mkdir /scratch
mkdir /scratch/condor
mkdir /scratch/condor/execute
mkdir /scratch/condor/log
mkdir /scratch/condor/spool
chmod a+rwx /scratch/condor/execute
chmod +t /scratch/condor/execute
</pre>
   * At the end you should see something like: <pre>
ls -l /scratch/condor/
total 12
drwxrwxrwt 2 condor root 4096 Oct 15 04:07 execute
drwxr-xr-x 2 condor root 4096 Oct 15 04:15 log
drwxr-xr-x 2 condor root 4096 Oct 15 04:02 spool
</pre>
   * Enable automatic startup:<pre>
cp /opt/condor/etc/examples/condor.boot /etc/init.d/condor
chkconfig --level 235 condor on
</pre>
   * Add Condor commands to the user environment (if you like to have condor commands in the path): <pre>
cp /opt/condor/condor.sh /opt/condor/consor.csh /etc/profile.d/
</pre>

---+++ Swap configuration on the condor submit host
If you have no swap space on the submit host all your job submissions will fail and in =/scratch/condor/log/SchedLog= you will see errors like "Swap space estimate reached! No more jobs can be run!". 

You can check your swap memory with =cat /proc/meminfo |grep Swap= and you will see something like:<pre> 
SwapCached:          0 kB
SwapTotal:     2097144 kB
SwapFree:      2097080 kB
</pre>
If the numbers above are all 0, you have no swap space.

The recommendation is to enable a swap space on your host. Here some information to [[http://www.cyberciti.biz/faq/linux-add-a-swap-file-howto/][add a swap file]] or [[http://www.xenotime.net/linux/doc/swap-mini-howto.txt][a swap-howto, see section 5]]. If you cannot, here is a workaround in Condor to avoid to use swap space. It will work for small test clusters.

In the file =condor_config.gc1-ui= file add the line: <pre>
RESERVED_SWAP = 0
</pre>

---++ Upgrades
As explained in the section about the directory structure, this Condor installation was structured to facilitate upgrades with minimal effort.  The Condor directory on each host is =/opt/condor=. This is a link to the exported =/nfs/condor/condor= that is a link to the real Condor installation directory that has also the version number in the name (e.g. =/nfs/condor/condor-2.7.4=).

Only one version of Condor will be used at the time but linking =/nfs/condor/condor= to a new directory, e.g. =/nfs/condor/condor-7.3.2= allows to install that version (similarly to what done above), test it and be able to revert back to the previous one if desired simply changing one link (e.g. if the new installation is not working properly). 

---++ Setup
Executing <pre>source /opt/condor/condor.sh</pre> will add the condor commands to your path and set the =CONDOR_LOCATION= variable.

---++ Testing the installation
You can see the resources in your Condor cluster using =condor_status=. Here is the output on =gs1-ce.mwt2.org= (the Condor headnode in the laptop):
<pre>
> condor_status

Name               OpSys      Arch   State     Activity LoadAv Mem   ActvtyTime

gs1-c001.mwt2.org  LINUX      INTEL  Owner     Idle     0.000   503  0+00:10:10
gs1-c002.mwt2.org  LINUX      INTEL  Owner     Idle     0.000   503  0+00:05:10

                     Total Owner Claimed Unclaimed Matched Preempting Backfill

         INTEL/LINUX     2     2       0         0       0          0        0

               Total     2     2       0         0       0          0        0
</pre>

To test condor you have to create  a submit file (=mytest.submit=) and submit a job.
   * submit file:<pre>
#  A test Condor submission file - mytest.submit
executable = /bin/hostname
universe = vanilla
log = test.log
output = test.out
error = test.err
queue
</pre>
   * Submit with:<pre>
condor_submit mytest.submit
</pre>
   * Check the jobs:<pre>
condor_q
</pre>
   * Inspect the files test.log/out/err. You can submit more than one test job. A different test can have =transfer_executable=true=.

---+++ A CPU intensive test
Here is a condor test to submit 20 CPU intensive jobs.
   * submit file (save it as =mytestcpu.sub=):<pre>
########################
# Submit description file for CPU test program
########################
Executable     = /home/gsadmin/use-processor.pl
# should_transfer_files  can be YES NO IF_NEEDED
should_transfer_files = YES
# when_to_transfer_output  can be ON_EXIT ON_EXIT_OR_EVICT 
when_to_transfer_output = ON_EXIT
Universe       = vanilla 
Output         = cputest.$(Process).out
Error          = cputest.$(Process).err
Log            = cputest.$(Process).log 
Queue 20
</pre>
   * Download the attached file [[%ATTACHURL%/use-processor.pl.txt][use-processor.pl]] and save it in =/home/gsadmin/= and make it executable, e.g. if you have wget: <pre>
wget https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/ClusterCondorInstall/use-processor.pl.txt
cp use-processor.pl.txt /home/gsadmin/use-processor.pl
chmod a+x /home/gsadmin/use-processor.pl
</pre>
   * Submit with:<pre>
condor_submit mytestcpu.sub
</pre>
   * Check the jobs:<pre>
condor_q
</pre>
   * Inspect the files =cputest.XX.log/out/err=, where XX is a number from 1 to 20. 

---++ More information
   * Examples running Condor jobs: http://www.cs.wisc.edu/condor/quick-start.html


%BR%
%COMPLETE2% %BR%
%RESPONSIBLE% Main.RobGardner - 23 Oct 2009 %BR%
%REVIEW%

---++ *Comments*
%COMMENT{type="tableappend"}%


<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/GridColombiaWorkshop2009/centerpageborder.css
-->