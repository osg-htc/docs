%META:TOPICINFO{author="KyleGross" date="1481047987" format="1.1" version="1.20"}%
%META:TOPICPARENT{name="SiteAdminsWorkshopTutorialsAug10"}%
---+!! *<nop>%SPACEOUT{%TOPIC%}%*
%DOC_STATUS_TABLE%
%TOC{depth="3"}%

---++ Introduction

This hands on will introduce you the basic Grid tools used in CMS for user analysis. CRAB (CMS Remote Analysis Builder) is the main tool for distributed user analysis. Please run and complete each of these exercises. At the end of this exercise, you should be able to install CRAB on a properly prepared machine and submit simple jobs to run on the GRID. For more complex uses of CRAB akin to the use cases for physicists, please see the resources under "More information."

---++ Requirements

To submit jobs to the Grid for this hands-on, you need a place where the following are all installed: gLite UI, CRAB and CMSSW 3_7_0_patch4. You also personally need a Grid certificate with the CMS VOMS membership installed on the same machine. If you want to install all of this on your own machine(s), please follow "Option A" below. Everything you need (except your certificate) is also available on FNAL or CERN computers. If you don't want to install or have trouble installing this software, you can just follow "Option B" below.

Certificate issues should already have been dealt with in the requirements before the workshop started.

---++ More Information

   * Official CRAB documentation for users: https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCrab
   * FAQ and troubleshooting page: https://twiki.cern.ch/twiki/bin/view/CMS/SWGuideCrabFaq
   * CRAB Feedback Hypernews: https://hypernews.cern.ch/HyperNews/CMS/get/crabFeedback.html (reporting problems not solved with FAQ)
   * gLite UI and CRAB installation: http://hep-t3.physics.umd.edu/HowToForAdmins/crab.html

---++ Option A: Install gLite, CRAB, and CMSSW on a Tier3

You may or may not want to install !CRAB locally for your users. It does complicate your install as you also need to install gLite-UI, but then your users can submit jobs from your local cluster. Alternatively, users can log into FNAL or CERN where CRAB is already installed.

To install CRAB on your own machine, you must install the gLite 3.2 client UI and the LCG CA bundle. I have generally found that the tarball of the gLite UI is easier, or at least more foolproof, to install than the RPM based UI. This page: http://hep-t3.physics.umd.edu/HowToForAdmins/crab.html by Malina Kirn provides very good instructions on installing the CA bundle, the gLite client (and CRAB). Some uses of CRAB may work with the OSG client only, but this is not a recommended or supported set up. Curious and ambitious administrators may attempt to follow this Hypernews message: https://hypernews.cern.ch/HyperNews/CMS/get/osg-tier3/249.html


After these prerequisites are taken care of, the CRAB install itself is quite simple:
   1 Point your browser to http://cmsdoc.cern.ch/cms/ccs/wm/scripts/Crab/
   1 Download the latest release that is not a pre-release (2.7.3_p(atch)1 at this time)
   1 =tar xvfz CRAB_XYZ.tgz=
   1 =cd CRAB_XYZ=
   1 =./configure=
That's it. CRAB is installed and should be ready to submit jobs

To complete the tutorial, you must also add CMSSW 3_7_0_patch4 to your installation. See Patrick's instructions from an earlier hands-on: https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/HandsOnCMSSWInstall#Installing_CMSSW_release_with_ap

---++ Option B: Use FNAL resources

This is much easier. Just log into cmslpc-sl5.fnal.gov if you have an account there. You may have to use kinit to get your kerberos ticket first.

Don't forget to install your grid certificate in =$HOME/.globus/= using the instructions at https://twiki.cern.ch/twiki/bin/view/Main/CRABPrerequisitesGRIDCredentials#Installation_of_the_certificate or by copying =$HOME/.globus/= from another machine and setting the permissions of the files and the directory correctly.

---++ Verify your grid certificate is OK

After you've installed your grid certificate, you need to verify it has all the information needed.

Login to *cmslpc-sl5.fnal.gov* or another machine with CRAB and gLite installed. First, set up the gLite UI. On *cmslpc-sl5* this is done by: 
<pre class="command">
source /uscmst1/prod/grid/gLite_SL5_CRAB_27x.csh
</pre>
Then intialize your proxy:
<pre class="command">
voms-proxy-init -voms cms
</pre>

Next run the following command:

<pre class="command">
voms-proxy-info -all | grep -Ei "role|subject"
</pre>
The response should look like this:
<pre class="output">
subject   : /DC=org/DC=doegrids/OU=People/CN=Eric Vaandering 123456/CN=proxy
subject   : /DC=org/DC=doegrids/OU=People/CN=Eric Vaandering 123456
attribute : /cms/Role=NULL/Capability=NULL
attribute : /cms/uscms/Role=NULL/Capability=NULL
</pre>
If you do not have the first attribute line listed above, you have not completed the VO registration above and you must complete it before continuing.

---++ Submit jobs with CRAB

This lengthy exercise is adapted from the latest !CRAB tutorial which is always in the CMS !WorkBook (https://twiki.cern.ch/twiki/bin/view/CMS/WorkBook) under !WorkBookCRABTutorial (https://twiki.cern.ch/twiki/bin/view/CMS/WorkBookCRABTutorial). This will be a better resource for you after the workshop has ended. The workbook pertains to running !CRAB at CERN, but there are just a couple of minor differences regarding software locations.

NOTICE: *Always use the latest production CRAB version*.

#PreRequisites
---+++ Prerequisites to run the tutorial. 

Just a reminder of the prerequisites before starting. You must
   * have a valid Grid certificate  
   * be registered in the CMS virtual organization
   * be registered in !SiteDB     

---+++ Recipe for the tutorial
For this tutorial we will use:

   * CMSSW_3_7_0_patch4   
   * The latest version of CRAB

The example is written to use the csh shell family. If you want to use sh, replace csh with sh.

---+++ Setup the CMS and gLite environments
Login to *cmslpc-sl5.fnal.gov* or another machine with CRAB and gLite installed.

Source the CMS and gLite UI setup script. At FNAL, you do this by
<verbatim class="command">
source /uscmst1/prod/sw/cms/cshrc prod
source /uscmst1/prod/grid/gLite_SL5_CRAB_27x.csh
</verbatim>

---+++ Setup analysis code

If you already have a CMSSW_3_7_0_patch4 area installed, great. If not, you must create one:
<verbatim class="command">
scram project CMSSW CMSSW_3_7_0_patch4 
</verbatim>

cd into the CMSSW src directory and setup the CMS environment before proceeding.

<verbatim class="command">
cd CMSSW_3_7_0_patch4/src
cmsenv 
</verbatim>
Next we have to download and build a simple little CMSSW example analysis (courtesy of Seema Sharma). Download  [[%ATTACHURL%/CRABAnalysis.tgz][CRABAnalysis.tgz]] into the =src/= directory (perhaps with wget) and then
<verbatim class="command">
tar xvfz CRABAnalysis.tgz
scram b  
</verbatim>

The example code is now compiled and built and will run when we type cmsRun.

---+++ !CRAB setup

To setup and use !CRAB, source the script =crab.(c)sh= located in =/uscmst1/prod/grid/CRAB=, which always points to the latest version of !CRAB.  (Note: At CERN and other sites, !CRAB is located in a different directory so the command will vary slightly.)

<verbatim class="command">
source /uscmst1/prod/grid/CRAB/crab.csh
</verbatim>

Notice: in order to have the correct environment, the setup order must always be 
   * source the gLite UI setup script
   * setup CMSSW software (=cmsenv=)
   * source the !CRAB setup script

---+++ Check your CMSSW code by running locally

It's always good to test analysis code on a small amount of data before submitting hundreds of jobs to the Grid. Let's try that now:

Simply type 
<verbatim> 
cd Analysis/SinglePions/test (assuming your CWD is CMSSW_3_7_0_patch4/src)
cmsRun singlepionsData_cfg.py
</verbatim>
and verify that a file named =IsolatedTracksTreeData.root= is created and no error messages are output by CMSSW. This works at FNAL where cmslpc-sl5 has access to the CMS data through it's logical file name (starting with =/store/data=). It may not work at your Tier3, in which case, don't worry.

---+++ Locate the dataset and prepare !CRAB submission

In this example we are going to use the dataset =/JetMETTau/Run2010A-Jun14thReReco_v2/RECO=. Generally users will find the data they want with the DBS Discovery page, but this is beyond the scope of this tutorial.

---+++ !CRAB configuration

Modify the !CRAB configuration file =crab.cfg= according to your needs: a fully documented template is available at =$CRABPATH/full_crab.cfg=, a template with essential parameters is available at =$CRABPATH/crab.cfg= . 

*Copy crab.cfg into your local area*. 

<pre class="command">
cp $CRABPATH/crab.cfg .
</pre>

For guidance, see the [[https://cmsweb.cern.ch/crabconf/crab-v2.6.4.html#configuration_parameters][list and description of configuration parameters]]. For this tutorial, the only relevant sections of the file are  =[CRAB]=, =[CMSSW]= and =[USER]= .

Edit =crab.cfg= to make it look like this (the order is not important, so it is just a few changes). Make sure you comment (with #) or delete other lines like =total_number_of_events=, =number_of_jobs=, and =output_file=.

<pre class="cfg">
[CMSSW]
total_number_of_lumis = -1
lumis_per_job = 200
pset = singlepionsData_cfg.py
datasetpath = /JetMETTau/Run2010A-Jun14thReReco_v2/RECO
get_edm_output = 1

[USER]
return_data=1

[CRAB]
scheduler = glidein
use_server = 1
jobtype = cmssw
</pre>

#MainConfiguration
---++++ Configuration parameters

Here is a short description of the most important parameters you may specify in your crab.cfg:
   * *pset*: the CMSSW configuration file name;
   * *output_file*: the output file name produced by your pset; if the output is defined in a !TFileService, the file is automatically handled by !CRAB, and there is no need to specify it explicitly;
   * *datasetpath*: the full dataset name you want to analyze;
   * *total_number_of_lumis*, *number_of_jobs*, *lumis_per_job*: you need to specify 2 of these parameters:
      * A lumi section is 23 seconds of CMS data. The number of lumi sections processed will either be *total_number_of_lumis* or *number_of_jobs*lumis_per_job*. The number of jobs and lumis processed by each job will only be approximately what you ask for.
   * *return_data*: this can be 0 or 1; if it is 1 you will retrieve your output files to your local working area;
   * *copy_data*: this can be 0 or 1; if it is 1 you will copy your output files to a remote Storage Element;
   * *publish_data*: this can be 0 or 1; if it is 1 you can publish your produced data to a local !DBS; 
   * *use_server*: send your jobs to a CRAB server. The server is picked automatically
   * *scheduler*: the name of the scheduler you want to use; when using CRAB server, this can be glite or glidein

#SetRunCrab
---+++Run Crab
Once your =crab.cfg= is ready and the underlying environment is set up, you can start running !CRAB.
!CRAB has command line help which can be useful for the first time. You can get it via:
<pre class="command">
crab -h
</pre>
in particular there is a  section HOW TO RUN !CRAB FOR THE IMPATIENT USER where the base commands are reported.

#JobCreation
---++++ Job Creation
The job creation checks the availability of the selected dataset and prepares *all* the jobs for submission according to the selected job splitting specified in the crab.cfg

The creation process creates a !CRAB project directory (default: crab_0_date_time) in the current working directory, where the related crab configuration file is cached for further usage,  avoiding  interference with other (already created) projects

!CRAB allows the user to chose the project directory name, so that it can be used later to distinguish multiple !CRAB projects in the same directory.

<pre class="command">
crab -create
</pre>

may ask for proxy/myproxy passwords and should produce output like this:

<verbatim style="font-size: 13px" class="output">
[ewv@cmslpc16 test]$ crab -create -cfg crab_server.cfg
crab:  Version 2.7.3 running on Wed Aug  4 11:22:56 2010 CST (16:22:56 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (use_server)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Contacting Data Discovery Services ...
crab:  Accessing DBS at: http://cmsdbsprod.cern.ch/cms_dbs_prod_global/servlet/DBSServlet
crab:  Requested dataset: /JetMETTau/Run2010A-Jun14thReReco_v2/RECO has 5228398 events in 1 blocks.

crab:  143 jobs created to run on 53403 lumis
crab:  Adding IsolatedTracksTreeData.root (from TFileService) to list of output files
crab:  Creating 143 jobs, please wait...
crab:  Total of 143 jobs created.

Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

#JobSubmission
---++++ Job Submission

With the submission command it's possible to specify a combination of jobs and job-ranges separated by comma (e.g.: =1,2,3-4), the default is all.
Since we're just testing, we'll submit just a few jobs. To submit the first ten jobs of  the last created project with the default name, execute the following command:
<verbatim class="command">
crab -submit 1-10
</verbatim>
to submit all the jobs of a specific project:
<verbatim class="command">
crab -submit -c  <dir name>
</verbatim>

which should produce output like this:

<verbatim  class="output" style="font-size: 13px">
[ewv@cmslpc16 test]$ crab -submit -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 11:24:27 2010 CST (16:24:27 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Registering credential to the server : glidein-2.t2.ucsd.edu
crab:  Credential successfully delegated to the server.

crab:  Starting sending the project to the storage glidein-2.t2.ucsd.edu...
crab:  Task crab_server successfully submitted to server glidein-2.t2.ucsd.edu

crab:  Total of 143 jobs submitted
Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

#JobStatusCheck
---++++ Job Status Check
Check the status of the jobs in the latest !CRAB project with the following command:
<verbatim class="command">
crab -status 
</verbatim>
to check a specific project:
<verbatim class="command">
crab -status -c  <dir name>
</verbatim>

which should produce output like this:
<verbatim  style="font-size: 13px" class="output">
[ewv@cmslpc16 test]$ crab -status -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 11:34:50 2010 CST (16:34:50 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:
ID     STATUS             E_HOST                               EXE_EXIT_CODE JOB_EXIT_STATUS
--------------------------------------------------------------------------------------------
1      Running            red.unl.edu
2      Running            egeece02.ifca.es
3      Running            sbgce1.in2p3.fr
4      Running            egeece01.ifca.es
5      Running            egeece02.ifca.es
[... edited for brevity ...]
143    Running            osg-gw-4.t2.ucsd.edu

crab:   143 Total Jobs
 >>>>>>>>> 143 Jobs Running
        List of jobs Running: 1-143

crab:  You can also follow the status of this task on :
        CMS Dashboard: http://dashb-cms-job-task.cern.ch/taskmon.html#task=ewv_crab_server_3t5rh6
        Server page: http://glidein-2.t2.ucsd.edu:8888/logginfo
        Your task name is: ewv_crab_server_3t5rh6

Log file is /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/log/crab.log
</verbatim>

eventually jobs will start to finish and you will see them all listed as "Done" like this:

<verbatim  style="font-size: 13px" class="output">
...
73     Done               osg-gw-4.t2.ucsd.edu                 0             0
74     Done               red.unl.edu                          0             0
75     Done               sbgce1.in2p3.fr                      0             0
76     Done               sbgce1.in2p3.fr                       
77     Done               sbgce1.in2p3.fr                       
78     Done               sbgce1.in2p3.fr                      0             0
79     Done               sbgce1.in2p3.fr                      0             0
80     Running            sbgce1.in2p3.fr
-------------------------------------------------------------------------------------------------------
81     Done               sbgce1.in2p3.fr                      0             0
82     Done               osg-gw-4.t2.ucsd.edu                 0             0
83     Done               osg-gw-4.t2.ucsd.edu                 0             0
...
</verbatim>

Note that some jobs are Done with exit code 0 and others have no exit code. The latter are not quite ready to be retrieved. Wait for them to show exit codes. As soon as you've gotten a few jobs to finish with exit code 0, you can go to the "Job Output Retrieval" step.

%BLUE%Also, you can sometimes (depending on the server you used) monitor the status of your job on the CRAB server with your web browser. Go to the "Server page" link with your web browser and paste the task name into the text field you see and press "Show".%ENDCOLOR%

---++++ Job Resubmission

Sometimes grid jobs will not complete successfully and need to be resubmitted. They will either be listed by =crab -status= as Aborted or Done with a non-zero error code. In this case, you can resubmit the jobs. If jobs 4, 6, 7, 8 failed, you can resubmit those four jobs with this command:

<pre class="command">
crab -resubmit 4,6-8
</pre>
or
<pre class="command">
crab -forceResubmit 4,6-8
</pre>
if CRAB refuses to resubmit the jobs.

#JobOutputRetrieval
---++++ Job Output Retrieval
For the jobs which are in the "Done" state you need to retrieve the files produced by the jobs. The following command retrieves the files of all "Done" jobs of the last created !CRAB project:
<pre class="command">
crab -getoutput 
</pre>
to get the output of a specific project:
<pre class="command">
crab -getoutput -c  dir_name
</pre>
the job results will be copied in the =res= subdirectory of your crab project:
<pre  style="font-size: 13px" class="output">
[ewv@cmslpc16 test]$ crab -getoutput -c crab_server
crab:  Version 2.7.3 running on Wed Aug  4 13:22:19 2010 CST (18:22:19 UTC)

crab. Working options:
        scheduler           glidein
        job type            CMSSW
        server              ON (default)
        working directory   /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/

crab:  Only 142 jobs will be retrieved  from 143 requested.
        (for details: crab -status)
crab:  Starting retrieving output from server glidein-2.t2.ucsd.edu...
crab:  Results of Jobs # 1 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 2 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 3 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
crab:  Results of Jobs # 4 are in /uscms/home/ewv/crab-test/CMSSW_3_7_0_patch4/src/Analysis/SinglePions/test/crab_server/res/
...
</pre>

---+++ Analysis of output

If you are doing a full blown analysis, you want to, of course, wait for all the results to be retrieved. Since we're just going to look at one histogram, we can start as soon as a few files are retrieved. With this particular analysis, a lot of the histograms are empty, so we want to find one of the largest files to look at:

<verbatim>
cd crab_0_[blah]/res/
ls -lSr *.root
root IsolatedTracksTreeData_xyz_1_ABC.root
</verbatim>

If you've never used root before, don't worry. What we're going to do is a very simple check on the output. At the prompt, type: =new TBrowser(); [enter]= and you should see a graphical window. In the right hand pane, click on "ROOT Files" -> the file name -> "IsoTracks" -> "Tracks" and then click on any histogram to view it. With luck, it will not be empty.

Were you doing an analysis, you would combine all the histogram files into a single file and use that in your analysis.

---++ Submit jobs to your Tier3

After finishing CRAB submission to Tier2 sites, you may want to test your own resources. Hopefully you transferred the dataset we are using to your own Trash/Tier3 as part of the Phedex hands-on. If so, you can try your hand at running jobs at your own site.

Doing so is easy. Just add the line 
<pre>
se_white_list = T3_US_[your site]
</pre>
to the =[GRID]= section of the =crab.cfg= file. If you don't have a =[GRID]= section, just create one.

Then, repeat the exercise above. Instead of seeing your jobs running at a variety of international CMS sites, they should all be sent to your Tier3. If you didn't transfer the data as part of the Phedex hands-on, you may want to try another Trash/Tier3 or just restrict things to a particular Tier2 to test out the white and black listing settings. (More configuration options are listed in the CRAB talk.) 

---++ Writing your data to a remote SE

If you have appropriate permissions, writing your data out to a storage element is easy. Just change the relevant portion of the =crab.cfg= file to look like this: 

<pre>
[USER]
return_data = 0
copy_data = 1
storage_element = TX_US_Somewhere
user_remote_dir = myDir
</pre>

This will write your output .root files to the LFN =/store/user/USERNAME/myDir= on the remote site TX_US_Somewhere. Both TX_US_Somewhere and USERNAME are translated using SiteDB. The latter is your Hypernews username as determined from your certificate DN. Since we did not ask you to have a /store/user area set up as part of the prerequisites for the hands-on, this is unlike to work right away. But you can pursue this on your own after the fact. You and your users have a designated Tier2 center that will provide you with a /store/user area. Instructions for requesting it are at http://www.uscms.org/uscms_at_work/software_computing/tier2/store_user.shtml. Of course you can set up /store/user areas at your own Trash/Tier3 as well.

---++ Presentation

To follow hands on

---++ References

---++ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = JamesWeichel

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = General

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Training

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = 
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = 
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %YES%
############################################################################################################
-->
-- Main.EricVaandering - 03 Feb 2010
   * [[%ATTACHURL%/CRABAnalysis.tgz][CRABAnalysis.tgz]]: tarball of the sample analysis for the tutorial

%META:FILEATTACHMENT{name="CRABAnalysis.tgz" attachment="CRABAnalysis.tgz" attr="" comment="tarball of the sample analysis for the tutorial" date="1280956392" path="CRABAnalysis.tgz" size="20483" stream="CRABAnalysis.tgz" tmpFilename="/usr/tmp/CGItemp18561" user="EricVaandering" version="1"}%
