%META:TOPICINFO{author="KyleGross" date="1481047986" format="1.1" version="1.51"}%
%META:TOPICPARENT{name="WorkerNodeClient"}%
%DOC_STATUS_TABLE%

---+!! Glexec Installation Guide

%TOC{depth="3"}%

---+ About This Document

%ICON{hand}% This document is intended for System Administrators that are installing the OSG version of glexec.

%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%

---+ Applicable Versions

The applicable software versions for this document are %RED%OSG Version >= 1.2.14%ENDCOLOR%.  The version of glexec installed should be %RED%0.6.8.3-osg3%ENDCOLOR%.

---+ About Glexec

Glexec is commonly used for what are referred to as "pilot" or "glidein" jobs.  

Traditionally, users submitted their jobs directly to a remote site (or compute element gatekeeper).   The user job was authenticated/authorized to run at that site based on the user's proxy credentials and run under the local unix account assigned.

In a pilot-based infrastructure, users submit their jobs to a centralized site (or queue).   The pilot/glidein software at the centralized site then recognizes there is a demand for computing resources.  It will then submit what is called a pilot/glidein job to a remote site.  This pilot job gets authenticated/authorized to run on a worker node in that site's cluster.  It will then "pull" down user jobs from the centralized queue and execute them.  Both the pilot and the user job are run under the pilot job's proxy certificate credentials and local unix account. This represents a security problem in pilot-based systems as there is no authentication/authorization of the individual user's proxy credentials and, thus, the user's jobs do not run using it's own local unix account.

Glexec is a security tool that can be used to resolve this problem.   It is meant to be used by VOs that run these pilot-based jobs.  It  has a number of authentication plugins and can be used both by EGEE LCAS/LCMAPS and by OSG with GUMS and SAZ.   

The pilot job will "pull" user jobs down from the central queue and invoke glexec which will then 
   1 authenticate the user job's proxy, 
   1 perform an authorization callout (GUMS/SAZ in the case of OSG) similar to that done by the gatekeeper,
   1 and then run the user job under the local account assigned by the authorization service (for OSG, this is GUMS) for that user.

In effect, glexec functions much the same as a compute element gatekeeper, except these functions are now performed on the individual worker node.  The pilot jobs authentication/authorization is done by the gatekeeper and the individual user jobs are now done by glexec on the individual worker node.

Many worker node clusters use shared file systems like NFS for much of their software and user home accounts.  Since glexec is an suid program, it must be installed on every single worker node individually.  Most shared file systems do not handle this correctly so it cannot and must not be NFS-exported.

For more information regarding pilot-based systems and glexec:
   1 <a href="http://www.uscms.org/SoftwareComputing/Grid/WMS/glideinWMS/doc.html">glideinWMS - The glidein based WMS</a>
   1 <a href="http://iopscience.iop.org/1742-6596/119/5/052029/pdf/jpconf8_119_052029.pdf">Addressing the pilot security problem with gLExec (pdf)</a>

---+ Engineering Considerations
This section describes any prerequisite software/considerations that must be taken into account before the glexec software installation is performed.  It should be reviewed completely before starting the installation process.

   1 The installation process described on this page is for a _pacman_ installation from the VDT.
   1 An alternative, RPM distribution, is under development/test and is available here: http://vdt.cs.wisc.edu/glexec-rpm/ . Keep in mind that this RPM distribution is under development and evolving.  When released for production this section will contain both installation methods.
   1 Glexec must be installed as root.  The =glexec= program is an =suid= program and thus must be installed on every single worker node individually.    It cannot and must not be exported over NFS and it is probably not a good idea to use any other shared filesystem either.  
   1 Same location on all worker nodes.  It is recommended that the glexec package be installed within the OSG Worker Node Client area because most of its dependencies are the same  as those within the worker node client.   It must be installed in the *same*  location on all worker nodes managed by the same Compute Element.   The reason for this is that the location of the glexec executable is defined on the CE. Details are described in the [[GlexecInstall#Configuring_the_CE_node][Configuring the CE node section]] of this document.

---+ How to get Help?

To get assistance please use [[HelpProcedure][this page]]. 

---+ Requirements

These are the requirements that must be met before proceeding with the glexec installation:

   1 Local user "glexec".  You must create a <b>"glexec" local user</b> before you attempt the installation.   Glexec uses it for privilege separation and the installation will fail without it.
   1 Group ids for tracking purposes.  Glexec also requires a range of <b>group ids</b> for tracking purposes. 
      * Define at least 4 group ids per batch slot per worker node.   A conservative way to handle this is to multiply the number of batch slots on the largest worker node by 6 and then share the group ids between all the worker nodes.
      * They must be consecutive and in any range (default range is 65000-65049, configured in post-install section below).   
      * The same group ids can be used on every worker node.
   1 CA Certificates.  Remember to install the CA Certificates as for every VDT install.  Follow the instructions in CaCertificatesInstall.
   1 A host cert or proxy on every node.  Each worker node which is executing =glexec= needs to have either a host certificate or a host proxy distributed from the gatekeeper.  If you don't have host certificates for the worker nodes, there is a host proxy distribution script located at  =http://fermigrid.fnal.gov/files/glexec/host_dist_latest.tgz= which can be installed on your gatekeeper and will automatically create a proxy from your gatekeeper's host certificate and push it out to =/etc/grid-security/hostproxy.pem= and =/etc/grid-security/hostproxykey.pem=.

---+ Installation Procedure
<b>Glexec must be installed as root</b>.

It is recommended that the glexec package be installed within the OSG Worker Node Client area because most of its dependencies are the same 
as those within the worker node client.   It must be installed in the *same*  location on all worker nodes managed by the same Compute Element.
The reason for this is that the location of the glexec executable is defined on the CE. Details are described in the  [[GlexecInstall#Configuring_the_CE_node][Configuring the CE node section]] of this document.

To install:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% pacman -get %CACHE%:Glexec
%UCL_PROMPT_ROOT% source ./setup.sh
%UCL_PROMPT_ROOT% vdt-post-install
</pre>

%NOTE% The =setup.sh= file is not changed to put the =glexec= binary into your path.

After the =pacman= installation is complete, perform these checks:
   1 The install will create config files under =/etc/glexec= if the directory does not already exist.
   1 The install should have created the =/var/log/glexec= directory. If it does not exist, create it with 555 permissions with root owner/group.
      * dr-xr-xr-x 2 root root 4096 Jun 29 03:32 /var/log/glexec
   1 Verify that the  =VDT_LOCATION/glexec-osg/sbin/glexec= binary has =suid= permissions. If it does not, check the =vdt_install.log= and the =post-install/README= file to see it sheds light on a problem.
      * -rwsr-sr-x 1 root root 157394 Jun 29 03:32 glexec. 




---+Post-install Configuration of glexec
The following steps need to be executed manually after the glexec installation is complete.

---++Modify /etc/glexec/contrib/gums_interface/getmapping.cfg

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg= to point to the correct location of your GUMS server.  

The =gums_url= variable is the only variable you should have to change unless you want to use the XACML2 interface; see the "Using the new XACML2 GUMS interface" section below. 

If you're using host proxies instead of host certificates, as described above, you will also need to change =host_x509_file= and =host_key_file= .

<pre class="file">
# gums_interface/getmapping config file
#
# this is a shell script that gets sourced
#

vdt_location=/opt/vdt
log_dir=/var/log/glexec

########################
# GUMS config parameters
########################
<b>
# Use this if you are using host proxies
#host_x509_file="/etc/grid-security/hostproxy.pem"
#host_key_file="/etc/grid-security/hostproxykey.pem"

# Use this if you are using host certs
host_x509_file="/etc/grid-security/hostcert.pem"
host_key_file="/etc/grid-security/hostkey.pem"
</b>
# Use this if you have a host cert mapfile
#host_mapfile=/etc/glexec/contrib/gums_interface/host_mapfile
<b>
##################################
# Mapping program
map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"
# Point it to your GUMS machine
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"
</b>
#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface
#map_exe="/opt/vdt/prima/bin/prima_scas_map_args"
# Point it to your GUMS machine
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"


######################
# Optional SAZ command
# Leave it blank if you don't use SAZ
sazcmd_location=
sazcfg=

# if set to 1, will map even if SAZ fails
saz_warn_only=0

####################
# monitoring program
# leave empty if none should be used
# if set, it will accept arguments
#   mon_pid log_dir job_id target_uid user_DN [vo issuer [user_FQAN]*]
monitor_exe=/opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor

# Should monitoring errors be nonfatal?
continue_on_monitor_error=0

</pre>

---++ Using the new XACML2 GUMS interface 
Starting with this version of VDT, GUMS and PRIMA can use two different ways to communicate with each other  
   * the old PRIMA legacy protocol, based on SAML v1
   * the new XACML2 protocol defined by the OSG-EGEE interoperability group

The default installation (describe above) will, for backward compatibility reasons, use the old PRIMA legacy protocol.
However, if you installed GUMS v1.3.0 or later, you can switch to the new interface.

Modify the file =/etc/glexec/contrib/gums_interface/getmapping.cfg=, 
commenting out the default =map_exe= and =gums_url=, then
uncomment and configure the alternative one.

<pre class="file">
##################################
# Mapping program<b>
#map_exe="/opt/vdt/prima/bin/gums_map_args /opt/vdt/prima/etc/opensaml/"</b>
# Point it to your GUMS machine<b>
#gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSAuthorizationServicePort"
</b>
#######################################
# Alternative mapping program
# using the new SAML2-XACML2 interface<b>
map_exe="/opt/vdt/prima/bin/prima_scas_map_args"</b>
# Point it to your GUMS machine<b>
gums_url="https://yourmachine.yourdomain:8443/gums/services/GUMSXACMLAuthorizationServicePort"
</b>
</pre>

---++Modify /etc/glexec/tracking_groups.cfg

Modify the file =/etc/glexec/tracking_groups.cfg= with the correct tracking group id range that you created in the pre-installation section.
<pre class="file">
# This file defines the range of GIDs
# used for tracking purposes
#<b>
# Make sure this range of GIDs is not used for any other purpose
#</b>
min_gid=65000
max_gid=65049
</pre>

---+ Testing glexec locally on the worker node

Now, __as a non-privileged user (not root)__ , do the following (where "yourvo" is your vo, etc.):
<pre class="screen">
%UCL_PROMPT% cd $VDT_LOCATION
%UCL_PROMPT% source setup.sh
%UCL_PROMPT% voms-proxy-init -voms yourvo:/yourvo -file /tmp/x509_xyz

%UCL_PROMPT% export GLEXEC_CLIENT_CERT=/tmp/x509_xyz
%UCL_PROMPT% $VDT_LOCATION/glexec-osg/sbin/glexec /usr/bin/id
It appears that the value of pthread_mutex_init is 10603520
uid=13160(fnalgrid) gid=9767(fnalgrid)
</pre>
If glexec is successful, it will print out the uid and gid that your proxy would normally be mapped to by your gums server.

.

---+ Glexec log files
There are four glexec log files that are made on each worker node and they have  four different time stamp formats.  They all live, by default, in =/var/log/glexec= directory:

---++ glexec_log
This is the main log of each glexec. It logs in GMT
<pre class="file">
glexec[11769] 20101015T080224Z: real_username(cdffgrid),real_groupname(cdf)->mapped_username(cdf),mapped_group(cdf) cmd: /bin/bash
glexec[21024] 20101015T092120Z: real_username(cdffgrid),real_groupname(cdf)->mapped_username(cdf),mapped_group(cdf) cmd: /bin/bash
glexec[21457] 20101015T092436Z: real_username(cdffgrid),real_groupname(cdf)->mapped_username(cdf),mapped_group(cdf) cmd: /bin/bash
glexec[21534] 20101015T092437Z: real_username(cdffgrid),real_groupname(cdf)->mapped_username(cdf),mapped_group(cdf) cmd: /bin/bash
</pre>

---++ gums_interface.log
This logs the gums mapping process. It logs in local time.
<pre class="file">
glegumsi[23138#23137] 2008-12-02T13:58:17Z Caller account (500,500)
glegumsi[23138#23137] 2008-12-02T13:58:17Z Base subject  '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872/CN=proxy'
glegumsi[23138#23137] 2008-12-02T13:58:17Z Found subject '/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Primary   FQAN '/cms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:17Z    Secondary FQAN '/cms/uscms/Role=NULL/Capability=NULL'
glegumsi[23138#23137] 2008-12-02T13:58:18Z GUMS mapped to user 'uscms466'
glegumsi[23138#23137] 2008-12-02T13:58:18Z Mapped as (504,504)
glegumsi[23138#23137] 2008-12-02T13:58:18Z Launching monitor /opt/vdt/glexec-osg/contrib/glexec_monitor/glexec_monitor
glegumsi[23138#23137] 2008-12-02T13:58:18Z Using tracking GID 65002
</pre>

---++ lcas_lcmaps.log
The log of the lcas and lcmaps portions of glexec.
Time is a mix of local time and GMT.
<pre class="file">
LCAS   1: 2010-10-15.04:24:36-21534 : 
LCAS   1: 2010-10-15.04:24:36-21534 : Initialization LCAS version 1.3.11.3
LCMAPS 1: 2010-10-15.04:24:36-21534 : 
LCMAPS 7: 2010-10-15.04:24:36-21534 : Initialization LCMAPS version 1.4.8-4
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-startPluginManager(): Reading LCMAPS database /etc/glexec/lcmaps/lcmaps-suexec.db
LCAS   0: 2010-10-15.04:24:36-21534 : LCAS already initialized
LCAS   2: 2010-10-15.04:24:36-21534 : LCAS authorization request
LCAS   1: 2010-10-15.04:24:36-21534 :   lcas_userban.mod-plugin_confirm_authorization(): checking banned users in /etc/glexec/lcas/ban_users.db
LCAS   1: 2010-10-15.04:24:36-21534 : lcas.mod-lcas_run_va(): succeeded
LCAS   1: 2010-10-15.04:24:36-21534 : Termination LCAS
LCAS   1: Termination LCAS
LCMAPS 0: 2010-10-15.04:24:36-21534 : LCMAPS already initialized
LCMAPS 5: 2010-10-15.04:24:36-21534 : LCMAPS credential mapping request
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-runPlugin(): found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_verify_proxy.mod
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-runPlugin(): running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_verify_proxy.mod
LCMAPS 0: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_verify_proxy-plugin_run(): verify proxy plugin succeeded
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-runPlugin(): found plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-runPlugin(): running plugin /usr/local/grid/glexec-osg/lib64/modules/lcmaps_gums.mod
LCMAPS 5: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_gums-plugin_run(): Adding UID :  1347
LCMAPS 5: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_gums-plugin_run(): Adding PRI_GID :  3200
LCMAPS 5: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_gums-plugin_run(): Adding SEC_GID :  65005
LCMAPS 5: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_gums-plugin_run(): Adding POOL_INDEX :  (void)
LCMAPS 0: 2010-10-15.04:24:36-21534 :   lcmaps_plugin_gums-plugin_run(): gums plugin succeeded
LCMAPS 0: 2010-10-15.04:24:36-21534 : lcmaps.mod-lcmaps_run_with_pem_and_return_account(): succeeded
LCMAPS 7: 2010-10-15.04:24:36-21534 : Termination LCMAPS
LCMAPS 1: 2010-10-15.04:24:36-21534 : lcmaps.mod-lcmaps_term(): terminating
LCMAPS 7: 2010-10-15.09:24:37 : Termination LCMAPS
LCMAPS 1: 2010-10-15.09:24:37 : lcmaps.mod-lcmaps_term(): terminating
</pre>

---++ glexec_monitor.log
The glexec utility also has a process called glexec_monitor which runs as root through the life of the glexec'd process.
It logs in  GMT format
<pre class="file">
glemon[23093#22904]: 2008-12-02T19:57:06Z Tracking gid: 65001
glemon[23093#22904]: 2008-12-02T19:57:06Z Monitor pid: 23095
glemon[23093#22904]: 2008-12-02T19:57:07Z Parents: 137980749#22904,137980747#22903,137477937#14901,137477234#14871,3906#3203
glemon[23093#22904]: 2008-12-02T19:57:07Z System boot time: 1226868018
glemon[23327#23137]: 2008-12-02T19:58:18Z Started, should use uid 504
glemon[23327#23137]: 2008-12-02T19:58:18Z Used DN: "/DC=org/DC=doegrids/OU=People/CN=Igor Sfiligoi 673872"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used VO: "cms" Issuer: "/DC=ch/DC=cern/OU=computers/CN=lcg-voms.cern.ch"
glemon[23327#23137]: 2008-12-02T19:58:18Z Used FQAN: "/cms/Role=NULL/Capability=NULL"
glemon[23327#23137]: 2008-12-02T19:58:18Z Tracking gid: 65002
glemon[23327#23137]: 2008-12-02T19:58:18Z Monitor pid: 23329
glemon[23327#23137]: 2008-12-02T19:58:19Z Parents: 137987916#23137,137987916#23136,137477937#14901,137477234#14871,3906#3203
glemon[23327#23137]: 2008-12-02T19:58:19Z System boot time: 1226868018
glemon[23093#22904]: 2008-12-02T19:59:07Z Terminated, CPU user 0 system 0
glemon[23093#22904]: 2008-12-02T19:59:08Z Killing procd 23096
glemon[23093#22904]: 2008-12-02T19:59:08Z All childs terminated
</pre>

---+  Configuring the CE node to use glexec
---++ config.ini changes
After all worker nodes have been configured for =glexec= , the CE node =$VDT_LOCATION/osg/etc/config.ini= file needs to be modified to tell the
location of the glexec executable.  As mentioned previously, this is why the =glexec= executable __must__ be installed in the same location on all worker nodes.
<pre class="file">
[Misc Services]
; If you have glexec installed on your worker nodes, enter the location
; of the glexec binary in this setting
glexec_location = YOUR_WN_VDT_LOCATION/sbin/glexec
</pre>

To effect the change, you need to run this on the CE node:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% source YOUR_CE_VDT_LOCATION/setup.sh
%UCL_PROMPT_ROOT% configure-osg -c
</pre>

This will populate the =OSG_GLEXEC_LOCATION= variable in =$VDT_LOCATION/osg/etc/osg-job-environment.conf= file which contains variables
available to all grid jobs.  This is how pilot jobs invoke =glexec= ..

---++ Testing the CE configuration
To test this, create a script like this to run on a worker node via your batch system:
<pre class="file">
#!/bin/bash
echo "OSG_GLEXEC_LOCATION=$OSG_GLEXEC_LOCATION"
export GLEXEC_CLIENT_CERT=$X509_USER_PROXY
"$OSG_GLEXEC_LOCATION" /usr/bin/id
echo glexec result: $?
</pre>

Submit the job via your batch system. The result should be something like this:
<pre class="screen">
OSG_GLEXEC_LOCATION=/opt/osg-wn-client/glexec-osg/sbin/glexec
uid=13160(fnalgrid) gid=9767(fnalgrid) groups=65000(glexec00)
glexec result: 0
It appears that the value of pthread_mutex_init is -43132256
</pre>

%BR%

---+ *Comments*
| PM2RPM_TASK = GLEXEC | Main.RobertEngel | 28 Aug 2011 - 05:32 |
%COMMENT{type="tableappend"}%


<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          =  DaveDykstra

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager|Documenter)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %NO%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = SuchandraThapa
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


   DEAR DOCUMENT TESTER
   ====================

   Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = AnthonyTiradani
  
   Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->