%META:TOPICINFO{author="KyleGross" date="1476285753" format="1.1" version="1.9"}%
%META:TOPICPARENT{name="GridColombiaWorkshop2009"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---++ Overview

For this demo we will be using Povray, which is a raytracer used to create three-dimensional graphics. A scene (including objects, textures, lights, cameras, ...) is described in a scene description language, and Povray then follow the light rays and creates a image with shadows, transparency and so on. This final output we are going to do in this demo is an image of a Bonsai tree:

<img src="%ATTACHURLPATH%/rendered.png" alt="rendered.png" width='400' height='300' />


Running Povray can be pretty compute intensive. Ho intensive depends on scene complexity and output size of the image. Running on a grid like OSG, one of the things you will have to do is break your problem up into sub problems. For our rendering problem, we will split the task of rendering the full image up into the problems of rendering tiles of the image, and then when we have all the tiles, we will put the tiles together into the final image. How many tiles you want to do is a user setting, but one possible break down would be 8x6 (48 tiles).

<img src="%ATTACHURLPATH%/rendered_split_lines.png" alt="rendered_split_lines.png" width='500' height='375' />


The way to describe a set of jobs in OSGMM is as a [[http://www.cs.wisc.edu/condor/manual/v7.2/2_10DAGMan_Applications.html][Condor DAGMan]]. The normal use case for DAGMans is that you have job dependencies, but in the case of rendering Povray tiles, the jobs are fully independent (you can render each tile without knowing what is going on with the neighboring tiles). But there are other advantages with DAGMan. It has hooks for pre/post scripts than can run locally before and after a job, and if case of job failures, DAGMan can resubmit the job. The steps necessary to create a workflow is:

   * for each tile, create a Condor submit file to describe the job and pass the needed parameters to the job
   * add all the Condor jobs to the DAGMan
   * submit the DAGMan


---++ Starting a Run

To start a run, user your assigned training account and ssh to gs-mm.uchicago.edu. Then, copy the example to your home directory, and see what files and directories exists in the example directory:

<pre class="screen">
[train01@gs-mm ~]$ <b>cp -r ~osgmm/povray-example ~/</b>
[train01@gs-mm ~]$ <b>cd povray-example</b>
[train01@gs-mm ~]$ <b>ls</b>
helpers
inputs
local-post-job
local-pre-job
remote-povray-wrapper
runs
submit
</pre>

Description of the files / directories:

   * *helpers* - This is a directory holding helper scripts. In the case of the povray example, there is only one helper, tiles-combine, which is used to combine all the individual tiles into one final image

   * *inputs* - Directory containing inputs for the jobs, which include the Povray scene description for the Bonsai scene

   * *local-pre-job* - It is a script which DAGMan runs before each job. It is mostly a placeholder in the Povray example.

   * *local-post-job* - It is a script which DAGMan runs after each job completes. This is a very important part of detecting job failures. Failures should be expected in any distributed system, so it is important to detect and handle failures accordingly. The local-post-job script checks the job output to determine if the job was successful or not, and if a failure is detected, the script exists with exit code of 1, which signals to the DAGMan that the job should be resubmitted.

   * *remote-povray-wrapper* - This is the actual job that runs on the remote side. In most cases you will have to wrap your executable in a job wrapper to do the staging in/out, work dir handling (moving to $OSG_WN_TMP for example for local disk I/O) and to do some extra error detection.

   * *runs* - This is a directory in which is used to old each "run". When you start a new run, a timestamped directory will be created in here to hold logs and outputs.

   * *submit* - This is the submit script. It creates the Condor jobs based on inputs/parameters, and a DAGMan description. Then the run is submitted to Condor for execution.


Let's start a run with the default parameters:

<pre class="screen">
[train01@gs-mm ~]$ <b>./submit</b>
Run id is 2009-10-21_202324
Generating job 1 X: 1 {1:200}, Y: 1 {1:150}
....
Generating job 16 X: 4 {601:800}, Y: 4 {451:600}

Checking all your submit files for log file names.
This might take a while...
Done.
-----------------------------------------------------------------------
File for submitting this DAG to Condor           : master.dag.condor.sub
Log of DAGMan debugging messages                 : master.dag.dagman.out
Log of Condor library output                     : master.dag.lib.out
Log of Condor library error messages             : master.dag.lib.err
Log of the life of condor_dagman itself          : master.dag.dagman.log

Condor Log file for all jobs of this DAG         : runs/2009-10-20_135040/alljobs.log
Submitting job(s).
Logging submit event(s).
1 job(s) submitted to cluster 849162.
-----------------------------------------------------------------------

</pre>

What was submitted was one DAGMan which will manager 16 tile jobs.


---++ Checking on the Jobs

We can check on jobs in the queue using the *condor_grid_overview* command:

<pre class="screen">
[train01@gs-mm ~]$ <b> condor_grid_overview</b>
ID         DAG              Owner        Resource              Status      Command       TimeInState
========== ================ ============ ===================== =========== ============= ===========
703        (DAGMan)         train01                            Running     condor_dagman     0:03:44
704          |-job_1        train01      UCHC_CBG              Stage out   remote-povray-    0:00:30
705          |-job_2        train01      LIGO_UWM_NEMO         Pending     remote-povray-    0:03:30
706          |-job_3        train01      Clemson-ciTeam        Stage out   remote-povray-    0:02:46
708          |-job_5        train01      NYSGRID_CORNELL_NYS1  Stage out   remote-povray-    0:00:31
709          |-job_6        train01      Firefly               Stage out   remote-povray-    0:00:31
710          |-job_8        train01      Firefly               Stage out   remote-povray-    0:01:28
711          |-job_10       train01      NYSGRID-CCR-U2        Running     remote-povray-    0:00:31
712          |-job_12       train01      Nebraska              Pending     remote-povray-    0:03:26
713          |-job_7        train01      UCHC_CBG              Stage out   remote-povray-    0:00:31
714          |-job_9        train01      LIGO_UWM_NEMO         Pending     remote-povray-    0:03:21
715          |-job_11       train01      Clemson-ciTeam        Stage out   remote-povray-    0:00:31
717          |-job_14       train01      NYSGRID_CORNELL_NYS1  Stage out   remote-povray-    0:00:31
718          |-job_15       train01      Firefly               Stage out   remote-povray-    0:00:31
720          |-job_4        train01      Firefly               Submitting  remote-povray-    0:00:25
721          |-job_13       train01      Clemson-ciTeam        Submitting  remote-povray-    0:00:25

Site                      Total  Subm Stage  Pend  Run  Other  Rank Succes
========================= ===== ===== ===== ===== ===== ===== ===== ======
AGLT2                         0     0     0     0     0     0   500   100%
BNL-ATLAS                     0     0     0     0     0     0   500   100%
BNL-ATLAS                     0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
CIT_CMS_T2                    0     0     0     0     0     0   500   100%
Clemson-ciTeam                3     1     2     0     0     0   947   100%
Duke                          0     0     0     0     0     0   500   100%
Firefly                       2     0     2     0     0     0   200   100%
Firefly                       2     1     1     0     0     0   950   100%
LIGO_UWM_NEMO                 2     0     0     2     0     0   200   100%
NYSGRID-CCR-U2                1     0     0     0     1     0   168    84%
NYSGRID_CORNELL_NYS1          2     0     2     0     0     0   200   100%
Nebraska                      1     0     0     1     0     0   200   100%
RENCI-Engagement              0     0     0     0     0     0     0     0%
SBGrid-Harvard-East           0     0     0     0     0     0   500   100%
SPRACE                        0     0     0     0     0     0   500   100%
UCHC_CBG                      2     0     2     0     0     0   200   100%
UFlorida-HPC                  0     0     0     0     0     0   200   100%
UJ-OSG                        0     0     0     0     0     0   500   100%
WQCG-Harvard-OSG              0     0     0     0     0     0   500   100%

16 jobs; 0 matching, 3 pending remotely, 2 running, 0 held, 11 other
</pre>

The first section of the output lists jobs. The second section lists compute sites. Interesting notes:

   * *Where does the site information come from?* - The site information comes from [[https://twiki.grid.iu.edu/bin/view/Trash/Trash/Trash/Trash/ResourceSelection/WebHome][ReSS]] which is one of the site information feeds in OSG. To see what the full Condor ClassAd for a site looks like, try this command: *condor_status -l RENCI-Trash/Engagement_belhaven-1.renci.org*  Information is advertised directly from the sites, and is less than 5 minutes old. Note that these are the set of sites which accepts OSGEDU VO jobs. The total number of sites on OSG is close to 100.

   * *Multiple Sites with the same name* - Some sites have multiple job gateways to spread the load out. The site entries in *condor_grid_overview* are one entry per gatekeeper.

   * *Rank* - Rank is used by Condor during the match making step. If a job matches multiple sites, the one with the highest rank will win. OSGMM continuously assigns and updates rank between 0 and 1000 depending on how well the sites is doing (running jobs, job success rates, etc). Ranks below 300 means that no more jobs will be sent to the site. If you keep on running *condor_grid_overview* you will see the rank column change.

   * *Why do the jobs not go to certain sites?* - The jobs depend on Povray being installed at the sites, and have that expressed as a requirement in the Condor job descriptions. Povray has not been built on all sites, and the jobs will not match those sites. 

   * *Jobs timing out* - the jobs are set up to time out after 10 minutes in being stuck pending. The result is that if a site is busy and not running the job, it will move to another site. Held state means that the job is being removed from the site, and will be resubmitted somewhere else.


---++ Run Directory

Outputs will appear in runs/{timestamp}, where the timestamp will be different for each submit. For example:

<pre class="screen">
[train01@gs-mm ~]$ <b>cd runs/2009-10-21_202324/</b>
[train01@gs-mm 2009-10-21_202324]$ <b>ls</b>
10.condor          1.outputs.tar.gz  9.condor
10.outputs.tar.gz  2.condor          9.outputs.tar.gz
11.condor          2.outputs.tar.gz  alljobs.log
11.outputs.tar.gz  3.condor          jobkey.txt
12.condor          3.outputs.tar.gz  logs
12.outputs.tar.gz  4.condor          master.dag
13.condor          4.outputs.tar.gz  master.dag.condor.sub
13.outputs.tar.gz  5.condor          master.dag.dagman.log
14.condor          5.outputs.tar.gz  master.dag.dagman.out
14.outputs.tar.gz  6.condor          master.dag.lib.err
15.condor          6.outputs.tar.gz  master.dag.lib.out
15.outputs.tar.gz  7.condor          outputs
16.condor          7.outputs.tar.gz  parameters.txt
16.outputs.tar.gz  8.condor          tiles-combine.log
1.condor           8.outputs.tar.gz
</pre>

In here you will find:

   * *DAGMan description and logs* - master.dag describes what jobs are part of the DAG.

   * *Condor* - The Condor submit files are the NN.condor files. They look complicated, but once you have this set up, you can submit and let the system handle job failures. Also note the "requirements" string which states that we need Povray.

   * *logs* - Contains outputs from the jobs. Also note alljobs.log where all the jobs log to the same file - this is so that DAGMan only have to check one file to know what is happening with the jobs belonging to the DAG.

   * *outputs* - This is where the povray outputs go, which we then tile together to the final image.


For convenience in this demo, the post script copies the final image to the public web space. So, as your jobs are running (at least one has to be completed), point a web browser to:

[[http://gs-mm.uchicago.edu/~train01/output.png][http://gs-mm.uchicago.edu/~train01/output.png]] (change train01 to the username of your demo account)

During the run you should see a partially tiled image. Example:

<img src="%ATTACHURLPATH%/output-tiles.png" alt="output-tiles.png" width='400' height='300' />


If the image is complete, you can start another run by running "./submit" again. You can also change number of tiles by changing *CHUNKSIZE_X* and *CHUNKSIZE_Y* (try 400 and 300) on the top of the *submit* script. But don't make them to small. There is an overhead (staging, scheduling at multiple levels, ...) running jobs on the grid, so making the jobs to short will just make the overall runtime longer. For production jobs, we usually try to create jobs that are 4-24 hours long, which sometimes mean grouping many task into jobs.

---++ More information

OSGMM:
http://osgmm.sourceforge.net/

Povray:
http://en.wikipedia.org/wiki/POV-Ray

Bonsai Povray scene:
http://www.zazzle.com/bonsais_poster-228003235318786172

%BR%
%COMPLETE1% %BR%
%RESPONSIBLE% Main.MatsRynge - 16 Oct 2009 %BR%
%REVIEW%


%META:FILEATTACHMENT{name="rendered.png" attachment="rendered.png" attr="" comment="" date="1255974825" path="rendered.png" size="201944" stream="rendered.png" tmpFilename="/usr/tmp/CGItemp18620" user="MatsRynge" version="1"}%
%META:FILEATTACHMENT{name="rendered_split_lines.png" attachment="rendered_split_lines.png" attr="" comment="" date="1255974838" path="rendered_split_lines.png" size="178227" stream="rendered_split_lines.png" tmpFilename="/usr/tmp/CGItemp18648" user="MatsRynge" version="1"}%
%META:FILEATTACHMENT{name="output-tiles.png" attachment="output-tiles.png" attr="" comment="" date="1256178047" path="output-tiles.png" size="55109" stream="output-tiles.png" tmpFilename="/usr/tmp/CGItemp18466" user="MatsRynge" version="1"}%
