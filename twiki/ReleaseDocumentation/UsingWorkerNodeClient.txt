%META:TOPICINFO{author="KyleGross" date="1481047991" format="1.1" version="1.30"}%
%META:TOPICPARENT{name="WebHome"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%DOC_STATUS_TABLE%
%TOC%

---++ Introduction

The Worker Node Client is a collection of useful software components that is guaranteed to be on every OSG worker node. In addition, a job running on a worker node can access a handful of environment variables that your job can use to locate the Worker Node Client software and other handy resources.

This page describes how to initialize the environment of your job to correctly access the execution and data areas from the worker node.

The OSG provides no scientific software dependencies or software build tools on the worker node; you are expected to bring along all application-level dependencies yourself. Sites are not required to provide any specific tools (gcc, svn, lapack, blas, etc.) beyond the ones in the OSG worker node client. 


---++ Setting Up Environment

All OSG sites should be configured so that the =$OSG_GRID= environment variable is already defined when your job starts running. The first command that a job should execute after it is started in a batch slot at an OSG site is:

<pre class="screen">
$ <userinput>. $OSG_GRID/setup.sh</userinput>
</pre>

=setup.sh= sets several environment variables and configures the worker node client tools.
The following is output from a test run of a job with the scripts configured correctly:

%TWISTY{
mode="div"
showlink="Show..."
hidelink="Hide"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
<pre class="screen">
<userinput>$ globus-job-run fgitb-gk/jobmanager-condor /usr/bin/printenv</userinput>
[bbockelm@t3-sl5 ~]$ globus-job-run red.unl.edu:/jobmanager-condor /usr/bin/printenv
_CONDOR_ANCESTOR_10629=11149:1288983808:3570854656
GRID3_DATA_DIR=/opt/osg/data
GRID3_TMP_WN_DIR=/scratch
OSG_GLEXEC_LOCATION=/usr/sbin/glexec
CHANGED_X509=/grid_home/uscmsPool018/.globus/job/red.unl.edu/2556.1288983706/x509_up
GLOBUS_LOCATION=/opt/osg/osg-wn-source/globus
TMPDIR=/scratch/condor/var/lib/condor/execute/dir_10629
_CONDOR_SCRATCH_DIR=/scratch/condor/var/lib/condor/execute/dir_10629
OLDPWD=/scratch/condor/var/lib/condor/execute/dir_10629
OSG_WN_TMP=/scratch/condor/var/lib/condor/execute/dir_10629
OSG_SQUID_LOCATION=red-squid1.unl.edu:3128
OSG_JOB_CONTACT=red.unl.edu/jobmanager-condor
OSG_SITE_WRITE=UNAVAILABLE
TEMP=/scratch/condor/var/lib/condor/execute/dir_10629
LD_LIBRARY_PATH=/opt/osg/osg-1.2/subversion/lib:/opt/osg/osg-1.2/apache/lib:/opt/osg/osg-1.2/MonaLisa/Service/VDTFarm/pgsql/lib:/opt/osg/osg-1.2/prima/lib:/opt/osg/osg-1.2/curl/lib:/opt/osg/osg-1.2/glite/lib64:/opt/osg/osg-1.2/glite/lib:/opt/osg/osg-1.2/mysql5/lib/mysql:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386/server:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386/client:/opt/osg/osg-1.2/berkeley-db/lib:/opt/osg/osg-1.2/expat/lib:/opt/osg/osg-1.2/globus/lib:/opt/osg/osg-1.2/subversion/lib:/opt/osg/osg-1.2/apache/lib:/opt/osg/osg-1.2/MonaLisa/Service/VDTFarm/pgsql/lib:/opt/osg/osg-1.2/prima/lib:/opt/osg/osg-1.2/curl/lib:/opt/osg/osg-1.2/glite/lib64:/opt/osg/osg-1.2/glite/lib:/opt/osg/osg-1.2/mysql5/lib/mysql:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386/server:/opt/osg/osg-1.2/jdk1.6/jre/lib/i386/client:/opt/osg/osg-1.2/berkeley-db/lib:/opt/osg/osg-1.2/expat/lib:/opt/osg/osg-1.2/subversion/lib:/opt/osg/osg-1.2/apache/lib:/opt/osg/osg-1.2/MonaLisa/Service/VDTFarm/pgsql/lib:/opt/osg/osg-1.2/glite/lib64:/opt/osg/osg-1.2/glite/lib:/opt/osg/osg-1.2/prima/lib:/opt/osg/osg-1.2/mysql5/lib/mysql:/opt/osg/osg-1.2/berkeley-db/lib:/opt/osg/osg-1.2/expat/lib::
OSG_LOCATION=/opt/osg/osg-1.2
OSG_SITE_READ=UNAVAILABLE
_CONDOR_ANCESTOR_4198=4208:1284756786:1513536384
GLOBUS_GRAM_MYJOB_CONTACT=URLx-nexus://red.unl.edu:60265/
MY_INITIAL_DIR=$_CONDOR_SCRATCH_DIR
OSG_DATA=/opt/osg/data
PWD=/scratch/condor/var/lib/condor/execute/dir_10629
OSG_APP=/opt/osg/app
_CONDOR_WRAPPER_ERROR_FILE=/scratch/condor/var/lib/condor/execute/dir_10629/.job_wrapper_failure
GRID3_TMP_DIR=/opt/osg/data
_CONDOR_SLOT=7
OSG_GRID=/opt/osg/osg-wn-source
OSG_HOSTNAME=red.unl.edu
GRID3_APP_DIR=/opt/osg/app
GRID3_SITE_NAME=Nebraska
SHLVL=0
HOME=/grid_home/uscmsPool018
_CONDOR_MACHINE_AD=/scratch/condor/var/lib/condor/execute/dir_10629/.machine.ad
OSG_STORAGE_ELEMENT=True
X509_USER_PROXY=/scratch/condor/var/lib/condor/execute/dir_10629/x509_up
OSG_DEFAULT_SE=srm.unl.edu
LOGNAME=uscmsPool018
TMP=/scratch/condor/var/lib/condor/execute/dir_10629
OSG_SITE_NAME=Nebraska
_CONDOR_ANCESTOR_4208=10629:1288983806:3772562235
_CONDOR_JOB_AD=/scratch/condor/var/lib/condor/execute/dir_10629/.job.ad
GLOBUS_GRAM_JOB_CONTACT=https://red.unl.edu:32943/2556/1288983706/
GRID3_BASE_DIR=/opt/osg/osg-1.2
</pre>
%ENDTWISTY%



---++ Common software available on worker nodes.

The OSG worker node client (called the =wn-client= package) contains the following software:

   %TWISTY{
mode="div"
showlink="Click to show software"
hidelink="Click to hide software"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%

   * The site's supported set of CA certificates (located in $X509_CERT_DIR after the environment is set up)
   * Basic VDT underlying infrastructure
   * Proxy management tools:
      * Create proxies: =voms-proxy-init= and =grid-proxy-init=
      * Show proxy info: =voms-proxy-info= and =grid-proxy-info=
      * Destroy the current proxy: =voms-proxy-destroy= and =grid-proxy-destroy=
   * Data transfer tools:
      * HTTP/plain FTP protocol tools:
         * =wget= and =curl=: standard tools for downloading files with HTTP and FTP
      * SRM clients
         * LCG SRM Client (=lcg-cp= and others)
         * LBNL SRM Client (=srm-copy= and others)
         * FNAL SRM Client (=srmcp= and others)
      * !GridFTP client
         * Globus !GridFTP client (=globus-url-copy=)
         * !UberFTP, another command-line client for !GridFTP; covers a wider variety of the !GridFTP protocol than just copying
      * Site-specific protocols
         * DCache client: =dccp=, a client specifically for sites running !dCache, about 5–10 of the 80 OSG sites
   * The Pegasus worker node software
   * !MyProxy client tools

Advanced users can read the Pacman source code to see what’s in the =wn-client= package for this release, OSG-%VERSION%:
   * http://software.grid.iu.edu/pacman/wn-client.pacman
%ENDTWISTY%

---++ Directories in the Worker Node Environment

The following table outlines the various important directories for the worker node environment. A job running on an OSG worker node can refer to each directory using the corresponding environment variable.

%EDITTABLE{ header="| *Environment Variable* | *Purpose of a directory* | *Notes* |" format="| text | textarea, 2x30 | textarea, 2x30 |" changerows="on" }%
| *Environment Variable* | *Purpose of a directory* | *Notes* |
| =$OSG_APP= | Location for users to install software. | This directory is used by VO's to install software that will be used when running on the cluster.  For example, a VO may install the BLAST executable here.  The VO would submit jobs that executed the blast executable.  Access to this area varies from site-to-site.  Most sites allow software installation only from the head node (jobmanager-fork), while others require you to access it via a special job description of VOMS role. |
| =$OSG_DATA= | Data files that are accessible via NFS from all batch slots, read-write. | %RED% Not all OSG sites have deployed this area %ENDCOLOR%  You can test for existance by comparing the environment variable =$OSG_DATA= to =UNAVAILABLE=.  If true, then the directory is not available. |
| =$OSG_GRID= | Contains the Worker Node Client software tools |  |
| =$OSG_WN_TMP= | Temporary storage area in which your job(s) run | Local to each batch slot.  Create a directory under this as your work area.  See NOTE below. |

%NOTE% Be careful with using =$OSG_WN_TMP=, this  directory might be shared with other VOs.  We recommend the following code (suppose =gpn= is your VO's name):

<pre class="screen">
mkdir -p $OSG_WN_TMP/gpn
export mydir=`mktemp -d -t gpn`
cd $mydir
# Run the rest of your application
rm -rf $mydir
</pre>

A significant number of sites use the batch system to make an independent directory for each user job, and change =$OSG_WN_TMP= on the fly to point to this directory.

There is no way to know in advance how much scratch disk space any given worker node has available, as OSG information systems don't advertise this.  Most of the times, it is shared among a number of job slots.

<!--
---++ Input and Output Files Specified Via Condor-G

Condor-G has the following attributes related to file transfers and your jobs:

| =executable= | one file |
| =output= | one file - stdout | 
| =error= | one file - stderr |
| =transfer_input_files= | comma separated list of files |
| =transfer_output_files= | comma separated list of files |

Do not use =transfer_input_files= or =transfer_output_files= to transfer more than a few megabytes: These files are transfered via the CE headnode and can cause serious loads, which can bring down the cluster.  Space on the headnode also tends to be limited, especially because some sites place a very small quota on your home directory. To transfer large files, we recommend pre-staging as covered by [[Storage/WebHome#Storage_for_the_End_User][the OSG-Storage documentation]].

-->

<!--
---++ Finding your way around

%NOTE% A definition of the concepts used in this document can be found in [[LocalStorageConfiguration][Local Storage Configuration]], including a section describing minimal requirements and some sample configurations.

Here we describe how to find the various storage locations. We start by describing how to find things after your job starts, and then complete the discussion by describing what you can determine from the outside, before you submit a job to the site. We deliberately do not describe what these various storage implementations are. That is done in the [[LocalStorageConfiguration][Local Storage Configuration]] document.




<br/>

See the [[ComputeElementInstall][CE Install Guide]] and [[LocalStorageConfiguration][Local Storage Configuration]] for more information. 

---+++ !!Getting information about CE storage before you submit a job

You can retrieve the storage information for some sites by querying the OSG information services. We do not recommend using this method — instead, have the job discover the correct directory through its environment. This will not work reliably at all sites.

We will illustrate this below using !ReSS.

%TWISTY{
mode="div"
showlink="Show unrecommended query method..."
hidelink="Hide queries"
showimgleft="%ICONURLPATH{toggleopen-small}%"
hideimgleft="%ICONURLPATH{toggleclose-small}%"
}%
The following query looks for storage directories for sites that support CMS (change "VO:cms" with the name of your VO):
<pre class="screen">
<userinput>condor_status -pool osg-ress-1.fnal.gov -const 'stringListIMember("VO:cms", GlueCEAccessControlBaseRule)' -format 'CE: %s ' GlueCEUniqueID -format 'OSG_WN_SCRATCH: %s ' GlueSubClusterWNTmpDir -format 'OSG_DATA: %s ' GlueCEInfoDataDir -format 'OSG_APP: %s\n' GlueCEInfoApplicationDir | sort | uniq</userinput>
</pre>
The reason the query is so long is due to the extensive format strings.  The above query shows the scratch, app, and data directories - one line per CE.  Here's a few lines of output:
<pre class="screen">
CE: antaeus.hpcc.ttu.edu:2119/jobmanager-sge-cms Scratch: /state/partition1 OSG_DATA: /lustre/hep/osg OSG_APP: /lustre/home/antaeus/apps
CE: antaeus.hpcc.ttu.edu:2119/jobmanager-sge-serial Scratch: /state/partition1 OSG_DATA: /lustre/hep/osg OSG_APP: /lustre/home/antaeus/apps
CE: belhaven-1.renci.org:2119/jobmanager-condor-default Scratch: /condor/osg_wn_tmp OSG_DATA: /nfs/osg-data OSG_APP: /nfs/osg-app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmshi Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmsprod Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_cmsuser Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: ce01.cmsaf.mit.edu:2119/jobmanager-condor-group_monitor Scratch: /osg/tmp OSG_DATA: /osg/data OSG_APP: /osg/app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_monitor Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_priority Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_production Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper2.ultralight.org:2119/jobmanager-condor-cms_user Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_monitor Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_priority Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_production Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cit-gatekeeper.ultralight.org:2119/jobmanager-condor-cms_user Scratch: /wntmp OSG_DATA: /raid2/osg-data OSG_APP: /raid1/osg-app
CE: cms-0.mps.ohio-state.edu:2119/jobmanager-condor-default Scratch: /hadoop/tmp OSG_DATA: /data/se/osg OSG_APP: /sharesoft/osg/app
</pre>

%ENDTWISTY%

%BR%

---+ Comments

| PM2RPM_TASK = WNCLIENT | Main.RobertEngel | 28 Aug 2011 - 05:49 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = BrianBockelman

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = User

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = Scientist

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Training
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = DerekWeitzel
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = DerekWeitzel
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %YES%
############################################################################################################
-->