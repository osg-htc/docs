%META:TOPICINFO{author="JamesWeichel" date="1317234349" format="1.1" version="1.18"}%
%META:TOPICPARENT{name="Trash.ReleaseDocumentationSiteAdminsWorkshopAug09Sessions"}%
---+!! *<nop>%SPACEOUT{ "%TOPIC%" }%*
%TOC%


---++Introduction
This is a tutorial to demonstrate how to install and setup a basic CE installation.  It will guide users through a basic CE installation step by step.  At the completion of this guide, you should have a simple CE available.

---++Requirements
You'll need a server with the following:
   * Using a rhel 4/5 based distribution or debian 4/5
   * root access
   * ~5 GB of free space
   * internet access 
   * Server, http, and rsv certificates

You'll also need the following:
   * Personal grid certificate

---+++ Preliminaries
---++++ Installing pacman
   * Download pacman from bu site
<pre class="screen">
cd /opt
wget http://atlas.bu.edu/~youssef/pacman/sample_cache/tarballs/pacman-latest.tar.gz
</pre>
   *  Untar and source pacman setup
<pre class="screen">
tar xzf pacman-latest.tar.gz 
ln -s pacman-3.29 pacman
cd pacman
source setup.sh
</pre>
---++++ Creating directories
   * Create osg directory
<pre class="screen">
mkdir /opt/osg-1.2
ln -s osg-1.2 /opt/osg
</pre>
   * Create data directory
<pre class="screen">
mkdir /opt/data
</pre>
   * Create app directory with correct permissions
<pre class="screen">
mkdir -p /opt/app/etc
chmod 1777 /opt/app/etc
</pre>
   * Create osg wn client directory
<pre class="screen">
mkdir /opt/wn-1.2
</pre>
   * Create grid security directory
<pre class="screen">
mkdir /etc/grid-security
mkdir /etc/grid-security/http
</pre>
   * Copy http cert and key to =/etc/grid-security/http=
   * Copy host cert and key to =/etc/grid-security=
   * Copy rsv cert and key to =/etc/grid-security=

---+++ Installing worker node software
   * Install WN stack
<pre class="screen">
cd /opt/wn-1.2/
pacman -allow trust-all-caches -get http://software.grid.iu.edu/osg-1.2:wn-client
</pre>

---+++ Installing CE software
   * Install CE stack
<pre class="screen">
cd /opt/osg-1.2/
pacman -allow trust-all-caches -get http://software.grid.iu.edu/osg-1.2:ce
</pre>
You should get the following output:
<pre class="screen">
Beginning VDT prerequisite checking script vdt-common/vdt-prereq-check...       

All prerequisite checks are satisfied.
                                                          


========== IMPORTANT ==========
Most of the software installed by the VDT *will not work* until you install
certificates.  To complete your CA certificate installation, see the notes
in the post-install/README file.



                                                                              
Pacman Installation of OSG-1.2.0 Complete
</pre>
   * Install managed fork
<pre class="screen">
pacman -allow trust-all-caches -get http://software.grid.iu.edu/osg-1.2:ManagedFork
</pre>
%NOTE% This assumes that you are using Condor as your batch manager.  If this isn't the case, you should install the appropriate jobmanager package (e.g. =Globus-PBS-Setup=, =Globus-SGE-Setup=, etc.)
   * Install jobmanager setup package
<pre class="screen">
pacman -allow trust-all-caches -get http://software.grid.iu.edu/osg-1.2:Globus-Condor-Setup
</pre>

---+++ Configure CE
   * Run post-install script
<pre class="screen">
source setup.sh 
vdt-post-install 
</pre>
You should get the following output:
<pre class="screen">
Starting...
Configuring PRIMA... Done.
Configuring EDG-Make-Gridmap... Done.
Configuring PRIMA-GT4... Done.
Completed all configuration.
</pre>
   * Setup CA certificates
<pre class="screen">
vdt-ca-manage setupca --location local --url osg
</pre>
You should get the following output:
<pre class="screen">
Setting CA Certificates for VDT installation at '/opt/osg-1.2'

Setup completed successfully.
</pre>
---++++ Modify config.ini
%NOTE% This assumes that you are using Condor as your batch manager.  If this isn't the case, you should fill in the appropriate section for your job manager, e.g. =PBS=, =SGE=, etc, instead of filling in the =Condor= section.  In addition, this job manager should also be given in the =batch= setting in =GIP= section
   * Edit =osg/etc/config.ini=
   * In =[Default]= section
      * Set =localhost= to the correct dns name for your CE (e.g. localhost = your.domain)
      * Set =admin_email= to your email address
   * In the =[Site Information]= section
      * Set =group= to OSG
      * Set =resource= to your site's name
      * Set =resource_group= to match the resource group that you used to register your site
      * =sponsor= For this workshop, use 'osg:100'
      * =site_policy= A url to your site's usage policy. Example: http://www.mwt2.org/policy.html
      * =contact=, set this to your email address
      * =city=
      * =country=
      * =latitude= You can set this to 0 if you do not know your lattitude
      * =longitude= Like latitude
   * In the =[Condor]= section
      * =enabled= , change the =%(unavailable)s= to =%(enable)s=
      * =condor_location=, set this to =/opt/osg-1.2/condor=
   * In the =[ManagedFork]= section
      * =enabled= , change the =%(unavailable)s= to =%(enable)s=
   * In the =[Misc Services]= section
      * =use_cert_updater= , change the =%(unavailable)s= to =%(enable)s=
      * =gums_host=, change the =%(unavailable)s= to the host name for your gums host
      * =authorization_method=, change the =%(unavailable)s= to prima
   * In the =[Storage]= section
      * =grid_dir= , change the =%(unavailable)s= to =/opt/wn-client=
      * =app_dir= , change the =%(unavailable)s= to =/opt/app=
      * =data_dir= , change the =%(unavailable)s= to =/opt/data=
      * =worker_node_temp= , change the =%(unavailable)s= to =/tmp=
      * =site_read= , change the =%(unavailable)s= to =/opt/data=
      * =site_write= , change the =%(unavailable)s= to =/opt/data=
   * In the  =[GIP]= section
      * Remove the =%(unavailable)s= and replace it with the correct values for the variables listed below
      * =batch= , change the =%(unavailable)s= to =condor=
   * In the  =[Subcluster CHANGEME]= section
      * Change name to =[Subcluster Main]=
      * =name=, change to =Main=
      * =node_count= , change the =NUMBER_OF_NODE= to the number of worker nodes your cluster has. For this tutorial, enter =1=
      * =ram_mb= , change the =MB_OF_RAM= to the ram that your cluster has in MB (hint: =cat /proc/meminfo=)
      * =cpu_module= , change the =CPU_MODEL_FROM_/proc/cpuinfo= to the model of your  cluster's cpus (hint: =cat /proc/cpuinfo=)
      * =cpu_vendor= , change the =VENDOR_AMD_OR_INTEL= to the vendor of your  cluster's cpus (e.g. =Intel=, =AMD=)
      * =cpu_speed_mhz= , change the =CLOCK_SPEED_MHZ= to the clock speed of your  cluster's cpus in MHz. For example, a 2.83GHz cpu runs at 2830 MHz.
      * =cpu_platform= , change the =x86_64_OR_i686= to either =i686= or =x86_64= depending on whether you have a 32bit or 64bit cpu
      * =cpus_per_node=, change =#_PHYSICAL_CHIPS_PER_NODE= to number of chips per node (e.g. =1=)
      * =cores_per_node=, change =#_CORES_PER_NODE= to number of cores total in the node (e.g. =8= for a dual socket, quad core node)
   * In the  =[SE CHANGEME]= section
      * Change enabled to =False=
   * In the =[RSV]= section
      * =enabled= , change the =%(disable)s= to =%(enable)s=
      * =rsv_user= , change the =%(unavailable)s= to name of the account that has been created for rsv ( rsv_user)
      * =gratia_probes= , change the =%(unavailable)s= to =metric, condor=
      * =enable_ce_probes= , change the =%(disable)s= to =%(enable)s=
      * =ce_hosts= , change the =%(unavailable)s= to =%(localhost)s=
      * =enable_gridftp_probes= , change the =%(disable)s= to =%(enable)s=
      * =gridftp_hosts= , change the =%(unavailable)s= to =%(localhost)s=
      * =use_service_cert= , change the =%(disable)s= to =%(enable)s=
      * =enable_gratia= , change the =%(disable)s= to =%(enable)s=
      * =setup_for_apache= , change the =%(disable)s= to =%(enable)s=

---++++ Run configure-osg
   * Verify configuration
<pre class="screen">
configure-osg -v
</pre>
You should get the following output:
<pre class="screen">
Using /opt/osg-1.2/osg/etc/config.ini for configuration information
Configuration verified successfully
</pre>
   * Configure system
<pre class="screen">
configure-osg -c
</pre>
You should get the following output:
<pre class="screen">
Using /opt/osg-1.2/osg/etc/config.ini for configuration information
running 'vdt-register-service --name fetch-crl --enable'... ok
Running /opt/osg-1.2/fetch-crl/share/doc/fetch-crl-2.6.6/fetch-crl.cron, this process make take some time to fetch all the crl updates
running 'vdt-register-service --name vdt-update-certs --enable'... ok
running 'vdt-register-service --name edg-mkgridmap --enable'... ok
running 'vdt-register-service --name gums-host-cron --disable'... ok
PRIMA for GT4 web services has been disabled
You will now be using a grid-mapfile for authorization.
Modifications to the /etc/sudoers file are still required.
You will need to restart the /etc/init.d/globus-ws container
to effect the changes.
Running /opt/osg-1.2/edg/sbin/edg-mkgridmap, this process make take some time to query vo and gums servers
The following consumer subscription has been installed:
	HOST:    https://osg-ress-4.fnal.gov:8443/ig/services/CEInfoCollector
	TOPIC:   OSG_CE
	DIALECT: OLD_CLASSAD

running 'vdt-register-service --name tomcat-55 --enable'... ok
The following consumer subscription has been installed:
	HOST:    http://is-itb.grid.iu.edu:14001
	TOPIC:   OSG_CE
	DIALECT: RAW

running 'vdt-register-service --name tomcat-55 --enable'... ok
running 'vdt-register-service --name mysql5 --enable'... ok
running 'vdt-register-service --name gsiftp --enable'... ok
Not defined: PER_JOB_HISTORY_DIR
running 'vdt-register-service --name gratia-condor --enable'... ok
INFO: Attempting to install latest RSV package from release repository. 
INFO: Attempting to install RSV consumers. 
INFO: Attempting to install RSV probes on appropriate URI(s). 
INFO: Creating .sub files for RSV probes of type OSG-Local-Monitor
	 for URI: uct3-edge5.uchicago.edu (host: uct3-edge5.uchicago.edu)
INFO: Creating .sub files for RSV probes of type OSG-CE
	 for URI: uct3-edge5.uchicago.edu (host: uct3-edge5.uchicago.edu)
INFO: Re-using metrics config file for uct3-edge5.uchicago.edu
 /opt/osg-1.2/osg-rsv/config/uct3-edge5.uchicago.edu_metrics.conf
 Existing settings like on/off and metric intervals will be re-used.
 Any new metrics found in probe set will be added with their default settings.

INFO: Creating .sub files for RSV probes of type OSG-GridFTP
	 for URI: uct3-edge5.uchicago.edu (host: uct3-edge5.uchicago.edu)
INFO: Re-using metrics config file for uct3-edge5.uchicago.edu
 /opt/osg-1.2/osg-rsv/config/uct3-edge5.uchicago.edu_metrics.conf
 Existing settings like on/off and metric intervals will be re-used.
 Any new metrics found in probe set will be added with their default settings.

running 'vdt-register-service --name condor-cron --enable'... ok
running 'vdt-register-service --name condor --enable'... ok
running 'vdt-register-service --name vdt-rotate-logs --enable'... ok
running 'vdt-register-service --name apache --enable'... ok
running 'vdt-register-service --name globus-gatekeeper --enable'... ok
running 'vdt-register-service --name globus-ws --enable'... ok
Configure-osg completed successfully
</pre>
   * Add your grid certificate DN to =/etc/grid-security/gridmap= if it's not there. E.g.
<pre class="screen">
echo "/DC=org/DC=doegrids/OU=People/CN=Suchandra Thapa 757586" sthapa > /etc/grid-security/gridmap
</pre>

----+++ Turn on software
   * Run vdt-control
<pre class="screen">
vdt-control --on
</pre>
You should get the following output:
<pre class="screen">
enabling cron service fetch-crl... ok
enabling cron service vdt-rotate-logs... ok
enabling cron service vdt-update-certs... ok
skipping init service 'gris' -- marked as disabled
enabling inetd service globus-gatekeeper... ok
enabling inetd service gsiftp... ok
enabling init service mysql5... ok
enabling init service globus-ws... ok
skipping cron service 'gums-host-cron' -- marked as disabled
skipping init service 'MLD' -- marked as disabled
enabling init service condor-cron... ok
enabling init service apache... ok
enabling init service tomcat-55... ok
enabling init service condor... ok
enabling cron service gratia-condor... ok
enabling cron service edg-mkgridmap... ok
enabling init service osg-rsv... ok
</pre>

----+++ Verification
   * Run site_verify
<pre class="screen">
 cd /opt/osg-1.2/verify
./site_verify.pl
</pre>
You should get something similar to the following output:
<pre class="screen">
===============================================================================
Info: Site verification initiated at Wed Aug  5 20:21:00 2009 GMT.
===============================================================================
-------------------------------------------------------------------------------
-------- Begin uct3-edge5.uchicago.edu at Wed Aug  5 20:21:00 2009 GMT --------
-------------------------------------------------------------------------------
Checking prerequisites needed for testing: PASS
Checking for a valid proxy for sthapa@uct3-edge5.uchicago.edu: PASS
Checking if remote host is reachable: PASS
Checking for a running gatekeeper: YES; port 2119
Checking authentication: PASS
Checking 'Hello, World' application: PASS
Checking remote host uptime: PASS
   15:21:03 up 15 days,  2:50,  2 users,  load average: 0.53, 0.36, 0.18
Checking remote Internet network services list: PASS
Checking remote Internet servers database configuration: PASS
Checking for GLOBUS_LOCATION: /opt/osg-1.2/globus
Checking expiration date of remote host certificate: Jul  9 20:45:23 2010 GMT
Checking for gatekeeper configuration file: YES
  /opt/osg-1.2/globus/etc/globus-gatekeeper.conf
Checking users in grid-mapfile, if none must be using Prima: alice,cdf,cigi,compbiogrid,des,dosar,engage,fermilab,geant4,glow,gpn,grase,grow,i2u2,icecube,ilc,ligo,mis,nanohub,nebiogrid,nwicg,nysgrid,ops,osg,osgedu,samgrid,sbgrid,star,sthapa,usatlas1,uscms01
Checking for remote globus-sh-tools-vars.sh: YES
Checking configured grid services: PASS
  jobmanager,jobmanager-condor,jobmanager-fork,jobmanager-managedfork
Checking for OSG osg-attributes.conf: YES
Checking scheduler types associated with remote jobmanagers: PASS
  jobmanager is of type managedfork
  jobmanager-condor is of type condor
  jobmanager-fork is of type managedfork
  jobmanager-managedfork is of type managedfork
Checking for paths to binaries of remote schedulers: PASS
  Path to condor binaries is /opt/osg-1.2/condor/bin
  Path to managedfork binaries is .
Checking remote scheduler status: PASS
  condor : 1 jobs running, 0 jobs idle/pending
Checking if Globus is deployed from the VDT: YES; version 2.0.0p7
Checking for OSG version: NO
Checking for OSG grid3-user-vo-map.txt: YES
  ops users: ops
  cms users: uscms01
  i2u2 users: i2u2
  geant4 users: geant4
  atlas users: usatlas1,usatlas3,usatlas4
  grow users: grow
  osgedu users: osgedu
  nanohub users: nanohub
  alice users: alice
  icecube users: icecube
  gpn users: gpn
  nebiogrid users: nebiogrid
  cdf users: cdf
  nwicg users: nwicg
  osg users: osg
  engage users: engage
  star users: star
  cigi users: cigi
  dosar users: dosar
  grase users: grase
  sbgrid users: sbgrid
  jdem users: jdem
  ligo users: ligo
  glow users: glow
  nysgrid users: nysgrid
  fermilab users: fermilab
  ilc users: ilc
  dzero users: samgrid
  compbiogrid users: compbiogrid
  mis users: mis
  des users: des
Checking for OSG site name: UC_ITB_2
Checking for OSG $GRID3 definition: /opt/osg-1.2
Checking for OSG $OSG_GRID definition: /opt/wn-1.2
Checking for OSG $APP definition: /opt/share/app
Checking for OSG $DATA definition: /opt/share/data
Checking for OSG $TMP definition: /opt/share/data
Checking for OSG $WNTMP definition: /tmp
Checking for OSG $OSG_GRID existence: FAIL
Checking for OSG $APP existence: PASS
Checking for OSG $DATA existence: PASS
Checking for OSG $TMP existence: PASS
Checking for OSG $APP writability: PASS
Checking for OSG $DATA writability: PASS
Checking for OSG $TMP writability: PASS
Checking for OSG $APP available space: 8.833 GB
Checking for OSG $DATA available space: 8.833 GB
Checking for OSG $TMP available space: 8.833 GB
Checking for OSG additional site-specific variable definitions: YES
  MountPoints
    SAMPLE_LOCATION default /SAMPLE-path
    SAMPLE_SCRATCH devel /SAMPLE-path
Checking for OSG execution jobmanager(s): uct3-edge5.uchicago.edu/jobmanager-condor
Checking for OSG utility jobmanager(s): uct3-edge5.uchicago.edu/jobmanager
Checking for OSG sponsoring VO: osg:100
Checking for OSG policy expression: NONE
Checking for OSG setup.sh: YES
Checking for OSG $Monalisa_HOME definition: FAIL
Checking for MonALISA configuration: UNTESTED
Checking for a running MonALISA: UNTESTED
Checking for a running GANGLIA gmond daemon: PASS (pid 3212 ...)
  /usr/sbin/gmond
  name = "part_max_used"
  owner = "unspecified"
  url = "unspecified"
Checking for a running GANGLIA gmetad daemon: NO
  gmetad does not appear to be running
Checking for a running gsiftp server: YES; port 2811
Checking gsiftp (local client, local host -> remote host): PASS
Checking gsiftp (local client, remote host -> local host): PASS
Checking that no differences exist between gsiftp'd files: PASS
-------------------------------------------------------------------------------
--------- End uct3-edge5.uchicago.edu at Wed Aug  5 20:23:11 2009 GMT ---------
-------------------------------------------------------------------------------
===============================================================================
Info: Site verification completed at Wed Aug  5 20:23:11 2009 GMT.
</pre>
   * Check rsv status
      * Go to http://your.host:8443/rsv/

---++Presentation


---++References

   1 [[WebHome][Release Documentation]]

%BR%

<!--
   * Set USERSTYLEURL = https://twiki.grid.iu.edu/twiki/pub/ReleaseDocumentation/HandsOn/centerpageborder.css
-->

%STOPINCLUDE%
%BR%



---++ *Comments*
%COMMENT{type="tableappend"}%

%META:FILEATTACHMENT{name="config.ini" attachment="config.ini" attr="" comment="Sample config.ini" date="1249588460" path="config.ini" size="29366" stream="config.ini" tmpFilename="/usr/tmp/CGItemp22765" user="SuchandraThapa" version="1"}%
