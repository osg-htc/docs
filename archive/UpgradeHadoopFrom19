%DOC_STATUS_TABLE%
---+!! Upgrading Hadoop from 0.19

%TOC{depth="2"}%

---+ About This Document

%ICON{hand}% This document is intended for System Administrators that want to upgrade their Hadoop install to the new OSG hosted rpms.

%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="CommandLine"}%

---+ Applicable Versions

This document is intended for those upgrading from  %RED%Hadoop Version 0.19%ENDCOLOR% hosted by Caltech to  %RED%Hadoop Version 0.20%ENDCOLOR% hosted by the new OSG repositories.

---+ Upgrade Procedure

---++ Description and Warnings

Upgrading your file system is a dangerous activity.  We have tested the upgrade several times, and it has been tested by the community in general.  Hadoop allows you to run 0.20 indefinitely without deleting the 0.19 data (on the datanodes, the on-disk metadata is recreated in a separate directory, and the data itself is hardlinked over. Nothing is deleted.), and allows you to rollback the upgrade

However, no software is bug-free, and no upgrade is 100% safe.  Read the entire upgrade instructions first, then upgrade.  Make the appropriate plans - irreplaceable data should be backed up.  If you suspect something has gone wrong, ask for help via the GOC (https://ticket.grid.iu.edu/goc/open) or the community on osg-hadoop@opensciencegrid.org sooner rather than later.

%NOTE% Before continuing we highly recommend you review carefully the [[InstallHadoopSE][main Hadoop 0.20 installation document]].

This document is split into two major parts:
   1 Upgrading the Hadoop service from 0.19 to 0.20
   1 Upgrading the rest of the Hadoop SE services

%NOTE% Throughout this document it will be stated which node the relevant instructions apply to.  It can apply to one of the following:
   * *Namenode*
   * *Datanode*
   * *Secondary Namenode*
   * *GridFTP node*
   *  *SRM node*

However the first steps are to upgrade to the OSG repo and to stop non-hadoop services.

---++ Initializing the YUM Repository

%NOTE% This must be done on *all nodes*

%INCLUDE{"Documentation.InstallVDTRepo"}%

%NOTE% You need to disable the caltech yum repository  on *all nodes* before proceeding.  Edit =/etc/yum.repos.d/osg-hadoop.repo= and ensure the following is set for all sections:

<pre class="file">enabled=0</pre>

---++ Stop Running Services

---+++ Stopping !BeStMan2

On your *SRM Node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service bestman2 stop
</pre>

Stopping hadoop is covered in the next section.

---+ Upgrading Hadoop

%NOTE% For this section *all nodes* is shorthand for hadoop specific nodes only, i.e. *Namenode*, *Secondary Namenode* and all *Datanodes*

There are six parts to the upgrade:
   1 Pre-upgrade activities.  Perform data backups, take a few metadata snapshots, shut down the cluster.
   1 Installing the new software.
   1 Configuration procedure
   1 Starting the upgrade
   1 Verifying the install
   1 Committing the install.  Once you have hit this step, rolling back to the previous version is not possible.

---++ Pre-upgrade Activities

Before your upgrade, we advise to run backups, snapshot current system, and turn off HDFS.  Turning off HDFS at this point is mandatory, while the rest are advised.  Having snapshots handy will be useful to compare the pre- and post- systems, and can be performed according to your level of paranoia.

   1 Back up the data on the current system according to your site policy and level of paranoia.
   1 Run fsck command: <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop fsck / -files -blocks -locations > hdfs-old-fsck.log</pre>  Fix HDFS to the point there are no errors. The resulting file will contain complete block map of the file system.
   1 Run lsr command:  <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop dfs -lsr / > hdfs-old-lsr.log </pre>  The resulting file will contain complete namespace of the file system.
   1 Run a node report:  <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop dfsadmin -report > hdfs-old-report.log </pre>  The resulting file will contain a list of all nodes participating in the cluster.
   1 Shut down the cluster.  Run the following on *every node*:  <pre class="rootscreen">%UCL_PROMPT_ROOT% service hadoop stop</pre>  You may want to use the =ps= command to verify all HDFS-related java processes have exited.
   1 Sometimes the Namenode shuts down incompletely.  Start it once again, watch its log (to verify it merges its binary log) until it starts accepting connections, and then stop it again.
   1 On your Namenode make a backup copy of =${dfs.name.dir}/edits= and =${dfs.name.dir}/image/fsimage=, where =${dfs.name.dir}= is the appropriate value for your cluster (defaults to $HADOOP_DATADIR/dfs/name in 0.19).

---++ Installing the RPMs

You will need to do the following on *all nodes*:
 <pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install hadoop-0.20-osg
</pre>

%NOTE% Be sure to log out and then back into the *Namenode* after installation.  Environment variables left over from the 0.19 install will cause problems during the rest of the upgrade process.

---++ Configuration Procedure

Between 0.19 and 0.20, there were several minor configuration option name changes.  Refer to the upstream documentation to see if any of the site customizations you performed are deprecated.  Most people will not need to change anything. This section assumes you use the =hadoop-firstboot= script to configure Hadoop.  If you prefer to manually reconfigure, refer to the [[#Tips_for_Manually_Configuring_Ha][tips below]] in the Advanced Topics section. A few changes must be made on *all nodes* in =/etc/sysconfig/hadoop=.

First, the config file location has to be updated to the correct location:
<pre class="file">
HADOOP_CONF_DIR=/etc/hadoop-0.20/conf
</pre>

Hadoop now is run as user =hdfs=.  The *Namenode* also runs as =hdfs= user now and no longer as =root=.  Make sure to change this:

<pre class="file">
HADOOP_USER=hdfs
</pre>

Once you are satisfied that everything looks good in =/etc/sysconfig/hadoop=, reconfigure hadoop:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service hadoop-firstboot start
</pre>

---++ Starting the upgrade

   1 First log into the *Namenode* and change the ownerships on your data directories to user hdfs: <pre class="rootscreen">%UCL_PROMPT_ROOT% chown -R hdfs:hadoop %RED%$HADOOP_DATADIR%ENDCOLOR%</pre> Where $HADOOP_DATADIR is the path specified in =/etc/sysconfig/hadoop=.
   1 Issue the following command: <pre class="rootscreen">%UCL_PROMPT_ROOT% su hdfs -s /bin/sh -c "/usr/bin/hadoop-daemon start namenode -upgrade"</pre>  Follow in the logs in =/var/log/hadoop= to verify the namenode upgrade completes successfully.
   1 Start your datanodes.  On each datanode, perform: <pre class="rootscreen">%UCL_PROMPT_ROOT% service hadoop start</pre>  The datanodes will contact the namenode, which will request they start the upgrade.
   1 Follow the progress using the following command on the namenode: <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop dfsadmin -upgradeProgress status</pre> 

Once all datanodes have completed the upgrade process, the namenode can leave safemode.  At this point, HDFS has a full version of the 0.20 and 0.19 directories, but will be using the 0.20 for any operations.

The 0.19 directories are frozen in time.  Any changes made to the file system (including deletions!) will only happen on the *new* version of the directories.  So, if you write files into HDFS for two weeks then rollback to 0.19, you will lose any changes you performed post-upgrade.

---++ Verify the install

If you chose to do the snapshots prior to upgrading, do a snapshot of the new system:
   1 Run fsck command:<pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop fsck / -files -blocks -locations > hdfs-new-fsck.log</pre>
   1 Run lsr command: <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop dfs -lsr / > hdfs-new-lsr.log</pre>
   1 Run a node report: <pre class="rootscreen">%UCL_PROMPT_ROOT% hadoop dfsadmin -report > hdfs-new-report.log</pre>
Compare the before-and-after picture.  The output of =lsr= should be identical. =fsck= is very hard to compare between versions on large clusters, but you should verify the post-upgrade cluster has no broken files.  Make sure all nodes have come up in the node report.

Run the cluster under typical load conditions until you are satisfied that no files are missing, no files have been corrupted, and the performance of HDFS 0.20 has not regressed.

---++ Committing the install
You can take as long as desired on the verification step - several days or weeks may be possible, depending on your level of free space.  At some point, however, you need to either commit or rollback.

If you decide to commit to the new system, the backup metadata directories will be deleted.  *Once you commit, you cannot rollback to the previous version*.  Perform the following command:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% hadoop dfsadmin -finalizeUpgrade
</pre>
This will instruct the system to remove all traces of the 0.19 install.

If you decide to rollback to the previous version, issue the following command:
<pre class="rootscreen">
%UCL_PROMPT_ROOT% hadoop dfsadmin -rollback
</pre>
However, there will be a few more manual downgrade steps (downgrading RPMs, restoring configuration files) not documented here.  Please contact osg-hadoop@opensciencegrid.org for support in this case, as we envision rollbacks will be handled on a case-by-case basis.

---+ Upgrading Other Services

For each service upgraded, be sure to make note of any warning messages regarding backups of config files saved to =*.rpmsave= and =*.rpmnew= locations.

---++ Upgrade !GridFTP
On your *GridFTP node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install osg-gridftp-hdfs
</pre>

---++ Upgrade !BeStMan2

On your *SRM node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install bestman2-server
</pre>

---++ Upgrade Gratia Transfer Probe

On your *GridFTP node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum update gratia-probe-gridftp-transfer
</pre>

---++ Upgrade Hadoop Storage Probe

On your *Namenode* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum update gratia-probe-hadoop-storage
</pre>

---++ Upgrade xrootd

On your *xrood nodes* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% yum install xrootd-hdfs xrootd-cmstfc xrootd-lcmaps
</pre>

The =xrootd-cmstfc= RPM is optional for non-CMS sites.

---+ Configuring Other Services

---++ Update Config Files as Needed

Review any =*.rpmsave= and =*.rpmnew=  files reported in the updates above and edit as needed.  A few of these locations have been changed for the new rpms.  Please refer to the [[InstallHadoopSE][main Hadoop install doc]] for more details. The most relevant changes are listed here:

| Service | Old Location | New Location |
| !BeStMan2 |/opt/bestman2/conf/bestman2.rc | /etc/bestman2/conf/bestman2.rc |
| Gratia Transfer Probe | /opt/vdt/gratia/probe/gridftp-transfer/ProbeConfig | /etc/gratia/gridftp-transfer/ProbeConfig |
| Hadoop Storage Probe | /opt/vdt/gratia/probe/hadoop-storage/ProbeConfig | /etc/gratia/hadoop-storage/ProbeConfig |
| Hadoop Storage Probe | /opt/vdt/gratia/probe/hadoop-storage/storage.cfg | /etc/gratia/hadoop-storage/storage.cfg |

---++ Notable Changes for !GridFTP

!GridFTP no longer uses =prima-authz= for gums authentication but instead uses =lcmaps=.  Please see the [[InstallHadoopSE#Installing_GridFTP][GridFTP section]] of the main Hadoop install doc on how to configure.

---++ Notable Changes for !BeStMan2

The file layout has changed considerably to conform to FHS standards.  In particular note the change in location of the =bestman2.rc= file listed above; review and edit as needed.  See the [[InstallHadoopSE#Installing_BeStMan2][BeStMan2 section]] of the main Hadoop install doc for more details of the new layout.

---++ Notable Changes for Gratia Transfer Probe

Note the change in location of the =ProbeConfig= file listed above; review and edit as needed.  In addition, the Transfer Probe no longer requires manual creation of the =user-vo-map= file.  Instead =gums-host-cron= is used.  To ensure the file gets generated, run it first by hand:

<pre class="rootscreen">
%UCL_PROMPT_ROOT%  gums-host-cron
</pre>

=user-vo-map= should be created in the following location:

=/var/lib/osg/user-vo-map=

Ensure =ProbeConfig= is updated to point to the new location of this file in the =UserVOMapFile= field.

To have cron regularly update this file start the following service:

<pre class="rootscreen">
%UCL_PROMPT_ROOT%  service gums-client-cron start
</pre>

---++ Notable Changes for Hadoop Storage Probe

Note the config file location changes listed above; review and edit as needed.  Please ensure the following lines are correct in =storage.cfg=:

<pre class="file">
gratia_location = /etc/gratia
ProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig
</pre>

---+ Starting Other Services

Assuming you finished updating the config files needed you can start up the services.

---++ Starting !GridFTP

!GridFTP is no longer controlled by xinetd but is now controlled by a startup daemon.  To start it, on your *GridFTP Node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service globus-gridftp-server start
</pre>

---++ Starting !BeStMan2

On your *SRM Node* run:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% service bestman2 start
</pre>

---+ Advanced Topics

---++ Tips for Manually Configuring Hadoop 0.20

For those who prefer to configure Hadoop manually instead of using the =hadoop-firstboot= script, the following tips may be useful:
   1 The name of the site-customization file has changed from =hadoop-site.xml= to =core-site.xml=.  You'll need to rename this in =/etc/hadoop=.
   1 The directory layout of configuration files has changed.  The configuration files are in =/etc/hadoop=, but that's actually a symlink maintained by the RHEL =alternatives= system.  We highly encourage sites to take advantage of this and create their own "site HDFS configuration RPM" to keep maintenance tidy.  Support for doing this is currently provided by the mail list.  <br/>If creating RPMs is not for you, you can continue to maintain the configuration files in =/etc/hadoop=.

Issues to watch for:
   1 =hadoop-env.sh=:
      * Remove the manual building of =HADOOP_CLASSPATH=; this is now taken care of automatically.
      * Do not redefine =HADOOP_CONF_DIR= here, as this will be taken care of by the RPMs.

---+ Comments
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = DouglasStrain

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|General|Trash/Trash/Integration|Monitoring|Operations|Security|Storage|Trash/Tier3|User|VO)
   * Local DOC_AREA       = Storage

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (Developer|Documenter|Scientist|Student|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (HowTo|Installation|Knowledge|Navigation|Planning|Training|Troubleshooting)
   * Local DOC_TYPE       = Installation
  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %YES%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

 change to %YES% once the document is ready to be tested and back to %NO% if that is not the case
   * Local TEST_READY     = %YES%

 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = NehaSharma
 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


 DEAR DOCUMENT TESTER
 ====================

 Thank for testing this document! Please fill in your FirstLast name here:
   * Local TESTER         = NehaSharma
 Please define the test status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local TEST_PASSED    = %IN_PROGRESS%
############################################################################################################
-->
-- Main.JamesWeichel - 03 Feb 2010
