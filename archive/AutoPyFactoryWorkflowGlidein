*WARNING: THIS DOCUMENTATION IS WORK IN PROGRESS!!!*

<!-- some useful definitions  (need 3 white spaces before * to enable it)
   * Set VERSION = 2.4.5
-->

---+!! Glidein style workflows with !AutoPyFactory
<!--
%DOC_STATUS_TABLE%
-->
%TOC{depth="3"}%

---# About this Document

This document explains how to setup AutoPyFactory to work in a glidein style fashion


%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Trash/DocumentationTeam/DocConventions" section="CommandLine"}%

%INCLUDE{"Documentation/Release3/AutoPyFactory" section="Version"}%

---# Configuration

In the case of workflows using glideins, the payloads are condor jobs submitted to a pool, usually with no startd's associated. 
Then, when the factory notices those IDLE jobs in the condor pool, submits pilots (glideins in this case) to remote resources. These glideins will be configured to join the pool and allow those IDLE jobs to finally run. 
In this scenario, in order to allow the !APF factory to see the IDLE jobs in the condor pool, they need to be submitted with an specific classad:

<pre class="file">
+MATCH_APF_QUEUE=&lt;label&gt;
</pre>

This line will typically be included in the condor description file being used to submit the payload jobs to the condor pool. 
The particular string =+MATCH_APF_QUEUE= tells the !AutoPyFactory that those jobs are to be managed by the factory. 
The different values of the =label= will allow the factory to treat them differently. All payload jobs with the same =label= will be managed together. Payload jobs with different =label= will be managed separately. 

As it was explained [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryConfiguration#5_3_queues_conf][here]], and !APFQueue can be interpreted as the unique combination of a wms queue and a batch queue. in this particular type of workflow, the value of the =+MATCH_APF_QUEUE= classad represents the wms queue. 

Then, the batch queue, the system where the pilots will be submitted to, can be, for example, a remote Compute Element, or another condor pool. 

---# Example 1

Let's say some of the payload jobs have been submitted to the empty pool by the end user with classad:

<pre class="file">
+MATCH_APF_QUEUE=montecarlo
</pre>

This first type of jobs are meant to be run in a remote grid site. 

In this case, the =WMSStatus= plugin is =Condor=, as the condor pool is our WMS service in this case. 
As the glideins will be submitted to a remote grid site -for example with a !Globus GT5 gatekeeper, the =BatchSubmitPlugin= is =CondorGT5=. 
To track the status of previously submitted glideins, as they are being submitted with condor-g, the =BatchStatusPlugin= is =Condor=.

So the basic configuration for this workflow in the =queues.conf= file would include a section similar to this:

<pre class="file">
[MC_SITE1_CE]
wmsqueue = montecarlo
wmsstatusplugin = Condor
batchstatusplugin = Condor
batchsubmitplugin = CondorGT5
...
</pre>
 
For the rest of variables, see the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryReferenceManual#6_queues_conf][Reference Manual]]


---# Example 2

In this case, the end user payload jobs have classad:

<pre class="file">
+MATCH_APF_QUEUE=analysis
</pre>
 
This second type of jobs are meant to run on a local condor cluster. 
In that case, the value of the =CondorSubmitPlugin= is =CondorLocal=. It is called "local" even if the schedd for this pool is in a remote host. 
So the  =queues.conf= file would have a section like this:

<pre class="file">
[ANALYSIS_CONDORPOOL_XYZ]
wmsqueue = analysis
wmsstatusplugin = Condor
batchstatusplugin = Condor
batchsubmitplugin = CondorLocal
batchsubmit.condorlocal.submitargs = -remote &lt;remote_schedd&gt;
...
</pre>
 
For the rest of variables, see the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryReferenceManual#6_queues_conf][Reference Manual]]


