<!-- some useful definitions  (need 3 white spaces before * to enable it)
   * Set UCL_PROMPT_ROOT = [root@factory ~]$
   * Set VERSION = 2.4.5
-->

---+!! Questions and Answers related !AutoPyFactory
<!--
%DOC_STATUS_TABLE%
-->
%TOC{depth="3"}%

---# About this Document

This document is just a somehow free-format list of questions asked about AutoPyFactory, and some of the answers provided. 
The answer may be straight forward, or may be just some hints about where to look.

It does not include the complete list of questions, as this documented was started on August 10th, 2015. 
It is not a FAQ page, therefore.

%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="Header"}%
%INCLUDE{"Documentation/DocumentationTeam/DocConventions" section="CommandLine"}%

---# Applicable versions
This document mostly applies to the version of !AutoPyFactory %VERSION% and higher. Some answer may be valid for older versions too, but that is not guaranteed. 

---# Questions and answers

This section contains also reported tracebacks and other misbehavior, and their possible explanation.
They may mean there is a bug in the code, otherwise, no traceback should appear in the log files. But bugs happen. And, even if they are fixed in the next release, the problem persist for those instances running old versions. 

---## how do I add several condor attributes to the HTCondor submit file, if one of them includes commas?

When using field =condor_attributes= to add several lines to the submit file, they are separated by comma. 
If some of the lines also include comma, they need to be escaped with a backslash character.

<pre class="file">
    batchsubmit.condorlocal.condor_attributes = foo=bar, foo2=bar2\,bar3
</pre>


---## how do I interpret comments like "fixed in version X" at the same time that "This documentation applies to the latest version of APF: Y"?

When the versions X and Y are different, usually that means that a bug was found in the version Y -the one the current documentation is for-, 
and a fix for that bug will be released in a future version -X or higher-, but not yet. 
<br>
It can also mean that we forgot to update the message "This documentation applies to the latest version of APF... " ;) 

---## what do I do if I want to change the path of the log files?

Some explanation can be found [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryConfiguration#5_2_logrotation][here]] 

---## how can I submit to a remote HTCondor schedd?

It is not needed the APF host also be an HTCondor schedd. 
Remote submission is allowed. 
The queryargs -for the !BatchStatus Plugin- and the submitargs -for the Submit Plugin- can be used to pass any arbitrary options, for example to refer to a remote schedd.

<pre class="file">
    batchsubmitplugin = CondorLocal
    batchsubmit.condorlocal.submitargs = -remote &lt;remote_schedd&gt;:&lt;port&gt;
 
    batchstatusplugin = Condor
    batchstatus.condor.queryargs = -pool &lt;remote_pool&gt; -name &lt;remote_schedd&gt;
</pre>


---## why pilots submitted to a CREAM CE fail?

If your pilots being submitted to a CREAM CE fail, with this type of content in the condor log file:

<pre class="file">
        009 (45454545.000.000) 12/10 20:35:01 Job was aborted by the user.
            CREAM error: BLAH error: submission command failed (exit code = 1) (stdout:) (stderr:) N/A (jobId = CREAM123456789)
</pre>

it is possible that the grid_resource line in the condor submit file is not correct. 
!AutoPyFactory creates a line like this

<pre class="file">
grid_resource = cream matrix.net:8443/ce-cream/services/CREAM2 pbs queuename
</pre>

while perhaps the CE needs a line like

<pre class="file">
grid_resource = cream matrix.net:8443/queuename
</pre>

If that is the case, or similar, there is dirty trick to fix it. Add this line to your queue configuration (typically in queues.conf):

<pre class="file">
batchsubmit.condorcream.condor_attributes.grid_resource = cream matrix.net:8443/queuename
</pre>



---## to which type of resource targets I can submit pilots with !AutoPyFactory?

In principle, to those supported by !HTCondor. That includes globus GT2, globus GT5, !HTCondor-CE, CREAM CE, !NorduGrid CE, Amazon EC2, a local or remote !HTCondor cluster,...
If !HTCondor cannot submit to a given resource, then !AutoPyFactory cannot either.

---## how do I find out the version of !AutoPyFactory I have installed?

A rpm command will give the answer:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% rpm -qa | grep autopyfactory
autopyfactory-tools-1.0.1-1.noarch
autopyfactory-2.4.1-1.noarch
</pre>

---## how should I configure the factory if I want to renew the proxies myself?

If you don't want the factory to renew the x509 proxies for you because you have in place a different mechanism for that, you only need to disable the renewal feature. 
But, if the factory is configured to submit to grid sites or any other resource that requires an x509 file, the proxymanager still must be enabled anyways. 
So, in your ==autopyfactory.conf== file:

<pre class="file">
proxymanager.enabled = True
</pre>

but in your =proxy.conf= file:
<pre class="file">
renew = False
</pre>

---## how can I add environment variables to the condor submit file?

Using the configuration variable =batchsubmit.&lt;plugin&gt;.environ=. 
For details, check the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryReferenceManual#6_5_Batch_Submit_Plugin_variable][Reference Manual]]

---## how can I add any arbitrary line to the condor submit file?

Using the configuration variable =batchsubmit.&lt;plugin&gt;.condor_attributes=. 
For details, check the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryReferenceManual#6_5_Batch_Submit_Plugin_variable][Reference Manual]]


---## how can I tune the number of pilots to be submitted depending on the number of activated jobs in !PanDA and the number of pilots still pending?

The sched plugin =Ready= checks the number of activated jobs, the number of idle pilots, and returns the difference. 
For details on the sched plugins parameters, check the [[https://twiki.grid.iu.edu/bin/view/Documentation/Release3/AutoPyFactoryReferenceManual#6_4_Sched_Plugin_variables][Reference Manual]]

---## how can I run a factory with a special setup?

If you need to ensure some environment variables are set when running the factory, for example, you can add any custom setup in the =/etc/sysconfig/autopyfactory= file. It is sourced by the daemon init script =/etc/init.d/autopyfactory= before start running.  

<pre class="file">
OPTIONS="--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log"
CONSOLE_LOG=/var/log/autopyfactory/console.log
env | sort >> $CONSOLE_LOG

source /path/to/my/custom/factory/setup.sh
</pre>

---## my factory is not submitting pilots anymore for a given queue, why?

First check that queue has no ==enabled=False== set in the configuration file =/etc/autopyfactory/queues.conf=
If that was not the problem, there are several main reasons why this can happens:

   * One reason is that there are no ready jobs in the WMS system (for example, in !PanDA). Standard factory configuration does not submit pilots when there are no jobs.  

   * Another reason may be data corruption from the CE/gatekeeper. Sometimes it happens the CE/gatekeeper reports back to condor a fake number of IDLE and RUNNING pilots. If the number of IDLE pilots returned is high enough (even if that number is not real), the factory most probably will stop submitting and wait for those pending pilot to start running and finish. 

   * The queue in the WMS system is disabled.

   * There is no !WMSStatus or !BatchStatus information available.

How to check?

   * for the first case, it is enough to check the !AutoPyFactory log files, with a command like this:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% grep ANALY_BNL_SHORT-gridgk04-htcondor /var/log/autopyfactory/autopyfactory.log | grep schedplugin
...
2015-08-10 18:38:20,363 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:34 calcSubmitNum(): Starting.
2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:60 _calc(): pending = 10 running = 17 offset = 0
2015-08-10 18:38:20,364 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ReadySchedPlugin.py:68 _calc(): input=0; activated=83; offset=0 pending=10; running=17; Return=73
2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ScaleSchedPlugin.py:31 calcSubmitNum(): Starting with n=73
2015-08-10 18:38:20,364 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] ScaleSchedPlugin.py:36 calcSubmitNum(): Return=19
2015-08-10 18:38:20,364 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPerCycleSchedPlugin.py:23 calcSubmitNum(): Starting with n=19
2015-08-10 18:38:20,365 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPerCycleSchedPlugin.py:35 calcSubmitNum(): input=19; Return=19
2015-08-10 18:38:20,365 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MinPerCycleSchedPlugin.py:24 calcSubmitNum(): Starting with n=19
2015-08-10 18:38:20,365 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MinPerCycleSchedPlugin.py:34 calcSubmitNum(): Return=19
2015-08-10 18:38:20,365 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:26 calcSubmitNum(): Starting.
2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:37 calcSubmitNum(): site status is online
2015-08-10 18:38:20,366 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusTestSchedPlugin.py:45 calcSubmitNum(): [Queue is not test] input=19; Return=19
2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:29 calcSubmitNum(): Starting.
2015-08-10 18:38:20,366 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:51 calcSubmitNum(): site status is online
2015-08-10 18:38:20,367 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] StatusOfflineSchedPlugin.py:66 calcSubmitNum(): [Queue is not offline] input=19; Return=19
2015-08-10 18:38:20,367 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:25 calcSubmitNum(): Starting with n=19
2015-08-10 18:38:20,367 (UTC) [ DEBUG ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:36 calcSubmitNum(): Pending is 10
2015-08-10 18:38:20,367 (UTC) [ INFO ] main.schedplugin[ANALY_BNL_SHORT-gridgk04-htcondor] MaxPendingSchedPlugin.py:53 calcSubmitNum(): Return=0 
</pre>

Pay attention to the messages coming from the !ReadySchedPlugin. Field *Activated* will tell you is there are jobs or not. 

   * For the second scenario, best way is to run condor_q and see how it says. When possible, print the !JobStatus information, or equivalent:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% condor_q  -format '%s.' ClusterId -format '%s ' ProcId -format ' MATCH_APF_QUEUE=%s' match_apf_queue -format ' JobStatus=%d\n' JobStatus 
1230.0 MATCH_APF_QUEUE=BNL_PROD-gridgk01-htcondor JobStatus=1
1231.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk04-htcondor JobStatus=2
1232.0 MATCH_APF_QUEUE=ANALY_BNL_LONG-gridgk01-htcondor JobStatus=2
1233.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk01-htcondor JobStatus=2
1234.0 MATCH_APF_QUEUE=BNL_ATLAS_2-gridgk07-htcondor JobStatus=2
1235.0 MATCH_APF_QUEUE=BNL_PROD_MCORE-gridgk07-htcondor JobStatus=1
</pre>

Also, when it is installed, try running command apf-queue-status

<pre class="rootscreen">
%UCL_PROMPT_ROOT% apf-queue-status
Mon Aug 10 15:12:24 2015
-----------------------------------------------------------------
ANALY_BNL_LONG-gridgk04-htcondor 	UNSUB = 0	IDLE = 4	RUNNING = 29	COMPLETE = 0	HELD = 0	ERROR = 0	REMOVED = 0	
ANALY_BNL_LONG-gridgk05      	UNSUB = 0	PENDING = 10	STAGE_IN = 0	ACTIVE = 10	STAGE_OUT = 0	SUSP = 0	DONE = 0	FAILED = 0	
</pre>


   * the 3rd case applies mostly to !PanDA. Check if the !PanDA queue is *offline*. In that case, !AutoPyFactory usually does not submit pilots.

   * Besides all of that, you can also check the queue object has actually been created. A grep command like this may help:

<pre class="rootscreen">
%UCL_PROMPT_ROOT% grep "APFQueue: Initializing object..." /var/log/autopyfactory/autopyfactory.log
...
2015-08-10 19:31:51,271 (UTC) [ DEBUG ] main.apfqueue[ANALY_BNL_LONG-gridgk02-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...
2015-08-10 19:31:51,557 (UTC) [ DEBUG ] main.apfqueue[BNL_ATLAS_RCF-gridgk02-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...
2015-08-10 19:31:51,859 (UTC) [ DEBUG ] main.apfqueue[BNL_ATLAS_RCF-gridgk06-htcondor] factory.py:870 __init__(): APFQueue: Initializing object...
...
</pre>

   * When there is no WMS Status or Batch Status information available, the Sched Plugin Ready returns 0. To check if that is the case, search for a line like this in the logs file:

<pre class="file">
2015-11-30 03:33:14,326 (UTC) [ WARNING ] main.schedplugin[A_QUEUE_AT_A_SOTE] ReadySchedPlugin.py:39 calcSubmitNum(): Missing info. wmsinfo is WMSQueueInfo: notready=0, ready=532,     running=314, done=0, failed=0, unknown=0 batchinfo is None
</pre>

If the !WMSQueueInfo is None, something is broken in the communication with the WMS service (for example, with a !PanDA server).
If the batchinfo is None, most probably the condor daemon is not running and command condor_q does not work.


---## I do not want my factory to show up in the generic !AutoPyFactory web monitor

Solution currently for this is to remove the config variable ==monitorsection== from file =/etc/autopyfactory/queues.conf=


---## how can I increase the verbosity level in the log files?

Edit file =/etc/sysconfig/autopyfactory= and change the loglevel from INFO level to DEBUG level:

<pre class="file">
OPTIONS="--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log"
</pre>

---## How can I change the non-root running user?

The default configuration sets the factory to run as user *autopyfactory*.
That can be changed in file =/etc/sysconfig/autopyfactory=, via the configuration variable ==--runas==

<pre class="file">
OPTIONS="--debug --sleep=60 --runas=autopyfactory --log=/var/log/autopyfactory/autopyfactory.log"
</pre>

Note that the RPM creates user account *autopyfactory* during the installation process. If you want to run as a different user, you will need to create that account manually.

---## How can I force the value of variable arguments in the condor submit file to be enclosed in between quotes?

If you want the condor submit file to look like this

<pre class="file">
arguments = "--wrapperloglevel=debug ...-p 25443 -u user -u managed"
</pre>

you only need to add the quoutes in the queues configuration file. Typically, like this:

<pre class="file">
[DEFAULT]
executable.defaultarguments = --wrapperloglevel=debug ...-p 25443 -u user

[ANALY_QUEUE]
executable.arguments = "%(executable.defaultarguments)s -u managed"
</pre>

---## I have variables in the =[DEFAULT]= section whose value requires interpolation, and they fail on some sections

When the =[DEFAULT]= section in a given configuration file contains variables like this

<pre class="file">
[DEFAULT]
foo = %(bar)s blah
</pre>

the variable =bar= is expected to be defined in every section of that configuration file.
When that is not the case, a traceback like this is printed out:

<pre class="rootscreen">
    self._interpolate_some(option, L, rawval, section, vars, 1)
  File "/usr/lib64/python2.6/ConfigParser.py", line 646, in _interpolate_some
    option, section, rest, var)
InterpolationMissingOptionError: Bad value substitution:
section: [ANALY_TRIUMF_ARC-ce3]
option : foo
key    : bar
rawval : blah
</pre>

This will mostly ocurr in the case of queues configuration files, where the variable =foo= is being resolved in a section where it is not needed. 
The simplest solution is to provide for a fake value for token =bar= to allow the interpolation to work.
If that is not desired because it is confusing, or there are too many sections where it happens, then a good idea is to split the configuration file into two separate files, each one with the right =[DEFAULT]= section, and list both of them, split by comma, in the factory configuration file.


---## what do I do when I don't want to use an X509 proxy?

To avoid !AutoPyFactory from trying to get X509 credentials, set the variable =batchsumit.&lt;your_submit_plugin&gt;.proxy= to =None=

---## why the factory keeps submitting jobs when there are already enough jobs in queue?

If the factory does not seem to respect the policies set in the sched plugins, and keeps submitting jobs when it should have already stop, 
it is possible the interval periods are not proper.

The queues have their own interval time, and the batchstatus plugin has its own one. 
If a queue interval is smaller than the batchstatus one, it does not see updated queue information, and therefore the queue keeps submitting. 

As a general rule, it is recommended to set the queues interval to be higher that twice the batchstatus interval.


---## qcl_dir

<pre class="file">
2017-03-03 08:29:53,565 (UTC) [ ERROR ] main factory.py:435 run(): Traceback (most recent call last):
  File "/usr/lib/python2.6/site-packages/autopyfactory/factory.py", line 411, in run
    f.run()
  File "/usr/lib/python2.6/site-packages/autopyfactory/factory.py", line 657, in run
    self.reconfig()
  File "/usr/lib/python2.6/site-packages/autopyfactory/factory.py", line 695, in reconfig
    tmpqcl = config_plugin.getConfig()
  File "/usr/lib/python2.6/site-packages/autopyfactory/plugins/factory/config/File.py", line 69, in getConfig
    raise ConfigFailure('Failed to create queues ConfigLoader: %s' %err)
ConfigFailure: Failed to create queues ConfigLoader: local variable 'qcl_dir' referenced before assignment
</pre>

A traceback related variable =qcl_dir= usually means the configuration variable =queuesDirConf= has been set in file =autopyfactory.con=, but the directory does not actually exists, or it is empty. 
<br>
Most probably, after a fresh deployment, it is set as =queuesDirConf = /etc/autopyfactory/queues.d/=, but that directory does not exists.
<br>
Note: fixed in version 2.4.10

---## condor_q against remote pool

<pre class="file">
2017-03-10 17:47:20,581 (UTC) [ WARNING ] main.condor condor.py:356 querycondor(): Leaving with bad return code. rc=1 err=Error: Collector has no record of schedd/submitter
</pre>

This happens due a bug in the code that creates the condor_q command line incorrectly, with a missing white space between arguments.
<br>
Note: fixed in version 2.4.10


