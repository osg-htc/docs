%SHOW_DOC_STATUS_TABLE%

---+!! Portable Batch System (PBS) Hints
%TOC{}%

---+ About this Document
<!-- useful variable definitions
   * Local UCL_HOST = ce
-->

%ICON{"hand"}% This document is for System Administrators of a %LINK_GLOSSARY_CE%. It describes all necessary steps to enable the [[http://en.wikipedia.org/wiki/Portable_Batch_System][Portable Batch System]] to work with the !OSG software stack.

---+ Requirements

   1 Working !PBS installation
   1 [[InstallComputeElement][Compute Element Installation]]
   1 [[InstallComputeElement#ConfigPBS][Additional Installation Instructions for PBS]]

---+ General Recommendations

   * Do not install the !PBS server and the !OSG %LINK_GLOSSARY_GATEKEEPER% on the same host to avoid problems with the gatekeeper bringing down the batch scheduler.
   * Export =/var/spool/pbs/server_priv/accounting= and =/var/spool/pbs/server_logs= from your !PBS server to the %LINK_GLOSSARY_GATEKEEPER% in order for Gratia and WS-GRAM to function correctly.
   * Multiple !OSG gatekeepers can be configured to access a single !PBS server.

---+ !PBS Configuration

!PBS maintains _stdout_ and _stderr_ on the execution node of the job; both are copied back to the !PBS server host upon completion of the job using =pbs_rcp= by default. If user home directories are shared across cluster nodes, you can edit =/var/spool/pbs/mom_priv/config= to manage copy-back by adding:

<pre class="file">
$usecp *.%UCL_HOST_FQDN%:/home /home
</pre>

%NOTE% Adjust the regular expression =*.%UCL_HOST_FQDN%= to match the node names and domain of your node network!

---+ Gatekeeper Configuration

The !Perl script =$GLOBUS_LOCATION/globus/lib/perl/Globus/GRAM/JobManager/pbs.pm= translates a Globus !RSL to the !PBS submission script. By _default_ the !Perl script assumes that the _default_ queue will be used. To change the queue name or any other !PBS directive, edit =$GLOBUS_LOCATION/globus/lib/perl/Globus/GRAM/JobManager/pbs.pm=. 

---++ Create a Default Queue

Edit =$GLOBUS_LOCATION/globus/lib/perl/Globus/GRAM/JobManager/pbs.pm= and add =%RED%#PBS -q &lt;queue name&gt;%ENDCOLOR%= to the file:

<pre class="file">
#! /bin/sh
# PBS batch job script built by Globus job manager
#
#PBS -S /bin/sh
#%RED%PBS -q &lt;queue name&gt;%ENDCOLOR%
EOF
</pre>

%NOTE% Change %RED%&lt;queue name&gt;%ENDCOLOR% with the name of the queue to be used by default. Grid users who specify a queue during job submission will not be affected by this setting. 

---++ Create a Dynamic OSG_WN_TMP Directory

   1 Create a prologue script that creates the temporary directory
   1 Modify =$GLOBUS_LOCATION/globus/lib/perl/Globus/GRAM/JobManager/pbs.pm=

<pre class="file">
#!/bin/bash
# Create a /scratch/jobid dir for every job run, and copy a user's keytab file from the PBS head node.
#
# 10/31/2006 - <sether@fnal.gov> added lots and lots of error checking
# 11/06/2006 - <jallen@fnal.gov> modified script to use a separate scratch filesystem for each job
# 12/15/2006 - <jallen@fnal.gov> write critical errors to ERROR_FILE to mom health script
# 05/04/2007 - Steven Timm add output at the end of script

JOBID=$1
USERNAME=$2
JOB_EXEC_GRP=$3
JOB_NAME=$4
RESOURCE_LIMITS=$5
QUEUE=$6
JOB_ACCOUNT=$7

SCRATCH_DIR="/scratch"
KRB_DIR="/var/adm/krb5"
JOBS_DIR="/var/spool/pbs/mom_priv/jobs"
HOME_MNT_POINT="/RunII/home"
ERROR_FILE="/var/spool/pbs/ERROR"

# Sanity checks
if [ ! ${JOBID} ]; then
        logger "PBS prologue: JOBID not specified"
        echo "JOBID not specified"
        exit 1;
fi

if [ ! ${USERNAME} ]; then
        logger "PBS prologue: USERNAME not specified"
        echo "USERNAME not specified"
        exit 1;
fi


if [ ! -d ${SCRATCH_DIR} ]; then
        err="$SCRATCH_DIR does not exist"
        logger "PBS prologue: $err"
        echo $err | tee $ERROR_FILE
        exit 1;
fi


# Job info
echo "---------------------------------------------------------"
echo "Start CAB Prolouge `date`"
printf "%-15s %s\n" "jobid:" $JOBID
printf "%-15s %s\n" "username:" $USERNAME
printf "%-15s %s\n" "job name:" $JOB_NAME
printf "%-15s %s\n" "limits:" $RESOURCE_LIMITS
printf "%-15s %s\n" "queue:" $QUEUE

#$SHORT_JOBID=${JOBID%%.*}

# Clean-up scratch
# Only running jobs should be using space.
for job_id in $(ls ${SCRATCH_DIR} | egrep "fnal.gov$"); do
        short_job_id=${job_id%%.*}
        j_cnt=$(ls $JOBS_DIR | egrep -c "^$short_job_id")
        if [ "$j_cnt" -eq  0  -a "$short_job_id" != "" ]; then
                rm -rf ${SCRATCH_DIR}/${job_id}
                logger "PBS prologue: Cleaning-up old scratch dir ${SCRATCH_DIR}
/${job_id}..."
        fi
done

# do we have scratch filesystems setup
sfs_cnt=$(ls ${SCRATCH_DIR} | egrep -c "_fs$")
if [ $sfs_cnt -lt 1 ]; then

        if [ -x "/var/spool/pbs/bin/mkscratch_fs.sh" ]; then
                err="Creating scratch filesystems. This will take awhile.."

                # Send errors to job output and to error file
                # so health check script will see it and set
                # set node offline.
                echo $err | tee $ERROR_FILE

                logger "PBS prologue: $err"
                /var/spool/pbs/bin/mkscratch_fs.sh
                exit 1;

        else
                err="Could not create scratch filesytems..."
                echo $err | tee $ERROR_FILE
                logger "PBS prologue: $err"
                exit 1;
        fi
fi

# Make sure scratch filesystems are mounted
i=0
while [ $i -lt $sfs_cnt ]; do
        scratch_mnt=${SCRATCH_DIR}/scratch_${i}
        scratch_mnt_info=$(mount | awk -v mnt="$scratch_mnt" '{ if ($3 == mnt) p
rint $0 }')
        if [ "$scratch_mnt_info" == "" ]; then
                mount -oloop ${scratch_mnt}_fs  ${scratch_mnt}
                if [ $? -ne 0 ]; then
                        err="Unable to mount $scratch_mnt. Mount cmd returned co
de $?."
                        echo $err | tee $ERROR_FILE
                        logger "PBS prologue: $err"
                        exit 1;
                fi
        fi
        i=$(($i+1))
done

# Link the job id to a free scratch area
i=0
linked=0
while [ $i -lt $sfs_cnt ]; do
        ls_out=$(ls -l ${SCRATCH_DIR} | egrep "\-> scratch_${i}")
        if [ "$ls_out" == "" ]; then
                cd $SCRATCH_DIR
                # clean-up if necessary
                rm -rf $SCRATCH_DIR/scratch_${i}/*
                ln -s  scratch_${i} $JOBID
                linked=1
                break
        fi
        i=$(($i+1))
done

if [ $linked -ne 1 ]; then
        err="Unable to link $JOBID to scratch dir..."
        logger "PBS prologue: $err"
        echo $err | tee $ERROR_FILE
        exit 1;
fi


# Make sure home dirs are mounted
HOME_MNT_INFO=$(mount | awk -v mnt=$HOME_MNT_POINT '{ if ($3 == mnt) print $0 }'
)
if [ "$HOME_MNT_INFO" == "" ]; then
   echo "Home directories not mounted!!"
   logger "PBS prologue: Home dirs not mounted. Rebooting."
   /sbin/shutdown -r now
fi

chmod 777 ${SCRATCH_DIR}/${JOBID}

echo "End CAB Prolouge `date`"
echo -e "---------------------------------------------------------\n"

#   OSG_WN_TMP now set dynamically on this cluster to /scratch/${PBS_JOB_ID}
print JOB "OSG_WN_TMP_OLD=\${OSG_WN_TMP}\n";
print JOB "export OSG_WN_TMP_OLD\n";
print JOB "OSG_WN_TMP=/scratch/\${PBS_JOBID}\n";
print JOB "export OSG_WN_TMP \n";
</pre>

---+ Troubleshooting

   * A simple grid job submission:<pre class="screen">
%UCL_PROMPT_SHORT% globus-job-run %UCL_HOST_FQDN%:2119/jobmanager-pbs /bin/hostname
</pre> will fail if the %RED%jobtype%ENDCOLOR% argument is not specified as part of the !RSL. The jobtype can be _single_, _multiple_ and _PBS_. The correct syntax is:<pre class="screen">
%UCL_PROMPT_SHORT% globus-job-run %UCL_HOST_FQDN%:2119/jobmanager-pbs -x '(%RED%jobtype%ENDCOLOR%=single)' /bin/hostname
</pre>

   * Unless ssh key-pairs have been configured for _all_ users, so that all the parts of the _multiple_ job can be launched with the gatekeeper, the job will fail with following error:<pre class="screen">
Host key verification failed.
/var/spool/pbs/mom_priv/jobs/302232.gaia.SC: line 87: [: too many arguments
</pre>
   *  If a !PBS job exceeds resource limits (_resources_default.mem_, _resources_default.walltime_) it will get killed unless the job explicitly request higher resource limits using the !RSL.
   *  If the job gets submitted to a !PBS queue that has reached its queue limit, the submission will fail and the job will be held with =Globus error 17=.

---+ *Comments*
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT

   DEAR DOCUMENT OWNER
   ===================

   Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER          = StevenTimm

   Please define the document area, choose one of the defined areas from the next line
   DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

   define the primary role the document serves, choose one of the defined roles from the next line
   DOC_ROLE = (Scientist|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

   Please define the document type, choose one of the defined types from the next line
   DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = HowTo
   
   Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

   Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

   change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%

   change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %YES%


   DEAR DOCUMENT REVIEWER
   ======================

   Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = MarcoMambelli
  
   Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %YES%
 
-->