%DOC_STATUS_TABLE%

%RED%This page is obsolete: the central service that GIP reports to (BDII), is no longer running on the OSG. Consequently, configuring the Generic Information Provider on a site is no longer necessary or useful.%ENDCOLOR%

---+!! Configuration of the Generic Information Provider
%TOC%

---+ About this Document
<!-- conventions used in this document
   * Local UCL_CWD  = $OSG_LOCATION
   * Local UCL_HOST = ce
   * Local TABLE_OPTS = cellpadding="5,2,5,2" cellspacing="0" databg="#FFFFFF" headerbg="#8D929A" valign="middle" databg="#FFFFFF,#DDDDDD" tablewidth="100%" columnwidths="150,150,"
   * Local UCL_CONFIG = =%UCL_CWD%/osg/etc/config.ini=
-->

%ICON{hand}% This document is for System Administrators. It contains the procedure to configure the %LINK_GLOSSARY_GIP%.

%STARTINCLUDE%

%STARTSECTION{"All"}%

%STARTSECTION{"ConfigurationStart"}%
---%SHIFT%+ Configuration of the Generic Information Provider (GIP)

One important aspect of a site being part of the %LINK_OSG% is the ability of the %LINK_GLOSSARY_CE% to describe the site to external users. The Generic Information Provider (GIP) generates site information and provides it in the [[http://glueschema.forge.cnaf.infn.it/][GLUE]] schema.

The GIP is collecting information about some aspects of your site including its hardware composition, the batch system and associated storage. It relies on the configuration files located in =/etc/osg/config.d= to collect the information.

The following sections detail information about the options in the =[GIP]=, =[Subcluster]=, and =[SE]= sections of the configuration files.

%ENDSECTION{"ConfigurationStart"}%

%STARTSECTION{"MultiCE"}%
---%SHIFT%++ For Sites with Multiple Compute Elements

If you would like to properly advertise multiple Compute Elements per resource, make sure to:

   * set the value of =cluster_name= in the =[GIP]= section to be the same for each CE
   * set the value of =other_ces= in the [GIP] section to be the hostname of the other CEs at your site; this should be comma separated.  So, if you have two CEs, ce1.example.com and ce2.example.com, the value of other_ces on ce1.example.com should be "ce2.example.com.  This assumes that the same queues are visible on each CE.
   * set the value of =resource_group= in the "Site Information" section to be the same for each CE. 
   * the value of =resource= in the "Site Information" section should be CE unique.
   * Have the *exact* same configuration values for the GIP, SE*, and Subcluster* sections in each CE.

It is good practice to run "diff" between the config files of the different CEs.  The only changes should be the value of localhost in the [DEFAULT] section and the value of other_ces in the [GIP] section.

%ENDSECTION{"MultiCE"}%

%STARTSECTION{"SubCluster"}%
---%SHIFT%++ Subcluster Configuration

Another aspect of the GIP is to advertise the physical hardware a job will encounter when submitted to your site.  This is information is provided to GIP in the =[Subcluster &lt;name&gt;]= section of the resource configuration file =/etc/osg/config.d/30-gip.ini=.

A _sub-cluster_ is defined to be a homogeneous set of worker node hardware. At least one Subcluster section _is required_. For WLCG sites, information filled in here will be advertised as part of your !MoU commitment, so please strive to make sure it is correct.  

For each sub-cluster constituting your site, fill in the information about the worker node hardware by creating a new section choosing a unique name using the following format:  =[Subcluster &lt;name&gt;]= where &lt;name&gt; is the sub-cluster name.

%IMPORTANT% This sub-cluster name must be unique for the entire OSG.  Do not pick something generic like =[Subcluster Opterons]=!

Examples can be found [[ReleaseDocumentation/ConfigurationFileGIPExamples][here]]. [[InformationServices/SubclusterMapping][This page]] shows the subcluster mapping from attribute names in =/etc/osg/config.d/30-gip.ini= to [[http://glueschema.forge.cnaf.infn.it/][GLUE]] attribute names.

The values for this section are relatively well-documented and self-explanatory and are given below:

%TWISTY{%TWISTY_OPTS_MORE%}%
%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|name|_String_|This is the same name that is in the Section label.  It should be globally unique!|
|node_count|_Positive Integer_|This is the number of worker nodes in the sub-cluster|
|ram_mb|_Positive Integer_|Megabytes of RAM per node.|
|cpu_model|_String_|CPU model, as taken from =/proc/cpuinfo=.  Please, no abbreviations!|
|cpu_vendor|_String_|Vendor's name: AMD, Intel, or any other.|
|cpu_speed_mhz|_Positive Integer_|Approximate speed in MHZ of the CPU as taken from =/proc/cpuinfo=|
|cpus_per_node|_Positive Integer_|Number of CPUs (physical chips) per node.|
|cores_per_node|_Positive Integer_|Number of cores per node.|
|inbound_network|_True_ or _False_|Set to true or false depending on inbound connectivity.  That is, external hosts can contact the worker nodes in this sub-cluster based on their hostname.|
|outbound_network|_True_ or _False_|Set to true or false depending on outbound connectivity.  Set to true if the worker nodes in this sub-cluster can communicate with the internet.|
|cpu_platform|_x86_64_ or _i686_|*NEW for OSG 1.2*.  Set according to your sub-cluster's processor architecture. |
|HEPSPEC|_Positive Integer_|Optional:  Publish the HEPSPEC number for the subcluster.|
|SI00|_Positive Integer_|Optional:  Publish the SpecInt2000 number for the subcluster.  Default: 2000|
|SF00|_Positive Integer_|Optional:  Publish the SpecFloat2000 number for the subcluster.  Default: 2000|
%ENDTWISTY%

%ENDSECTION{"SubCluster"}%

%STARTSECTION{"ForComputeElements"}%
---%SHIFT%++ For Compute Elements

The GIP queries the batch system specified and enabled in =/etc/osg/config.d/20-&lt;batch system&gt;.ini=. Alternatively you may manually specify which batch system to query by setting the =batch= attribute in the =[GIP]= section. 

%AH_CONDOR_BEGIN%---%SHIFT%+++ Information on Condor
GIP has several options that will control how it interacts with the Condor Batch system.  You can determine which Condor daemon GIP queries as well as set whitelists and black lists that GIP will use to properly report which VOs are assigned to which Condor Groups.

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|use_collector|_True_ or _False_|Ordinarily GIP queries the Negotiator.  This option allows the administrator to tell GIP to query the collector instead of the negotiator.  This option is overridden by =query_only_local_condor=|
|query_only_local_condor|_True_ or _False_|This option overrides =use_collector=.  It causes GIP to query the local Condor SCHEDD rather than the Negotiator or Collector.  If not explicitly set, this option defaults to =False=|
|exclude_schedds|_String_|This is a comma separated list of schedds that should be excluded during a condor_status -submitter query|
|subtract_owner|_True_ or _False_|By default, owner VMs are not accounted for the total CPU numbers published by GIP since according to Condor they aren't available for user jobs.  Set to =False= to tell GIP to not subtract CPUs provided by owner VMs|
|(groupname)_blacklist|_String_|Either set to "*" which denies access to all VOs to the Condor group identified by (groupname) or set to a comma separate list of VOs you wish to deny access.|
|(groupname)_whitelist|_String_|Either set to "*" which allows access to all VOs to the Condor group identified by (groupname) or set to a comma separate list of VOs you wish to allow access.|
|jobs_constraint|_String_|adds a =--constraint= option to the condor_q command that GIP will execute.  %NOTE: for big sites this could cause rather large performance issues|
|status_constraint|_String_|adds a =--constraint= option to the condor_status command that GIP will execute.  %NOTE: for big sites this could cause rather large performance issues|
|max_wall|_Integer_|This allows an administrator to publish the maximum wall time allowed on the Condor pool|
%NOTE% To generate a list of supported VOs for a queue, the blacklist is evaluated before the whitelist.

%AH_CONDOR_END%

%AH_PBS_BEGIN%---%SHIFT%+++ Information on PBS

By default any authorized grid user may submit to every PBS queue listed in =/usr/share/globus/globus_gram_job_manager/pbs.rvf=. In this case GIP automatically advertises all listed queues. 

GIP will also detect if queues restrict access to certain users (=acl_users=) or certain groups (=acl_groups=). A reverse mapping from these users and groups to their associated %LINK_GLOSSARY_VO% provides access information of VOs to certain queues which will be advertised by the GIP instead.

This process is not perfect and sometimes fails to generate the right information. In this case you may _manually_ blacklist and whitelist PBS queues for listed VOs in the =[PBS]= section of the resource configuration file =/etc/osg/config.d/20-pbs.ini=.

GIP also allows you to completely exclude queues from the list of queues that GIP will advertise.

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|host|_String_|Optional - This is the hostname that will be appended to PBS commands if set.  Example: =qstat -B -f host=|
|preemption|_Integer_|Optional - Set to 1 in order enable reporting that preemption is enabled on the cluster.  Default is 0|
|pbs_path|_String_|Optional - This allows an administrator to set a non standard PBS location or a PBS location that is not in the system search path|
|(queuename)_blacklist|_String_|Either set to "*" which denies access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to deny access.|
|(queuename)_whitelist|_String_|Either set to "*" which allows access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to allow access.|
|queue_exclude|_String_|Comma-separated list of queue names that GIP should exclude from the list of queues to publish|

%NOTE% To generate a list of supported VOs for a queue, the blacklist is evaluated before the whitelist.

%AH_PBS_END%

%AH_SGE_BEGIN%---%SHIFT%+++ Information on SGE

The =home= attribute in the =[SGE]= section of the resource configuration file =/etc/osg/config.d/20-sge.ini= has been replaced by =sge_root= and =sge_cell=, which should be set to the value of  of =$SGE_ROOT= and =$SGE_CELL=, respectively.  The GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|preemption|_Integer_|Optional - Set to 1 in order enable reporting that preemption is enabled on the cluster.  Default is 0|
|sge_path|_String_|Optional - This allows an administrator to set a non standard SGE location or a SGE location that is not in the system search path|
|sge_root|_String_|This value should be set to =$SGE_ROOT= - GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.|
|sge_cell|_String_|This value should be set to =$SGE_CELL= - GIP assumes that it can source =$SGE_ROOT/$SGE_CELL/common/settings.sh= to create a working SGE environment.|
|(queuename)_blacklist|_String_|Either set to "*" which denies access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to deny access.|
|(queuename)_whitelist|_String_|Either set to "*" which allows access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to allow access.|
|queue_exclude|_String_|Comma-separated list of queue names that GIP should exclude from the list of queues to publish|

%AH_SGE_END%

%AH_LSF_BEGIN%---%SHIFT%+++ Information on LSF

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|host|_String_|Optional - This is the hostname that will be appended to LSF commands if set.  Example: =lshosts -w host=|
|preemption|_Integer_|Optional - Set to 1 in order enable reporting that preemption is enabled on the cluster.  Default is 0|
|lsf_location|_String_|Optional - This allows an administrator to set a non standard LSF location or a LSF location that is not in the system search path|
|lsf_profile|_String_|Optional - Set this to specify the location of the LSF profile file.  GIP will source this to extract a working LSF environment.  Defaults to =/lsf/conf/profile.lsf=|
|(queuename)_blacklist|_String_|Either set to "*" which denies access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to deny access.|
|(queuename)_whitelist|_String_|Either set to "*" which allows access to all VOs to the queue identified by (queuename) or set to a comma separate list of VOs you wish to allow access.|
|queue_exclude|_String_|Comma-separated list of queue names that GIP should exclude from the list of queues to publish|

%AH_LSF_END%

---%SHIFT%+++ Link the Compute Element to an external Storage Element

A compute element may provide mass storage to grid users on an _external_ storage element. To advertise the available storage space on the external storage element create the file =/etc/gip/gip.conf= with following content:

%CODE{"scheme"}%
[cesebind]
; Advertise the availability of an external storage element for mass storage use on the compute element.
simple = False
se_list = <coma-separated list of the SRM endpoint of storage elements>
%ENDCODE%

%ENDSECTION{"ForComputeElements"}%


%STARTSECTION{"ForStorageElements"}%

#StorageElementConfiguration
---%SHIFT%++ For Storage Elements

For each %LINK_GLOSSARY_SE% add a new section to the resource configuration file =/etc/osg/config.d/30-gip.ini= named =[SE_&lt;name&gt;]= where &lt;name&gt; is the unique name of your SE. The following sections illustrate the configuration process for a:

   1 Generic Storage Element
   1 %BESTMAN% Storage Element
   1 dCache Storage Element

See following [[ConfigurationFileGIPExamples][examples]] for working SE configuration sections.

%NOTE% WLCG sites _must_ use the =bestman= or =dcache19= provider in order to be advertised correctly.

---%SHIFT%+++ Generic Storage Element

Following attributes are available to configure the information provided by GIP for a generic storage element:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|enabled |_True_ or _False_|Indicates whether or not this SE section is enabled for the GIP.|
|name |_String_|Name of the Storage Element as registered in %LINK_OIM%.|
|srm_endpoint |_String_|The endpoint of the SE.  It MUST contain the _hostname_, _port_, and the _server location_ (such as /srm/v2/server).  It MUST NOT have the ?SFN= string.  Example:  srm://srm.example.com:8443/srm/v2/server |
|srm_port |_String_|Sets the specific port number that will appear in the srm_endpoint. |
|provider_implementation |String|Set to =static= for a generic SE.%BR%Set to =bestman= for %BESTMAN% SE.%BR%Set to =dcache= or =dcache19= for dCache SE.|
|implementation |_String_|Name of the SE implementation.|
|version |_String_|Version number of the SE implementation.|
|default_path |_String_|Default storage path.  For the srm command line this corresponds to the path that appears after the question mark.  If a path has the string VONAME in it, that string will be replaced with the appropriate vo name. |
|use_df |_True_ or _False_|Set to _True_ if the SE provider may use =df= on the directory referenced in =default_path= to obtain the size of the SE.|
|vo_dirs |_Comma-separated List_|A comma-separated list of &lt;VONAME&gt;:&lt;PATH&gt; pairs. The PATH will override the =default_path= attribute for VONAME.|
|allowed_vos|_Comma-separated List_|By default all VOs are advertised to have access to the SE. To advertise only a subset of VOs, provide a comma-separated list here.|
|mount_point|_String_|Used if your SE provides a POSIX-like mount (e.g. Lustre, HDFS, xrootdfs).  The value of `mount_point` should be two paths; first, the path where the file system is mounted on the worker nodes, followed by the exported directory of the file system.|

---%SHIFT%+++ %BESTMAN% Storage Element

For %BESTMAN% storage elements, there are a few notes to consider for the SE attributes:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/v2/server.|
|provider_implementation|_String_|Set to =bestman=.|
|implementation|_String_|Set to =bestman=.|

The %BESTMAN% provider may use =srm-ping= to query a %BESTMAN% storage element for information.  This means that the Unix =tomcat= user will need to access to a valid =http= service certificate in =/etc/grid-security/http/httpcert.pem= and its associated key. Instructions on how to request and install a service certificate can be found [[Documentation/Release3.GetHostServiceCertificates][here]].

---%SHIFT%+++ dCache Storage Element

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|srm_endpoint|_String_|The endpoint of the SE.  It is most likely of the form srm://srm.example.com:8443/srm/managerv2.|
|provider_implementation|_String_|Set to =dcache19= for dCache 1.9 sites.%BR%Set to =static= for default values.%BR%If you use the dcache provider with dCache 1.8, see [[InformationServices/DcacheGip][this page to complete installation]].  If you use the dcache19 provider, you must fill in the location of your dCache's information provider|
|infoprovider_endpoint|_String_|Url to the dcache information provider.  Only required for the dcache19 provider.|
|implementation|_String_|Set to =dcache=.|
|version|_String_|The dCache version.%BR%Will be _auto-detected_ in the case of non-static providers.|

The implementation of dCache version 1.9 added a new read-only XML information service which feeds the dcache19 provider.  The location of the info provider is most likely =http://dcache_head_node.example.com:2288/info=. It runs on the same node that the dCache web interface. Also consult your dCache documentation on how to enable the information service.

---%SHIFT%+++ Advertise Available Storage Space

The GIP has a concept of _space_ as the logical grouping of available storage space. The =static= provider just advertises the storage element as one homogeneous storage space. The dCache and %BESTMAN% can also partition the storage space into several sub-spaces.

   * For dCache a space may be either a _link group_ or a _pool_ group. If a link group contains one or more pool groups, only the link group will be advertised. The space name is the name of the link group or pool group.

   * For %BESTMAN% a space is a SRM _space token_. The name of the space is the description of the space token.


Both dCache and %BESTMAN% storage elements will try to detect the correct values automatically. To _manually_ change attributes in the SE section of the resource configuration file =/etc/osg/config.d/30-gip.ini= consult the table of available space-related attributes:


%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|spaces|_Comma-separated List_|List of all the spaces that should be restricted by VO.|
|space_&lt;NAME&gt;_vos|_Comma-separated List_|By default, all VOs are allowed to access all spaces. To restrict access by VO provide a list of VOs that may access the storage space &lt;NAME&gt;.|
|space_&lt;NAME&gt;_default_path|_String_|The default storage path for VOs for the space named &lt;NAME&gt;.  As in the "default_path" option, the word VONAME is evaluated to be the VO's name.|
|space_&lt;NAME&gt;_path|_Comma-separated List_|A list of &lt;VONAME&gt;:&lt;PATH&gt; pairs for this space.|

%ENDSECTION{"ForStorageElements"}%


%STARTSECTION{"AdvertiseAvailableServices"}%
---%SHIFT%++ Advertise Available Services

The =[GIP]= section of the resource configuration file =/etc/osg/config.d/30-gip.ini= provides the possibility to advertise services available at your site.

However, Multi-CE sites *MUST* edit both =cluster_name= and =other_ces=.

All options are given in the table below:

%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
| advertise_gums  | _True_ or _False_  | Defaults to False. If you want GIP to query and advertise your gums server set this to True. |
| advertise_gsiftp  | _True_ or _False_  | Defaults to True.  If you don't want GIP to advertise your gridftp server set this to False. |
| gsiftp_host  | _String_  | This should be set to the name of the gridftp server GIP will advertise if the advertise_gridftp setting is set to True. |
| cluster_name | _String_  | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the FQDN of the head node of the cluster. |
| other_ces| _String_  | This should *only* be set if you run multiple gatekeepers for the same cluster; if you do, set this value to the comma-separate list of FQDNs for the other CEs at this site. |
| local_template_dirs| _String_ |Support for local template dirs (contributed by Sam Morrison @ ARCS)|

If you want to advertise glexec support you must set =glexec_location= in the =[Misc]= section.  The value should be the location of glexec on the worker node.

%ENDSECTION{"AdvertiseAvailableServices"}%

%STARTSECTION{"AdvertiseWLCGAttributes"}%
---%SHIFT%++ Advertise WLCG Attributes
Sites that are interoperable with the WLCG will need to publish certain extra attributes that help identify them in the information systems.  These options go in the =[Site Information]= section in =/etc/osg/config.d/40-siteinfo.ini=.  Some of this information is available at [[http://gridops.cern.ch/mou/site/][the CERN gridops site]].
%TABLE{ %TABLE_OPTS% }%
|*Option* | *Valid Values* | *Explanation* |
|wlcg_tier|_Positive_Integer_ |The WLCG computing model is composed of tiers.  Put the Tier number that your site belongs to here.|
|wlcg_parent|_String_ |Put your wlcg_tier - 1 site name that your site is assigned to here.  For example, if your site is a Tier-2 site, then put the site name of the Tier-1 site that you are assigned to.|
|wlcg_name|_String_ |String giving the name that the WLCG uses to identify the site|
|wlcg_grid|_String_|String defining which native grid the site belongs to.  If not set, it will default to =OSG=.  Example:  OSG sites would set this value to =OSG=|

%ENDSECTION{"AdvertiseWLCGAttributes"}%

%STARTSECTION{"StandAloneSE"}%
---%SHIFT%---++ Configuring GIP for SE Only

%RED%NOTE:%ENDCOLOR% The following instructions are preserved for OSG 1.2.x installations.  

Now, you have to configure GIP. Several manual steps are required to do so:
   1. Create ==gip.conf== file.<pre class="rootscreen">
<code># cd %RED%VDT_LOCATION%ENDCOLOR%/gip/etc
# cp gip.se_only.example.conf gip.conf </code>
</pre>
   1. Edit ==gip.conf== file and modify this content:<pre class="file">
[gip]
override = False
check_osg = False
osg_config = %RED%VDT_LOCATION%ENDCOLOR%/gip/etc/config.ini

[vo]
user_vo_map = %RED%VDT_LOCATION%ENDCOLOR%/gip/etc/osg-user-vo-map.txt
</pre>the following way:<pre class="file">
[gip]
override = False
check_osg = False
osg_config = %RED%VDT_LOCATION%ENDCOLOR%/gip/etc/config.ini

[vo]
user_vo_map = %RED%VDT_LOCATION%ENDCOLOR%/monitoring/osg-user-vo-map.txt
</pre>
   1. Fill out a config.ini for the SE only site.  The known minimum sections are: [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileHelp#Default_variables][DEFAULT]], [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileHelp#Site_Information][Site Information]], [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileHelp#Storage][Storage]], [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/ConfigurationFileHelp#GIP][GIP]], and [[https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GipConfiguration#For_Storage_Elements][SE &lt;name&gt;]].
   * the Storage section only needs:<pre class="file">   se_available = %(enable)s
   default_se = %RED%&lt;se fqdn&gt;%ENDCOLOR%
</pre>
   * the GIP section only needs:<pre class="file">   advertise_gsiftp = %(disable)s</pre> In order to modify the file you will need first to copy ==config.ini.se_only.example== to ==config.ini== first.<pre class="rootscreen">
<code># cp config.ini.se_only.example config.ini
# vi config.ini</code>
</pre> You will need to modify ==config.ini== accordingly following the instruction given above. Here are two examples for 
   * !dCache  installation: %TWISTY%<pre class="file">
[DEFAULT]
unavailable = UNAVAILABLE
default = UNAVAILABLE
disable = False
enable = True
localhost = fg0x5.fnal.gov
admin_email = tlevshin@fnal.gov
osg_location = /opt/vdt_cemon

[Site Information]
resource =  FNAL-ITB-GIP-TEST1
resource_group =  FNAL-ITB-GIP-TEST1
sponsor = cms:100
site_policy = http://www.uscms.org/SoftwareComputing/Grid/Policy/OSG_VO.html
contact = cms-t1@fnal.gov
city = Batavia, IL
country = USA
longitude = -88.2546
latitude = 41.8412
group = OSG-ITB
email = %(admin_email)s
host_name = %(localhost)s

[Storage]
se_available = %(enable)s
default_se = cmssrm.fnal.gov

[GIP]
advertise_gsiftp = %(disable)s

[SE dCache]
enabled = True
name = cmssrm.fnal.gov
srm_endpoint = httpg://gwdca04.fnal.gov:8443/srm/managerv2
provider_implementation = dcache19
implementation = dcache
version = 1.9.5-19
default_path = /
infoprovider_endpoint = http://gwdca06.fnal.gov/site/info
</pre>%ENDTWISTY% 
   * !BeStMan installation:%TWISTY%<pre class="file">
[DEFAULT]
unavailable = UNAVAILABLE
default = UNAVAILABLE
disable = False
enable = True
localhost = fg0x5.fnal.gov
admin_email = tlevshin@fnal.gov
osg_location = /opt/vdt_cemon

[Site Information]
resource = FNAL-ITB-GIP-TEST1
resource_group = FNAL-ITB-GIP-TEST1
sponsor = cms:100
site_policy = http://www.uscms.org/SoftwareComputing/Grid/Policy/OSG_VO.html
contact = tlevshin@fnal.gov
city = Batavia, IL
country = USA
longitude = -88.2546
latitude = 41.8412
group = OSG-ITB
email = %(admin_email)s
host_name = %(localhost)s

[Storage]
se_available = %(enable)s
default_se = fg0x5.fnal.gov

[GIP]
advertise_gsiftp = %(disable)s
advertise_accesspoints = False

[SE BeStMan]
enabled = True
name = fg0x5.fnal.gov
srm_endpoint = httpg://fg0x5.fnal.gov:10443/srm/v2/server
provider_implementation = BeStMan
implementation = BeStMan
version = 2.2.2.0.0
default_path = /cache
</pre>%ENDTWISTY%

Next step is to prepare ==run_gip.sh== script: 
<pre class="rootscreen"><code># cd gip/bin
# cp run_gip.sh.se_only.example run_gip.sh
# vi run_gip.sh</code></pre>modify ==run_gip.sh== accordingly:<pre class="file">
export VDT_LOCATION=%RED%VDT_LOCATION%ENDCOLOR%
export GIP_LOCATION=%RED%VDT_LOCATION%ENDCOLOR%/gip
$GIP_LOCATION/bin/gip_info
</pre>

You will need to set right permission on ==$VDT_LOCATION/gip/var== directory. It should belong to the user that is running tomcat service (==daemon==).<pre class="rootscreen">
<code># cd $VDT_LOCATION/gip
# chown daemon.daemon var</code>
</pre>


%ENDSECTION{"StandAloneSE"}%

%ENDSECTION{"All"}%
%STOPINCLUDE%

---+ Comments
https://twiki.grid.iu.edu/bin/view/ReleaseDocumentation/GipConfiguration
| PM2RPM_TASK = CE | Main.RobertEngel | 28 Aug 2011 - 06:06 |
%COMMENT{type="tableappend"}%

<!-- CONTENT MANAGEMENT PROJECT
############################################################################################################
 DEAR DOCUMENT OWNER
 ===================

 Thank you for claiming ownership for this document! Please fill in your FirstLast name here:
   * Local OWNER = AnthonyTiradani 

 Please define the document area, choose one of the defined areas from the next line
 DOC_AREA = (ComputeElement|Storage|VO|Security|User|Monitoring|General|Trash/Trash/Integration|Operations|Tier3)
   * Local DOC_AREA       = ComputeElement

 define the primary role the document serves, choose one of the defined roles from the next line
 DOC_ROLE = (EndUser|Student|Developer|SysAdmin|VOManager)
   * Local DOC_ROLE       = SysAdmin

 Please define the document type, choose one of the defined types from the next line
 DOC_TYPE = (Troubleshooting|Training|Installation|HowTo|Planning|Navigation|Knowledge)
   * Local DOC_TYPE       = Installation

  Please define if this document in general needs to be reviewed before release ( %YES% | %NO% )
   * Local INCLUDE_REVIEW = %YES%

 Please define if this document in general needs to be tested before release ( %YES% | %NO% )
   * Local INCLUDE_TEST   = %NO%

 change to %YES% once the document is ready to be reviewed and back to %NO% if that is not the case
   * Local REVIEW_READY   = %YES%


 change to %YES% only if the document has passed the review and the test (if applicable) and is ready for release
   * Local RELEASE_READY  = %NO%


 DEAR DOCUMENT REVIEWER
 ======================

 Thank for reviewing this document! Please fill in your FirstLast name here:
   * Local REVIEWER       = BrianBockelman

 Please define the review status for this document to be in progress ( %IN_PROGRESS% ), failed ( %NO% ) or passed ( %YES% )
   * Local REVIEW_PASSED  = %IN_PROGRESS%


############################################################################################################
-->




-- Main.AnthonyTiradani - 10 Jan 2012