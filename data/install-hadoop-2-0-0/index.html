<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Install HDFS - OSG Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Install HDFS";
    var mkdocs_page_input_path = "data/install-hadoop-2-0-0.md";
    var mkdocs_page_url = "/data/install-hadoop-2-0-0/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-69012-29', 'opensciencegrid.github.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> OSG Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
  
    <a class="" href="../..">Home</a>
  
	    </li>
          
            <li class="toctree-l1">
		
  
    <a class="" href="../../site-planning/">Site Planning</a>
  
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Compute Element</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../compute-element/htcondor-ce-overview/">HTCondor-CE Overview</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../compute-element/install-htcondor-ce/">Install HTCondor-CE</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../compute-element/job-router-recipes/">Job Router Recipes</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../compute-element/troubleshoot-htcondor-ce/">Troubleshooting HTCondor-CE</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../compute-element/submit-htcondor-ce/">Submitting to HTCondor-CE</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Worker Node</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../worker-node/using-wn/">Worker Node Overview</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-wn/">Worker Node via RPM</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-wn-tarball/">Worker Node via Tarball</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-wn-oasis/">Worker Node via OASIS</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-cvmfs/">CVMFS</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-singularity/">Singularity</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Data - HTTP & CVMFS</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../frontier-squid/">Install HTTP Cache</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../update-oasis/">Update OASIS Shared Repo</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../external-oasis-repos/">Install an OASIS Repo</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Data - POSIX Storage</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../gridftp/">Install GridFTP Server</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../load-balanced-gridftp/">Install Load Balanced GridFTP</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Data - HDFS</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../hadoop-overview/">HDFS Overview</a>
  
        </li>
        <li class=" current">
          
  
    <a class="current" href="./">Install HDFS</a>
  
  <ul class="subnav">
      
    <li class="toctree-l3"><a href="#hadoop-200-cdh4">Hadoop 2.0.0 (CDH4)</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#introduction">Introduction</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#requirements">Requirements</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#architecture">Architecture</a></li>
        
            <li><a class="toctree-l4" href="#host-and-os">Host and OS</a></li>
        
            <li><a class="toctree-l4" href="#users">Users</a></li>
        
            <li><a class="toctree-l4" href="#certificates">Certificates</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#installation">Installation</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#namenode-installation">Namenode Installation</a></li>
        
            <li><a class="toctree-l4" href="#secondary-namenode-installation">Secondary Namenode Installation</a></li>
        
            <li><a class="toctree-l4" href="#datanode-installation">Datanode Installation</a></li>
        
            <li><a class="toctree-l4" href="#clientfuse-installation">Client/FUSE Installation</a></li>
        
            <li><a class="toctree-l4" href="#standalone-gridftp-node-installation">Standalone Gridftp Node Installation</a></li>
        
            <li><a class="toctree-l4" href="#srm-node-installation">SRM Node Installation</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#configuration">Configuration</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#hadoop-configuration">Hadoop Configuration</a></li>
        
            <li><a class="toctree-l4" href="#fuse-client-configuration">FUSE Client Configuration</a></li>
        
            <li><a class="toctree-l4" href="#creating-vo-and-user-areas">Creating VO and User Areas</a></li>
        
            <li><a class="toctree-l4" href="#gridftp-configuration">GridFTP Configuration</a></li>
        
            <li><a class="toctree-l4" href="#gridftp-gratia-transfer-probe-configuration">GridFTP Gratia Transfer Probe Configuration</a></li>
        
            <li><a class="toctree-l4" href="#bestman-configuration">BeStMan Configuration</a></li>
        
            <li><a class="toctree-l4" href="#hadoop-storage-probe-configuration">Hadoop Storage Probe Configuration</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#running-services">Running Services</a></li>
    

    <li class="toctree-l3"><a href="#validation_1">Validation</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#gridftp-validation">GridFTP Validation</a></li>
        
            <li><a class="toctree-l4" href="#bestman-validation">BeStMan Validation</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#troubleshooting">Troubleshooting</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#hadoop">Hadoop</a></li>
        
            <li><a class="toctree-l4" href="#fuse">FUSE</a></li>
        
            <li><a class="toctree-l4" href="#gridftp">GridFTP</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#file-locations">File Locations</a></li>
    

    <li class="toctree-l3"><a href="#known-issues">Known Issues</a></li>
    
        <ul>
        
            <li><a class="toctree-l4" href="#replicas">Replicas</a></li>
        
            <li><a class="toctree-l4" href="#copyfromlocal-java-ioexception">copyFromLocal java IOException</a></li>
        
        </ul>
    

    <li class="toctree-l3"><a href="#how-to-get-help">How to get Help?</a></li>
    

    <li class="toctree-l3"><a href="#references">References</a></li>
    

  </ul>
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Data - XRootD</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../xrootd-overview/">XRootD Overview</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../install-xrootd/">Install XRootD</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../install-xroot-client/">Install XRootD Client</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Security</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../security/lcmaps-voms-authentication/">LCMAPS-VOMS authentication</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../security/host-certs/">Host Certificates</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../security/user-certs/">User Certificates</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../security/certificate-management/">Certificate Management Reference</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Common</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../common/install-best-practices/">Install Best Practices</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../common/ca/">CA Certificates</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../common/yum/">Yum Repos</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../common/openjdk7/">Java 7</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/configuration-with-osg-configure/">Configuration with OSG-Configure</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Other</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../other/gsissh/">GSI-enabled SSH</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/install-gwms-frontend/">GlideinWMS Frontend</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/install-cvmfs-stratum1/">Install a CVMFS Stratum 1</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/network-performance-toolkit/">Network Performance Toolkit</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/schedd-filebeats/">Transfer log filebeats installation</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Release Information</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../release/notes/">Release Notes</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../release/release_series/">OSG Release Series</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../release/supported_platforms/">Supported Platforms</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/install-osg-upcoming-software/">OSG upcoming software</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../release/yum-basics/">Yum Basics</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../release/signing/">RPM Signing</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../release/contrib/">Contrib Software</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Monitoring</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../../monitoring/install-rsv/">Install RSV</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../monitoring/advanced-rsv-configuration/">Advanced RSV Configuration</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../monitoring/rsv-control/">Manage RSV via rsv-control</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../monitoring/install-rsv-gwms-tester/">RSV GlideinWMS Tester</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  <span class="caption-text">Deprecated Software</span>
  <ul class="subnav">
        <li class="">
          
  
    <a class="" href="../bestman-install/">BeStMan Install</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../install-bestman-xrootd/">BeStMan Xrootd SE</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../security/edg-mkgridmap/">EDG-Mkgridmap Install</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../security/install-gums/">GUMS</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../worker-node/install-glexec/">Worker node glexec Install</a>
  
        </li>
        <li class="">
          
  
    <a class="" href="../../other/install-voms/">Install VOMS</a>
  
        </li>
  </ul>
	    </li>
          
            <li class="toctree-l1">
		
  
    <a class="" href="../../common/help/">Get Help</a>
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
            <li class="toctree-l1">
		
  
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">OSG Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Data - HDFS &raquo;</li>
        
      
    
    <li>Install HDFS</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/opensciencegrid/docs/edit/master/docs/data/install-hadoop-2-0-0.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="hadoop-200-cdh4">Hadoop 2.0.0 (CDH4)<a class="headerlink" href="#hadoop-200-cdh4" title="Permanent link">&para;</a></h1>
<p>The purpose of this document is to provide Hadoop based SE administrators the information on how to prepare, install
and validate OSG storage based on the Hadoop Distributed File System (HDFS).  Currently, OSG supports a patched version
of the CDH4 distribution of HDFS.</p>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p><a href="http://hadoop.apache.org/">Hadoop Distributed File System</a> (HDFS) is a scalable reliable distributed file system developed in the Apache project. It is based on map-reduce framework and design of the Google file system. The VDT distribution of Hadoop includes all components needed to operate a multi-terabyte storage site. Included are:</p>
<ul>
<li><a href="http://hadoop.apache.org/">Apache Hadoop</a></li>
<li>A <a href="http://fuse.sourceforge.net/">FUSE interface</a> for localized POSIX access.</li>
<li>GridFTP and XRootD for offsite transfers</li>
</ul>
<p>The OSG packaging and distribution of Hadoop is based on YUM. All components are packaged as RPMs and are available
from the OSG repositories. It is also recommended that you enable <a href="http://fedoraproject.org/wiki/EPEL">EPEL</a> repos.</p>
<div class="admonition warning">
<p class="admonition-title">Deprecation Notice</p>
<p>This installation page additionally includes integration with both the BestMan SRM server and GUMS authorization
service.  Both of these are deprecated as of June 2017 and support will end May 2018.</p>
</div>
<h1 id="requirements">Requirements<a class="headerlink" href="#requirements" title="Permanent link">&para;</a></h1>
<h2 id="architecture">Architecture<a class="headerlink" href="#architecture" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There are several important components to a storage element installation. Throughout this document, it will be stated which node the relevant installation instructions apply to. It can apply to one of the following:</p>
</div>
<ul>
<li><strong>Namenode</strong>: You will have at least one namenode. The name node functions as the directory server and coordinator of the hadoop cluster. It houses all the meta-data for the hadoop cluster. <span style="color:red">The namenode and secondary namenode need to have a directory that they can both access on a shared filesystem so that they can exchange filesystem checkpoints.</span></li>
<li><strong>Secondary Namenode</strong>: This is a secondary machine that periodically merges updates to the HDFS file system back into the fsimage. This dramatically improves startup and restart times.</li>
<li><strong>Datanode</strong>: You will have many datanodes. Each data node stores large blocks of files to be stored on the hadoop cluster.</li>
<li><strong>Client</strong>: This is a documentation shorthand that refers to any machine with the hadoop client commands and <a href="http://fuse.sourceforge.net/">FUSE</a> mount. Any machine that needs a FUSE mount to access data in a POSIX-like fashion will need this.</li>
<li><strong>GridFTP node</strong>: This is a node with <a href="../gridftp">Globus GridFTP</a>. The GridFTP server for Hadoop can be very memory-hungry, up to 500MB/transfer in the default configuration. You should plan accordingly to provision enough GridFTP servers to handle the bandwidth that your site can support.</li>
</ul>
<p>Note that these components are not necessarily mutually exclusive. For instance, you may consider having your GridFTP server co-located on the SRM node. Alternatively, you can locate a client (or even a GridFTP node) co-located on each data node. That way, each data node also acts as an access point to the hadoop cluster.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Total installation time, on an average, should not exceed 8 to 24 man-hours. If your site needs further assistance
to help expedite, please email <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#104;&#101;&#108;&#112;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;">&#104;&#101;&#108;&#112;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;</a>.</p>
</div>
<h2 id="host-and-os">Host and OS<a class="headerlink" href="#host-and-os" title="Permanent link">&para;</a></h2>
<p>Hadoop will run anywhere that Java is supported (including Solaris). However, these instructions are for RedHat derivants (including Scientific Linux) because of the RPM based installation. The current supported Operating Systems supported by the OSG are Red Hat Enterprise Linux 6, 7, and variants (see <a href="../../release/supported_platforms">details...</a>).</p>
<p>The HDFS prerequisites are:</p>
<ul>
<li>Minimum of 1 headnode (the namenode)</li>
<li>At least one node which will hold data, preferably at least 2. Most sites will have 20 to 200 datanodes.</li>
<li>Working Yum and RPM installation on every system.</li>
<li><code>fuse</code> kernel module and <code>fuse-libs</code>.</li>
<li>Java RPM. If java isn't already installed we supply the Oracle jdk 1.6.0 rpm and it will come in as a dependency. Oracle jdk is currently the only jdk supported by OSG so we highly recommend you use the version supplied.</li>
</ul>
<h2 id="users">Users<a class="headerlink" href="#users" title="Permanent link">&para;</a></h2>
<p>This installation will create following users unless they are already created.</p>
<table>
<thead>
<tr>
<th align="left">User</th>
<th align="left">Comment</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><code>hdfs</code></td>
<td align="left">Used by Hadoop to store data blocks and meta-data</td>
</tr>
</tbody>
</table>
<p>For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.</p>
<p>For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.</p>
<p>For gums users, this means that each user that can be authenticated by gums should be created on the server.</p>
<p>Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.</p>
<h2 id="certificates">Certificates<a class="headerlink" href="#certificates" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th align="left">Certificate</th>
<th align="left">User that owns certificate</th>
<th align="left">Path to certificate</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Host certificate</td>
<td align="left"><code>root</code></td>
<td align="left"><code>/etc/grid-security/hostcert.pem</code> <br> <code>/etc/grid-security/hostkey.pem</code></td>
</tr>
</tbody>
</table>
<p><a href="../../security/host-certs">Instructions</a> to request a service certificate.</p>
<p>You will also need a copy of CA certificates; see the <a href="../../common/ca">CA certificate installation document</a> if you are
unfamiliar with this procedure.  This is needed by GridFTP and SRM nodes, but it is recommended for all nodes in the
cluster.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Make sure you enable <a href="../../common/ca#install-fetch-crl">fetch-crl</a></p>
</div>
<h1 id="installation">Installation<a class="headerlink" href="#installation" title="Permanent link">&para;</a></h1>
<p>Installation depends on the node you are installing:</p>
<h2 id="namenode-installation">Namenode Installation<a class="headerlink" href="#namenode-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-namenode
</pre></div>


<h2 id="secondary-namenode-installation">Secondary Namenode Installation<a class="headerlink" href="#secondary-namenode-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-secondarynamenode
</pre></div>


<h2 id="datanode-installation">Datanode Installation<a class="headerlink" href="#datanode-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-datanode
</pre></div>


<h2 id="clientfuse-installation">Client/FUSE Installation<a class="headerlink" href="#clientfuse-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-client
</pre></div>


<h2 id="standalone-gridftp-node-installation">Standalone Gridftp Node Installation<a class="headerlink" href="#standalone-gridftp-node-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-gridftp
</pre></div>


<p>If you are using GUMS authorization, the follow rpms need to be installed as well:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install lcmaps-plugins-gums-client
<span class="gp">root@host #</span> yum install lcmaps-plugins-basic
</pre></div>


<h2 id="srm-node-installation">SRM Node Installation<a class="headerlink" href="#srm-node-installation" title="Permanent link">&para;</a></h2>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> yum install osg-se-hadoop-srm
</pre></div>


<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using a single system to host the SRM software and the gridftp node, you'll also need to install the <code>osg-se-hadoop-gridftp</code> rpm as well.</p>
</div>
<h1 id="configuration">Configuration<a class="headerlink" href="#configuration" title="Permanent link">&para;</a></h1>
<h2 id="hadoop-configuration">Hadoop Configuration<a class="headerlink" href="#hadoop-configuration" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Needed by: Hadoop namenode, Hadoop datanodes, Hadoop client, GridFTP, SRM</p>
</div>
<p>Hadoop configuration is needed by every node in the hadoop cluster. However, in most cases, you can do the configuration once and copy it to all nodes in the cluster (possibly using your favorite configuration management tool). Special configuration for various special components is given in the below sections.</p>
<p>Hadoop configuration is stored in <code>/etc/hadoop/conf</code>. However, by default, these files are mostly blank. OSG provides a sample configuration in <code>/etc/hadoop/conf.osg</code> with most common values filled in. You will need to copy these into <code>/etc/hadoop/conf</code> before they become active. Please let us know if there are any common values that should be added/changed across the whole grid. You will likely need to modify <code>hdfs-site.xml</code> and <code>core-site.xml</code>. Review all the settings in these files, but listed below are common settings to modify:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>File</td>
<td>Setting</td>
<td>Example</td>
<td>Comments</td>
</tr>
<tr>
<td><code>core-site.xml</code></td>
<td>fs.default.name</td>
<td>hdfs://namenode.domain.tld.:9000</td>
<td>This is the address of the namenode</td>
</tr>
<tr>
<td><code>core-site.xml</code></td>
<td>hadoop.tmp.dir</td>
<td>/data/scratch</td>
<td>Scratch temp directory used by Hadoop</td>
</tr>
<tr>
<td><code>core-site.xml</code></td>
<td>hadoop.log.dir</td>
<td>/var/log/hadoop-hdfs</td>
<td>Log directory used by Hadoop</td>
</tr>
<tr>
<td><code>core-site.xml</code></td>
<td>dfs.umaskmode</td>
<td>002</td>
<td>umask for permissions used by default</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.block.size</td>
<td>134217728</td>
<td>Block size: 128MB by default</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.replication</td>
<td>2</td>
<td>Default replication factor. Generally the same as dfs.replication.min/max</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.datanode.du.reserved</td>
<td>100000000</td>
<td>How much free space hadoop will reserve for non-Hadoop usage</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.datanode.handler.count</td>
<td>20</td>
<td>Number of server threads for datanodes. Increase if you have many more client connections</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.namenode.handler.count</td>
<td>40</td>
<td>Number of server threads for namenodes. Increase if you need more connections</td>
</tr>
<tr>
<td><code>hdfs-site.xml</code></td>
<td>dfs.http.address</td>
<td>namenode.domain.tld.:50070</td>
<td>Web address for dfs health monitoring page</td>
</tr>
</tbody>
</table>
<p>See <a href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a> for more parameters to configure.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Namenodes must have a <code>/etc/hosts_exclude</code> present</p>
</div>
<h3 id="special-namenode-instructions-for-brand-new-installs">Special namenode instructions for brand new installs<a class="headerlink" href="#special-namenode-instructions-for-brand-new-installs" title="Permanent link">&para;</a></h3>
<p>If this is a new installation (<span style="color:red">and only if this is a brand new installation<span class="twiki-macro ENDCOLOR"></span>), you should run the following command as the <code>hdfs</code> user. (Otherwise, be sure to <code>chown</code> your storage directory to hdfs after running):</p>
<div class="codehilite"><pre><span></span><span class="go">hadoop namenode -format</span>
</pre></div>


<p>This will initialize the storage directory on your namenode</p>
<h2 id="fuse-client-configuration">FUSE Client Configuration<a class="headerlink" href="#fuse-client-configuration" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Needed by: Hadoop client and SRM node. Recommended but not neccessary for GridFTP nodes.</p>
</div>
<p>A FUSE mount is required on any node that you would like to use standard POSIX-like commands on the Hadoop filesystem. FUSE (or "file system in user space") is a way to access Hadoop using typical UNIX directory commands (ie POSIX-like access). Note that not all advanced functions of a full POSIX-compliant file system are necessarily available.</p>
<p>FUSE is typically installed as part of this installation, but, if you are running a customized or non-standard system, make sure that the fuse kernel module is installed and loaded with <code>modprobe fuse</code>.</p>
<p>You can add the FUSE to be mounted at boot time by adding the following line to <code>/etc/fstab</code>:</p>
<div class="codehilite"><pre><span></span>hadoop-fuse-dfs# <span style="color:red">/mnt/hadoop</span> fuse server=<span style="color:red">namenode.host</span>,port=9000,rdbuffer=131072,allow_other 0 0
</pre></div>


<p>Be sure to change the <code>/mnt/hadoop</code> mount point and <code>namenode.host</code> to match your local configuration. To match the help documents, we recommend using <code>/mnt/hadoop</code> as your mountpoint.</p>
<p>Once your <code>/etc/fstab</code> is updated, to mount FUSE run:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> mkdir /mnt/hadoop
<span class="gp">root@host #</span> mount /mnt/hadoop
</pre></div>


<p>When mounting the HDFS FUSE mount, you will see the following harmless warnings printed to the screen:</p>
<div class="codehilite"><pre><span></span><span class="gp">#</span> mount /mnt/hadoop
<span class="go">INFO fuse_options.c:162 Adding FUSE arg /mnt/hadoop</span>
<span class="go">INFO fuse_options.c:110 Ignoring option allow_other</span>
</pre></div>


<p>If you have troubles mounting FUSE refer to <a href="#running-fuse-in-debug-mode">Running FUSE in Debug Mode</a> in the Troubleshooting section.</p>
<h2 id="creating-vo-and-user-areas">Creating VO and User Areas<a class="headerlink" href="#creating-vo-and-user-areas" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Grid Users are needed by GridFTP and SRM nodes. VO areas are common to all nodes.</p>
</div>
<p>For this package to function correctly, you will have to create the users needed for grid operation. Any user that can be authenticated should be created.</p>
<p>For grid-mapfile users, each line of the grid-mapfile is a certificate/user pair. Each user in this file should be created on the server.</p>
<p>For gums users, this means that each user that can be authenticated by gums should be created on the server.</p>
<p>Note that these users must be kept in sync with the authentication method. For instance, if new users or rules are added in gums, then new users should also be added here.</p>
<p>Prior to starting basic day-to-day operations, it is important to create dedicated areas for each VO and/or user. This is similar to user management in simple UNIX filesystems. Create (and maintain) usernames and groups with UIDs and GIDs on <strong>all nodes</strong>. These are maintained in basic system files such as <code>/etc/passwd</code> and <code>/etc/group</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the examples below It is assumed a FUSE mount is set to <code>/mnt/hadoop</code>. As an alternative <code>hadoop fs</code> commands could have been used.</p>
</div>
<p>For clean HDFS operations and filesystem management:</p>
<p>(a) Create top-level VO subdirectories under <code>/mnt/hadoop</code>.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> mkdir /mnt/hadoop/cms
<span class="gp">root@host #</span> mkdir /mnt/hadoop/dzero
<span class="gp">root@host #</span> mkdir /mnt/hadoop/sbgrid
<span class="gp">root@host #</span> mkdir /mnt/hadoop/fermigrid
<span class="gp">root@host #</span> mkdir /mnt/hadoop/cmstest
<span class="gp">root@host #</span> mkdir /mnt/hadoop/osg
</pre></div>


<p>(b) Create individual top-level user areas, under each VO area, as needed.</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> mkdir -p /mnt/hadoop/cms/store/user/tanyalevshina
<span class="gp">root@host #</span> mkdir -p /mnt/hadoop/cms/store/user/michaelthomas
<span class="gp">root@host #</span> mkdir -p /mnt/hadoop/cms/store/user/brianbockelman
<span class="gp">root@host #</span> mkdir -p /mnt/hadoop/cms/store/user/douglasstrain
<span class="gp">root@host #</span> mkdir -p /mnt/hadoop/cms/store/user/abhisheksinghrana
</pre></div>


<p>(c) Adjust username:group ownership of each area.</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> chown -R cms:cms /mnt/hadoop/cms
<span class="gp">root@host #</span> chown -R sam:sam /mnt/hadoop/dzero

<span class="gp">root@host #</span> chown -R michaelthomas:cms /mnt/hadoop/cms/store/user/michaelthomas
</pre></div>


<h2 id="gridftp-configuration">GridFTP Configuration<a class="headerlink" href="#gridftp-configuration" title="Permanent link">&para;</a></h2>
<p>gridftp-hdfs reads the Hadoop configuration file to learn how to talk to Hadoop. By now, you should have followed the instruction for installing hadoop as detailed in the previous section as well as created the proper users/directories.</p>
<p>The default settings in <code>/etc/gridftp.conf</code> along with <code>/etc/gridftp.d/gridftp-hdfs.conf</code> are used by the init.d script and should be ok for most installations. The file <code>/etc/gridftp-hdfs/gridftp-debug.conf</code> is used by <code>/usr/bin/gridftp-hdfs-standalone</code> for starting up the GridFTP server in a testing mode. Any additional config files under <code>/etc/gridftp.d</code> will be used for both the init.d and standalone GridFTP server. <code>/etc/sysconfig/gridftp-hdfs</code> contains additional site-specific environment variables that are used by the gridftp-hdfs dsi module in both the init.d and standalone GridFTP server. Some of the environment variables that can be used in <code>/etc/sysconfig/gridftp-hdfs</code> include:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Option Name</td>
<td>Needs Editing?</td>
<td>Suggested value</td>
</tr>
<tr>
<td>GRIDFTP_HDFS_REPLICA_MAP</td>
<td>No</td>
<td>File containing a list of paths and replica values for setting the default # of replicas for specific file paths</td>
</tr>
<tr>
<td>GRIDFTP_BUFFER_COUNT</td>
<td>No</td>
<td>The number of 1MB memory buffers used to reorder data streams before writing them to Hadoop</td>
</tr>
<tr>
<td>GRIDFTP_FILE_BUFFER_COUNT</td>
<td>No</td>
<td>The number of 1MB file-based buffers used to reorder data streams before writing them to Hadoop</td>
</tr>
<tr>
<td>GRIDFTP_SYSLOG</td>
<td>No</td>
<td>Set this to 1 in case if you want to send transfer activity data to syslog (only used for the HadoopViz application)</td>
</tr>
<tr>
<td>GRIDFTP_HDFS_MOUNT_POINT</td>
<td>Maybe</td>
<td>The location of the FUSE mount point used during the Hadoop installation. Defaults to /mnt/hadoop. This is needed so that gridftp-hdfs can convert fuse paths on the incoming URL to native Hadoop paths. <strong>Note:</strong> this does not imply you need FUSE mounted on GridFTP nodes!</td>
</tr>
<tr>
<td>GRIDFTP_LOAD_LIMIT</td>
<td>No</td>
<td>GridFTP will refuse to start new transfers if the load on the GridFTP host is higher than this number; defaults to 20.</td>
</tr>
<tr>
<td>TMPDIR</td>
<td>Maybe</td>
<td>The temp directory where the file-based buffers are stored. Defaults to /tmp.</td>
</tr>
</tbody>
</table>
<p><code>/etc/sysconfig/gridftp-hdfs</code> is also a good place to increase per-process resource limits. For example, many installations will require more than the default number of open files (<code>ulimit -n</code>).</p>
<p>Lastly, you will need to configure an authentication mechanism for GridFTP.</p>
<h3 id="configuring-authentication">Configuring authentication<a class="headerlink" href="#configuring-authentication" title="Permanent link">&para;</a></h3>
<p>For information on how to configure authentication for your GridFTP installation, please refer to the <a href="../gridftp#configuring-authentication">configuring authentication section of the GridFTP guide</a>.</p>
<h2 id="gridftp-gratia-transfer-probe-configuration">GridFTP Gratia Transfer Probe Configuration<a class="headerlink" href="#gridftp-gratia-transfer-probe-configuration" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Needed by GridFTP node only.</p>
</div>
<p>The Gratia probe requires the file <code>user-vo-map</code> to exist and be up to date. This file is created and updated by the <code>gums-client</code> package that comes in as a dependency of <code>osg-se-hadoop-gridftp</code> or <code>osg-gridftp-hdfs</code>. Assuming you installed GridFTP using the <code>osg-se-hadoop-gridftp</code> rpm, the Gratia Transfer Probe will already be installed.</p>
<p>Here are the most relevant file and directory locations:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Purpose</td>
<td>Needs Editing?</td>
<td>Location</td>
</tr>
<tr>
<td>Probe Configuration</td>
<td>Yes</td>
<td>/etc/gratia/gridftp-transfer/ProbeConfig</td>
</tr>
<tr>
<td>Probe Executables</td>
<td>No</td>
<td>/usr/share/gratia/gridftp-transfer</td>
</tr>
<tr>
<td>Log files</td>
<td>No</td>
<td>/var/log/gratia</td>
</tr>
<tr>
<td>Temporary files</td>
<td>No</td>
<td>/var/lib/gratia/tmp</td>
</tr>
<tr>
<td>Gums configuration</td>
<td>Yes</td>
<td>/etc/gums/gums-client.properties</td>
</tr>
</tbody>
</table>
<p>The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by the file</p>
<div class="codehilite"><pre><span></span>/etc/gratia/gridftp-transfer/ProbeConfig
</pre></div>


<p>This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Attribute</td>
<td>Needs Editing</td>
<td>Value</td>
</tr>
<tr>
<td>ProbeName</td>
<td>Maybe</td>
<td>This should be set to "gridftp-transfer:<hostname>", where <hostname> is the fully-qualified domain name of your gridftp host.</td>
</tr>
<tr>
<td>CollectorHost</td>
<td>Maybe</td>
<td>Set to the hostname and port of the central collector. By default it sends to the OSG collector. See below.</td>
</tr>
<tr>
<td>SiteName</td>
<td>Yes</td>
<td>Set to the resource group name of your site as registered in OIM.</td>
</tr>
<tr>
<td>GridftpLogDir</td>
<td>Yes</td>
<td>Set to /var/log, or wherever your current gridftp logs are located</td>
</tr>
<tr>
<td>Grid</td>
<td>Maybe</td>
<td>Set to "ITB" if this is a test resource; otherwise, leave as OSG.</td>
</tr>
<tr>
<td>UserVOMapFile</td>
<td>No</td>
<td>This should be set to /var/lib/osg/user-vo-map; see below for information about this file.</td>
</tr>
<tr>
<td>SuppressUnknownVORecords</td>
<td>Maybe</td>
<td>Set to 1 to suppress any records that can't be matched to a VO; 0 is strongly recommended.</td>
</tr>
<tr>
<td>SuppressNoDNRecords</td>
<td>Maybe</td>
<td>Set to 1 to suppress records that can't be matched to a DN; 0 is strongly recommended.</td>
</tr>
<tr>
<td>EnableProbe</td>
<td>Yes</td>
<td>Set to 1 to enable the probe.</td>
</tr>
</tbody>
</table>
<h3 id="selecting-a-collector-host">Selecting a collector host<a class="headerlink" href="#selecting-a-collector-host" title="Permanent link">&para;</a></h3>
<p>The collector is the central server which logs the GridFTP transfers into a database. There are usually three options:</p>
<ol>
<li><strong>OSG Transfer Collector</strong>: This is the primary collector for transfers in the OSG. Use <code>CollectorHost="gratia-osg-prod.opensciencegrid.org:80"</code>.</li>
<li><strong>OSG-ITB Transfer Collector</strong>: This is the test collector for transfers in the OSG. Use <code>CollectorHost=" gratia-osg-itb.opensciencegrid.org:80"</code>.</li>
</ol>
<h3 id="validation">Validation<a class="headerlink" href="#validation" title="Permanent link">&para;</a></h3>
<p>Run the Gratia probe once by hand to check for functionality:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> /usr/share/gratia/gridftp-transfer/GridftpTransferProbeDriver
</pre></div>


<p>Look for any abnormal termination and report it if it is a non-trivial site issue. Look in the log files in <code>/var/log/gratia/&lt;date&gt;.log</code> and make sure there are no error messages printed.</p>
<h2 id="bestman-configuration">BeStMan Configuration<a class="headerlink" href="#bestman-configuration" title="Permanent link">&para;</a></h2>
<div class="admonition warning">
<p class="admonition-title">Deprecation Warning</p>
<p>As of June 2017, support for the <code>bestman2</code> software has been deprecated.  Support will end in May 2018</p>
</div>
<p>See the <a href="../bestman-install#authorization">bestman2 documentation</a> for instructions on how to install and configure
<code>bestman2</code>.</p>
<p>BeStMan2 SRM uses the Hadoop FUSE mount to perform namespace operations via common POSIX tools, such as <code>mkdir</code>, <code>rm</code>,
and <code>ls</code>.  It is <strong>not</strong> necessary (or even recommended) to start any HDFS services on the <code>bestman2</code> host.</p>
<p>Make sure that you modify <code>localPathListAllowed</code> to use the Hadoop mount in <code>/etc/bestman2/conf/bestman2.rc</code>.</p>
<h2 id="hadoop-storage-probe-configuration">Hadoop Storage Probe Configuration<a class="headerlink" href="#hadoop-storage-probe-configuration" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is only needed by the Hadoop Namenode</p>
</div>
<p>Here are the most relevant file and directory locations:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Purpose</td>
<td>Needs Editing?</td>
<td>Location</td>
</tr>
<tr>
<td>Probe Configuration</td>
<td>Yes</td>
<td>/etc/gratia/hadoop-storage/ProbeConfig</td>
</tr>
<tr>
<td>Probe Executable</td>
<td>No</td>
<td>/usr/share/gratia/hadoop-storage/hadoop_storage_probe</td>
</tr>
<tr>
<td>Log files</td>
<td>No</td>
<td>/var/log/gratia</td>
</tr>
<tr>
<td>Temporary files</td>
<td>No</td>
<td>/var/lib/gratia/tmp</td>
</tr>
</tbody>
</table>
<p>The RPM installs the Gratia probe into the system crontab, but does not configure it. The configuration of the probe is controlled by two files</p>
<div class="codehilite"><pre><span></span>/etc/gratia/hadoop-storage/ProbeConfig
/etc/gratia/hadoop-storage/storage.cfg
</pre></div>


<h3 id="probeconfig">ProbeConfig<a class="headerlink" href="#probeconfig" title="Permanent link">&para;</a></h3>
<p>This is usually one XML node spread over multiple lines. Note that comments (#) have no effect on this file. You will need to edit the following:</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Attribute</td>
<td>Needs Editing</td>
<td>Value</td>
</tr>
<tr>
<td>CollectorHost</td>
<td>Maybe</td>
<td>Set to the hostname and port of the central collector. By default it sends to the OSG collector. You probably do not want to change it.</td>
</tr>
<tr>
<td>SiteName</td>
<td>Yes</td>
<td>Set to the resource group name of your SE as registered in OIM.</td>
</tr>
<tr>
<td>Grid</td>
<td>Maybe</td>
<td>Set to "ITB" if this is a test resource; otherwise, leave as OSG.</td>
</tr>
<tr>
<td>EnableProbe</td>
<td>Yes</td>
<td>Set to 1 to enable the probe.</td>
</tr>
</tbody>
</table>
<h3 id="storagecfg">storage.cfg<a class="headerlink" href="#storagecfg" title="Permanent link">&para;</a></h3>
<p>This file controls which paths in HDFS should be monitored. This is in the Windows INI format.</p>
<p><strong>Note: for the current version of the storage.cfg, there is an error, and you may need to delete the "probe/" subdirectory for the ProbeConfig location</strong></p>
<div class="codehilite"><pre><span></span>ProbeConfig = /etc/gratia/<span style="color:red">probe/</span>hadoop-storage/ProbeConfig
</pre></div>


<p>For each logical "area" (arbitrarily defined by you), specify both a given name and a list of paths that belong to that area. Unix globs are accepted.</p>
<p>To configure an area named "CMS /store" that monitors the space usage in the paths /user/cms/store/*, one would add the following to the storage.cfg file.</p>
<div class="codehilite"><pre><span></span><span class="k">[Area CMS /store]</span>
<span class="na">Name</span> <span class="o">=</span> <span class="s">CMS /store</span>
<span class="na">Path</span> <span class="o">=</span> <span class="s">/user/cms/store/*</span>
<span class="na">Trim</span> <span class="o">=</span> <span class="s">/user/cms</span>
</pre></div>


<p>For each such area, add a section to your configuration file.</p>
<h4 id="example-file">Example file<a class="headerlink" href="#example-file" title="Permanent link">&para;</a></h4>
<p>Below is a configuration file that includes three distinct areas. Note that you shouldn't have to touch the [Gratia] section if you edited the ProbeConfig above:</p>
<div class="codehilite"><pre><span></span><span class="k">[Gratia]</span>
<span class="na">gratia_location</span> <span class="o">=</span> <span class="s">/opt/vdt/gratia</span>
<span class="na">ProbeConfig</span> <span class="o">=</span> <span class="s">%(gratia_location)s/probe/hadoop-storage/ProbeConfig</span>

<span class="k">[Area /store]</span>
<span class="na">Name</span> <span class="o">=</span> <span class="s">CMS /store</span>
<span class="na">Path</span> <span class="o">=</span> <span class="s">/store/*</span>

<span class="k">[Area /store/user]</span>
<span class="na">Name</span> <span class="o">=</span> <span class="s">CMS /store/user</span>
<span class="na">Path</span> <span class="o">=</span> <span class="s">/store/user/*</span>

<span class="k">[Area /user]</span>
<span class="na">Name</span> <span class="o">=</span> <span class="s">Hadoop /user</span>
<span class="na">Path</span> <span class="o">=</span> <span class="s">/user/*</span>
</pre></div>


<p>*<strong>NOTE These lines in the [gratia] section are wrong and need to be changed to the following by hand for now until the rpm is updated:</strong></p>
<div class="codehilite"><pre><span></span>gratia_location = /etc/gratia
ProbeConfig = %(gratia_location)s/hadoop-storage/ProbeConfig
</pre></div>


<h1 id="running-services">Running Services<a class="headerlink" href="#running-services" title="Permanent link">&para;</a></h1>
<p>Namenode:</p>
<div class="codehilite"><pre><span></span><span class="gp">#</span>Starting namenode
<span class="go">service hadoop-hdfs-namenode start</span>
<span class="gp">#</span>Stopping namenode
<span class="go">service hadoop-hdfs-namenode stop</span>
</pre></div>


<p>Secondary Namenode:</p>
<div class="codehilite"><pre><span></span><span class="gp">#</span>Starting secondary namenode
<span class="go">service hadoop-hdfs-secondarynamenode start</span>
<span class="gp">#</span>Stopping secondary namenode
<span class="go">service hadoop-hdfs-secondarynamenode stop</span>
</pre></div>


<p>Datanode:</p>
<div class="codehilite"><pre><span></span><span class="gp">#</span>Starting namenode
<span class="go">service hadoop-hdfs-datanode start</span>
<span class="gp">#</span>Stopping namenode
<span class="go">service hadoop-hdfs-datanode stop</span>
</pre></div>


<p>GridFTP:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> service globus-gridftp-server start
</pre></div>


<p>To start Gridftp automatically at boot time</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> chkconfig globus-gridftp-server on
</pre></div>


<p>Stopping GridFTP:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> service globus-gridftp-server stop
</pre></div>


<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> service bestman2 start
</pre></div>


<p>To start Bestman automatically at boot time</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> chkconfig bestman2 on
</pre></div>


<h1 id="validation_1">Validation<a class="headerlink" href="#validation_1" title="Permanent link">&para;</a></h1>
<p>The first thing you may want to do after installing and starting your <strong>Namenode</strong> is to verify that the web interface works. In your web browser go to:</p>
<div class="codehilite"><pre><span></span>http://<span style="color:red">namenode.hostname</span>:50070/dfshealth.jsp
</pre></div>


<p>Get familiar with Hadoop commands. Run hadoop with no arguments to see the list of commands.</p>
<p><details>
  <summary>Show detailed ouput</summary>
   <p></p>
<div class="codehilite"><pre><span></span><span class="go">user$ hadoop</span>
<span class="go">Usage: hadoop [--config confdir] COMMAND</span>
<span class="go">where COMMAND is one of:</span>
<span class="go">  namenode -format     format the DFS filesystem</span>
<span class="go">  secondarynamenode    run the DFS secondary namenode</span>
<span class="go">  namenode             run the DFS namenode</span>
<span class="go">  datanode             run a DFS datanode</span>
<span class="go">  dfsadmin             run a DFS admin client</span>
<span class="go">  mradmin              run a Map-Reduce admin client</span>
<span class="go">  fsck                 run a DFS filesystem checking utility</span>
<span class="go">  fs                   run a generic filesystem user client</span>
<span class="go">  balancer             run a cluster balancing utility</span>
<span class="go">  fetchdt              fetch a delegation token from the NameNode</span>
<span class="go">  jobtracker           run the MapReduce job Tracker node</span>
<span class="go">  pipes                run a Pipes job</span>
<span class="go">  tasktracker          run a MapReduce task Tracker node</span>
<span class="go">  job                  manipulate MapReduce jobs</span>
<span class="go">  queue                get information regarding JobQueues</span>
<span class="go">  version              print the version</span>
<span class="go">  jar &lt;jar&gt;            run a jar file</span>
<span class="go">  distcp &lt;srcurl&gt; &lt;desturl&gt; copy file or directories recursively</span>
<span class="go">  archive -archiveName NAME -p &lt;parent path&gt; &lt;src&gt;* &lt;dest&gt; create a hadoop archive</span>
<span class="go">  oiv                  apply the offline fsimage viewer to an fsimage</span>
<span class="go">  classpath            prints the class path needed to get the</span>
<span class="go">                       Hadoop jar and the required libraries</span>
<span class="go">  daemonlog            get/set the log level for each daemon</span>
<span class="go"> or</span>
<span class="go">  CLASSNAME            run the class named CLASSNAME</span>
<span class="go">Most commands print help when invoked w/o parameters.</span>
</pre></div>


<p></p>
</details></p>
<p>For a list of supported filesystem commands:</p>
<p><details>
  <summary>Show 'hadoop fs' detailed ouput</summary>
   <p></p>
<div class="codehilite"><pre><span></span><span class="go">user$ hadoop fs</span>
<span class="go">Usage: java FsShell</span>
<span class="go">           [-ls &lt;path&gt;]</span>
<span class="go">           [-lsr &lt;path&gt;]</span>
<span class="go">           [-df [&lt;path&gt;]]</span>
<span class="go">           [-du &lt;path&gt;]</span>
<span class="go">           [-dus &lt;path&gt;]</span>
<span class="go">           [-count[-q] &lt;path&gt;]</span>
<span class="go">           [-mv &lt;src&gt; &lt;dst&gt;]</span>
<span class="go">           [-cp &lt;src&gt; &lt;dst&gt;]</span>
<span class="go">           [-rm [-skipTrash] &lt;path&gt;]</span>
<span class="go">           [-rmr [-skipTrash] &lt;path&gt;]</span>
<span class="go">           [-expunge]</span>
<span class="go">           [-put &lt;localsrc&gt; ... &lt;dst&gt;]</span>
<span class="go">           [-copyFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span>
<span class="go">           [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]</span>
<span class="go">           [-get [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]</span>
<span class="go">           [-getmerge &lt;src&gt; &lt;localdst&gt; [addnl]]</span>
<span class="go">           [-cat &lt;src&gt;]</span>
<span class="go">           [-text &lt;src&gt;]</span>
<span class="go">           [-copyToLocal [-ignoreCrc] [-crc] &lt;src&gt; &lt;localdst&gt;]</span>
<span class="go">           [-moveToLocal [-crc] &lt;src&gt; &lt;localdst&gt;]</span>
<span class="go">           [-mkdir &lt;path&gt;]</span>
<span class="go">           [-setrep [-R] [-w] &lt;rep&gt; &lt;path/file&gt;]</span>
<span class="go">           [-touchz &lt;path&gt;]</span>
<span class="go">           [-test -[ezd] &lt;path&gt;]</span>
<span class="go">           [-stat [format] &lt;path&gt;]</span>
<span class="go">           [-tail [-f] &lt;file&gt;]</span>
<span class="go">           [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span>
<span class="go">           [-chown [-R] [OWNER][:[GROUP]] PATH...]</span>
<span class="go">           [-chgrp [-R] GROUP PATH...]</span>
<span class="go">           [-help [cmd]]</span>

<span class="go">Generic options supported are</span>
<span class="go">-conf &lt;configuration file&gt;     specify an application configuration file</span>
<span class="go">-D &lt;property=value&gt;            use value for given property</span>
<span class="go">-fs &lt;local|namenode:port&gt;      specify a namenode</span>
<span class="go">-jt &lt;local|jobtracker:port&gt;    specify a job tracker</span>
<span class="go">-files &lt;comma separated list of files&gt;    specify comma separated files to be copied to the map reduce cluster</span>
<span class="go">-libjars &lt;comma separated list of jars&gt;    specify comma separated jar files to include in the classpath.</span>
<span class="go">-archives &lt;comma separated list of archives&gt;    specify comma separated archives to be unarchived on the compute machines.</span>

<span class="go">The general command line syntax is</span>
<span class="go">bin/hadoop command [genericOptions] [commandOptions]</span>
</pre></div>


<p></p>
</details></p>
<p>An online guide is also available at <a href="https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/CommandsManual.html">Apache Hadoop commands manual</a>. You can use Hadoop commands to perform filesystem operations with more consistency.</p>
<p>Example, to look into the internal hadoop namespace:</p>
<div class="codehilite"><pre><span></span><span class="go">user$ hadoop fs -ls /</span>
<span class="go">Found 1 items</span>
<span class="go">drwxrwxr-x   - engage engage          0 2011-07-25 06:32 /engage</span>
</pre></div>


<p>Example, to adjust ownership of filesystem areas (there is usually no need to specify the mount itself <code>/mnt/hadoop</code> in Hadoop commands):</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> hadoop fs -chown -R engage:engage /engage
</pre></div>


<p>Example, compare <code>hadoop fs</code> command vs. using FUSE mount:</p>
<div class="codehilite"><pre><span></span><span class="go">user$ hadoop fs -ls /engage</span>
<span class="go">Found 3 items</span>
<span class="go">-rw-rw-r--   2 engage engage  733669376 2011-06-15 16:55 /engage/CentOS-5.6-x86_64-LiveCD.iso</span>
<span class="go">-rw-rw-r--   2 engage engage  215387183 2011-06-15 16:28 /engage/condor-7.6.1-x86_rhap_5-stripped.tar.gz</span>
<span class="go">-rw-rw-r--   2 engage engage    9259360 2011-06-15 16:32 /engage/glideinWMS_v2_5_1.tgz</span>

<span class="go">user$ ls -l /mnt/hadoop/engage</span>
<span class="go">total 935855</span>
<span class="go">-rw-rw-r-- 1 engage engage 733669376 Jun 15 16:55 CentOS-5.6-x86_64-LiveCD.iso</span>
<span class="go">-rw-rw-r-- 1 engage engage 215387183 Jun 15 16:28 condor-7.6.1-x86_rhap_5-stripped.tar.gz</span>
<span class="go">-rw-rw-r-- 1 engage engage   9259360 Jun 15 16:32 glideinWMS_v2_5_1.tgz</span>
</pre></div>


<h2 id="gridftp-validation">GridFTP Validation<a class="headerlink" href="#gridftp-validation" title="Permanent link">&para;</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The commands used to verify GridFTP below assume you have access to a node where you can first generate a valid proxy using <code>voms-proxy-init</code> or <code>grid-proxy-init</code>. Obtaining grid credentials is beyond the scope of this document.</p>
</div>
<div class="codehilite"><pre><span></span><span class="go">user$ globus-url-copy file:///home/users/jdost/test.txt gsiftp://devg-7.t2.ucsd.edu:2811/mnt/hadoop/engage/test.txt</span>
</pre></div>


<p>If you are having troubles running GridFTP refer to <a href="#starting-gridftp-in-standalone-mode">Starting GridFTP in Standalone Mode</a> in the Troubleshooting section.</p>
<h2 id="bestman-validation">BeStMan Validation<a class="headerlink" href="#bestman-validation" title="Permanent link">&para;</a></h2>
<p>There are three ways of validating BeStMan: * SrmTester: BeStMan testing application * InstallRSV: RSV monitoring tools * BestMan client tools</p>
<p>See the relevant pages for the first two options. This section will detail some basic client commands to validate. You will need grid credentials in order to test using client tools.</p>
<div class="codehilite"><pre><span></span><span class="go">srm-ping srm://BeStMan_host:secured_http_port/srm/v2/server</span>
<span class="go">srm-copy file:////tmp/test1  srm://BeStMan_host:secured_http_port/srm/v2/server\?SFN=/mnt/hadoop/VONAME/test_1</span>
</pre></div>


<p>The <code>srm-ping</code> tool should return a valid mapping <code>gumsIDMapped</code> that is not null</p>
<h1 id="troubleshooting">Troubleshooting<a class="headerlink" href="#troubleshooting" title="Permanent link">&para;</a></h1>
<h2 id="hadoop">Hadoop<a class="headerlink" href="#hadoop" title="Permanent link">&para;</a></h2>
<p>To view all of the currently configured settings of Hadoop from the web interface, enter the following url in your browser:</p>
<div class="codehilite"><pre><span></span>http://<span style="color:red">namenode.hostname</span>:50070/conf
</pre></div>


<p>You will see the entire configuration in XML format, for example:</p>
<p><details>
  <summary>Expand XML configuration</summary>
    <p></p>
<div class="codehilite"><pre><span></span><span class="cp">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><span class="nt">&lt;configuration&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3n.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.s3native.NativeS3FileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.cache.levels<span class="nt">&lt;/name&gt;&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>map.sort.class<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.util.QuickSort<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.tmp.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/data1/hadoop//scratch<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.native.lib<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.decommission.nodes.per.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.https.need.client.auth<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.client.idlethreshold<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.system.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/mapred/system<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir.perm<span class="nt">&lt;/name&gt;&lt;value&gt;</span>755<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.persist.jobstatus.hours<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.logging.level<span class="nt">&lt;/name&gt;&lt;value&gt;</span>all<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50010<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.skip.checksum.errors<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.block.access.token.enable<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from Unknown--&gt;</span><span class="nt">&lt;name&gt;</span>fs.default.name<span class="nt">&lt;/name&gt;&lt;value&gt;</span>hdfs://nagios.t2.ucsd.edu:9000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.child.tmp<span class="nt">&lt;/name&gt;&lt;value&gt;</span>./tmp<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.har.impl.disable.cache<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.skip.reduce.max.skip.groups<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.safemode.threshold.pct<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.999f<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.heartbeats.in.second<span class="nt">&lt;/name&gt;&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.handler.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>40<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.blockreport.initialDelay<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.instrumentation<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.mapred.JobTrackerMetricsInst<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.dns.nameserver<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.sort.factor<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.timeout<span class="nt">&lt;/name&gt;&lt;value&gt;</span>600000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.max.tracker.failures<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.rpc.socket.factory.class.default<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.net.StandardSocketFactory<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.jobhistory.lru.cache.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.hdfs.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.hdfs.DistributedFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.skip.map.auto.incr.proc.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.block.access.key.update.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>600<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.job.complete.cancel.delegation.tokens<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.mapfile.bloom.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1048576<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.reduce.shuffle.connect.timeout<span class="nt">&lt;/name&gt;&lt;value&gt;</span>180000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.safemode.extension<span class="nt">&lt;/name&gt;&lt;value&gt;</span>30000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>tasktracker.http.threads<span class="nt">&lt;/name&gt;&lt;value&gt;</span>50<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.shuffle.merge.percent<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.66<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.ftp.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.ftp.FTPFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.output.compress<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.bytes.per.checksum<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.healthChecker.script.timeout<span class="nt">&lt;/name&gt;&lt;value&gt;</span>600000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>topology.node.switch.mapping.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.net.ScriptBasedMapping<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.https.server.keystore.resource<span class="nt">&lt;/name&gt;&lt;value&gt;</span>ssl-server.xml<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.slowstart.completed.maps<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.05<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.max.attempts<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.ramfs.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.InMemoryFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.block.access.token.lifetime<span class="nt">&lt;/name&gt;&lt;value&gt;</span>600<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.skip.map.max.skip.records<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.name.edits.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">dfs</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span><span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.security.group.mapping<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.security.ShellBasedUnixGroupsMapping<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.persist.jobstatus.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/jobtracker/jobsInfo<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.log.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/var/log/hadoop<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3.buffer.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/s3<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.block.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>134217728<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>job.end.retry.attempts<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.file.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.LocalFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.output.compression.type<span class="nt">&lt;/name&gt;&lt;value&gt;</span>RECORD<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.local.dir.minspacestart<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.ipc.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50020<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.permissions<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>topology.script.number.args<span class="nt">&lt;/name&gt;&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.mapfile.bloom.error.rate<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.005<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.max.tracker.blacklists<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.profile.maps<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0-2<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.https.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50475<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.umaskmode<span class="nt">&lt;/name&gt;&lt;value&gt;</span>002<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.userlog.retain.hours<span class="nt">&lt;/name&gt;&lt;value&gt;</span>24<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.secondary.http.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>gratia-1:50090<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.replication.max<span class="nt">&lt;/name&gt;&lt;value&gt;</span>32<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.persist.jobstatus.active<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.security.authorization<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>local.cache.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10737418240<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.min.split.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.delegation.token.renew-interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>86400000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.map.tasks<span class="nt">&lt;/name&gt;&lt;value&gt;</span>7919<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.child.java.opts<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-Xmx200m<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.https.client.keystore.resource<span class="nt">&lt;/name&gt;&lt;value&gt;</span>ssl-client.xml<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from Unknown--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.startup<span class="nt">&lt;/name&gt;&lt;value&gt;</span>REGULAR<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.queue.name<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.retiredjobs.cache.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.https.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50470<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.balance.bandwidthPerSec<span class="nt">&lt;/name&gt;&lt;value&gt;</span>2000000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.server.listen.queue.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>128<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>job.end.retry.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>30000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.inmem.merge.threshold<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.skip.attempts.to.start.skipping<span class="nt">&lt;/name&gt;&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.checkpoint.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/var/hadoop/checkpoint-a<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.tasks<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1543<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.merge.recordsBeforeProgress<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.userlog.limit.kb<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>webinterface.private.actions<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.max.objects<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.shuffle.input.buffer.percent<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.70<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.sort.spill.percent<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.80<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.map.tasks.speculative.execution<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.util.hash.type<span class="nt">&lt;/name&gt;&lt;value&gt;</span>murmur<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.dns.nameserver<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.blockreport.intervalMsec<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3600000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.map.max.attempts<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.job.acl-view-job<span class="nt">&lt;/name&gt;&lt;value&gt;</span> <span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.handler.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.client.block.write.retries<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.max.reduces.per.node<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.reduce.shuffle.read.timeout<span class="nt">&lt;/name&gt;&lt;value&gt;</span>180000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.expiry.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>600000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.https.enable<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.maxtasks.per.job<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.job.history.block.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3145728<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>keep.failed.task.files<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.failed.volumes.tolerated<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.profile.reduces<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0-2<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.client.tcpnodelay<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.output.compression.codec<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.io.compress.DefaultCodec<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.map.index.skip<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.server.tcpnodelay<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.delegation.key.update-interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>86400000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.running.map.limit<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>jobclient.progress.monitor.poll.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.default.chunk.view.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>32768<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.logfile.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.tasks.speculative.execution<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.tasktracker.outofband.heartbeat<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3n.block.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>67108864<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.du.reserved<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10000000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.security.authentication<span class="nt">&lt;/name&gt;&lt;value&gt;</span>simple<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.checkpoint.period<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3600<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.running.reduce.limit<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.reuse.jvm.num.tasks<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.web.ugi<span class="nt">&lt;/name&gt;&lt;value&gt;</span>webuser,webgroup<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.completeuserjobs.maximum<span class="nt">&lt;/name&gt;&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.df.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>60000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.tracker.task-controller<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.mapred.DefaultTaskController<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.data.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/data1/hadoop//data<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3.maxRetries<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.dns.interface<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.support.append<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.job.acl-modify-job<span class="nt">&lt;/name&gt;&lt;value&gt;</span> <span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.local.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/mapred/local<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.hftp.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.hdfs.HftpFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.permissions.supergroup<span class="nt">&lt;/name&gt;&lt;value&gt;</span>root<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.trash.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3.sleepTimeSeconds<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.submit.replication<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.replication.min<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.har.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.HarFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.map.output.compression.codec<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.io.compress.DefaultCodec<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.dns.interface<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.decommission.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>30<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from Unknown--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.http.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>nagios:50070<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker<span class="nt">&lt;/name&gt;&lt;value&gt;</span>nagios:9000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.heartbeat.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.seqfile.sorter.recordlimit<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.name.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/dfs/name<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.line.input.format.linespermap<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.taskScheduler<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.mapred.JobQueueTaskScheduler<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.instrumentation<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.mapred.TaskTrackerMetricsInst<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.http.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50075<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>jobclient.completion.poll.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.max.maps.per.node<span class="nt">&lt;/name&gt;&lt;value&gt;</span>-1<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.local.dir.minspacekill<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.replication.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.sort.record.percent<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.05<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.kfs.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.kfs.KosmosFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.temp.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/mapred/temp<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.reduce.tasks.maximum<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;&lt;value&gt;</span>2<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.checkpoint.edits.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">fs</span><span class="o">.</span><span class="n">checkpoint</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span><span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.tasks.sleeptime-before-sigkill<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.reduce.input.buffer.percent<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.indexcache.mb<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.job.split.metainfo.maxsize<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.skip.reduce.auto.incr.proc.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>hadoop.logfile.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.automatic.close<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.seqfile.compress.blocksize<span class="nt">&lt;/name&gt;&lt;value&gt;</span>1000000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.hosts.exclude<span class="nt">&lt;/name&gt;&lt;value&gt;</span>/etc/hadoop-0.20/conf/hosts_exclude<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3.block.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>67108864<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.taskmemorymanager.monitoring-interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.acls.enabled<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapreduce.jobtracker.staging.root.dir<span class="nt">&lt;/name&gt;&lt;value&gt;</span><span class="cp">${</span><span class="n">hadoop</span><span class="o">.</span><span class="n">tmp</span><span class="o">.</span><span class="n">dir</span><span class="cp">}</span>/mapred/staging<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.queue.names<span class="nt">&lt;/name&gt;&lt;value&gt;</span>default<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.access.time.precision<span class="nt">&lt;/name&gt;&lt;value&gt;</span>3600000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.hsftp.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.hdfs.HsftpFileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.tracker.http.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50060<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.parallel.copies<span class="nt">&lt;/name&gt;&lt;value&gt;</span>5<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.seqfile.lazydecompress<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.safemode.min.datanodes<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.sort.mb<span class="nt">&lt;/name&gt;&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.client.connection.maxidletime<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.compress.map.output<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.tracker.report.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>127.0.0.1:0<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.healthChecker.interval<span class="nt">&lt;/name&gt;&lt;value&gt;</span>60000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.client.kill.max<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>ipc.client.connect.max.retries<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.s3.impl<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.fs.s3.S3FileSystem<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.job.tracker.http.address<span class="nt">&lt;/name&gt;&lt;value&gt;</span>0.0.0.0:50030<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.file.buffer.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4096<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.jobtracker.restart.recover<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.serializations<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.io.serializer.WritableSerialization<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.task.profile<span class="nt">&lt;/name&gt;&lt;value&gt;</span>false<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.datanode.handler.count<span class="nt">&lt;/name&gt;&lt;value&gt;</span>10<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.reduce.copy.backoff<span class="nt">&lt;/name&gt;&lt;value&gt;</span>300<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.replication.considerLoad<span class="nt">&lt;/name&gt;&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>jobclient.output.filter<span class="nt">&lt;/name&gt;&lt;value&gt;</span>FAILED<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from hdfs-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>dfs.namenode.delegation.token.max-lifetime<span class="nt">&lt;/name&gt;&lt;value&gt;</span>604800000<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from mapred-site.xml--&gt;</span><span class="nt">&lt;name&gt;</span>mapred.tasktracker.map.tasks.maximum<span class="nt">&lt;/name&gt;&lt;value&gt;</span>4<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>io.compression.codecs<span class="nt">&lt;/name&gt;&lt;value&gt;</span>org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span><span class="c">&lt;!--Loaded from core-default.xml--&gt;</span><span class="nt">&lt;name&gt;</span>fs.checkpoint.size<span class="nt">&lt;/name&gt;&lt;value&gt;</span>67108864<span class="nt">&lt;/value&gt;&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span>
</pre></div>


<p></p>
</details></p>
<p>Please refer to the <a href="http://wiki.apache.org/hadoop/FAQ">Apache Hadoop FAQ webpage</a> for answers to common questions/concerns</p>
<h2 id="fuse">FUSE<a class="headerlink" href="#fuse" title="Permanent link">&para;</a></h2>
<h3 id="notes-on-building-a-fuse-module">Notes on Building a FUSE Module<a class="headerlink" href="#notes-on-building-a-fuse-module" title="Permanent link">&para;</a></h3>
<p>If you are running a custom kernel, then be sure to enable the <code>fuse</code> module with <code>CONFIG_FUSE_FS=m</code> in your kernel config. Building and installing a <code>fuse</code> kernel module for your custom kernel is beyond the scope of this document.</p>
<h3 id="running-fuse-in-debug-mode">Running FUSE in Debug Mode<a class="headerlink" href="#running-fuse-in-debug-mode" title="Permanent link">&para;</a></h3>
<p>To start the FUSE mount in debug mode, you can run the FUSE mount command by hand:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span>  /usr/bin/hadoop-fuse-dfs  /mnt/hadoop -o rw,server<span class="o">=</span><span style="color:red">namenode.host</span>,port<span class="o">=</span><span class="m">9000</span>,rdbuffer<span class="o">=</span><span class="m">131072</span>,allow_other -d
</pre></div>


<p>Debug output will be printed to stderr, which you will probably want to redirect to a file. Most FUSE-related problems can be tackled by reading through the stderr and looking for error messages.</p>
<h2 id="gridftp">GridFTP<a class="headerlink" href="#gridftp" title="Permanent link">&para;</a></h2>
<p>#GridFTPStand</p>
<h3 id="starting-gridftp-in-standalone-mode">Starting GridFTP in Standalone Mode<a class="headerlink" href="#starting-gridftp-in-standalone-mode" title="Permanent link">&para;</a></h3>
<p>If you would like to test the gridftp-hdfs server in a debug standalone mode, you can run the command:</p>
<div class="codehilite"><pre><span></span><span class="gp">root@host #</span> gridftp-hdfs-standalone
</pre></div>


<p>The standalone server runs on port 5002, handles a single GridFTP request, and will log output to stdout/stderr.</p>
<h1 id="file-locations">File Locations<a class="headerlink" href="#file-locations" title="Permanent link">&para;</a></h1>
<table>
<thead>
<tr>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Component</td>
<td>File Type</td>
<td>Location</td>
<td>Needs editing?</td>
</tr>
<tr>
<td>Hadoop</td>
<td>Log files</td>
<td><code>/var/log/hadoop/*</code></td>
<td>No</td>
</tr>
<tr>
<td>Hadoop</td>
<td>PID files</td>
<td><code>/var/run/hadoop/*.pid</code></td>
<td>No</td>
</tr>
<tr>
<td>Hadoop</td>
<td>init scripts</td>
<td><code>/etc/init.d/hadoop</code></td>
<td>No</td>
</tr>
<tr>
<td>Hadoop</td>
<td>init script config file</td>
<td><code>/etc/sysconfig/hadoop</code></td>
<td>Yes</td>
</tr>
<tr>
<td>Hadoop</td>
<td>runtime config files</td>
<td><code>/etc/hadoop/conf/*</code></td>
<td>Maybe</td>
</tr>
<tr>
<td>Hadoop</td>
<td>System binaries</td>
<td><code>/usr/bin/hadoop</code></td>
<td>No</td>
</tr>
<tr>
<td>Hadoop</td>
<td>JARs</td>
<td><code>/usr/lib/hadoop/*</code></td>
<td>No</td>
</tr>
<tr>
<td>Hadoop</td>
<td>runtime config files</td>
<td><code>/etc/hosts_exclude</code></td>
<td>Yes, must be present on namenodes</td>
</tr>
<tr>
<td>GridFTP</td>
<td>Log files</td>
<td><code>/var/log/gridftp-auth.log</code>, <code>/var/log/gridftp.log</code></td>
<td>No</td>
</tr>
<tr>
<td>GridFTP</td>
<td>init.d script</td>
<td><code>/etc/init.d/globus-gridftp-server</code></td>
<td>No</td>
</tr>
<tr>
<td>GridFTP</td>
<td>runtime config files</td>
<td><code>/etc/gridftp-hdfs/*</code>, <code>/etc/sysconfig/gridftp-hdfs</code></td>
<td>Maybe</td>
</tr>
<tr>
<td>GridFTP</td>
<td>System binaries</td>
<td><code>/usr/bin/gridftp-hdfs-standalone</code>, <code>/usr/sbin/globus-gridftp-server</code></td>
<td>No</td>
</tr>
<tr>
<td>GridFTP</td>
<td>System libraries</td>
<td><code>/usr/lib64/libglobus_gridftp_server_hdfs.so*</code></td>
<td>No</td>
</tr>
<tr>
<td>GridFTP</td>
<td>GUMS client (called by LCMAPS) configuration</td>
<td><code>/etc/lcmaps.db</code></td>
<td>Yes</td>
</tr>
<tr>
<td>GridFTP</td>
<td>CA certificates</td>
<td><code>/etc/grid-security/certificates/*</code></td>
<td>No</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th align="left">Service/Process</th>
<th align="left">Configuration File</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">BeStMan2</td>
<td align="left"><code>/etc/bestman2/conf/bestman2.rc</code></td>
<td align="left">Main Configuration file</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/etc/sysconfig/bestman2</code></td>
<td align="left">Environment variables used by BeStMan2</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/etc/sysconfig/bestman2lib</code></td>
<td align="left">Environment variables that store values of various client and server libraries used by BeStMan2</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/etc/bestman2/conf/*</code></td>
<td align="left">Other runtime configuration files</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/etc/init.d/bestman2</code></td>
<td align="left">init.d startup script</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/etc/gridftp.conf</code></td>
<td align="left">Startup parameters</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th align="left">Service/Process</th>
<th align="left">Log File</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">BeStMan2</td>
<td align="left"><code>/var/log/bestman2/bestman2.log</code></td>
<td align="left">BeStMan2 server log and errors</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/var/log/bestman2/event.srm.log</code></td>
<td align="left">Records all SRM transactions</td>
</tr>
<tr>
<td align="left">GridFTP</td>
<td align="left"><code>/var/log/gridftp.log</code></td>
<td align="left">Transfer log</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/var/log/gridftp-auth.log</code></td>
<td align="left">Authentication log</td>
</tr>
<tr>
<td align="left"></td>
<td align="left"><code>/var/log/messages</code></td>
<td align="left">Main system log (look here for LCMAPS errors)</td>
</tr>
</tbody>
</table>
<h1 id="known-issues">Known Issues<a class="headerlink" href="#known-issues" title="Permanent link">&para;</a></h1>
<h3 id="replicas">Replicas<a class="headerlink" href="#replicas" title="Permanent link">&para;</a></h3>
<p>You may need to change the following line in <code>/usr/share/gridftp-hdfs/gridftp-hdfs-environment</code>:</p>
<div class="codehilite"><pre><span></span>export GRIDFTP_HDFS_REPLICAS=2
</pre></div>


<h3 id="copyfromlocal-java-ioexception">copyFromLocal java IOException<a class="headerlink" href="#copyfromlocal-java-ioexception" title="Permanent link">&para;</a></h3>
<p>When trying to copy a local file into Hadoop you may come across the following java exception:</p>
<p><details>
  <summary>Show detailed java exception</summary>
    <p></p>
<div class="codehilite"><pre><span></span><span class="go">11/06/24 11:10:50 WARN hdfs.DFSClient: Error Recovery for block null bad datanode[0]</span>
<span class="go">nodes == null</span>
<span class="go">11/06/24 11:10:50 WARN hdfs.DFSClient: Could not get block locations. Source file</span>
<span class="go">&quot;/osg/ddd&quot; - Aborting...</span>
<span class="go">copyFromLocal: java.io.IOException: File /osg/ddd could only be replicated to 0</span>
<span class="go">nodes, instead of 1</span>
<span class="go">11/06/24 11:10:50 ERROR hdfs.DFSClient: Exception closing file /osg/ddd :</span>
<span class="go">org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /osg/ddd could only</span>
<span class="go">be replicated to 0 nodes, instead of 1</span>
<span class="go">        at</span>
<span class="go">org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1415)</span>
<span class="go">        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:588)</span>
<span class="go">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span>
<span class="go">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)</span>
<span class="go">        at</span>
<span class="go">sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)</span>
<span class="go">        at java.lang.reflect.Method.invoke(Method.java:597)</span>
<span class="go">        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:528)</span>
<span class="go">        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1319)</span>
<span class="go">        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1315)</span>
<span class="go">        at java.security.AccessController.doPrivileged(Native Method)</span>
<span class="go">        at javax.security.auth.Subject.doAs(Subject.java:396)</span>
<span class="go">        at</span>
<span class="go">org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)</span>
<span class="go">        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1313)</span>
</pre></div>


<p></p>
</details></p>
<p>This can occur if you try to install a Datanode on a machine with less than 10GB of disk space available. This can be changed by lowering the value of the following property in <code>/usr/lib/hadoop-0.20/conf/hdfs-site.xml</code>:</p>
<div class="codehilite"><pre><span></span><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.du.reserved<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>10000000000<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</pre></div>


<p>Hadoop always requires this amount of disk space to be available for non-hdfs usage on the machine.</p>
<h1 id="how-to-get-help">How to get Help?<a class="headerlink" href="#how-to-get-help" title="Permanent link">&para;</a></h1>
<p>If you cannot resolve the problem, there are several ways to receive help:</p>
<ul>
<li>For bug support and issues, submit a ticket to the <a href="https://ticket.opensciencegrid.org/goc">Grid Operations Center</a>.</li>
<li>For community support and best-effort software team support contact <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#111;&#115;&#103;&#45;&#115;&#111;&#102;&#116;&#119;&#97;&#114;&#101;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;">&#111;&#115;&#103;&#45;&#115;&#111;&#102;&#116;&#119;&#97;&#114;&#101;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;</a>.</li>
<li>For additional community support, contact <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#111;&#115;&#103;&#45;&#104;&#97;&#100;&#111;&#111;&#112;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;">&#111;&#115;&#103;&#45;&#104;&#97;&#100;&#111;&#111;&#112;&#64;&#111;&#112;&#101;&#110;&#115;&#99;&#105;&#101;&#110;&#99;&#101;&#103;&#114;&#105;&#100;&#46;&#111;&#114;&#103;</a>. Note, this is only best-effort help from OSG Software team.</li>
</ul>
<p>For a full set of help options, see <a href="../../common/help">Help Procedure</a>.</p>
<h1 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h1>
<ul>
<li><a href="http://www.iop.org/EJ/article/1742-6596/180/1/012047/jpconf9_180_012047.pdf">Using Hadoop as a Grid Storage Element</a>, <em>Journal of Physics Conference Series, 2009</em>.</li>
<li><a href="http://osg-docdb.opensciencegrid.org/0009/000911/001/Hadoop.pdf">Hadoop Distributed File System for the Grid</a>, <em>IEEE Nuclear Science Symposium, 2009</em>.</li>
</ul>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../xrootd-overview/" class="btn btn-neutral float-right" title="XRootD Overview">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../hadoop-overview/" class="btn btn-neutral" title="HDFS Overview"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/opensciencegrid/docs/" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../hadoop-overview/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../xrootd-overview/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/require.js"></script>
      <script src="../../search/search.js"></script>

</body>
</html>
